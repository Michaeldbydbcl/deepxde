Using TensorFlow 2 backend.

              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.382039 s

'compile' took 1.974292 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 1.00e+02, 2.74e+00]    [0.00e+00, 9.88e+01, 2.74e+00]    [9.88e+01, 9.88e+01, 3.08e+00, 3.08e+00]    
1000      [5.45e+01, 2.05e-03, 3.55e+00]    [0.00e+00, 6.91e+01, 3.55e+00]    [6.90e+01, 6.91e+01, 3.56e+01, 3.54e+01]    
2000      [4.56e+01, 8.84e-03, 3.56e+00]    [0.00e+00, 6.00e+01, 3.56e+00]    [6.00e+01, 6.00e+01, 2.96e+01, 2.94e+01]    
3000      [3.70e+01, 1.52e-02, 3.63e+00]    [0.00e+00, 4.88e+01, 3.63e+00]    [4.90e+01, 4.88e+01, 2.55e+01, 2.55e+01]    
4000      [2.65e+01, 5.69e-02, 3.78e+00]    [0.00e+00, 3.42e+01, 3.78e+00]    [3.43e+01, 3.42e+01, 2.25e+01, 2.23e+01]    
5000      [1.42e+01, 2.58e-03, 4.00e+00]    [0.00e+00, 1.76e+01, 4.00e+00]    [1.77e+01, 1.76e+01, 1.81e+01, 1.80e+01]    
6000      [7.34e+00, 7.61e-02, 4.10e+00]    [0.00e+00, 8.82e+00, 4.10e+00]    [8.82e+00, 8.82e+00, 1.04e+01, 1.03e+01]    
7000      [5.02e+00, 9.32e-03, 4.07e+00]    [0.00e+00, 5.97e+00, 4.07e+00]    [5.96e+00, 5.97e+00, 6.88e+00, 6.88e+00]    
8000      [4.35e+00, 2.28e-03, 4.02e+00]    [0.00e+00, 5.46e+00, 4.02e+00]    [5.48e+00, 5.46e+00, 5.06e+00, 5.07e+00]    
9000      [3.95e+00, 2.01e-02, 3.95e+00]    [0.00e+00, 4.82e+00, 3.95e+00]    [4.86e+00, 4.82e+00, 3.84e+00, 3.86e+00]    
10000     [3.59e+00, 8.07e-03, 3.90e+00]    [0.00e+00, 4.10e+00, 3.90e+00]    [4.14e+00, 4.10e+00, 3.04e+00, 3.03e+00]    
11000     [3.42e+00, 2.77e-02, 3.85e+00]    [0.00e+00, 3.97e+00, 3.85e+00]    [3.99e+00, 3.97e+00, 2.57e+00, 2.49e+00]    
12000     [3.26e+00, 2.03e-02, 3.79e+00]    [0.00e+00, 3.69e+00, 3.79e+00]    [3.72e+00, 3.69e+00, 2.36e+00, 2.25e+00]    
13000     [3.16e+00, 9.77e-03, 3.74e+00]    [0.00e+00, 3.61e+00, 3.74e+00]    [3.65e+00, 3.61e+00, 2.33e+00, 2.18e+00]    
14000     [3.13e+00, 2.41e-02, 3.70e+00]    [0.00e+00, 3.38e+00, 3.70e+00]    [3.42e+00, 3.38e+00, 2.23e+00, 2.05e+00]    
15000     [3.04e+00, 4.55e-02, 3.66e+00]    [0.00e+00, 3.30e+00, 3.66e+00]    [3.34e+00, 3.30e+00, 2.23e+00, 1.99e+00]    
16000     [2.96e+00, 2.27e-02, 3.62e+00]    [0.00e+00, 3.21e+00, 3.62e+00]    [3.26e+00, 3.21e+00, 2.25e+00, 1.96e+00]    
17000     [2.90e+00, 2.99e-03, 3.58e+00]    [0.00e+00, 3.17e+00, 3.58e+00]    [3.22e+00, 3.17e+00, 2.25e+00, 1.92e+00]    
18000     [2.85e+00, 5.80e-02, 3.54e+00]    [0.00e+00, 3.09e+00, 3.54e+00]    [3.16e+00, 3.09e+00, 2.16e+00, 1.79e+00]    
19000     [2.80e+00, 1.49e-02, 3.50e+00]    [0.00e+00, 3.09e+00, 3.50e+00]    [3.16e+00, 3.09e+00, 2.17e+00, 1.74e+00]    
20000     [2.78e+00, 5.63e-02, 3.47e+00]    [0.00e+00, 3.07e+00, 3.47e+00]    [3.15e+00, 3.07e+00, 2.18e+00, 1.70e+00]    
21000     [2.73e+00, 2.78e-02, 3.43e+00]    [0.00e+00, 3.05e+00, 3.43e+00]    [3.17e+00, 3.05e+00, 2.10e+00, 1.64e+00]    
22000     [2.70e+00, 2.08e-02, 3.40e+00]    [0.00e+00, 3.02e+00, 3.40e+00]    [3.18e+00, 3.02e+00, 2.05e+00, 1.58e+00]    
23000     [2.67e+00, 3.14e-02, 3.37e+00]    [0.00e+00, 3.00e+00, 3.37e+00]    [3.19e+00, 3.00e+00, 1.99e+00, 1.52e+00]    
24000     [2.66e+00, 2.29e-02, 3.34e+00]    [0.00e+00, 2.99e+00, 3.34e+00]    [3.19e+00, 2.99e+00, 1.97e+00, 1.46e+00]    
25000     [2.68e+00, 3.96e-02, 3.31e+00]    [0.00e+00, 2.87e+00, 3.31e+00]    [3.09e+00, 2.87e+00, 1.88e+00, 1.43e+00]    
26000     [2.62e+00, 6.19e-03, 3.29e+00]    [0.00e+00, 2.86e+00, 3.29e+00]    [3.10e+00, 2.86e+00, 1.85e+00, 1.38e+00]    
27000     [2.59e+00, 1.98e-02, 3.26e+00]    [0.00e+00, 2.89e+00, 3.26e+00]    [3.14e+00, 2.89e+00, 1.91e+00, 1.35e+00]    
28000     [2.56e+00, 1.50e-02, 3.24e+00]    [0.00e+00, 2.81e+00, 3.24e+00]    [3.08e+00, 2.81e+00, 1.83e+00, 1.34e+00]    
29000     [2.56e+00, 6.77e-02, 3.21e+00]    [0.00e+00, 2.90e+00, 3.21e+00]    [3.19e+00, 2.90e+00, 1.93e+00, 1.32e+00]    
30000     [2.54e+00, 2.97e-02, 3.19e+00]    [0.00e+00, 2.86e+00, 3.19e+00]    [3.18e+00, 2.86e+00, 1.88e+00, 1.32e+00]    

Best model at step 30000:
  train loss: 5.76e+00
  test loss: 6.05e+00
  test metric: [3.18e+00, 2.86e+00, 1.88e+00, 1.32e+00]

'train' took 74.941702 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.412896 s

'compile' took 1.889945 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 9.97e+01, 2.91e+00]    [0.00e+00, 1.02e+02, 2.91e+00]    [1.02e+02, 1.02e+02, 2.74e+00, 2.74e+00]    
1000      [5.71e+01, 4.80e+01, 3.71e+00]    [0.00e+00, 5.40e+01, 3.71e+00]    [5.43e+01, 5.40e+01, 3.41e+01, 3.39e+01]    
2000      [2.81e+01, 3.09e-02, 4.46e+00]    [0.00e+00, 3.04e+01, 4.46e+00]    [3.08e+01, 3.04e+01, 2.08e+01, 1.86e+01]    
3000      [1.03e+01, 2.08e-03, 4.80e+00]    [0.00e+00, 1.32e+01, 4.80e+00]    [1.30e+01, 1.32e+01, 1.07e+01, 8.72e+00]    
4000      [6.19e+00, 5.68e-02, 4.79e+00]    [0.00e+00, 7.87e+00, 4.79e+00]    [8.10e+00, 7.87e+00, 6.08e+00, 5.84e+00]    
5000      [4.10e+00, 8.79e-03, 4.73e+00]    [0.00e+00, 4.86e+00, 4.73e+00]    [4.92e+00, 4.86e+00, 3.81e+00, 3.90e+00]    
6000      [3.44e+00, 1.13e-02, 4.66e+00]    [0.00e+00, 4.17e+00, 4.66e+00]    [4.21e+00, 4.17e+00, 3.01e+00, 3.20e+00]    
7000      [3.20e+00, 2.17e-02, 4.59e+00]    [0.00e+00, 3.78e+00, 4.59e+00]    [3.81e+00, 3.78e+00, 2.81e+00, 3.05e+00]    
8000      [3.08e+00, 8.13e-03, 4.51e+00]    [0.00e+00, 3.65e+00, 4.51e+00]    [3.64e+00, 3.65e+00, 2.71e+00, 2.90e+00]    
9000      [3.03e+00, 2.82e-02, 4.44e+00]    [0.00e+00, 3.48e+00, 4.44e+00]    [3.46e+00, 3.48e+00, 2.69e+00, 2.84e+00]    
10000     [2.96e+00, 2.59e-02, 4.38e+00]    [0.00e+00, 3.34e+00, 4.38e+00]    [3.30e+00, 3.34e+00, 2.72e+00, 2.84e+00]    
11000     [2.92e+00, 6.16e-03, 4.32e+00]    [0.00e+00, 3.36e+00, 4.32e+00]    [3.27e+00, 3.36e+00, 2.61e+00, 2.66e+00]    
12000     [2.87e+00, 1.36e-02, 4.26e+00]    [0.00e+00, 3.31e+00, 4.26e+00]    [3.24e+00, 3.31e+00, 2.39e+00, 2.43e+00]    
13000     [2.93e+00, 2.15e-03, 4.21e+00]    [0.00e+00, 3.42e+00, 4.21e+00]    [3.37e+00, 3.42e+00, 2.33e+00, 2.38e+00]    
14000     [2.81e+00, 2.64e-02, 4.16e+00]    [0.00e+00, 3.32e+00, 4.16e+00]    [3.28e+00, 3.32e+00, 2.21e+00, 2.23e+00]    
15000     [2.81e+00, 7.90e-03, 4.11e+00]    [0.00e+00, 3.28e+00, 4.11e+00]    [3.26e+00, 3.28e+00, 2.19e+00, 2.20e+00]    
16000     [2.78e+00, 1.39e-02, 4.06e+00]    [0.00e+00, 3.35e+00, 4.06e+00]    [3.34e+00, 3.35e+00, 2.20e+00, 2.21e+00]    
17000     [2.76e+00, 5.59e-03, 4.02e+00]    [0.00e+00, 3.32e+00, 4.02e+00]    [3.32e+00, 3.32e+00, 2.19e+00, 2.19e+00]    
18000     [2.74e+00, 2.05e-02, 3.97e+00]    [0.00e+00, 3.31e+00, 3.97e+00]    [3.32e+00, 3.31e+00, 2.18e+00, 2.17e+00]    
19000     [2.78e+00, 1.29e-02, 3.93e+00]    [0.00e+00, 3.39e+00, 3.93e+00]    [3.40e+00, 3.39e+00, 2.23e+00, 2.22e+00]    
20000     [2.74e+00, 2.03e-02, 3.89e+00]    [0.00e+00, 3.29e+00, 3.89e+00]    [3.31e+00, 3.29e+00, 2.16e+00, 2.15e+00]    
21000     [2.71e+00, 5.28e-03, 3.86e+00]    [0.00e+00, 3.33e+00, 3.86e+00]    [3.36e+00, 3.33e+00, 2.15e+00, 2.13e+00]    
22000     [2.74e+00, 9.97e-03, 3.82e+00]    [0.00e+00, 3.32e+00, 3.82e+00]    [3.35e+00, 3.32e+00, 2.09e+00, 2.07e+00]    
23000     [2.72e+00, 3.27e-03, 3.78e+00]    [0.00e+00, 3.32e+00, 3.78e+00]    [3.37e+00, 3.32e+00, 2.09e+00, 2.06e+00]    
24000     [2.70e+00, 2.21e-02, 3.75e+00]    [0.00e+00, 3.40e+00, 3.75e+00]    [3.46e+00, 3.40e+00, 2.14e+00, 2.10e+00]    
25000     [2.68e+00, 1.82e-02, 3.72e+00]    [0.00e+00, 3.31e+00, 3.72e+00]    [3.36e+00, 3.31e+00, 2.09e+00, 2.05e+00]    
26000     [2.66e+00, 9.04e-03, 3.68e+00]    [0.00e+00, 3.36e+00, 3.68e+00]    [3.42e+00, 3.36e+00, 2.08e+00, 2.04e+00]    
27000     [2.64e+00, 4.03e-03, 3.65e+00]    [0.00e+00, 3.33e+00, 3.65e+00]    [3.41e+00, 3.33e+00, 2.08e+00, 2.02e+00]    
28000     [2.64e+00, 7.00e-03, 3.62e+00]    [0.00e+00, 3.36e+00, 3.62e+00]    [3.43e+00, 3.36e+00, 2.06e+00, 2.00e+00]    
29000     [2.62e+00, 7.66e-03, 3.60e+00]    [0.00e+00, 3.33e+00, 3.60e+00]    [3.41e+00, 3.33e+00, 2.04e+00, 1.98e+00]    
30000     [2.64e+00, 1.52e-02, 3.57e+00]    [0.00e+00, 3.30e+00, 3.57e+00]    [3.39e+00, 3.30e+00, 2.01e+00, 1.94e+00]    

Best model at step 29000:
  train loss: 6.22e+00
  test loss: 6.92e+00
  test metric: [3.41e+00, 3.33e+00, 2.04e+00, 1.98e+00]

'train' took 70.738799 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.497671 s

'compile' took 2.227044 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 9.99e+01, 2.67e+00]    [0.00e+00, 1.01e+02, 2.67e+00]    [1.01e+02, 1.01e+02, 1.47e+00, 1.47e+00]    
1000      [5.68e+01, 4.56e+01, 3.43e+00]    [0.00e+00, 5.31e+01, 3.43e+00]    [5.34e+01, 5.31e+01, 3.82e+01, 3.82e+01]    
2000      [3.15e+01, 1.78e-02, 4.08e+00]    [0.00e+00, 3.40e+01, 4.08e+00]    [3.45e+01, 3.40e+01, 2.33e+01, 2.27e+01]    
3000      [1.34e+01, 3.77e-02, 4.40e+00]    [0.00e+00, 1.23e+01, 4.40e+00]    [1.38e+01, 1.23e+01, 7.74e+00, 7.43e+00]    
4000      [4.99e+00, 3.24e-02, 4.55e+00]    [0.00e+00, 5.91e+00, 4.55e+00]    [5.88e+00, 5.91e+00, 4.90e+00, 4.62e+00]    
5000      [3.93e+00, 5.29e-02, 4.46e+00]    [0.00e+00, 5.04e+00, 4.46e+00]    [4.65e+00, 5.04e+00, 3.55e+00, 3.25e+00]    
6000      [3.35e+00, 4.34e-03, 4.36e+00]    [0.00e+00, 3.91e+00, 4.36e+00]    [3.57e+00, 3.91e+00, 2.51e+00, 2.40e+00]    
7000      [3.12e+00, 1.97e-02, 4.27e+00]    [0.00e+00, 3.83e+00, 4.27e+00]    [3.59e+00, 3.83e+00, 2.13e+00, 2.22e+00]    
8000      [2.98e+00, 1.13e-02, 4.19e+00]    [0.00e+00, 3.75e+00, 4.19e+00]    [3.58e+00, 3.75e+00, 1.88e+00, 2.03e+00]    
9000      [2.85e+00, 1.25e-02, 4.13e+00]    [0.00e+00, 3.66e+00, 4.13e+00]    [3.52e+00, 3.66e+00, 1.69e+00, 1.82e+00]    
10000     [2.76e+00, 9.11e-03, 4.08e+00]    [0.00e+00, 3.61e+00, 4.08e+00]    [3.50e+00, 3.61e+00, 1.68e+00, 1.84e+00]    
11000     [2.84e+00, 2.03e-03, 4.02e+00]    [0.00e+00, 3.47e+00, 4.02e+00]    [3.38e+00, 3.47e+00, 1.66e+00, 1.79e+00]    
12000     [2.71e+00, 1.49e-02, 3.97e+00]    [0.00e+00, 3.50e+00, 3.97e+00]    [3.41e+00, 3.50e+00, 1.69e+00, 1.84e+00]    
13000     [2.67e+00, 2.47e-02, 3.92e+00]    [0.00e+00, 3.49e+00, 3.92e+00]    [3.41e+00, 3.49e+00, 1.70e+00, 1.84e+00]    
14000     [2.67e+00, 1.64e-02, 3.87e+00]    [0.00e+00, 3.43e+00, 3.87e+00]    [3.37e+00, 3.43e+00, 1.66e+00, 1.78e+00]    
15000     [2.63e+00, 8.11e-04, 3.83e+00]    [0.00e+00, 3.40e+00, 3.83e+00]    [3.35e+00, 3.40e+00, 1.69e+00, 1.81e+00]    
16000     [2.65e+00, 8.92e-04, 3.79e+00]    [0.00e+00, 3.35e+00, 3.79e+00]    [3.31e+00, 3.35e+00, 1.67e+00, 1.77e+00]    
17000     [2.61e+00, 1.66e-02, 3.75e+00]    [0.00e+00, 3.36e+00, 3.75e+00]    [3.31e+00, 3.36e+00, 1.74e+00, 1.77e+00]    
18000     [2.60e+00, 5.70e-03, 3.71e+00]    [0.00e+00, 3.42e+00, 3.71e+00]    [3.38e+00, 3.42e+00, 1.72e+00, 1.74e+00]    
19000     [2.57e+00, 3.20e-02, 3.67e+00]    [0.00e+00, 3.34e+00, 3.67e+00]    [3.32e+00, 3.34e+00, 1.65e+00, 1.67e+00]    
20000     [2.56e+00, 3.41e-03, 3.64e+00]    [0.00e+00, 3.35e+00, 3.64e+00]    [3.34e+00, 3.35e+00, 1.67e+00, 1.68e+00]    
21000     [2.57e+00, 2.73e-02, 3.60e+00]    [0.00e+00, 3.39e+00, 3.60e+00]    [3.39e+00, 3.39e+00, 1.71e+00, 1.71e+00]    
22000     [2.54e+00, 3.37e-03, 3.57e+00]    [0.00e+00, 3.29e+00, 3.57e+00]    [3.31e+00, 3.29e+00, 1.68e+00, 1.67e+00]    
23000     [2.51e+00, 4.81e-03, 3.54e+00]    [0.00e+00, 3.31e+00, 3.54e+00]    [3.34e+00, 3.31e+00, 1.70e+00, 1.68e+00]    
24000     [2.50e+00, 2.55e-02, 3.50e+00]    [0.00e+00, 3.28e+00, 3.50e+00]    [3.32e+00, 3.28e+00, 1.72e+00, 1.69e+00]    
25000     [2.52e+00, 1.22e-02, 3.47e+00]    [0.00e+00, 3.31e+00, 3.47e+00]    [3.37e+00, 3.31e+00, 1.74e+00, 1.70e+00]    
26000     [2.47e+00, 1.52e-02, 3.44e+00]    [0.00e+00, 3.28e+00, 3.44e+00]    [3.34e+00, 3.28e+00, 1.74e+00, 1.68e+00]    
27000     [2.48e+00, 1.29e-02, 3.42e+00]    [0.00e+00, 3.23e+00, 3.42e+00]    [3.30e+00, 3.23e+00, 1.73e+00, 1.65e+00]    
28000     [2.50e+00, 4.73e-03, 3.39e+00]    [0.00e+00, 3.21e+00, 3.39e+00]    [3.29e+00, 3.21e+00, 1.74e+00, 1.64e+00]    
29000     [2.44e+00, 1.38e-02, 3.36e+00]    [0.00e+00, 3.24e+00, 3.36e+00]    [3.32e+00, 3.24e+00, 1.75e+00, 1.62e+00]    
30000     [2.45e+00, 1.59e-02, 3.34e+00]    [0.00e+00, 3.28e+00, 3.34e+00]    [3.38e+00, 3.28e+00, 1.80e+00, 1.64e+00]    

Best model at step 30000:
  train loss: 5.80e+00
  test loss: 6.62e+00
  test metric: [3.38e+00, 3.28e+00, 1.80e+00, 1.64e+00]

'train' took 75.705514 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.380980 s

'compile' took 1.972724 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.81e+01, 9.67e+01, 2.72e+00]    [0.00e+00, 9.87e+01, 2.72e+00]    [9.87e+01, 9.87e+01, 2.50e+00, 2.50e+00]    
1000      [8.73e+01, 1.65e-02, 2.71e+00]    [0.00e+00, 8.63e+01, 2.71e+00]    [8.63e+01, 8.63e+01, 2.56e+01, 2.57e+01]    
2000      [8.22e+01, 1.10e-02, 2.79e+00]    [0.00e+00, 8.14e+01, 2.79e+00]    [8.14e+01, 8.14e+01, 2.66e+01, 2.65e+01]    
3000      [7.79e+01, 2.44e-01, 2.90e+00]    [0.00e+00, 7.67e+01, 2.90e+00]    [7.68e+01, 7.67e+01, 2.87e+01, 2.86e+01]    
4000      [7.43e+01, 9.24e-03, 3.01e+00]    [0.00e+00, 7.36e+01, 3.01e+00]    [7.37e+01, 7.36e+01, 2.69e+01, 2.70e+01]    
5000      [6.99e+01, 9.18e-02, 3.16e+00]    [0.00e+00, 7.00e+01, 3.16e+00]    [7.02e+01, 7.00e+01, 2.42e+01, 2.43e+01]    
6000      [6.42e+01, 4.38e-02, 3.36e+00]    [0.00e+00, 6.48e+01, 3.36e+00]    [6.50e+01, 6.48e+01, 2.16e+01, 2.17e+01]    
7000      [5.62e+01, 2.21e-01, 3.64e+00]    [0.00e+00, 5.74e+01, 3.64e+00]    [5.75e+01, 5.74e+01, 1.84e+01, 1.82e+01]    
8000      [4.46e+01, 1.16e-01, 3.99e+00]    [0.00e+00, 4.65e+01, 3.99e+00]    [4.67e+01, 4.65e+01, 1.49e+01, 1.47e+01]    
9000      [2.80e+01, 1.43e-01, 4.46e+00]    [0.00e+00, 3.03e+01, 4.46e+00]    [3.05e+01, 3.03e+01, 1.33e+01, 1.32e+01]    
10000     [1.09e+01, 3.16e-01, 4.94e+00]    [0.00e+00, 1.52e+01, 4.94e+00]    [1.53e+01, 1.52e+01, 1.29e+01, 1.28e+01]    
11000     [8.94e+00, 3.42e-01, 4.92e+00]    [0.00e+00, 1.25e+01, 4.92e+00]    [1.25e+01, 1.25e+01, 1.18e+01, 1.18e+01]    
12000     [7.63e+00, 5.13e-01, 4.82e+00]    [0.00e+00, 1.02e+01, 4.82e+00]    [1.03e+01, 1.02e+01, 1.06e+01, 1.06e+01]    
13000     [6.65e+00, 1.91e-01, 4.75e+00]    [0.00e+00, 8.68e+00, 4.75e+00]    [8.71e+00, 8.68e+00, 9.73e+00, 9.71e+00]    
14000     [5.85e+00, 2.49e-01, 4.68e+00]    [0.00e+00, 7.57e+00, 4.68e+00]    [7.57e+00, 7.57e+00, 8.85e+00, 8.82e+00]    
15000     [5.53e+00, 6.60e-01, 4.60e+00]    [0.00e+00, 6.96e+00, 4.60e+00]    [6.96e+00, 6.96e+00, 8.41e+00, 8.38e+00]    
16000     [5.29e+00, 1.95e-01, 4.53e+00]    [0.00e+00, 6.69e+00, 4.53e+00]    [6.67e+00, 6.69e+00, 7.96e+00, 7.92e+00]    
17000     [5.12e+00, 1.92e-02, 4.46e+00]    [0.00e+00, 6.41e+00, 4.46e+00]    [6.39e+00, 6.41e+00, 7.67e+00, 7.64e+00]    
18000     [4.99e+00, 5.16e-01, 4.40e+00]    [0.00e+00, 6.16e+00, 4.40e+00]    [6.15e+00, 6.16e+00, 7.35e+00, 7.33e+00]    
19000     [4.81e+00, 1.41e-01, 4.35e+00]    [0.00e+00, 5.91e+00, 4.35e+00]    [5.89e+00, 5.91e+00, 7.20e+00, 7.18e+00]    
20000     [4.67e+00, 2.16e-01, 4.30e+00]    [0.00e+00, 5.71e+00, 4.30e+00]    [5.69e+00, 5.71e+00, 7.01e+00, 6.98e+00]    
21000     [4.55e+00, 5.72e-01, 4.26e+00]    [0.00e+00, 5.52e+00, 4.26e+00]    [5.50e+00, 5.52e+00, 6.62e+00, 6.60e+00]    
22000     [4.47e+00, 6.63e-01, 4.21e+00]    [0.00e+00, 5.35e+00, 4.21e+00]    [5.33e+00, 5.35e+00, 6.72e+00, 6.70e+00]    
23000     [4.35e+00, 3.14e-01, 4.17e+00]    [0.00e+00, 5.23e+00, 4.17e+00]    [5.21e+00, 5.23e+00, 6.30e+00, 6.29e+00]    
24000     [4.30e+00, 5.59e-01, 4.14e+00]    [0.00e+00, 5.13e+00, 4.14e+00]    [5.12e+00, 5.13e+00, 6.00e+00, 5.98e+00]    
25000     [4.21e+00, 6.72e-02, 4.11e+00]    [0.00e+00, 5.02e+00, 4.11e+00]    [5.01e+00, 5.02e+00, 6.00e+00, 5.99e+00]    
26000     [4.12e+00, 3.75e-01, 4.07e+00]    [0.00e+00, 4.85e+00, 4.07e+00]    [4.83e+00, 4.85e+00, 5.77e+00, 5.76e+00]    
27000     [4.09e+00, 5.59e-01, 4.04e+00]    [0.00e+00, 4.73e+00, 4.04e+00]    [4.73e+00, 4.73e+00, 5.86e+00, 5.85e+00]    
28000     [3.99e+00, 6.57e-02, 4.01e+00]    [0.00e+00, 4.62e+00, 4.01e+00]    [4.61e+00, 4.62e+00, 5.57e+00, 5.57e+00]    
29000     [3.95e+00, 3.47e-01, 3.99e+00]    [0.00e+00, 4.54e+00, 3.99e+00]    [4.53e+00, 4.54e+00, 5.57e+00, 5.56e+00]    
30000     [3.88e+00, 2.25e-01, 3.96e+00]    [0.00e+00, 4.48e+00, 3.96e+00]    [4.47e+00, 4.48e+00, 5.25e+00, 5.24e+00]    

Best model at step 30000:
  train loss: 8.06e+00
  test loss: 8.44e+00
  test metric: [4.47e+00, 4.48e+00, 5.25e+00, 5.24e+00]

'train' took 85.113352 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.504650 s

'compile' took 2.374648 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.96e+01, 1.01e+02, 2.73e+00]    [0.00e+00, 1.01e+02, 2.73e+00]    [1.01e+02, 1.01e+02, 2.22e+00, 2.22e+00]    
1000      [6.35e+01, 2.87e-02, 3.46e+00]    [0.00e+00, 6.15e+01, 3.46e+00]    [6.17e+01, 6.15e+01, 3.52e+01, 3.48e+01]    
2000      [5.13e+01, 2.86e-02, 3.58e+00]    [0.00e+00, 5.03e+01, 3.58e+00]    [5.08e+01, 5.03e+01, 3.05e+01, 3.03e+01]    
3000      [3.77e+01, 2.12e-03, 3.83e+00]    [0.00e+00, 3.58e+01, 3.83e+00]    [3.65e+01, 3.58e+01, 2.26e+01, 2.25e+01]    
4000      [1.18e+01, 2.81e-02, 4.36e+00]    [0.00e+00, 1.17e+01, 4.36e+00]    [1.21e+01, 1.17e+01, 1.01e+01, 9.80e+00]    
5000      [4.09e+00, 3.66e-02, 4.38e+00]    [0.00e+00, 5.75e+00, 4.38e+00]    [5.66e+00, 5.75e+00, 4.95e+00, 5.62e+00]    
6000      [3.53e+00, 4.79e-02, 4.27e+00]    [0.00e+00, 5.22e+00, 4.27e+00]    [4.83e+00, 5.22e+00, 4.55e+00, 5.21e+00]    
7000      [3.34e+00, 9.42e-03, 4.16e+00]    [0.00e+00, 5.10e+00, 4.16e+00]    [4.72e+00, 5.10e+00, 3.88e+00, 4.72e+00]    
8000      [3.15e+00, 2.84e-02, 4.06e+00]    [0.00e+00, 4.83e+00, 4.06e+00]    [4.46e+00, 4.83e+00, 3.17e+00, 4.07e+00]    
9000      [3.02e+00, 2.52e-02, 3.99e+00]    [0.00e+00, 4.71e+00, 3.99e+00]    [4.34e+00, 4.71e+00, 2.66e+00, 3.61e+00]    
10000     [2.94e+00, 1.03e-02, 3.92e+00]    [0.00e+00, 4.64e+00, 3.92e+00]    [4.27e+00, 4.64e+00, 2.43e+00, 3.46e+00]    
11000     [2.87e+00, 6.86e-03, 3.87e+00]    [0.00e+00, 4.55e+00, 3.87e+00]    [4.18e+00, 4.55e+00, 2.29e+00, 3.39e+00]    
12000     [2.80e+00, 1.36e-02, 3.82e+00]    [0.00e+00, 4.43e+00, 3.82e+00]    [4.06e+00, 4.43e+00, 2.13e+00, 3.25e+00]    
13000     [2.76e+00, 5.56e-02, 3.77e+00]    [0.00e+00, 4.33e+00, 3.77e+00]    [3.97e+00, 4.33e+00, 2.01e+00, 3.14e+00]    
14000     [2.71e+00, 2.22e-02, 3.72e+00]    [0.00e+00, 4.25e+00, 3.72e+00]    [3.87e+00, 4.25e+00, 2.01e+00, 3.12e+00]    
15000     [2.70e+00, 2.76e-02, 3.68e+00]    [0.00e+00, 4.15e+00, 3.68e+00]    [3.75e+00, 4.15e+00, 1.95e+00, 3.03e+00]    
16000     [2.62e+00, 4.80e-02, 3.64e+00]    [0.00e+00, 4.17e+00, 3.64e+00]    [3.77e+00, 4.17e+00, 2.07e+00, 3.14e+00]    
17000     [2.59e+00, 1.25e-02, 3.60e+00]    [0.00e+00, 4.15e+00, 3.60e+00]    [3.74e+00, 4.15e+00, 2.05e+00, 3.09e+00]    
18000     [2.60e+00, 3.16e-02, 3.56e+00]    [0.00e+00, 4.20e+00, 3.56e+00]    [3.78e+00, 4.20e+00, 2.09e+00, 3.12e+00]    
19000     [2.53e+00, 1.88e-02, 3.53e+00]    [0.00e+00, 4.15e+00, 3.53e+00]    [3.72e+00, 4.15e+00, 2.07e+00, 3.08e+00]    
20000     [2.52e+00, 3.82e-02, 3.50e+00]    [0.00e+00, 4.20e+00, 3.50e+00]    [3.76e+00, 4.20e+00, 2.15e+00, 3.16e+00]    
21000     [2.52e+00, 6.08e-03, 3.46e+00]    [0.00e+00, 4.11e+00, 3.46e+00]    [3.69e+00, 4.11e+00, 2.08e+00, 3.08e+00]    
22000     [2.48e+00, 1.07e-02, 3.43e+00]    [0.00e+00, 4.17e+00, 3.43e+00]    [3.75e+00, 4.17e+00, 2.15e+00, 3.15e+00]    
23000     [2.47e+00, 6.82e-03, 3.41e+00]    [0.00e+00, 4.23e+00, 3.41e+00]    [3.80e+00, 4.23e+00, 2.21e+00, 3.20e+00]    
24000     [2.44e+00, 1.02e-02, 3.38e+00]    [0.00e+00, 4.18e+00, 3.38e+00]    [3.75e+00, 4.18e+00, 2.11e+00, 3.08e+00]    
25000     [2.41e+00, 1.52e-02, 3.35e+00]    [0.00e+00, 4.18e+00, 3.35e+00]    [3.76e+00, 4.18e+00, 2.17e+00, 3.13e+00]    
26000     [2.44e+00, 2.91e-02, 3.33e+00]    [0.00e+00, 4.24e+00, 3.33e+00]    [3.81e+00, 4.24e+00, 2.23e+00, 3.18e+00]    
27000     [2.38e+00, 1.48e-02, 3.30e+00]    [0.00e+00, 4.20e+00, 3.30e+00]    [3.78e+00, 4.20e+00, 2.20e+00, 3.14e+00]    
28000     [2.38e+00, 2.72e-02, 3.28e+00]    [0.00e+00, 4.22e+00, 3.28e+00]    [3.79e+00, 4.22e+00, 2.20e+00, 3.12e+00]    
29000     [2.38e+00, 2.67e-02, 3.25e+00]    [0.00e+00, 4.24e+00, 3.25e+00]    [3.81e+00, 4.24e+00, 2.31e+00, 3.21e+00]    
30000     [2.33e+00, 4.86e-02, 3.23e+00]    [0.00e+00, 4.20e+00, 3.23e+00]    [3.77e+00, 4.20e+00, 2.24e+00, 3.12e+00]    

Best model at step 30000:
  train loss: 5.61e+00
  test loss: 7.43e+00
  test metric: [3.77e+00, 4.20e+00, 2.24e+00, 3.12e+00]

'train' took 92.628255 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.616358 s

'compile' took 2.649913 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+02, 1.00e+02, 2.83e+00]    [0.00e+00, 1.02e+02, 2.83e+00]    [1.02e+02, 1.02e+02, 3.04e+00, 3.04e+00]    
1000      [6.85e+01, 6.67e+01, 3.17e+00]    [0.00e+00, 7.02e+01, 3.17e+00]    [7.06e+01, 7.02e+01, 3.13e+01, 3.14e+01]    
2000      [2.84e+01, 3.17e-02, 4.16e+00]    [0.00e+00, 3.11e+01, 4.16e+00]    [3.19e+01, 3.11e+01, 3.10e+01, 3.06e+01]    
3000      [2.02e+01, 1.78e-02, 4.18e+00]    [0.00e+00, 2.33e+01, 4.18e+00]    [2.43e+01, 2.33e+01, 2.51e+01, 2.49e+01]    
4000      [1.25e+01, 7.68e-03, 4.31e+00]    [0.00e+00, 1.35e+01, 4.31e+00]    [1.41e+01, 1.35e+01, 1.70e+01, 1.67e+01]    
5000      [7.54e+00, 3.88e-02, 4.40e+00]    [0.00e+00, 8.26e+00, 4.40e+00]    [8.11e+00, 8.26e+00, 9.87e+00, 9.27e+00]    
6000      [6.34e+00, 1.31e-02, 4.35e+00]    [0.00e+00, 6.79e+00, 4.35e+00]    [6.56e+00, 6.79e+00, 7.14e+00, 6.53e+00]    
7000      [5.51e+00, 4.18e-02, 4.31e+00]    [0.00e+00, 5.60e+00, 4.31e+00]    [5.36e+00, 5.60e+00, 5.07e+00, 4.68e+00]    
8000      [4.74e+00, 1.10e-02, 4.28e+00]    [0.00e+00, 4.44e+00, 4.28e+00]    [4.11e+00, 4.44e+00, 3.74e+00, 3.46e+00]    
9000      [4.25e+00, 3.66e-03, 4.23e+00]    [0.00e+00, 3.78e+00, 4.23e+00]    [3.46e+00, 3.78e+00, 3.14e+00, 3.06e+00]    
10000     [3.95e+00, 1.43e-02, 4.18e+00]    [0.00e+00, 3.64e+00, 4.18e+00]    [3.38e+00, 3.64e+00, 2.70e+00, 2.74e+00]    
11000     [3.72e+00, 2.67e-02, 4.13e+00]    [0.00e+00, 3.45e+00, 4.13e+00]    [3.23e+00, 3.45e+00, 2.55e+00, 2.58e+00]    
12000     [3.53e+00, 8.95e-03, 4.10e+00]    [0.00e+00, 3.25e+00, 4.10e+00]    [3.04e+00, 3.25e+00, 2.34e+00, 2.50e+00]    
13000     [3.33e+00, 1.91e-02, 4.06e+00]    [0.00e+00, 2.98e+00, 4.06e+00]    [2.79e+00, 2.98e+00, 2.28e+00, 2.44e+00]    
14000     [3.27e+00, 3.93e-05, 4.01e+00]    [0.00e+00, 3.13e+00, 4.01e+00]    [2.86e+00, 3.13e+00, 2.09e+00, 2.12e+00]    
15000     [3.20e+00, 7.07e-03, 3.97e+00]    [0.00e+00, 3.04e+00, 3.97e+00]    [2.81e+00, 3.04e+00, 2.05e+00, 2.04e+00]    
16000     [3.11e+00, 2.40e-03, 3.93e+00]    [0.00e+00, 3.12e+00, 3.93e+00]    [2.94e+00, 3.12e+00, 1.90e+00, 1.91e+00]    
17000     [3.07e+00, 3.66e-02, 3.89e+00]    [0.00e+00, 3.21e+00, 3.89e+00]    [3.06e+00, 3.21e+00, 1.78e+00, 1.79e+00]    
18000     [3.09e+00, 4.22e-02, 3.86e+00]    [0.00e+00, 2.98e+00, 3.86e+00]    [2.88e+00, 2.98e+00, 1.85e+00, 1.86e+00]    
19000     [2.99e+00, 7.54e-03, 3.83e+00]    [0.00e+00, 2.97e+00, 3.83e+00]    [2.93e+00, 2.97e+00, 1.78e+00, 1.82e+00]    
20000     [2.94e+00, 9.80e-03, 3.80e+00]    [0.00e+00, 2.95e+00, 3.80e+00]    [2.92e+00, 2.95e+00, 1.76e+00, 1.78e+00]    
21000     [2.93e+00, 1.22e-02, 3.76e+00]    [0.00e+00, 2.90e+00, 3.76e+00]    [2.91e+00, 2.90e+00, 1.73e+00, 1.73e+00]    
22000     [2.89e+00, 3.67e-02, 3.73e+00]    [0.00e+00, 2.95e+00, 3.73e+00]    [2.97e+00, 2.95e+00, 1.67e+00, 1.64e+00]    
23000     [2.87e+00, 5.52e-02, 3.69e+00]    [0.00e+00, 2.92e+00, 3.69e+00]    [2.98e+00, 2.92e+00, 1.65e+00, 1.63e+00]    
24000     [2.89e+00, 4.55e-02, 3.66e+00]    [0.00e+00, 2.91e+00, 3.66e+00]    [2.99e+00, 2.91e+00, 1.62e+00, 1.57e+00]    
25000     [2.87e+00, 4.55e-02, 3.63e+00]    [0.00e+00, 2.88e+00, 3.63e+00]    [2.98e+00, 2.88e+00, 1.59e+00, 1.52e+00]    
26000     [2.81e+00, 6.68e-03, 3.60e+00]    [0.00e+00, 2.79e+00, 3.60e+00]    [2.92e+00, 2.79e+00, 1.58e+00, 1.52e+00]    
27000     [2.76e+00, 1.06e-02, 3.57e+00]    [0.00e+00, 2.74e+00, 3.57e+00]    [2.89e+00, 2.74e+00, 1.54e+00, 1.49e+00]    
28000     [2.80e+00, 5.61e-03, 3.55e+00]    [0.00e+00, 2.75e+00, 3.55e+00]    [2.92e+00, 2.75e+00, 1.54e+00, 1.45e+00]    
29000     [2.86e+00, 4.16e-02, 3.52e+00]    [0.00e+00, 2.61e+00, 3.52e+00]    [2.77e+00, 2.61e+00, 1.55e+00, 1.55e+00]    
30000     [2.75e+00, 3.38e-03, 3.49e+00]    [0.00e+00, 2.62e+00, 3.49e+00]    [2.83e+00, 2.62e+00, 1.51e+00, 1.48e+00]    

Best model at step 30000:
  train loss: 6.25e+00
  test loss: 6.12e+00
  test metric: [2.83e+00, 2.62e+00, 1.51e+00, 1.48e+00]

'train' took 93.008265 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.510633 s

'compile' took 2.280897 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.95e+01, 9.05e+01, 2.67e+00]    [0.00e+00, 9.92e+01, 2.67e+00]    [9.92e+01, 9.92e+01, 2.82e+00, 2.82e+00]    
1000      [8.36e+01, 4.08e-02, 2.57e+00]    [0.00e+00, 9.13e+01, 2.57e+00]    [9.13e+01, 9.13e+01, 1.50e+01, 1.50e+01]    
2000      [8.28e+01, 1.68e-01, 2.49e+00]    [0.00e+00, 9.05e+01, 2.49e+00]    [9.05e+01, 9.05e+01, 1.51e+01, 1.51e+01]    
3000      [8.21e+01, 1.83e-01, 2.44e+00]    [0.00e+00, 8.98e+01, 2.44e+00]    [8.98e+01, 8.98e+01, 1.49e+01, 1.49e+01]    
4000      [8.11e+01, 1.33e-02, 2.41e+00]    [0.00e+00, 8.89e+01, 2.41e+00]    [8.89e+01, 8.89e+01, 1.45e+01, 1.46e+01]    
5000      [7.99e+01, 5.66e-02, 2.41e+00]    [0.00e+00, 8.78e+01, 2.41e+00]    [8.78e+01, 8.78e+01, 1.43e+01, 1.43e+01]    
6000      [7.85e+01, 2.07e-01, 2.44e+00]    [0.00e+00, 8.63e+01, 2.44e+00]    [8.63e+01, 8.63e+01, 1.41e+01, 1.42e+01]    
7000      [7.66e+01, 3.25e-02, 2.49e+00]    [0.00e+00, 8.44e+01, 2.49e+00]    [8.44e+01, 8.44e+01, 1.43e+01, 1.43e+01]    
8000      [7.41e+01, 5.87e-01, 2.57e+00]    [0.00e+00, 8.18e+01, 2.57e+00]    [8.18e+01, 8.18e+01, 1.50e+01, 1.50e+01]    
9000      [7.09e+01, 3.13e-01, 2.68e+00]    [0.00e+00, 7.84e+01, 2.68e+00]    [7.85e+01, 7.84e+01, 1.63e+01, 1.63e+01]    
10000     [6.67e+01, 3.15e-01, 2.83e+00]    [0.00e+00, 7.41e+01, 2.83e+00]    [7.41e+01, 7.41e+01, 1.86e+01, 1.86e+01]    
11000     [6.15e+01, 4.00e-01, 3.00e+00]    [0.00e+00, 6.85e+01, 3.00e+00]    [6.85e+01, 6.85e+01, 2.23e+01, 2.24e+01]    
12000     [5.46e+01, 2.02e-01, 3.22e+00]    [0.00e+00, 6.13e+01, 3.22e+00]    [6.13e+01, 6.13e+01, 2.76e+01, 2.77e+01]    
13000     [4.73e+01, 2.35e-01, 3.45e+00]    [0.00e+00, 5.51e+01, 3.45e+00]    [5.51e+01, 5.51e+01, 2.96e+01, 2.96e+01]    
14000     [4.27e+01, 1.08e+00, 3.61e+00]    [0.00e+00, 5.08e+01, 3.61e+00]    [5.08e+01, 5.08e+01, 2.77e+01, 2.77e+01]    
15000     [3.86e+01, 5.85e-01, 3.77e+00]    [0.00e+00, 4.48e+01, 3.77e+00]    [4.49e+01, 4.48e+01, 2.60e+01, 2.60e+01]    
16000     [3.43e+01, 4.86e-01, 3.95e+00]    [0.00e+00, 3.81e+01, 3.95e+00]    [3.81e+01, 3.81e+01, 2.46e+01, 2.46e+01]    
17000     [2.92e+01, 2.75e-01, 4.19e+00]    [0.00e+00, 3.00e+01, 4.19e+00]    [3.01e+01, 3.00e+01, 2.26e+01, 2.26e+01]    
18000     [2.44e+01, 2.81e-01, 4.39e+00]    [0.00e+00, 2.48e+01, 4.39e+00]    [2.48e+01, 2.48e+01, 1.88e+01, 1.88e+01]    
19000     [1.87e+01, 1.01e+00, 4.64e+00]    [0.00e+00, 1.91e+01, 4.64e+00]    [1.91e+01, 1.91e+01, 1.42e+01, 1.42e+01]    
20000     [1.36e+01, 3.76e-01, 4.84e+00]    [0.00e+00, 1.36e+01, 4.84e+00]    [1.36e+01, 1.36e+01, 1.04e+01, 1.04e+01]    
21000     [9.61e+00, 1.70e-01, 4.99e+00]    [0.00e+00, 9.35e+00, 4.99e+00]    [9.31e+00, 9.35e+00, 8.38e+00, 8.33e+00]    
22000     [6.89e+00, 8.07e-01, 5.07e+00]    [0.00e+00, 7.17e+00, 5.07e+00]    [7.17e+00, 7.17e+00, 6.98e+00, 6.97e+00]    
23000     [6.04e+00, 4.70e-01, 4.99e+00]    [0.00e+00, 5.92e+00, 4.99e+00]    [5.87e+00, 5.92e+00, 7.43e+00, 7.37e+00]    
24000     [5.55e+00, 3.21e-01, 4.90e+00]    [0.00e+00, 5.42e+00, 4.90e+00]    [5.33e+00, 5.42e+00, 8.25e+00, 8.17e+00]    
25000     [5.32e+00, 7.63e-01, 4.78e+00]    [0.00e+00, 5.18e+00, 4.78e+00]    [5.13e+00, 5.18e+00, 7.89e+00, 7.83e+00]    
26000     [5.19e+00, 1.90e+00, 4.68e+00]    [0.00e+00, 5.09e+00, 4.68e+00]    [5.03e+00, 5.09e+00, 7.33e+00, 7.28e+00]    
27000     [5.03e+00, 1.28e+00, 4.59e+00]    [0.00e+00, 4.98e+00, 4.59e+00]    [4.96e+00, 4.98e+00, 7.35e+00, 7.32e+00]    
28000     [4.87e+00, 8.07e-01, 4.51e+00]    [0.00e+00, 4.91e+00, 4.51e+00]    [4.86e+00, 4.91e+00, 6.82e+00, 6.77e+00]    
29000     [4.74e+00, 2.73e-01, 4.44e+00]    [0.00e+00, 4.82e+00, 4.44e+00]    [4.79e+00, 4.82e+00, 6.52e+00, 6.49e+00]    
30000     [4.68e+00, 8.35e-01, 4.38e+00]    [0.00e+00, 4.78e+00, 4.38e+00]    [4.77e+00, 4.78e+00, 6.27e+00, 6.25e+00]    

Best model at step 29000:
  train loss: 9.45e+00
  test loss: 9.26e+00
  test metric: [4.79e+00, 4.82e+00, 6.52e+00, 6.49e+00]

'train' took 82.594091 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.490689 s

'compile' took 2.273917 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.92e+01, 2.77e+00]    [0.00e+00, 1.00e+02, 2.77e+00]    [1.00e+02, 1.00e+02, 1.91e+00, 1.91e+00]    
1000      [5.98e+01, 5.03e+01, 3.49e+00]    [0.00e+00, 5.68e+01, 3.49e+00]    [5.70e+01, 5.68e+01, 3.57e+01, 3.56e+01]    
2000      [3.10e+01, 4.51e-02, 4.24e+00]    [0.00e+00, 3.42e+01, 4.24e+00]    [3.46e+01, 3.42e+01, 2.34e+01, 2.21e+01]    
3000      [1.76e+01, 1.61e-02, 4.46e+00]    [0.00e+00, 1.78e+01, 4.46e+00]    [1.91e+01, 1.78e+01, 1.30e+01, 1.29e+01]    
4000      [6.94e+00, 1.28e-02, 4.73e+00]    [0.00e+00, 8.52e+00, 4.73e+00]    [8.63e+00, 8.52e+00, 7.26e+00, 6.88e+00]    
5000      [4.41e+00, 1.42e-02, 4.70e+00]    [0.00e+00, 5.73e+00, 4.70e+00]    [5.83e+00, 5.73e+00, 4.26e+00, 4.24e+00]    
6000      [3.54e+00, 5.78e-02, 4.60e+00]    [0.00e+00, 4.37e+00, 4.60e+00]    [4.39e+00, 4.37e+00, 3.04e+00, 3.04e+00]    
7000      [3.21e+00, 8.36e-04, 4.52e+00]    [0.00e+00, 3.70e+00, 4.52e+00]    [3.70e+00, 3.70e+00, 2.65e+00, 2.63e+00]    
8000      [2.99e+00, 3.14e-02, 4.45e+00]    [0.00e+00, 3.22e+00, 4.45e+00]    [3.21e+00, 3.22e+00, 2.34e+00, 2.22e+00]    
9000      [2.91e+00, 4.05e-03, 4.38e+00]    [0.00e+00, 3.05e+00, 4.38e+00]    [3.03e+00, 3.05e+00, 2.24e+00, 2.06e+00]    
10000     [2.84e+00, 2.93e-02, 4.31e+00]    [0.00e+00, 2.95e+00, 4.31e+00]    [2.95e+00, 2.95e+00, 2.26e+00, 2.05e+00]    
11000     [2.78e+00, 8.86e-03, 4.25e+00]    [0.00e+00, 2.87e+00, 4.25e+00]    [2.93e+00, 2.87e+00, 2.21e+00, 2.02e+00]    
12000     [2.74e+00, 1.72e-02, 4.19e+00]    [0.00e+00, 2.85e+00, 4.19e+00]    [2.94e+00, 2.85e+00, 2.15e+00, 1.97e+00]    
13000     [2.71e+00, 5.36e-02, 4.14e+00]    [0.00e+00, 2.89e+00, 4.14e+00]    [3.02e+00, 2.89e+00, 2.06e+00, 1.92e+00]    
14000     [2.69e+00, 3.94e-03, 4.09e+00]    [0.00e+00, 2.86e+00, 4.09e+00]    [3.03e+00, 2.86e+00, 2.03e+00, 1.89e+00]    
15000     [2.66e+00, 1.22e-02, 4.04e+00]    [0.00e+00, 2.89e+00, 4.04e+00]    [3.09e+00, 2.89e+00, 2.05e+00, 1.90e+00]    
16000     [2.67e+00, 1.18e-02, 3.99e+00]    [0.00e+00, 2.87e+00, 3.99e+00]    [3.09e+00, 2.87e+00, 1.99e+00, 1.88e+00]    
17000     [2.63e+00, 3.14e-02, 3.95e+00]    [0.00e+00, 2.89e+00, 3.95e+00]    [3.14e+00, 2.89e+00, 2.03e+00, 1.92e+00]    
18000     [2.62e+00, 1.43e-02, 3.91e+00]    [0.00e+00, 2.94e+00, 3.91e+00]    [3.20e+00, 2.94e+00, 2.06e+00, 1.92e+00]    
19000     [2.61e+00, 1.88e-02, 3.86e+00]    [0.00e+00, 2.91e+00, 3.86e+00]    [3.18e+00, 2.91e+00, 1.99e+00, 1.89e+00]    
20000     [2.62e+00, 1.32e-02, 3.82e+00]    [0.00e+00, 2.90e+00, 3.82e+00]    [3.18e+00, 2.90e+00, 2.00e+00, 1.90e+00]    
21000     [2.58e+00, 2.63e-03, 3.78e+00]    [0.00e+00, 2.91e+00, 3.78e+00]    [3.20e+00, 2.91e+00, 2.00e+00, 1.88e+00]    
22000     [2.58e+00, 2.40e-02, 3.75e+00]    [0.00e+00, 3.00e+00, 3.75e+00]    [3.30e+00, 3.00e+00, 2.04e+00, 1.87e+00]    
23000     [2.55e+00, 3.93e-04, 3.71e+00]    [0.00e+00, 2.94e+00, 3.71e+00]    [3.25e+00, 2.94e+00, 2.04e+00, 1.89e+00]    
24000     [2.55e+00, 5.44e-03, 3.68e+00]    [0.00e+00, 3.00e+00, 3.68e+00]    [3.32e+00, 3.00e+00, 2.06e+00, 1.88e+00]    
25000     [2.53e+00, 5.48e-03, 3.64e+00]    [0.00e+00, 3.02e+00, 3.64e+00]    [3.35e+00, 3.02e+00, 2.09e+00, 1.89e+00]    
26000     [2.50e+00, 6.22e-03, 3.61e+00]    [0.00e+00, 2.98e+00, 3.61e+00]    [3.32e+00, 2.98e+00, 2.03e+00, 1.87e+00]    
27000     [2.50e+00, 3.54e-03, 3.58e+00]    [0.00e+00, 3.00e+00, 3.58e+00]    [3.36e+00, 3.00e+00, 2.07e+00, 1.88e+00]    
28000     [2.47e+00, 4.50e-03, 3.55e+00]    [0.00e+00, 3.00e+00, 3.55e+00]    [3.36e+00, 3.00e+00, 2.05e+00, 1.87e+00]    
29000     [2.48e+00, 4.02e-02, 3.52e+00]    [0.00e+00, 2.98e+00, 3.52e+00]    [3.35e+00, 2.98e+00, 1.99e+00, 1.86e+00]    
30000     [2.49e+00, 2.89e-03, 3.49e+00]    [0.00e+00, 3.03e+00, 3.49e+00]    [3.41e+00, 3.03e+00, 2.06e+00, 1.85e+00]    

Best model at step 30000:
  train loss: 5.98e+00
  test loss: 6.51e+00
  test metric: [3.41e+00, 3.03e+00, 2.06e+00, 1.85e+00]

'train' took 88.804482 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.494677 s

'compile' took 2.538213 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.84e+01, 2.83e+00]    [0.00e+00, 1.00e+02, 2.83e+00]    [1.00e+02, 1.00e+02, 1.27e+00, 1.27e+00]    
1000      [8.95e+01, 4.91e-02, 2.83e+00]    [0.00e+00, 8.82e+01, 2.83e+00]    [8.80e+01, 8.82e+01, 2.74e+01, 2.73e+01]    
2000      [8.70e+01, 6.28e-02, 2.80e+00]    [0.00e+00, 8.59e+01, 2.80e+00]    [8.57e+01, 8.59e+01, 2.56e+01, 2.52e+01]    
3000      [8.36e+01, 2.43e-01, 2.83e+00]    [0.00e+00, 8.25e+01, 2.83e+00]    [8.23e+01, 8.25e+01, 2.45e+01, 2.39e+01]    
4000      [8.01e+01, 1.90e-01, 2.89e+00]    [0.00e+00, 7.86e+01, 2.89e+00]    [7.84e+01, 7.86e+01, 2.52e+01, 2.43e+01]    
5000      [7.66e+01, 4.43e-02, 2.95e+00]    [0.00e+00, 7.45e+01, 2.95e+00]    [7.44e+01, 7.45e+01, 2.61e+01, 2.51e+01]    
6000      [7.27e+01, 2.51e-01, 3.02e+00]    [0.00e+00, 7.03e+01, 3.02e+00]    [7.02e+01, 7.03e+01, 2.61e+01, 2.50e+01]    
7000      [6.78e+01, 1.67e-01, 3.11e+00]    [0.00e+00, 6.52e+01, 3.11e+00]    [6.52e+01, 6.52e+01, 2.52e+01, 2.40e+01]    
8000      [6.15e+01, 2.85e-01, 3.23e+00]    [0.00e+00, 5.89e+01, 3.23e+00]    [5.93e+01, 5.89e+01, 2.23e+01, 2.15e+01]    
9000      [5.35e+01, 1.00e-01, 3.39e+00]    [0.00e+00, 5.10e+01, 3.39e+00]    [5.19e+01, 5.10e+01, 1.81e+01, 1.81e+01]    
10000     [4.33e+01, 8.56e-01, 3.59e+00]    [0.00e+00, 4.09e+01, 3.59e+00]    [4.20e+01, 4.09e+01, 1.43e+01, 1.45e+01]    
11000     [3.02e+01, 7.96e-01, 3.85e+00]    [0.00e+00, 2.77e+01, 3.85e+00]    [2.91e+01, 2.77e+01, 1.24e+01, 1.30e+01]    
12000     [1.67e+01, 5.88e-02, 4.08e+00]    [0.00e+00, 1.42e+01, 4.08e+00]    [1.48e+01, 1.42e+01, 1.24e+01, 1.22e+01]    
13000     [8.65e+00, 5.53e-01, 4.14e+00]    [0.00e+00, 7.73e+00, 4.14e+00]    [7.99e+00, 7.73e+00, 6.98e+00, 8.02e+00]    
14000     [6.25e+00, 3.10e-01, 4.12e+00]    [0.00e+00, 6.17e+00, 4.12e+00]    [5.02e+00, 6.17e+00, 5.79e+00, 6.42e+00]    
15000     [5.16e+00, 7.52e-02, 4.09e+00]    [0.00e+00, 5.53e+00, 4.09e+00]    [4.13e+00, 5.53e+00, 4.32e+00, 4.83e+00]    
16000     [4.51e+00, 4.59e-01, 4.05e+00]    [0.00e+00, 4.92e+00, 4.05e+00]    [3.58e+00, 4.92e+00, 3.39e+00, 4.07e+00]    
17000     [4.16e+00, 3.50e-01, 3.99e+00]    [0.00e+00, 4.31e+00, 3.99e+00]    [3.36e+00, 4.31e+00, 2.55e+00, 3.54e+00]    
18000     [4.00e+00, 1.29e+00, 3.94e+00]    [0.00e+00, 4.58e+00, 3.94e+00]    [3.39e+00, 4.58e+00, 2.35e+00, 3.32e+00]    
19000     [3.82e+00, 3.85e-01, 3.89e+00]    [0.00e+00, 4.26e+00, 3.89e+00]    [3.23e+00, 4.26e+00, 2.25e+00, 3.10e+00]    
20000     [3.65e+00, 8.70e-02, 3.85e+00]    [0.00e+00, 4.43e+00, 3.85e+00]    [3.42e+00, 4.43e+00, 2.09e+00, 2.93e+00]    
21000     [3.54e+00, 3.74e-02, 3.81e+00]    [0.00e+00, 4.34e+00, 3.81e+00]    [3.43e+00, 4.34e+00, 1.97e+00, 2.91e+00]    
22000     [3.48e+00, 5.61e-01, 3.78e+00]    [0.00e+00, 4.35e+00, 3.78e+00]    [3.42e+00, 4.35e+00, 1.96e+00, 2.79e+00]    
23000     [3.41e+00, 1.09e-01, 3.75e+00]    [0.00e+00, 4.04e+00, 3.75e+00]    [3.30e+00, 4.04e+00, 1.83e+00, 2.73e+00]    
24000     [3.35e+00, 2.16e-01, 3.72e+00]    [0.00e+00, 3.95e+00, 3.72e+00]    [3.23e+00, 3.95e+00, 1.82e+00, 2.66e+00]    
25000     [3.33e+00, 8.09e-01, 3.69e+00]    [0.00e+00, 3.94e+00, 3.69e+00]    [3.17e+00, 3.94e+00, 1.86e+00, 2.58e+00]    
26000     [3.27e+00, 5.00e-02, 3.66e+00]    [0.00e+00, 3.71e+00, 3.66e+00]    [3.06e+00, 3.71e+00, 1.81e+00, 2.51e+00]    
27000     [3.38e+00, 1.10e+00, 3.64e+00]    [0.00e+00, 3.57e+00, 3.64e+00]    [2.94e+00, 3.57e+00, 1.93e+00, 2.37e+00]    
28000     [3.21e+00, 4.30e-01, 3.62e+00]    [0.00e+00, 3.54e+00, 3.62e+00]    [2.96e+00, 3.54e+00, 1.80e+00, 2.33e+00]    
29000     [3.19e+00, 2.20e-01, 3.60e+00]    [0.00e+00, 3.48e+00, 3.60e+00]    [2.94e+00, 3.48e+00, 1.82e+00, 2.38e+00]    
30000     [3.16e+00, 2.30e-01, 3.57e+00]    [0.00e+00, 3.39e+00, 3.57e+00]    [2.89e+00, 3.39e+00, 1.80e+00, 2.32e+00]    

Best model at step 30000:
  train loss: 6.97e+00
  test loss: 6.96e+00
  test metric: [2.89e+00, 3.39e+00, 1.80e+00, 2.32e+00]

'train' took 80.303217 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.525598 s

'compile' took 2.439478 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.93e+01, 2.66e+00]    [0.00e+00, 1.01e+02, 2.66e+00]    [1.01e+02, 1.01e+02, 1.70e+00, 1.70e+00]    
1000      [5.98e+01, 4.77e+01, 3.39e+00]    [0.00e+00, 6.36e+01, 3.39e+00]    [6.39e+01, 6.36e+01, 2.24e+01, 2.24e+01]    
2000      [3.02e+01, 3.78e-02, 4.14e+00]    [0.00e+00, 3.45e+01, 4.14e+00]    [3.53e+01, 3.45e+01, 2.27e+01, 2.26e+01]    
3000      [1.35e+01, 1.21e-02, 4.53e+00]    [0.00e+00, 1.53e+01, 4.53e+00]    [1.58e+01, 1.53e+01, 1.29e+01, 1.27e+01]    
4000      [5.80e+00, 1.47e-02, 4.64e+00]    [0.00e+00, 6.97e+00, 4.64e+00]    [6.83e+00, 6.97e+00, 6.24e+00, 6.02e+00]    
5000      [3.91e+00, 1.80e-02, 4.56e+00]    [0.00e+00, 3.73e+00, 4.56e+00]    [3.63e+00, 3.73e+00, 2.77e+00, 2.81e+00]    
6000      [3.35e+00, 5.42e-03, 4.47e+00]    [0.00e+00, 3.25e+00, 4.47e+00]    [2.89e+00, 3.25e+00, 2.04e+00, 2.30e+00]    
7000      [3.26e+00, 2.11e-02, 4.38e+00]    [0.00e+00, 3.44e+00, 4.38e+00]    [2.88e+00, 3.44e+00, 1.91e+00, 2.12e+00]    
8000      [3.04e+00, 2.11e-03, 4.30e+00]    [0.00e+00, 3.54e+00, 4.30e+00]    [3.02e+00, 3.54e+00, 1.54e+00, 1.93e+00]    
9000      [2.92e+00, 2.90e-02, 4.22e+00]    [0.00e+00, 3.60e+00, 4.22e+00]    [3.14e+00, 3.60e+00, 1.40e+00, 1.92e+00]    
10000     [2.87e+00, 6.37e-03, 4.15e+00]    [0.00e+00, 3.69e+00, 4.15e+00]    [3.25e+00, 3.69e+00, 1.42e+00, 1.96e+00]    
11000     [2.83e+00, 1.51e-02, 4.09e+00]    [0.00e+00, 3.68e+00, 4.09e+00]    [3.26e+00, 3.68e+00, 1.42e+00, 1.96e+00]    
12000     [2.81e+00, 1.47e-02, 4.04e+00]    [0.00e+00, 3.71e+00, 4.04e+00]    [3.30e+00, 3.71e+00, 1.44e+00, 1.96e+00]    
13000     [2.73e+00, 3.36e-02, 3.99e+00]    [0.00e+00, 3.62e+00, 3.99e+00]    [3.26e+00, 3.62e+00, 1.40e+00, 1.95e+00]    
14000     [2.75e+00, 2.80e-03, 3.95e+00]    [0.00e+00, 3.57e+00, 3.95e+00]    [3.22e+00, 3.57e+00, 1.41e+00, 1.92e+00]    
15000     [2.69e+00, 2.75e-02, 3.90e+00]    [0.00e+00, 3.61e+00, 3.90e+00]    [3.28e+00, 3.61e+00, 1.42e+00, 1.92e+00]    
16000     [2.67e+00, 2.80e-02, 3.86e+00]    [0.00e+00, 3.65e+00, 3.86e+00]    [3.33e+00, 3.65e+00, 1.48e+00, 1.98e+00]    
17000     [2.67e+00, 6.68e-03, 3.82e+00]    [0.00e+00, 3.59e+00, 3.82e+00]    [3.27e+00, 3.59e+00, 1.46e+00, 1.91e+00]    
18000     [2.61e+00, 1.38e-02, 3.79e+00]    [0.00e+00, 3.63e+00, 3.79e+00]    [3.32e+00, 3.63e+00, 1.46e+00, 1.92e+00]    
19000     [2.64e+00, 4.11e-02, 3.75e+00]    [0.00e+00, 3.59e+00, 3.75e+00]    [3.29e+00, 3.59e+00, 1.50e+00, 1.94e+00]    
20000     [2.58e+00, 1.69e-02, 3.72e+00]    [0.00e+00, 3.62e+00, 3.72e+00]    [3.32e+00, 3.62e+00, 1.51e+00, 1.94e+00]    
21000     [2.58e+00, 4.58e-03, 3.68e+00]    [0.00e+00, 3.61e+00, 3.68e+00]    [3.32e+00, 3.61e+00, 1.54e+00, 1.95e+00]    
22000     [2.58e+00, 6.32e-03, 3.65e+00]    [0.00e+00, 3.58e+00, 3.65e+00]    [3.31e+00, 3.58e+00, 1.50e+00, 1.88e+00]    
23000     [2.56e+00, 2.75e-03, 3.62e+00]    [0.00e+00, 3.59e+00, 3.62e+00]    [3.32e+00, 3.59e+00, 1.53e+00, 1.91e+00]    
24000     [2.49e+00, 6.83e-03, 3.59e+00]    [0.00e+00, 3.65e+00, 3.59e+00]    [3.40e+00, 3.65e+00, 1.55e+00, 1.93e+00]    
25000     [2.53e+00, 1.46e-02, 3.56e+00]    [0.00e+00, 3.63e+00, 3.56e+00]    [3.38e+00, 3.63e+00, 1.59e+00, 1.95e+00]    
26000     [2.53e+00, 2.47e-02, 3.54e+00]    [0.00e+00, 3.71e+00, 3.54e+00]    [3.47e+00, 3.71e+00, 1.60e+00, 1.97e+00]    
27000     [2.46e+00, 4.88e-03, 3.51e+00]    [0.00e+00, 3.61e+00, 3.51e+00]    [3.37e+00, 3.61e+00, 1.57e+00, 1.91e+00]    
28000     [2.50e+00, 7.13e-04, 3.48e+00]    [0.00e+00, 3.56e+00, 3.48e+00]    [3.33e+00, 3.56e+00, 1.57e+00, 1.89e+00]    
29000     [2.44e+00, 4.30e-03, 3.46e+00]    [0.00e+00, 3.62e+00, 3.46e+00]    [3.40e+00, 3.62e+00, 1.56e+00, 1.89e+00]    
30000     [2.41e+00, 3.52e-03, 3.43e+00]    [0.00e+00, 3.60e+00, 3.43e+00]    [3.38e+00, 3.60e+00, 1.56e+00, 1.88e+00]    

Best model at step 30000:
  train loss: 5.85e+00
  test loss: 7.03e+00
  test metric: [3.38e+00, 3.60e+00, 1.56e+00, 1.88e+00]

'train' took 81.309532 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[2.864327390477897, 3.3282511103091865, 3.2847852314557793, 4.475133071141867, 4.196465306419439, 2.623590364605735, 4.81935659866893, 3.0271809049981773, 3.390448234377325, 3.5998238727192375]
E* 1 3.560936208517357 0.6806637153234485
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.490685 s

'compile' took 2.148254 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.93e+01, 9.90e+01, 2.63e+00]    [0.00e+00, 9.92e+01, 2.63e+00]    [9.92e+01, 9.92e+01, 1.93e+00, 1.93e+00]    
1000      [8.56e+01, 4.86e+01, 2.74e+00]    [0.00e+00, 8.38e+01, 2.74e+00]    [8.36e+01, 8.38e+01, 2.53e+01, 2.50e+01]    
2000      [7.53e+01, 3.94e+01, 3.00e+00]    [0.00e+00, 7.38e+01, 3.00e+00]    [7.36e+01, 7.38e+01, 2.24e+01, 2.15e+01]    
3000      [5.54e+01, 1.65e+01, 3.54e+00]    [0.00e+00, 5.48e+01, 3.54e+00]    [5.53e+01, 5.48e+01, 2.24e+01, 2.11e+01]    
4000      [3.75e+01, 1.16e-02, 3.91e+00]    [0.00e+00, 3.87e+01, 3.91e+00]    [3.85e+01, 3.87e+01, 2.49e+01, 2.23e+01]    
5000      [2.92e+01, 2.08e-01, 4.02e+00]    [0.00e+00, 3.14e+01, 4.02e+00]    [3.20e+01, 3.14e+01, 1.87e+01, 1.81e+01]    
6000      [2.29e+01, 5.41e-02, 4.07e+00]    [0.00e+00, 2.43e+01, 4.07e+00]    [2.47e+01, 2.43e+01, 1.52e+01, 1.51e+01]    
7000      [1.69e+01, 1.28e-02, 4.12e+00]    [0.00e+00, 1.75e+01, 4.12e+00]    [1.78e+01, 1.75e+01, 1.26e+01, 1.26e+01]    
8000      [1.14e+01, 3.24e-02, 4.17e+00]    [0.00e+00, 1.12e+01, 4.17e+00]    [1.12e+01, 1.12e+01, 1.14e+01, 1.14e+01]    
9000      [8.14e+00, 4.19e-02, 4.21e+00]    [0.00e+00, 7.29e+00, 4.21e+00]    [7.25e+00, 7.29e+00, 1.12e+01, 1.10e+01]    
10000     [7.16e+00, 2.40e-01, 4.20e+00]    [0.00e+00, 6.54e+00, 4.20e+00]    [6.72e+00, 6.54e+00, 1.02e+01, 9.96e+00]    
11000     [6.48e+00, 1.51e-01, 4.18e+00]    [0.00e+00, 5.83e+00, 4.18e+00]    [6.16e+00, 5.83e+00, 9.12e+00, 8.91e+00]    
12000     [6.15e+00, 1.23e-01, 4.14e+00]    [0.00e+00, 5.48e+00, 4.14e+00]    [5.93e+00, 5.48e+00, 8.21e+00, 8.06e+00]    
13000     [5.86e+00, 3.96e-02, 4.11e+00]    [0.00e+00, 5.24e+00, 4.11e+00]    [5.73e+00, 5.24e+00, 7.31e+00, 7.18e+00]    
14000     [5.60e+00, 1.04e-02, 4.08e+00]    [0.00e+00, 5.05e+00, 4.08e+00]    [5.52e+00, 5.05e+00, 6.60e+00, 6.47e+00]    
15000     [5.41e+00, 1.51e-01, 4.05e+00]    [0.00e+00, 4.94e+00, 4.05e+00]    [5.41e+00, 4.94e+00, 5.98e+00, 5.83e+00]    
16000     [5.24e+00, 1.50e-01, 4.02e+00]    [0.00e+00, 4.72e+00, 4.02e+00]    [5.16e+00, 4.72e+00, 5.38e+00, 5.23e+00]    
17000     [4.99e+00, 3.04e-01, 3.99e+00]    [0.00e+00, 4.33e+00, 3.99e+00]    [4.73e+00, 4.33e+00, 4.58e+00, 4.42e+00]    
18000     [4.97e+00, 2.55e-01, 3.97e+00]    [0.00e+00, 4.40e+00, 3.97e+00]    [4.77e+00, 4.40e+00, 4.26e+00, 4.08e+00]    
19000     [4.72e+00, 2.05e-01, 3.94e+00]    [0.00e+00, 4.01e+00, 3.94e+00]    [4.34e+00, 4.01e+00, 3.39e+00, 3.24e+00]    
20000     [4.58e+00, 1.35e-01, 3.92e+00]    [0.00e+00, 3.87e+00, 3.92e+00]    [4.17e+00, 3.87e+00, 2.82e+00, 2.66e+00]    
21000     [4.43e+00, 1.06e-01, 3.90e+00]    [0.00e+00, 3.78e+00, 3.90e+00]    [4.07e+00, 3.78e+00, 2.26e+00, 2.08e+00]    
22000     [4.31e+00, 2.88e-01, 3.88e+00]    [0.00e+00, 3.53e+00, 3.88e+00]    [3.79e+00, 3.53e+00, 1.66e+00, 1.47e+00]    
23000     [4.16e+00, 3.83e-03, 3.86e+00]    [0.00e+00, 3.51e+00, 3.86e+00]    [3.76e+00, 3.51e+00, 1.50e+00, 1.30e+00]    
24000     [4.13e+00, 3.36e-01, 3.84e+00]    [0.00e+00, 3.45e+00, 3.84e+00]    [3.67e+00, 3.45e+00, 1.46e+00, 1.27e+00]    
25000     [3.94e+00, 7.27e-02, 3.83e+00]    [0.00e+00, 3.04e+00, 3.83e+00]    [3.24e+00, 3.04e+00, 1.56e+00, 1.42e+00]    
26000     [3.84e+00, 2.47e-01, 3.81e+00]    [0.00e+00, 2.94e+00, 3.81e+00]    [3.07e+00, 2.94e+00, 1.58e+00, 1.36e+00]    
27000     [3.81e+00, 3.97e-01, 3.78e+00]    [0.00e+00, 2.99e+00, 3.78e+00]    [3.19e+00, 2.99e+00, 1.71e+00, 1.56e+00]    
28000     [3.68e+00, 3.15e-01, 3.76e+00]    [0.00e+00, 2.90e+00, 3.76e+00]    [3.05e+00, 2.90e+00, 1.48e+00, 1.25e+00]    
29000     [3.68e+00, 2.27e-01, 3.74e+00]    [0.00e+00, 2.94e+00, 3.74e+00]    [3.10e+00, 2.94e+00, 1.59e+00, 1.31e+00]    
30000     [3.58e+00, 2.44e-01, 3.72e+00]    [0.00e+00, 2.87e+00, 3.72e+00]    [3.04e+00, 2.87e+00, 1.43e+00, 1.21e+00]    

Best model at step 30000:
  train loss: 7.54e+00
  test loss: 6.59e+00
  test metric: [3.04e+00, 2.87e+00, 1.43e+00, 1.21e+00]

'train' took 80.902613 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.479718 s

'compile' took 2.235022 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.98e+01, 2.68e+00]    [0.00e+00, 1.01e+02, 2.68e+00]    [1.01e+02, 1.01e+02, 2.58e+00, 2.58e+00]    
1000      [5.48e+01, 5.32e+01, 3.41e+00]    [0.00e+00, 5.93e+01, 3.41e+00]    [5.98e+01, 5.93e+01, 3.63e+01, 3.65e+01]    
2000      [2.08e+01, 8.67e+00, 4.19e+00]    [0.00e+00, 2.53e+01, 4.19e+00]    [2.59e+01, 2.53e+01, 1.55e+01, 1.42e+01]    
3000      [7.14e+00, 2.06e-02, 4.39e+00]    [0.00e+00, 9.62e+00, 4.39e+00]    [9.87e+00, 9.62e+00, 7.60e+00, 7.43e+00]    
4000      [5.50e+00, 2.04e-02, 4.35e+00]    [0.00e+00, 7.09e+00, 4.35e+00]    [7.20e+00, 7.09e+00, 5.08e+00, 4.98e+00]    
5000      [4.64e+00, 6.45e-03, 4.29e+00]    [0.00e+00, 5.66e+00, 4.29e+00]    [5.67e+00, 5.66e+00, 3.61e+00, 3.61e+00]    
6000      [4.01e+00, 1.46e-02, 4.24e+00]    [0.00e+00, 4.81e+00, 4.24e+00]    [4.72e+00, 4.81e+00, 2.51e+00, 2.56e+00]    
7000      [3.61e+00, 1.22e-02, 4.19e+00]    [0.00e+00, 4.17e+00, 4.19e+00]    [3.96e+00, 4.17e+00, 2.11e+00, 2.14e+00]    
8000      [3.27e+00, 8.43e-03, 4.15e+00]    [0.00e+00, 3.69e+00, 4.15e+00]    [3.33e+00, 3.69e+00, 2.05e+00, 2.04e+00]    
9000      [3.05e+00, 1.22e-02, 4.11e+00]    [0.00e+00, 3.41e+00, 4.11e+00]    [2.94e+00, 3.41e+00, 2.11e+00, 2.11e+00]    
10000     [2.92e+00, 2.18e-03, 4.07e+00]    [0.00e+00, 3.36e+00, 4.07e+00]    [2.86e+00, 3.36e+00, 2.01e+00, 2.18e+00]    
11000     [2.84e+00, 1.39e-03, 4.02e+00]    [0.00e+00, 3.23e+00, 4.02e+00]    [2.88e+00, 3.23e+00, 1.81e+00, 2.23e+00]    
12000     [2.77e+00, 6.22e-03, 3.99e+00]    [0.00e+00, 3.05e+00, 3.99e+00]    [2.91e+00, 3.05e+00, 1.69e+00, 2.32e+00]    
13000     [2.72e+00, 5.00e-03, 3.95e+00]    [0.00e+00, 3.01e+00, 3.95e+00]    [2.97e+00, 3.01e+00, 1.63e+00, 2.27e+00]    
14000     [2.67e+00, 2.13e-02, 3.91e+00]    [0.00e+00, 3.00e+00, 3.91e+00]    [2.97e+00, 3.00e+00, 1.60e+00, 2.34e+00]    
15000     [2.66e+00, 2.30e-02, 3.87e+00]    [0.00e+00, 2.99e+00, 3.87e+00]    [2.95e+00, 2.99e+00, 1.63e+00, 2.31e+00]    
16000     [2.65e+00, 1.29e-02, 3.83e+00]    [0.00e+00, 3.08e+00, 3.83e+00]    [3.05e+00, 3.08e+00, 1.66e+00, 2.24e+00]    
17000     [2.62e+00, 1.35e-02, 3.80e+00]    [0.00e+00, 3.02e+00, 3.80e+00]    [2.95e+00, 3.02e+00, 1.62e+00, 2.33e+00]    
18000     [2.61e+00, 2.40e-02, 3.76e+00]    [0.00e+00, 3.02e+00, 3.76e+00]    [2.93e+00, 3.02e+00, 1.65e+00, 2.34e+00]    
19000     [2.63e+00, 1.14e-02, 3.73e+00]    [0.00e+00, 3.05e+00, 3.73e+00]    [3.03e+00, 3.05e+00, 1.74e+00, 2.26e+00]    
20000     [2.57e+00, 2.65e-02, 3.70e+00]    [0.00e+00, 2.99e+00, 3.70e+00]    [2.96e+00, 2.99e+00, 1.68e+00, 2.37e+00]    
21000     [2.58e+00, 1.94e-02, 3.67e+00]    [0.00e+00, 3.02e+00, 3.67e+00]    [2.96e+00, 3.02e+00, 1.67e+00, 2.37e+00]    
22000     [2.56e+00, 6.10e-03, 3.63e+00]    [0.00e+00, 3.00e+00, 3.63e+00]    [2.90e+00, 3.00e+00, 1.68e+00, 2.34e+00]    
23000     [2.54e+00, 1.05e-02, 3.60e+00]    [0.00e+00, 2.98e+00, 3.60e+00]    [2.93e+00, 2.98e+00, 1.69e+00, 2.36e+00]    
24000     [2.63e+00, 5.28e-03, 3.57e+00]    [0.00e+00, 3.09e+00, 3.57e+00]    [2.83e+00, 3.09e+00, 1.66e+00, 2.31e+00]    
25000     [2.54e+00, 4.38e-03, 3.54e+00]    [0.00e+00, 2.98e+00, 3.54e+00]    [2.96e+00, 2.98e+00, 1.71e+00, 2.33e+00]    
26000     [2.53e+00, 1.14e-02, 3.51e+00]    [0.00e+00, 2.98e+00, 3.51e+00]    [2.96e+00, 2.98e+00, 1.70e+00, 2.33e+00]    
27000     [2.52e+00, 3.16e-03, 3.49e+00]    [0.00e+00, 2.98e+00, 3.49e+00]    [2.92e+00, 2.98e+00, 1.69e+00, 2.33e+00]    
28000     [2.54e+00, 5.65e-03, 3.46e+00]    [0.00e+00, 2.97e+00, 3.46e+00]    [2.93e+00, 2.97e+00, 1.73e+00, 2.31e+00]    
29000     [2.50e+00, 1.88e-02, 3.43e+00]    [0.00e+00, 3.01e+00, 3.43e+00]    [2.92e+00, 3.01e+00, 1.70e+00, 2.32e+00]    
30000     [2.48e+00, 4.59e-03, 3.41e+00]    [0.00e+00, 2.99e+00, 3.41e+00]    [2.93e+00, 2.99e+00, 1.72e+00, 2.33e+00]    

Best model at step 30000:
  train loss: 5.89e+00
  test loss: 6.40e+00
  test metric: [2.93e+00, 2.99e+00, 1.72e+00, 2.33e+00]

'train' took 80.942509 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.451790 s

'compile' took 2.160220 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+02, 1.02e+02, 2.69e+00]    [0.00e+00, 1.02e+02, 2.69e+00]    [1.02e+02, 1.02e+02, 3.92e+00, 3.92e+00]    
1000      [8.53e+01, 4.61e+01, 3.14e+00]    [0.00e+00, 8.46e+01, 3.14e+00]    [8.46e+01, 8.46e+01, 2.77e+01, 2.91e+01]    
2000      [4.03e+01, 2.73e+01, 3.85e+00]    [0.00e+00, 4.10e+01, 3.85e+00]    [4.24e+01, 4.10e+01, 2.61e+01, 2.62e+01]    
3000      [1.19e+01, 9.66e-02, 4.39e+00]    [0.00e+00, 1.24e+01, 4.39e+00]    [1.33e+01, 1.24e+01, 1.58e+01, 1.53e+01]    
4000      [9.70e+00, 1.05e-01, 4.34e+00]    [0.00e+00, 1.05e+01, 4.34e+00]    [1.07e+01, 1.05e+01, 1.35e+01, 1.28e+01]    
5000      [8.32e+00, 3.29e-02, 4.30e+00]    [0.00e+00, 9.10e+00, 4.30e+00]    [9.23e+00, 9.10e+00, 1.15e+01, 1.09e+01]    
6000      [7.34e+00, 3.35e-02, 4.27e+00]    [0.00e+00, 8.10e+00, 4.27e+00]    [8.22e+00, 8.10e+00, 9.54e+00, 9.09e+00]    
7000      [6.58e+00, 1.95e-02, 4.24e+00]    [0.00e+00, 6.86e+00, 4.24e+00]    [7.00e+00, 6.86e+00, 8.10e+00, 7.88e+00]    
8000      [5.91e+00, 3.18e-02, 4.21e+00]    [0.00e+00, 5.95e+00, 4.21e+00]    [5.94e+00, 5.95e+00, 6.86e+00, 6.71e+00]    
9000      [5.17e+00, 6.42e-02, 4.20e+00]    [0.00e+00, 5.22e+00, 4.20e+00]    [5.18e+00, 5.22e+00, 5.38e+00, 5.45e+00]    
10000     [4.52e+00, 9.24e-02, 4.19e+00]    [0.00e+00, 4.47e+00, 4.19e+00]    [4.44e+00, 4.47e+00, 3.73e+00, 4.07e+00]    
11000     [3.94e+00, 1.68e-01, 4.19e+00]    [0.00e+00, 3.58e+00, 4.19e+00]    [3.48e+00, 3.58e+00, 2.37e+00, 3.06e+00]    
12000     [3.48e+00, 3.47e-02, 4.17e+00]    [0.00e+00, 3.25e+00, 4.17e+00]    [3.09e+00, 3.25e+00, 1.76e+00, 2.61e+00]    
13000     [3.32e+00, 8.57e-02, 4.15e+00]    [0.00e+00, 3.07e+00, 4.15e+00]    [2.85e+00, 3.07e+00, 1.53e+00, 2.54e+00]    
14000     [3.11e+00, 5.47e-03, 4.12e+00]    [0.00e+00, 3.10e+00, 4.12e+00]    [2.89e+00, 3.10e+00, 1.50e+00, 2.47e+00]    
15000     [2.97e+00, 7.43e-03, 4.09e+00]    [0.00e+00, 3.18e+00, 4.09e+00]    [2.91e+00, 3.18e+00, 1.46e+00, 2.43e+00]    
16000     [2.92e+00, 7.65e-02, 4.06e+00]    [0.00e+00, 3.24e+00, 4.06e+00]    [2.99e+00, 3.24e+00, 1.51e+00, 2.35e+00]    
17000     [2.82e+00, 5.63e-02, 4.02e+00]    [0.00e+00, 3.17e+00, 4.02e+00]    [2.83e+00, 3.17e+00, 1.42e+00, 2.48e+00]    
18000     [2.79e+00, 1.21e-02, 3.98e+00]    [0.00e+00, 3.21e+00, 3.98e+00]    [2.87e+00, 3.21e+00, 1.44e+00, 2.52e+00]    
19000     [2.71e+00, 2.52e-02, 3.95e+00]    [0.00e+00, 3.13e+00, 3.95e+00]    [2.81e+00, 3.13e+00, 1.37e+00, 2.59e+00]    
20000     [2.69e+00, 3.71e-02, 3.91e+00]    [0.00e+00, 3.09e+00, 3.91e+00]    [2.85e+00, 3.09e+00, 1.37e+00, 2.54e+00]    
21000     [2.69e+00, 7.88e-02, 3.88e+00]    [0.00e+00, 3.05e+00, 3.88e+00]    [2.78e+00, 3.05e+00, 1.29e+00, 2.62e+00]    
22000     [2.69e+00, 8.05e-02, 3.84e+00]    [0.00e+00, 3.00e+00, 3.84e+00]    [2.77e+00, 3.00e+00, 1.25e+00, 2.64e+00]    
23000     [2.61e+00, 4.96e-02, 3.80e+00]    [0.00e+00, 2.96e+00, 3.80e+00]    [2.83e+00, 2.96e+00, 1.22e+00, 2.55e+00]    
24000     [2.63e+00, 8.35e-02, 3.77e+00]    [0.00e+00, 2.91e+00, 3.77e+00]    [2.80e+00, 2.91e+00, 1.18e+00, 2.60e+00]    
25000     [2.65e+00, 1.09e-01, 3.74e+00]    [0.00e+00, 2.93e+00, 3.74e+00]    [2.77e+00, 2.93e+00, 1.18e+00, 2.60e+00]    
26000     [2.55e+00, 1.65e-02, 3.71e+00]    [0.00e+00, 2.91e+00, 3.71e+00]    [2.94e+00, 2.91e+00, 1.23e+00, 2.44e+00]    
27000     [2.52e+00, 5.78e-02, 3.69e+00]    [0.00e+00, 2.90e+00, 3.69e+00]    [2.96e+00, 2.90e+00, 1.23e+00, 2.40e+00]    
28000     [2.54e+00, 7.22e-02, 3.66e+00]    [0.00e+00, 2.82e+00, 3.66e+00]    [2.87e+00, 2.82e+00, 1.14e+00, 2.52e+00]    
29000     [2.48e+00, 3.22e-02, 3.63e+00]    [0.00e+00, 2.90e+00, 3.63e+00]    [2.97e+00, 2.90e+00, 1.22e+00, 2.35e+00]    
30000     [2.47e+00, 2.00e-02, 3.60e+00]    [0.00e+00, 2.88e+00, 3.60e+00]    [2.96e+00, 2.88e+00, 1.18e+00, 2.42e+00]    

Best model at step 30000:
  train loss: 6.09e+00
  test loss: 6.49e+00
  test metric: [2.96e+00, 2.88e+00, 1.18e+00, 2.42e+00]

'train' took 82.657920 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.483705 s

'compile' took 2.261950 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.95e+01, 9.72e+01, 2.92e+00]    [0.00e+00, 9.88e+01, 2.92e+00]    [9.88e+01, 9.88e+01, 2.88e+00, 2.88e+00]    
1000      [8.20e+01, 4.15e+01, 3.10e+00]    [0.00e+00, 8.10e+01, 3.10e+00]    [8.10e+01, 8.10e+01, 2.47e+01, 2.47e+01]    
2000      [6.05e+01, 2.21e-01, 3.74e+00]    [0.00e+00, 6.35e+01, 3.74e+00]    [6.36e+01, 6.35e+01, 3.23e+01, 3.21e+01]    
3000      [5.21e+01, 4.23e-02, 3.78e+00]    [0.00e+00, 5.55e+01, 3.78e+00]    [5.56e+01, 5.55e+01, 3.07e+01, 3.04e+01]    
4000      [4.51e+01, 3.81e-02, 3.87e+00]    [0.00e+00, 4.90e+01, 3.87e+00]    [4.92e+01, 4.90e+01, 2.60e+01, 2.58e+01]    
5000      [3.72e+01, 8.85e-02, 4.00e+00]    [0.00e+00, 4.19e+01, 4.00e+00]    [4.21e+01, 4.19e+01, 2.07e+01, 2.07e+01]    
6000      [2.76e+01, 3.22e-02, 4.19e+00]    [0.00e+00, 3.26e+01, 4.19e+00]    [3.26e+01, 3.26e+01, 1.64e+01, 1.64e+01]    
7000      [1.55e+01, 2.51e-02, 4.43e+00]    [0.00e+00, 2.03e+01, 4.43e+00]    [2.02e+01, 2.03e+01, 1.30e+01, 1.30e+01]    
8000      [1.01e+01, 8.43e-02, 4.53e+00]    [0.00e+00, 1.34e+01, 4.53e+00]    [1.34e+01, 1.34e+01, 1.11e+01, 1.13e+01]    
9000      [7.75e+00, 1.06e-01, 4.54e+00]    [0.00e+00, 1.04e+01, 4.54e+00]    [1.04e+01, 1.04e+01, 1.02e+01, 1.03e+01]    
10000     [6.27e+00, 1.12e-01, 4.51e+00]    [0.00e+00, 8.15e+00, 4.51e+00]    [8.20e+00, 8.15e+00, 9.68e+00, 9.79e+00]    
11000     [5.60e+00, 2.77e-01, 4.47e+00]    [0.00e+00, 7.23e+00, 4.47e+00]    [7.32e+00, 7.23e+00, 9.08e+00, 9.19e+00]    
12000     [5.21e+00, 1.71e-01, 4.40e+00]    [0.00e+00, 6.53e+00, 4.40e+00]    [6.63e+00, 6.53e+00, 8.48e+00, 8.60e+00]    
13000     [4.97e+00, 1.29e-01, 4.33e+00]    [0.00e+00, 6.18e+00, 4.33e+00]    [6.29e+00, 6.18e+00, 8.22e+00, 8.34e+00]    
14000     [4.78e+00, 1.56e-02, 4.28e+00]    [0.00e+00, 5.80e+00, 4.28e+00]    [5.93e+00, 5.80e+00, 8.01e+00, 8.15e+00]    
15000     [4.61e+00, 1.15e-01, 4.22e+00]    [0.00e+00, 5.55e+00, 4.22e+00]    [5.66e+00, 5.55e+00, 7.82e+00, 7.93e+00]    
16000     [4.46e+00, 1.54e-01, 4.18e+00]    [0.00e+00, 5.36e+00, 4.18e+00]    [5.46e+00, 5.36e+00, 7.57e+00, 7.67e+00]    
17000     [4.32e+00, 1.95e-01, 4.14e+00]    [0.00e+00, 5.28e+00, 4.14e+00]    [5.45e+00, 5.28e+00, 7.03e+00, 7.17e+00]    
18000     [4.18e+00, 1.51e-01, 4.10e+00]    [0.00e+00, 5.20e+00, 4.10e+00]    [5.37e+00, 5.20e+00, 6.89e+00, 7.02e+00]    
19000     [4.12e+00, 2.73e-01, 4.07e+00]    [0.00e+00, 5.19e+00, 4.07e+00]    [5.37e+00, 5.19e+00, 6.41e+00, 6.53e+00]    
20000     [4.01e+00, 2.16e-01, 4.03e+00]    [0.00e+00, 5.07e+00, 4.03e+00]    [5.27e+00, 5.07e+00, 6.23e+00, 6.36e+00]    
21000     [3.88e+00, 1.32e-01, 4.00e+00]    [0.00e+00, 4.89e+00, 4.00e+00]    [5.11e+00, 4.89e+00, 6.26e+00, 6.42e+00]    
22000     [3.77e+00, 8.61e-02, 3.97e+00]    [0.00e+00, 4.79e+00, 3.97e+00]    [5.02e+00, 4.79e+00, 5.99e+00, 6.14e+00]    
23000     [3.73e+00, 1.72e-01, 3.94e+00]    [0.00e+00, 4.70e+00, 3.94e+00]    [4.94e+00, 4.70e+00, 5.64e+00, 5.79e+00]    
24000     [3.62e+00, 5.63e-02, 3.91e+00]    [0.00e+00, 4.53e+00, 3.91e+00]    [4.79e+00, 4.53e+00, 5.64e+00, 5.82e+00]    
25000     [3.58e+00, 2.55e-01, 3.89e+00]    [0.00e+00, 4.46e+00, 3.89e+00]    [4.73e+00, 4.46e+00, 5.56e+00, 5.76e+00]    
26000     [3.51e+00, 1.43e-01, 3.86e+00]    [0.00e+00, 4.34e+00, 3.86e+00]    [4.61e+00, 4.34e+00, 5.25e+00, 5.46e+00]    
27000     [3.44e+00, 5.68e-02, 3.83e+00]    [0.00e+00, 4.29e+00, 3.83e+00]    [4.57e+00, 4.29e+00, 5.15e+00, 5.40e+00]    
28000     [3.46e+00, 2.10e-01, 3.81e+00]    [0.00e+00, 4.18e+00, 3.81e+00]    [4.48e+00, 4.18e+00, 5.19e+00, 5.46e+00]    
29000     [3.38e+00, 4.05e-02, 3.78e+00]    [0.00e+00, 4.15e+00, 3.78e+00]    [4.45e+00, 4.15e+00, 4.98e+00, 5.24e+00]    
30000     [3.43e+00, 4.29e-02, 3.76e+00]    [0.00e+00, 4.04e+00, 3.76e+00]    [4.35e+00, 4.04e+00, 4.99e+00, 5.28e+00]    

Best model at step 29000:
  train loss: 7.20e+00
  test loss: 7.93e+00
  test metric: [4.45e+00, 4.15e+00, 4.98e+00, 5.24e+00]

'train' took 86.581219 s


Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.456777 s

'compile' took 2.306830 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.95e+01, 2.53e+00]    [0.00e+00, 9.99e+01, 2.53e+00]    [9.99e+01, 9.99e+01, 9.98e-01, 9.98e-01]    
1000      [5.73e+01, 2.34e+01, 3.40e+00]    [0.00e+00, 5.78e+01, 3.40e+00]    [5.80e+01, 5.78e+01, 3.68e+01, 3.65e+01]    
2000      [3.16e+01, 9.82e-03, 3.85e+00]    [0.00e+00, 3.78e+01, 3.85e+00]    [3.82e+01, 3.78e+01, 2.54e+01, 2.54e+01]    
3000      [1.49e+01, 2.33e-02, 4.10e+00]    [0.00e+00, 1.52e+01, 4.10e+00]    [1.54e+01, 1.52e+01, 1.35e+01, 1.34e+01]    
4000      [3.74e+00, 2.74e-02, 4.24e+00]    [0.00e+00, 6.57e+00, 4.24e+00]    [6.50e+00, 6.57e+00, 4.90e+00, 5.02e+00]    
5000      [3.02e+00, 1.28e-02, 4.15e+00]    [0.00e+00, 5.44e+00, 4.15e+00]    [5.29e+00, 5.44e+00, 3.15e+00, 3.30e+00]    
6000      [2.82e+00, 1.24e-02, 4.07e+00]    [0.00e+00, 4.91e+00, 4.07e+00]    [4.74e+00, 4.91e+00, 2.58e+00, 2.79e+00]    
7000      [2.67e+00, 1.82e-02, 4.00e+00]    [0.00e+00, 4.68e+00, 4.00e+00]    [4.51e+00, 4.68e+00, 2.49e+00, 2.72e+00]    
8000      [2.60e+00, 6.72e-03, 3.94e+00]    [0.00e+00, 4.55e+00, 3.94e+00]    [4.37e+00, 4.55e+00, 2.29e+00, 2.50e+00]    
9000      [2.61e+00, 1.34e-02, 3.89e+00]    [0.00e+00, 4.61e+00, 3.89e+00]    [4.43e+00, 4.61e+00, 2.31e+00, 2.54e+00]    
10000     [2.54e+00, 2.55e-02, 3.84e+00]    [0.00e+00, 4.45e+00, 3.84e+00]    [4.27e+00, 4.45e+00, 2.11e+00, 2.32e+00]    
11000     [2.51e+00, 7.51e-03, 3.79e+00]    [0.00e+00, 4.42e+00, 3.79e+00]    [4.25e+00, 4.42e+00, 2.16e+00, 2.38e+00]    
12000     [2.47e+00, 2.29e-02, 3.75e+00]    [0.00e+00, 4.47e+00, 3.75e+00]    [4.29e+00, 4.47e+00, 2.17e+00, 2.40e+00]    
13000     [2.45e+00, 1.16e-02, 3.70e+00]    [0.00e+00, 4.42e+00, 3.70e+00]    [4.25e+00, 4.42e+00, 2.12e+00, 2.34e+00]    
14000     [2.40e+00, 2.42e-02, 3.66e+00]    [0.00e+00, 4.42e+00, 3.66e+00]    [4.25e+00, 4.42e+00, 2.19e+00, 2.42e+00]    
15000     [2.41e+00, 1.48e-02, 3.62e+00]    [0.00e+00, 4.44e+00, 3.62e+00]    [4.27e+00, 4.44e+00, 2.23e+00, 2.46e+00]    
16000     [2.43e+00, 1.13e-02, 3.58e+00]    [0.00e+00, 4.44e+00, 3.58e+00]    [4.28e+00, 4.44e+00, 2.21e+00, 2.43e+00]    
17000     [2.35e+00, 1.89e-02, 3.55e+00]    [0.00e+00, 4.34e+00, 3.55e+00]    [4.19e+00, 4.34e+00, 2.15e+00, 2.36e+00]    
18000     [2.33e+00, 2.41e-02, 3.51e+00]    [0.00e+00, 4.33e+00, 3.51e+00]    [4.19e+00, 4.33e+00, 2.19e+00, 2.40e+00]    
19000     [2.31e+00, 1.12e-02, 3.48e+00]    [0.00e+00, 4.33e+00, 3.48e+00]    [4.19e+00, 4.33e+00, 2.23e+00, 2.44e+00]    
20000     [2.31e+00, 1.26e-02, 3.44e+00]    [0.00e+00, 4.36e+00, 3.44e+00]    [4.22e+00, 4.36e+00, 2.26e+00, 2.46e+00]    
21000     [2.29e+00, 1.75e-02, 3.41e+00]    [0.00e+00, 4.34e+00, 3.41e+00]    [4.20e+00, 4.34e+00, 2.24e+00, 2.44e+00]    
22000     [2.28e+00, 2.94e-02, 3.38e+00]    [0.00e+00, 4.32e+00, 3.38e+00]    [4.19e+00, 4.32e+00, 2.27e+00, 2.47e+00]    
23000     [2.30e+00, 8.72e-03, 3.35e+00]    [0.00e+00, 4.24e+00, 3.35e+00]    [4.11e+00, 4.24e+00, 2.21e+00, 2.39e+00]    
24000     [2.28e+00, 1.45e-02, 3.32e+00]    [0.00e+00, 4.36e+00, 3.32e+00]    [4.22e+00, 4.36e+00, 2.31e+00, 2.50e+00]    
25000     [2.27e+00, 8.71e-03, 3.29e+00]    [0.00e+00, 4.25e+00, 3.29e+00]    [4.12e+00, 4.25e+00, 2.23e+00, 2.40e+00]    
26000     [2.28e+00, 1.14e-02, 3.26e+00]    [0.00e+00, 4.39e+00, 3.26e+00]    [4.25e+00, 4.39e+00, 2.41e+00, 2.57e+00]    
27000     [2.27e+00, 2.02e-02, 3.23e+00]    [0.00e+00, 4.25e+00, 3.23e+00]    [4.10e+00, 4.25e+00, 2.30e+00, 2.43e+00]    
28000     [2.25e+00, 7.75e-03, 3.21e+00]    [0.00e+00, 4.30e+00, 3.21e+00]    [4.14e+00, 4.30e+00, 2.28e+00, 2.39e+00]    
29000     [2.23e+00, 4.44e-03, 3.18e+00]    [0.00e+00, 4.31e+00, 3.18e+00]    [4.14e+00, 4.31e+00, 2.30e+00, 2.39e+00]    
30000     [2.30e+00, 1.48e-02, 3.15e+00]    [0.00e+00, 4.19e+00, 3.15e+00]    [4.02e+00, 4.19e+00, 2.33e+00, 2.42e+00]    

Best model at step 29000:
  train loss: 5.42e+00
  test loss: 7.49e+00
  test metric: [4.14e+00, 4.31e+00, 2.30e+00, 2.39e+00]

'train' took 91.775537 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.514624 s

'compile' took 2.378645 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.96e+01, 1.00e+02, 2.72e+00]    [0.00e+00, 9.98e+01, 2.72e+00]    [9.98e+01, 9.98e+01, 5.32e-01, 5.32e-01]    
1000      [5.60e+01, 3.33e+01, 3.46e+00]    [0.00e+00, 6.41e+01, 3.46e+00]    [6.47e+01, 6.41e+01, 3.81e+01, 3.77e+01]    
2000      [3.88e+01, 2.01e-02, 3.91e+00]    [0.00e+00, 4.50e+01, 3.91e+00]    [4.41e+01, 4.50e+01, 3.15e+01, 2.86e+01]    
3000      [2.80e+01, 3.64e-02, 3.84e+00]    [0.00e+00, 2.92e+01, 3.84e+00]    [2.95e+01, 2.92e+01, 2.72e+01, 2.52e+01]    
4000      [2.06e+01, 2.75e-02, 3.89e+00]    [0.00e+00, 2.18e+01, 3.89e+00]    [2.22e+01, 2.18e+01, 2.31e+01, 2.18e+01]    
5000      [1.33e+01, 6.43e-02, 3.99e+00]    [0.00e+00, 1.51e+01, 3.99e+00]    [1.54e+01, 1.51e+01, 1.69e+01, 1.61e+01]    
6000      [8.02e+00, 4.99e-02, 4.06e+00]    [0.00e+00, 9.36e+00, 4.06e+00]    [9.17e+00, 9.36e+00, 1.10e+01, 1.03e+01]    
7000      [5.04e+00, 1.87e-02, 4.10e+00]    [0.00e+00, 6.45e+00, 4.10e+00]    [6.19e+00, 6.45e+00, 4.44e+00, 4.08e+00]    
8000      [4.06e+00, 1.46e-02, 4.07e+00]    [0.00e+00, 5.07e+00, 4.07e+00]    [5.00e+00, 5.07e+00, 2.37e+00, 3.03e+00]    
9000      [3.72e+00, 4.66e-02, 4.01e+00]    [0.00e+00, 4.54e+00, 4.01e+00]    [4.48e+00, 4.54e+00, 2.09e+00, 2.90e+00]    
10000     [3.44e+00, 5.08e-02, 3.96e+00]    [0.00e+00, 4.23e+00, 3.96e+00]    [4.16e+00, 4.23e+00, 2.27e+00, 3.05e+00]    
11000     [3.29e+00, 2.75e-02, 3.91e+00]    [0.00e+00, 3.81e+00, 3.91e+00]    [3.66e+00, 3.81e+00, 2.48e+00, 3.08e+00]    
12000     [3.18e+00, 3.22e-02, 3.86e+00]    [0.00e+00, 3.69e+00, 3.86e+00]    [3.44e+00, 3.69e+00, 2.51e+00, 2.89e+00]    
13000     [3.11e+00, 6.66e-02, 3.81e+00]    [0.00e+00, 3.69e+00, 3.81e+00]    [3.44e+00, 3.69e+00, 2.53e+00, 2.83e+00]    
14000     [3.05e+00, 3.47e-02, 3.77e+00]    [0.00e+00, 3.64e+00, 3.77e+00]    [3.41e+00, 3.64e+00, 2.45e+00, 2.64e+00]    
15000     [2.96e+00, 3.46e-02, 3.72e+00]    [0.00e+00, 3.74e+00, 3.72e+00]    [3.54e+00, 3.74e+00, 2.38e+00, 2.56e+00]    
16000     [2.90e+00, 3.45e-02, 3.68e+00]    [0.00e+00, 3.79e+00, 3.68e+00]    [3.61e+00, 3.79e+00, 2.35e+00, 2.51e+00]    
17000     [2.86e+00, 3.86e-02, 3.65e+00]    [0.00e+00, 3.78e+00, 3.65e+00]    [3.64e+00, 3.78e+00, 2.31e+00, 2.44e+00]    
18000     [2.83e+00, 6.99e-03, 3.61e+00]    [0.00e+00, 3.81e+00, 3.61e+00]    [3.69e+00, 3.81e+00, 2.34e+00, 2.44e+00]    
19000     [2.78e+00, 1.44e-02, 3.58e+00]    [0.00e+00, 3.76e+00, 3.58e+00]    [3.69e+00, 3.76e+00, 2.25e+00, 2.35e+00]    
20000     [2.74e+00, 5.48e-02, 3.55e+00]    [0.00e+00, 3.71e+00, 3.55e+00]    [3.68e+00, 3.71e+00, 2.16e+00, 2.26e+00]    
21000     [2.72e+00, 1.24e-02, 3.51e+00]    [0.00e+00, 3.69e+00, 3.51e+00]    [3.67e+00, 3.69e+00, 2.09e+00, 2.15e+00]    
22000     [2.70e+00, 5.00e-02, 3.48e+00]    [0.00e+00, 3.77e+00, 3.48e+00]    [3.76e+00, 3.77e+00, 2.10e+00, 2.14e+00]    
23000     [2.64e+00, 1.23e-02, 3.46e+00]    [0.00e+00, 3.72e+00, 3.46e+00]    [3.72e+00, 3.72e+00, 2.00e+00, 2.01e+00]    
24000     [2.59e+00, 1.44e-02, 3.43e+00]    [0.00e+00, 3.73e+00, 3.43e+00]    [3.74e+00, 3.73e+00, 1.91e+00, 1.89e+00]    
25000     [2.55e+00, 1.44e-02, 3.40e+00]    [0.00e+00, 3.70e+00, 3.40e+00]    [3.72e+00, 3.70e+00, 1.86e+00, 1.81e+00]    
26000     [2.52e+00, 2.80e-02, 3.38e+00]    [0.00e+00, 3.69e+00, 3.38e+00]    [3.71e+00, 3.69e+00, 1.81e+00, 1.74e+00]    
27000     [2.50e+00, 2.35e-02, 3.35e+00]    [0.00e+00, 3.62e+00, 3.35e+00]    [3.65e+00, 3.62e+00, 1.81e+00, 1.71e+00]    
28000     [2.51e+00, 3.64e-02, 3.33e+00]    [0.00e+00, 3.57e+00, 3.33e+00]    [3.61e+00, 3.57e+00, 1.72e+00, 1.61e+00]    
29000     [2.46e+00, 2.35e-02, 3.30e+00]    [0.00e+00, 3.57e+00, 3.30e+00]    [3.62e+00, 3.57e+00, 1.76e+00, 1.63e+00]    
30000     [2.43e+00, 2.47e-02, 3.28e+00]    [0.00e+00, 3.63e+00, 3.28e+00]    [3.68e+00, 3.63e+00, 1.78e+00, 1.62e+00]    

Best model at step 30000:
  train loss: 5.73e+00
  test loss: 6.91e+00
  test metric: [3.68e+00, 3.63e+00, 1.78e+00, 1.62e+00]

'train' took 87.610666 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.477720 s

'compile' took 2.325776 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 1.04e+02, 2.71e+00]    [0.00e+00, 1.00e+02, 2.71e+00]    [1.00e+02, 1.00e+02, 1.26e+00, 1.26e+00]    
1000      [9.22e+01, 2.75e-01, 2.75e+00]    [0.00e+00, 9.07e+01, 2.75e+00]    [9.08e+01, 9.07e+01, 2.39e+01, 2.40e+01]    
2000      [8.16e+01, 3.78e-02, 2.79e+00]    [0.00e+00, 8.57e+01, 2.79e+00]    [8.57e+01, 8.57e+01, 2.19e+01, 2.19e+01]    
3000      [7.93e+01, 1.12e-01, 2.78e+00]    [0.00e+00, 8.40e+01, 2.78e+00]    [8.40e+01, 8.40e+01, 2.08e+01, 2.08e+01]    
4000      [7.66e+01, 2.01e-01, 2.80e+00]    [0.00e+00, 8.21e+01, 2.80e+00]    [8.21e+01, 8.21e+01, 1.94e+01, 1.95e+01]    
5000      [7.38e+01, 1.38e-01, 2.84e+00]    [0.00e+00, 7.97e+01, 2.84e+00]    [7.97e+01, 7.97e+01, 1.76e+01, 1.77e+01]    
6000      [7.03e+01, 8.31e-02, 2.91e+00]    [0.00e+00, 7.67e+01, 2.91e+00]    [7.68e+01, 7.67e+01, 1.61e+01, 1.61e+01]    
7000      [6.60e+01, 7.31e-02, 3.02e+00]    [0.00e+00, 7.26e+01, 3.02e+00]    [7.26e+01, 7.26e+01, 1.60e+01, 1.60e+01]    
8000      [6.07e+01, 5.51e-02, 3.17e+00]    [0.00e+00, 6.72e+01, 3.17e+00]    [6.72e+01, 6.72e+01, 1.74e+01, 1.74e+01]    
9000      [5.41e+01, 1.65e-01, 3.35e+00]    [0.00e+00, 6.04e+01, 3.35e+00]    [6.04e+01, 6.04e+01, 2.05e+01, 2.05e+01]    
10000     [4.61e+01, 1.85e-01, 3.57e+00]    [0.00e+00, 5.19e+01, 3.57e+00]    [5.19e+01, 5.19e+01, 2.56e+01, 2.57e+01]    
11000     [3.92e+01, 2.77e-01, 3.76e+00]    [0.00e+00, 4.55e+01, 3.76e+00]    [4.55e+01, 4.55e+01, 2.64e+01, 2.64e+01]    
12000     [3.47e+01, 4.69e-01, 3.85e+00]    [0.00e+00, 4.08e+01, 3.85e+00]    [4.08e+01, 4.08e+01, 2.50e+01, 2.50e+01]    
13000     [3.01e+01, 2.93e-01, 3.90e+00]    [0.00e+00, 3.51e+01, 3.90e+00]    [3.51e+01, 3.51e+01, 2.41e+01, 2.41e+01]    
14000     [2.51e+01, 2.27e-01, 3.97e+00]    [0.00e+00, 2.95e+01, 3.97e+00]    [2.95e+01, 2.95e+01, 2.19e+01, 2.19e+01]    
15000     [2.04e+01, 9.38e-01, 4.02e+00]    [0.00e+00, 2.41e+01, 4.02e+00]    [2.41e+01, 2.41e+01, 1.94e+01, 1.94e+01]    
16000     [1.61e+01, 3.13e-01, 4.06e+00]    [0.00e+00, 1.99e+01, 4.06e+00]    [1.99e+01, 1.99e+01, 1.65e+01, 1.65e+01]    
17000     [1.34e+01, 1.82e-01, 4.05e+00]    [0.00e+00, 1.82e+01, 4.05e+00]    [1.82e+01, 1.82e+01, 1.30e+01, 1.30e+01]    
18000     [1.13e+01, 3.99e-01, 4.01e+00]    [0.00e+00, 1.65e+01, 4.01e+00]    [1.65e+01, 1.65e+01, 1.11e+01, 1.11e+01]    
19000     [9.83e+00, 7.22e-01, 3.99e+00]    [0.00e+00, 1.49e+01, 3.99e+00]    [1.49e+01, 1.49e+01, 1.05e+01, 1.05e+01]    
20000     [8.78e+00, 8.72e-01, 3.96e+00]    [0.00e+00, 1.32e+01, 3.96e+00]    [1.32e+01, 1.32e+01, 1.01e+01, 1.01e+01]    
21000     [8.05e+00, 4.66e-01, 3.94e+00]    [0.00e+00, 1.18e+01, 3.94e+00]    [1.18e+01, 1.18e+01, 1.01e+01, 1.01e+01]    
22000     [7.47e+00, 3.87e-01, 3.93e+00]    [0.00e+00, 1.08e+01, 3.93e+00]    [1.08e+01, 1.08e+01, 9.55e+00, 9.55e+00]    
23000     [6.98e+00, 2.41e-01, 3.91e+00]    [0.00e+00, 1.00e+01, 3.91e+00]    [1.00e+01, 1.00e+01, 8.95e+00, 8.95e+00]    
24000     [6.65e+00, 7.52e-01, 3.89e+00]    [0.00e+00, 9.32e+00, 3.89e+00]    [9.32e+00, 9.32e+00, 8.53e+00, 8.52e+00]    
25000     [6.13e+00, 3.61e-01, 3.87e+00]    [0.00e+00, 8.52e+00, 3.87e+00]    [8.52e+00, 8.52e+00, 7.84e+00, 7.83e+00]    
26000     [5.78e+00, 4.91e-01, 3.85e+00]    [0.00e+00, 7.95e+00, 3.85e+00]    [7.95e+00, 7.95e+00, 7.29e+00, 7.29e+00]    
27000     [5.56e+00, 4.48e-01, 3.84e+00]    [0.00e+00, 7.56e+00, 3.84e+00]    [7.56e+00, 7.56e+00, 6.96e+00, 6.96e+00]    
28000     [5.28e+00, 2.89e-01, 3.83e+00]    [0.00e+00, 6.98e+00, 3.83e+00]    [6.98e+00, 6.98e+00, 6.61e+00, 6.60e+00]    
29000     [5.16e+00, 1.17e+00, 3.81e+00]    [0.00e+00, 6.57e+00, 3.81e+00]    [6.57e+00, 6.57e+00, 6.40e+00, 6.39e+00]    
30000     [4.86e+00, 3.86e-01, 3.80e+00]    [0.00e+00, 6.04e+00, 3.80e+00]    [6.04e+00, 6.04e+00, 5.80e+00, 5.79e+00]    

Best model at step 30000:
  train loss: 9.04e+00
  test loss: 9.84e+00
  test metric: [6.04e+00, 6.04e+00, 5.80e+00, 5.79e+00]

'train' took 88.018581 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.529582 s

'compile' took 2.300844 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.79e+01, 9.99e+01, 2.71e+00]    [0.00e+00, 9.77e+01, 2.71e+00]    [9.77e+01, 9.77e+01, 3.58e+00, 3.58e+00]    
1000      [5.89e+01, 2.38e+01, 3.57e+00]    [0.00e+00, 5.82e+01, 3.57e+00]    [5.79e+01, 5.82e+01, 3.74e+01, 3.63e+01]    
2000      [3.43e+01, 5.58e+00, 3.95e+00]    [0.00e+00, 3.54e+01, 3.95e+00]    [3.59e+01, 3.54e+01, 2.75e+01, 2.66e+01]    
3000      [1.32e+01, 6.41e-01, 4.23e+00]    [0.00e+00, 1.24e+01, 4.23e+00]    [1.31e+01, 1.24e+01, 8.50e+00, 7.87e+00]    
4000      [4.56e+00, 1.60e+00, 4.30e+00]    [0.00e+00, 6.35e+00, 4.30e+00]    [6.28e+00, 6.35e+00, 4.09e+00, 4.44e+00]    
5000      [4.34e+00, 1.72e-02, 4.21e+00]    [0.00e+00, 6.02e+00, 4.21e+00]    [6.00e+00, 6.02e+00, 3.31e+00, 3.42e+00]    
6000      [3.76e+00, 1.81e-02, 4.15e+00]    [0.00e+00, 5.40e+00, 4.15e+00]    [5.39e+00, 5.40e+00, 3.13e+00, 3.15e+00]    
7000      [3.40e+00, 1.17e-02, 4.09e+00]    [0.00e+00, 4.74e+00, 4.09e+00]    [4.75e+00, 4.74e+00, 3.10e+00, 3.05e+00]    
8000      [3.19e+00, 1.84e-02, 4.03e+00]    [0.00e+00, 4.50e+00, 4.03e+00]    [4.57e+00, 4.50e+00, 2.91e+00, 2.83e+00]    
9000      [3.09e+00, 3.49e-02, 3.97e+00]    [0.00e+00, 4.48e+00, 3.97e+00]    [4.62e+00, 4.48e+00, 2.69e+00, 2.64e+00]    
10000     [3.04e+00, 1.11e-02, 3.92e+00]    [0.00e+00, 4.47e+00, 3.92e+00]    [4.62e+00, 4.47e+00, 2.57e+00, 2.50e+00]    
11000     [2.97e+00, 1.80e-02, 3.86e+00]    [0.00e+00, 4.32e+00, 3.86e+00]    [4.47e+00, 4.32e+00, 2.36e+00, 2.31e+00]    
12000     [2.92e+00, 2.15e-02, 3.81e+00]    [0.00e+00, 4.22e+00, 3.81e+00]    [4.38e+00, 4.22e+00, 2.24e+00, 2.19e+00]    
13000     [2.94e+00, 2.23e-02, 3.76e+00]    [0.00e+00, 4.13e+00, 3.76e+00]    [4.29e+00, 4.13e+00, 2.10e+00, 2.06e+00]    
14000     [2.87e+00, 6.40e-03, 3.72e+00]    [0.00e+00, 4.11e+00, 3.72e+00]    [4.27e+00, 4.11e+00, 2.13e+00, 2.07e+00]    
15000     [2.84e+00, 7.27e-03, 3.68e+00]    [0.00e+00, 4.09e+00, 3.68e+00]    [4.25e+00, 4.09e+00, 2.14e+00, 2.07e+00]    
16000     [2.82e+00, 9.96e-03, 3.64e+00]    [0.00e+00, 4.06e+00, 3.64e+00]    [4.22e+00, 4.06e+00, 2.06e+00, 2.00e+00]    
17000     [2.84e+00, 6.65e-03, 3.60e+00]    [0.00e+00, 4.10e+00, 3.60e+00]    [4.26e+00, 4.10e+00, 2.05e+00, 1.98e+00]    
18000     [2.79e+00, 6.38e-03, 3.56e+00]    [0.00e+00, 4.07e+00, 3.56e+00]    [4.23e+00, 4.07e+00, 2.06e+00, 1.98e+00]    
19000     [2.77e+00, 2.74e-03, 3.52e+00]    [0.00e+00, 3.99e+00, 3.52e+00]    [4.15e+00, 3.99e+00, 1.93e+00, 1.86e+00]    
20000     [2.77e+00, 1.20e-02, 3.49e+00]    [0.00e+00, 3.94e+00, 3.49e+00]    [4.09e+00, 3.94e+00, 1.93e+00, 1.86e+00]    
21000     [2.74e+00, 1.80e-02, 3.46e+00]    [0.00e+00, 4.00e+00, 3.46e+00]    [4.15e+00, 4.00e+00, 1.91e+00, 1.83e+00]    
22000     [2.74e+00, 1.97e-02, 3.43e+00]    [0.00e+00, 3.98e+00, 3.43e+00]    [4.12e+00, 3.98e+00, 2.01e+00, 1.92e+00]    
23000     [2.68e+00, 3.25e-03, 3.40e+00]    [0.00e+00, 3.97e+00, 3.40e+00]    [4.11e+00, 3.97e+00, 1.91e+00, 1.82e+00]    
24000     [2.67e+00, 3.12e-03, 3.37e+00]    [0.00e+00, 3.91e+00, 3.37e+00]    [4.06e+00, 3.91e+00, 1.93e+00, 1.84e+00]    
25000     [2.71e+00, 2.68e-02, 3.34e+00]    [0.00e+00, 3.83e+00, 3.34e+00]    [3.98e+00, 3.83e+00, 1.75e+00, 1.66e+00]    
26000     [2.65e+00, 3.21e-02, 3.31e+00]    [0.00e+00, 3.85e+00, 3.31e+00]    [4.00e+00, 3.85e+00, 1.81e+00, 1.71e+00]    
27000     [2.61e+00, 1.86e-02, 3.28e+00]    [0.00e+00, 3.90e+00, 3.28e+00]    [4.05e+00, 3.90e+00, 1.92e+00, 1.81e+00]    
28000     [2.60e+00, 1.66e-02, 3.26e+00]    [0.00e+00, 3.89e+00, 3.26e+00]    [4.03e+00, 3.89e+00, 1.90e+00, 1.79e+00]    
29000     [2.61e+00, 8.49e-03, 3.23e+00]    [0.00e+00, 3.92e+00, 3.23e+00]    [4.07e+00, 3.92e+00, 1.96e+00, 1.85e+00]    
30000     [2.59e+00, 5.16e-04, 3.21e+00]    [0.00e+00, 3.83e+00, 3.21e+00]    [3.98e+00, 3.83e+00, 1.88e+00, 1.77e+00]    

Best model at step 30000:
  train loss: 5.80e+00
  test loss: 7.04e+00
  test metric: [3.98e+00, 3.83e+00, 1.88e+00, 1.77e+00]

'train' took 86.953431 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.651258 s

'compile' took 2.532227 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+02, 1.20e+02, 2.69e+00]    [0.00e+00, 1.01e+02, 2.69e+00]    [1.01e+02, 1.01e+02, 4.16e+00, 4.16e+00]    
1000      [8.33e+01, 2.50e+01, 2.61e+00]    [0.00e+00, 9.19e+01, 2.61e+00]    [9.19e+01, 9.19e+01, 1.53e+01, 1.53e+01]    
2000      [8.13e+01, 3.35e-01, 2.78e+00]    [0.00e+00, 8.40e+01, 2.78e+00]    [8.40e+01, 8.40e+01, 2.03e+01, 2.03e+01]    
3000      [7.71e+01, 1.52e-01, 2.79e+00]    [0.00e+00, 8.04e+01, 2.79e+00]    [8.04e+01, 8.04e+01, 1.87e+01, 1.86e+01]    
4000      [7.38e+01, 3.30e-02, 2.83e+00]    [0.00e+00, 7.72e+01, 2.83e+00]    [7.72e+01, 7.72e+01, 1.70e+01, 1.69e+01]    
5000      [7.02e+01, 1.51e-01, 2.89e+00]    [0.00e+00, 7.36e+01, 2.89e+00]    [7.36e+01, 7.36e+01, 1.50e+01, 1.49e+01]    
6000      [6.58e+01, 2.03e-01, 2.98e+00]    [0.00e+00, 6.90e+01, 2.98e+00]    [6.91e+01, 6.90e+01, 1.27e+01, 1.26e+01]    
7000      [6.05e+01, 7.43e-01, 3.11e+00]    [0.00e+00, 6.34e+01, 3.11e+00]    [6.34e+01, 6.34e+01, 1.02e+01, 1.02e+01]    
8000      [5.37e+01, 1.12e-01, 3.27e+00]    [0.00e+00, 5.63e+01, 3.27e+00]    [5.63e+01, 5.63e+01, 8.64e+00, 8.61e+00]    
9000      [4.53e+01, 2.21e-01, 3.48e+00]    [0.00e+00, 4.75e+01, 3.48e+00]    [4.76e+01, 4.75e+01, 9.60e+00, 9.66e+00]    
10000     [3.51e+01, 4.39e-01, 3.72e+00]    [0.00e+00, 3.65e+01, 3.72e+00]    [3.66e+01, 3.65e+01, 1.36e+01, 1.37e+01]    
11000     [2.43e+01, 5.64e-01, 3.95e+00]    [0.00e+00, 2.64e+01, 3.95e+00]    [2.66e+01, 2.64e+01, 1.31e+01, 1.33e+01]    
12000     [1.39e+01, 1.82e-01, 4.16e+00]    [0.00e+00, 1.56e+01, 4.16e+00]    [1.57e+01, 1.56e+01, 1.56e+01, 1.58e+01]    
13000     [1.10e+01, 4.90e-02, 4.18e+00]    [0.00e+00, 1.23e+01, 4.18e+00]    [1.23e+01, 1.23e+01, 1.44e+01, 1.46e+01]    
14000     [1.01e+01, 1.11e+00, 4.11e+00]    [0.00e+00, 1.11e+01, 4.11e+00]    [1.09e+01, 1.11e+01, 1.26e+01, 1.27e+01]    
15000     [9.04e+00, 4.35e-01, 4.05e+00]    [0.00e+00, 9.51e+00, 4.05e+00]    [9.37e+00, 9.51e+00, 1.04e+01, 1.05e+01]    
16000     [8.19e+00, 9.76e-02, 4.00e+00]    [0.00e+00, 8.12e+00, 4.00e+00]    [8.07e+00, 8.12e+00, 8.87e+00, 9.08e+00]    
17000     [7.46e+00, 5.44e-02, 3.96e+00]    [0.00e+00, 7.23e+00, 3.96e+00]    [7.08e+00, 7.23e+00, 7.61e+00, 7.79e+00]    
18000     [7.04e+00, 6.43e-01, 3.93e+00]    [0.00e+00, 6.42e+00, 3.93e+00]    [6.28e+00, 6.42e+00, 6.66e+00, 6.90e+00]    
19000     [6.62e+00, 3.48e-01, 3.89e+00]    [0.00e+00, 5.89e+00, 3.89e+00]    [5.63e+00, 5.89e+00, 6.13e+00, 6.28e+00]    
20000     [6.23e+00, 4.54e-01, 3.87e+00]    [0.00e+00, 5.46e+00, 3.87e+00]    [5.20e+00, 5.46e+00, 5.35e+00, 5.52e+00]    
21000     [5.96e+00, 4.14e-01, 3.84e+00]    [0.00e+00, 5.37e+00, 3.84e+00]    [5.10e+00, 5.37e+00, 4.88e+00, 5.06e+00]    
22000     [5.75e+00, 1.66e-01, 3.82e+00]    [0.00e+00, 5.30e+00, 3.82e+00]    [5.02e+00, 5.30e+00, 4.51e+00, 4.72e+00]    
23000     [5.54e+00, 1.70e-01, 3.81e+00]    [0.00e+00, 5.26e+00, 3.81e+00]    [4.97e+00, 5.26e+00, 4.05e+00, 4.29e+00]    
24000     [5.35e+00, 2.02e-01, 3.80e+00]    [0.00e+00, 5.24e+00, 3.80e+00]    [4.94e+00, 5.24e+00, 3.76e+00, 4.03e+00]    
25000     [5.17e+00, 5.22e-01, 3.78e+00]    [0.00e+00, 5.08e+00, 3.78e+00]    [4.78e+00, 5.08e+00, 3.23e+00, 3.53e+00]    
26000     [4.99e+00, 3.61e-01, 3.77e+00]    [0.00e+00, 4.99e+00, 3.77e+00]    [4.68e+00, 4.99e+00, 2.84e+00, 3.18e+00]    
27000     [4.91e+00, 6.90e-01, 3.76e+00]    [0.00e+00, 4.99e+00, 3.76e+00]    [4.68e+00, 4.99e+00, 2.47e+00, 2.89e+00]    
28000     [4.70e+00, 1.84e-02, 3.74e+00]    [0.00e+00, 4.83e+00, 3.74e+00]    [4.50e+00, 4.83e+00, 2.14e+00, 2.61e+00]    
29000     [4.55e+00, 4.59e-01, 3.73e+00]    [0.00e+00, 4.83e+00, 3.73e+00]    [4.49e+00, 4.83e+00, 1.94e+00, 2.48e+00]    
30000     [4.38e+00, 2.69e-01, 3.73e+00]    [0.00e+00, 4.57e+00, 3.73e+00]    [4.24e+00, 4.57e+00, 1.91e+00, 2.50e+00]    

Best model at step 30000:
  train loss: 8.38e+00
  test loss: 8.31e+00
  test metric: [4.24e+00, 4.57e+00, 1.91e+00, 2.50e+00]

'train' took 86.390934 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.480709 s

'compile' took 2.275909 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.01e+02, 2.83e+00]    [0.00e+00, 1.01e+02, 2.83e+00]    [1.01e+02, 1.01e+02, 2.46e+00, 2.46e+00]    
1000      [8.55e+01, 4.92e+01, 2.87e+00]    [0.00e+00, 8.41e+01, 2.87e+00]    [8.41e+01, 8.41e+01, 2.43e+01, 2.43e+01]    
2000      [7.53e+01, 4.08e+01, 3.11e+00]    [0.00e+00, 7.35e+01, 3.11e+00]    [7.35e+01, 7.35e+01, 2.39e+01, 2.37e+01]    
3000      [5.58e+01, 2.06e+01, 3.63e+00]    [0.00e+00, 5.77e+01, 3.63e+00]    [5.77e+01, 5.77e+01, 2.41e+01, 2.39e+01]    
4000      [3.53e+01, 3.64e-02, 4.08e+00]    [0.00e+00, 4.05e+01, 4.08e+00]    [4.06e+01, 4.05e+01, 2.84e+01, 2.84e+01]    
5000      [2.66e+01, 2.62e-02, 4.15e+00]    [0.00e+00, 2.91e+01, 4.15e+00]    [2.92e+01, 2.91e+01, 2.48e+01, 2.48e+01]    
6000      [1.99e+01, 7.69e-02, 4.22e+00]    [0.00e+00, 2.46e+01, 4.22e+00]    [2.46e+01, 2.46e+01, 1.91e+01, 1.90e+01]    
7000      [1.51e+01, 2.14e-02, 4.24e+00]    [0.00e+00, 2.08e+01, 4.24e+00]    [2.08e+01, 2.08e+01, 1.43e+01, 1.43e+01]    
8000      [1.10e+01, 5.29e-01, 4.23e+00]    [0.00e+00, 1.55e+01, 4.23e+00]    [1.55e+01, 1.55e+01, 1.19e+01, 1.19e+01]    
9000      [8.06e+00, 2.55e-01, 4.21e+00]    [0.00e+00, 1.15e+01, 4.21e+00]    [1.15e+01, 1.15e+01, 9.63e+00, 9.62e+00]    
10000     [6.56e+00, 1.01e-01, 4.17e+00]    [0.00e+00, 9.06e+00, 4.17e+00]    [9.06e+00, 9.06e+00, 7.61e+00, 7.60e+00]    
11000     [5.54e+00, 1.97e-01, 4.13e+00]    [0.00e+00, 7.42e+00, 4.13e+00]    [7.41e+00, 7.42e+00, 6.07e+00, 6.06e+00]    
12000     [4.89e+00, 5.88e-01, 4.09e+00]    [0.00e+00, 6.26e+00, 4.09e+00]    [6.25e+00, 6.26e+00, 5.07e+00, 5.04e+00]    
13000     [4.38e+00, 1.66e-01, 4.05e+00]    [0.00e+00, 5.53e+00, 4.05e+00]    [5.54e+00, 5.53e+00, 4.58e+00, 4.56e+00]    
14000     [4.05e+00, 3.43e-01, 4.01e+00]    [0.00e+00, 5.40e+00, 4.01e+00]    [5.43e+00, 5.40e+00, 3.57e+00, 3.55e+00]    
15000     [3.76e+00, 2.13e-01, 3.98e+00]    [0.00e+00, 5.25e+00, 3.98e+00]    [5.27e+00, 5.25e+00, 2.98e+00, 2.97e+00]    
16000     [3.60e+00, 2.99e-01, 3.96e+00]    [0.00e+00, 5.04e+00, 3.96e+00]    [5.06e+00, 5.04e+00, 2.89e+00, 2.87e+00]    
17000     [3.37e+00, 1.34e-01, 3.94e+00]    [0.00e+00, 4.75e+00, 3.94e+00]    [4.78e+00, 4.75e+00, 2.77e+00, 2.77e+00]    
18000     [3.27e+00, 2.63e-01, 3.91e+00]    [0.00e+00, 4.83e+00, 3.91e+00]    [4.86e+00, 4.83e+00, 2.39e+00, 2.39e+00]    
19000     [3.17e+00, 1.37e-01, 3.89e+00]    [0.00e+00, 4.84e+00, 3.89e+00]    [4.85e+00, 4.84e+00, 2.30e+00, 2.30e+00]    
20000     [3.14e+00, 1.35e-01, 3.87e+00]    [0.00e+00, 4.89e+00, 3.87e+00]    [4.90e+00, 4.89e+00, 2.32e+00, 2.31e+00]    
21000     [3.07e+00, 6.90e-02, 3.85e+00]    [0.00e+00, 4.85e+00, 3.85e+00]    [4.85e+00, 4.85e+00, 2.26e+00, 2.26e+00]    
22000     [3.07e+00, 3.18e-01, 3.83e+00]    [0.00e+00, 4.82e+00, 3.83e+00]    [4.81e+00, 4.82e+00, 2.12e+00, 2.11e+00]    
23000     [2.99e+00, 5.40e-03, 3.81e+00]    [0.00e+00, 4.69e+00, 3.81e+00]    [4.68e+00, 4.69e+00, 2.22e+00, 2.21e+00]    
24000     [2.95e+00, 1.05e-01, 3.79e+00]    [0.00e+00, 4.61e+00, 3.79e+00]    [4.60e+00, 4.61e+00, 2.19e+00, 2.18e+00]    
25000     [2.90e+00, 4.98e-02, 3.78e+00]    [0.00e+00, 4.54e+00, 3.78e+00]    [4.52e+00, 4.54e+00, 2.24e+00, 2.23e+00]    
26000     [2.89e+00, 2.30e-01, 3.76e+00]    [0.00e+00, 4.49e+00, 3.76e+00]    [4.46e+00, 4.49e+00, 2.16e+00, 2.16e+00]    
27000     [2.92e+00, 3.01e-01, 3.75e+00]    [0.00e+00, 4.41e+00, 3.75e+00]    [4.38e+00, 4.41e+00, 2.37e+00, 2.35e+00]    
28000     [2.85e+00, 1.68e-01, 3.73e+00]    [0.00e+00, 4.35e+00, 3.73e+00]    [4.31e+00, 4.35e+00, 2.31e+00, 2.30e+00]    
29000     [2.78e+00, 3.12e-02, 3.72e+00]    [0.00e+00, 4.39e+00, 3.72e+00]    [4.35e+00, 4.39e+00, 2.20e+00, 2.19e+00]    
30000     [2.76e+00, 5.74e-02, 3.70e+00]    [0.00e+00, 4.35e+00, 3.70e+00]    [4.31e+00, 4.35e+00, 2.19e+00, 2.18e+00]    

Best model at step 30000:
  train loss: 6.52e+00
  test loss: 8.05e+00
  test metric: [4.31e+00, 4.35e+00, 2.19e+00, 2.18e+00]

'train' took 81.955794 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[2.872625346650846, 2.9869647606933505, 2.8825307820180748, 4.147797210390874, 4.312497833742238, 3.6258418270356656, 6.038931750067133, 3.8290353528554775, 4.57356279517723, 4.350461245226219]
E* 2 3.962024890385711 0.9181117892194388
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.460771 s

'compile' took 2.205102 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.98e+01, 1.02e+02, 2.62e+00]    [0.00e+00, 1.00e+02, 2.62e+00]    [1.00e+02, 1.00e+02, 1.86e+00, 1.86e+00]    
1000      [8.22e+01, 5.10e+01, 2.56e+00]    [0.00e+00, 9.09e+01, 2.56e+00]    [9.09e+01, 9.09e+01, 1.41e+01, 1.42e+01]    
2000      [7.75e+01, 3.57e+01, 2.77e+00]    [0.00e+00, 8.02e+01, 2.77e+00]    [8.02e+01, 8.02e+01, 2.37e+01, 2.38e+01]    
3000      [6.68e+01, 2.24e+01, 3.09e+00]    [0.00e+00, 6.84e+01, 3.09e+00]    [6.85e+01, 6.84e+01, 2.26e+01, 2.24e+01]    
4000      [4.76e+01, 7.93e+00, 3.57e+00]    [0.00e+00, 5.00e+01, 3.57e+00]    [5.02e+01, 5.00e+01, 3.18e+01, 3.16e+01]    
5000      [3.72e+01, 5.73e-02, 3.79e+00]    [0.00e+00, 4.40e+01, 3.79e+00]    [4.40e+01, 4.40e+01, 2.70e+01, 2.65e+01]    
6000      [3.26e+01, 6.47e-01, 3.82e+00]    [0.00e+00, 3.78e+01, 3.82e+00]    [3.79e+01, 3.78e+01, 2.53e+01, 2.49e+01]    
7000      [2.74e+01, 3.47e-01, 3.89e+00]    [0.00e+00, 3.09e+01, 3.89e+00]    [3.11e+01, 3.09e+01, 2.34e+01, 2.31e+01]    
8000      [2.18e+01, 2.48e-01, 4.00e+00]    [0.00e+00, 2.33e+01, 4.00e+00]    [2.35e+01, 2.33e+01, 2.12e+01, 2.11e+01]    
9000      [1.56e+01, 1.07e+00, 4.15e+00]    [0.00e+00, 1.53e+01, 4.15e+00]    [1.54e+01, 1.53e+01, 1.88e+01, 1.87e+01]    
10000     [1.10e+01, 6.54e-01, 4.26e+00]    [0.00e+00, 1.14e+01, 4.26e+00]    [1.14e+01, 1.14e+01, 1.41e+01, 1.41e+01]    
11000     [7.89e+00, 5.04e-01, 4.34e+00]    [0.00e+00, 8.87e+00, 4.34e+00]    [8.88e+00, 8.87e+00, 9.54e+00, 9.55e+00]    
12000     [6.78e+00, 3.79e-02, 4.33e+00]    [0.00e+00, 7.02e+00, 4.33e+00]    [7.05e+00, 7.02e+00, 6.69e+00, 6.74e+00]    
13000     [5.98e+00, 1.52e-01, 4.33e+00]    [0.00e+00, 5.76e+00, 4.33e+00]    [5.79e+00, 5.76e+00, 5.35e+00, 5.38e+00]    
14000     [5.38e+00, 3.50e-01, 4.32e+00]    [0.00e+00, 5.37e+00, 4.32e+00]    [5.39e+00, 5.37e+00, 4.03e+00, 4.05e+00]    
15000     [4.97e+00, 1.14e-01, 4.30e+00]    [0.00e+00, 4.99e+00, 4.30e+00]    [5.01e+00, 4.99e+00, 3.18e+00, 3.20e+00]    
16000     [4.64e+00, 2.49e-01, 4.29e+00]    [0.00e+00, 4.40e+00, 4.29e+00]    [4.41e+00, 4.40e+00, 2.55e+00, 2.55e+00]    
17000     [4.36e+00, 2.11e-01, 4.26e+00]    [0.00e+00, 4.14e+00, 4.26e+00]    [4.15e+00, 4.14e+00, 2.37e+00, 2.37e+00]    
18000     [4.24e+00, 6.93e-01, 4.23e+00]    [0.00e+00, 3.87e+00, 4.23e+00]    [3.90e+00, 3.87e+00, 2.26e+00, 2.26e+00]    
19000     [3.98e+00, 2.29e-01, 4.21e+00]    [0.00e+00, 3.59e+00, 4.21e+00]    [3.62e+00, 3.59e+00, 2.22e+00, 2.18e+00]    
20000     [3.88e+00, 5.60e-01, 4.19e+00]    [0.00e+00, 3.35e+00, 4.19e+00]    [3.40e+00, 3.35e+00, 2.13e+00, 2.07e+00]    
21000     [3.65e+00, 1.01e-01, 4.17e+00]    [0.00e+00, 3.17e+00, 4.17e+00]    [3.22e+00, 3.17e+00, 2.07e+00, 1.96e+00]    
22000     [3.61e+00, 4.60e-01, 4.15e+00]    [0.00e+00, 3.03e+00, 4.15e+00]    [3.07e+00, 3.03e+00, 2.01e+00, 1.83e+00]    
23000     [3.44e+00, 4.70e-02, 4.13e+00]    [0.00e+00, 2.99e+00, 4.13e+00]    [3.04e+00, 2.99e+00, 1.96e+00, 1.71e+00]    
24000     [3.35e+00, 8.41e-02, 4.11e+00]    [0.00e+00, 2.95e+00, 4.11e+00]    [3.01e+00, 2.95e+00, 1.90e+00, 1.60e+00]    
25000     [3.27e+00, 1.97e-01, 4.09e+00]    [0.00e+00, 3.01e+00, 4.09e+00]    [3.08e+00, 3.01e+00, 1.73e+00, 1.41e+00]    
26000     [3.19e+00, 1.88e-01, 4.07e+00]    [0.00e+00, 3.01e+00, 4.07e+00]    [3.09e+00, 3.01e+00, 1.65e+00, 1.30e+00]    
27000     [3.17e+00, 3.27e-01, 4.05e+00]    [0.00e+00, 2.96e+00, 4.05e+00]    [3.05e+00, 2.96e+00, 1.62e+00, 1.26e+00]    
28000     [3.09e+00, 9.74e-02, 4.02e+00]    [0.00e+00, 2.97e+00, 4.02e+00]    [3.07e+00, 2.97e+00, 1.60e+00, 1.23e+00]    
29000     [3.03e+00, 2.38e-01, 4.00e+00]    [0.00e+00, 3.06e+00, 4.00e+00]    [3.17e+00, 3.06e+00, 1.60e+00, 1.30e+00]    
30000     [3.02e+00, 4.19e-01, 3.98e+00]    [0.00e+00, 3.07e+00, 3.98e+00]    [3.20e+00, 3.07e+00, 1.61e+00, 1.40e+00]    

Best model at step 28000:
  train loss: 7.21e+00
  test loss: 6.99e+00
  test metric: [3.07e+00, 2.97e+00, 1.60e+00, 1.23e+00]

'train' took 80.020975 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.459771 s

'compile' took 2.281896 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.99e+01, 2.68e+00]    [0.00e+00, 1.01e+02, 2.68e+00]    [1.01e+02, 1.01e+02, 1.66e+00, 1.66e+00]    
1000      [5.96e+01, 6.63e+01, 3.28e+00]    [0.00e+00, 6.10e+01, 3.28e+00]    [6.19e+01, 6.10e+01, 2.86e+01, 2.84e+01]    
2000      [1.20e+01, 1.95e-02, 4.39e+00]    [0.00e+00, 1.38e+01, 4.39e+00]    [1.50e+01, 1.38e+01, 1.29e+01, 1.07e+01]    
3000      [7.19e+00, 1.79e-02, 4.38e+00]    [0.00e+00, 9.18e+00, 4.38e+00]    [9.63e+00, 9.18e+00, 8.21e+00, 7.10e+00]    
4000      [5.44e+00, 2.10e-02, 4.34e+00]    [0.00e+00, 6.91e+00, 4.34e+00]    [7.17e+00, 6.91e+00, 5.33e+00, 4.84e+00]    
5000      [4.42e+00, 1.46e-02, 4.29e+00]    [0.00e+00, 5.14e+00, 4.29e+00]    [5.24e+00, 5.14e+00, 3.38e+00, 3.25e+00]    
6000      [3.64e+00, 1.76e-02, 4.25e+00]    [0.00e+00, 3.71e+00, 4.25e+00]    [3.68e+00, 3.71e+00, 2.24e+00, 2.28e+00]    
7000      [3.23e+00, 1.17e-02, 4.20e+00]    [0.00e+00, 3.07e+00, 4.20e+00]    [3.02e+00, 3.07e+00, 1.55e+00, 1.71e+00]    
8000      [3.10e+00, 1.05e-02, 4.15e+00]    [0.00e+00, 2.96e+00, 4.15e+00]    [2.89e+00, 2.96e+00, 1.50e+00, 1.43e+00]    
9000      [2.99e+00, 9.36e-03, 4.10e+00]    [0.00e+00, 2.83e+00, 4.10e+00]    [2.74e+00, 2.83e+00, 1.55e+00, 1.57e+00]    
10000     [2.97e+00, 6.80e-03, 4.05e+00]    [0.00e+00, 2.84e+00, 4.05e+00]    [2.73e+00, 2.84e+00, 1.58e+00, 1.53e+00]    
11000     [2.89e+00, 5.47e-03, 4.00e+00]    [0.00e+00, 2.79e+00, 4.00e+00]    [2.66e+00, 2.79e+00, 1.56e+00, 1.62e+00]    
12000     [2.87e+00, 8.08e-03, 3.96e+00]    [0.00e+00, 2.79e+00, 3.96e+00]    [2.59e+00, 2.79e+00, 1.61e+00, 1.63e+00]    
13000     [2.86e+00, 1.37e-02, 3.91e+00]    [0.00e+00, 2.81e+00, 3.91e+00]    [2.68e+00, 2.81e+00, 1.58e+00, 1.67e+00]    
14000     [2.81e+00, 1.82e-02, 3.87e+00]    [0.00e+00, 2.83e+00, 3.87e+00]    [2.66e+00, 2.83e+00, 1.54e+00, 1.69e+00]    
15000     [2.78e+00, 8.10e-03, 3.82e+00]    [0.00e+00, 2.83e+00, 3.82e+00]    [2.64e+00, 2.83e+00, 1.55e+00, 1.71e+00]    
16000     [2.76e+00, 9.78e-03, 3.78e+00]    [0.00e+00, 2.84e+00, 3.78e+00]    [2.62e+00, 2.84e+00, 1.52e+00, 1.72e+00]    
17000     [2.77e+00, 5.64e-03, 3.75e+00]    [0.00e+00, 2.88e+00, 3.75e+00]    [2.59e+00, 2.88e+00, 1.51e+00, 1.74e+00]    
18000     [2.80e+00, 2.45e-02, 3.71e+00]    [0.00e+00, 2.95e+00, 3.71e+00]    [2.55e+00, 2.95e+00, 1.51e+00, 1.79e+00]    
19000     [2.73e+00, 1.78e-02, 3.67e+00]    [0.00e+00, 2.92e+00, 3.67e+00]    [2.58e+00, 2.92e+00, 1.51e+00, 1.80e+00]    
20000     [2.70e+00, 1.41e-02, 3.64e+00]    [0.00e+00, 2.88e+00, 3.64e+00]    [2.61e+00, 2.88e+00, 1.51e+00, 1.82e+00]    
21000     [2.69e+00, 7.26e-03, 3.61e+00]    [0.00e+00, 2.90e+00, 3.61e+00]    [2.61e+00, 2.90e+00, 1.51e+00, 1.86e+00]    
22000     [2.72e+00, 3.01e-02, 3.57e+00]    [0.00e+00, 2.86e+00, 3.57e+00]    [2.65e+00, 2.86e+00, 1.55e+00, 1.89e+00]    
23000     [2.67e+00, 2.32e-02, 3.54e+00]    [0.00e+00, 2.87e+00, 3.54e+00]    [2.62e+00, 2.87e+00, 1.52e+00, 1.95e+00]    
24000     [2.66e+00, 1.46e-02, 3.51e+00]    [0.00e+00, 2.94e+00, 3.51e+00]    [2.53e+00, 2.94e+00, 1.56e+00, 1.97e+00]    
25000     [2.65e+00, 5.03e-03, 3.48e+00]    [0.00e+00, 2.94e+00, 3.48e+00]    [2.52e+00, 2.94e+00, 1.58e+00, 1.97e+00]    
26000     [2.63e+00, 7.89e-03, 3.45e+00]    [0.00e+00, 2.95e+00, 3.45e+00]    [2.57e+00, 2.95e+00, 1.57e+00, 2.02e+00]    
27000     [2.63e+00, 1.19e-02, 3.42e+00]    [0.00e+00, 3.01e+00, 3.42e+00]    [2.50e+00, 3.01e+00, 1.59e+00, 2.00e+00]    
28000     [2.59e+00, 2.74e-02, 3.40e+00]    [0.00e+00, 3.00e+00, 3.40e+00]    [2.53e+00, 3.00e+00, 1.58e+00, 2.01e+00]    
29000     [2.59e+00, 1.27e-02, 3.37e+00]    [0.00e+00, 2.95e+00, 3.37e+00]    [2.56e+00, 2.95e+00, 1.61e+00, 2.01e+00]    
30000     [2.59e+00, 2.56e-02, 3.34e+00]    [0.00e+00, 3.03e+00, 3.34e+00]    [2.49e+00, 3.03e+00, 1.61e+00, 2.01e+00]    

Best model at step 30000:
  train loss: 5.96e+00
  test loss: 6.38e+00
  test metric: [2.49e+00, 3.03e+00, 1.61e+00, 2.01e+00]

'train' took 80.479746 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.499661 s

'compile' took 2.229038 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 9.96e+01, 2.78e+00]    [0.00e+00, 9.89e+01, 2.78e+00]    [9.89e+01, 9.89e+01, 2.97e+00, 2.97e+00]    
1000      [6.72e+01, 5.63e+01, 3.29e+00]    [0.00e+00, 6.53e+01, 3.29e+00]    [6.55e+01, 6.53e+01, 2.68e+01, 2.68e+01]    
2000      [2.62e+01, 5.65e+00, 4.39e+00]    [0.00e+00, 2.21e+01, 4.39e+00]    [2.31e+01, 2.21e+01, 2.10e+01, 2.04e+01]    
3000      [1.20e+01, 1.16e-02, 4.62e+00]    [0.00e+00, 1.30e+01, 4.62e+00]    [1.36e+01, 1.30e+01, 9.61e+00, 1.01e+01]    
4000      [8.45e+00, 5.05e-02, 4.58e+00]    [0.00e+00, 9.36e+00, 4.58e+00]    [9.68e+00, 9.36e+00, 8.21e+00, 8.62e+00]    
5000      [6.14e+00, 6.89e-03, 4.54e+00]    [0.00e+00, 6.62e+00, 4.54e+00]    [6.67e+00, 6.62e+00, 5.27e+00, 5.34e+00]    
6000      [5.29e+00, 4.07e-02, 4.48e+00]    [0.00e+00, 5.59e+00, 4.48e+00]    [5.51e+00, 5.59e+00, 3.68e+00, 3.67e+00]    
7000      [4.67e+00, 5.58e-03, 4.43e+00]    [0.00e+00, 4.94e+00, 4.43e+00]    [4.77e+00, 4.94e+00, 3.32e+00, 3.33e+00]    
8000      [4.13e+00, 2.74e-02, 4.40e+00]    [0.00e+00, 4.66e+00, 4.40e+00]    [4.46e+00, 4.66e+00, 3.20e+00, 3.27e+00]    
9000      [3.87e+00, 4.80e-02, 4.36e+00]    [0.00e+00, 4.37e+00, 4.36e+00]    [4.28e+00, 4.37e+00, 3.02e+00, 3.33e+00]    
10000     [3.60e+00, 2.11e-02, 4.32e+00]    [0.00e+00, 4.28e+00, 4.32e+00]    [4.26e+00, 4.28e+00, 2.78e+00, 3.21e+00]    
11000     [3.45e+00, 4.76e-03, 4.28e+00]    [0.00e+00, 4.08e+00, 4.28e+00]    [4.19e+00, 4.08e+00, 2.48e+00, 3.15e+00]    
12000     [3.36e+00, 3.89e-02, 4.24e+00]    [0.00e+00, 3.99e+00, 4.24e+00]    [4.20e+00, 3.99e+00, 2.30e+00, 3.08e+00]    
13000     [3.24e+00, 9.93e-03, 4.21e+00]    [0.00e+00, 3.81e+00, 4.21e+00]    [4.13e+00, 3.81e+00, 2.07e+00, 3.06e+00]    
14000     [3.18e+00, 4.66e-03, 4.17e+00]    [0.00e+00, 3.68e+00, 4.17e+00]    [4.08e+00, 3.68e+00, 1.92e+00, 3.00e+00]    
15000     [3.11e+00, 6.03e-03, 4.14e+00]    [0.00e+00, 3.62e+00, 4.14e+00]    [4.05e+00, 3.62e+00, 1.85e+00, 2.94e+00]    
16000     [3.07e+00, 1.75e-02, 4.11e+00]    [0.00e+00, 3.53e+00, 4.11e+00]    [3.99e+00, 3.53e+00, 1.78e+00, 2.89e+00]    
17000     [3.08e+00, 5.79e-02, 4.08e+00]    [0.00e+00, 3.58e+00, 4.08e+00]    [4.05e+00, 3.58e+00, 1.83e+00, 2.81e+00]    
18000     [3.01e+00, 5.92e-02, 4.05e+00]    [0.00e+00, 3.46e+00, 4.05e+00]    [3.92e+00, 3.46e+00, 1.73e+00, 2.75e+00]    
19000     [2.91e+00, 2.39e-02, 4.02e+00]    [0.00e+00, 3.44e+00, 4.02e+00]    [3.90e+00, 3.44e+00, 1.74e+00, 2.69e+00]    
20000     [2.89e+00, 1.94e-02, 3.99e+00]    [0.00e+00, 3.43e+00, 3.99e+00]    [3.84e+00, 3.43e+00, 1.81e+00, 2.63e+00]    
21000     [2.91e+00, 4.52e-02, 3.96e+00]    [0.00e+00, 3.43e+00, 3.96e+00]    [3.80e+00, 3.43e+00, 1.89e+00, 2.56e+00]    
22000     [2.88e+00, 5.70e-02, 3.93e+00]    [0.00e+00, 3.37e+00, 3.93e+00]    [3.72e+00, 3.37e+00, 1.92e+00, 2.54e+00]    
23000     [2.85e+00, 5.91e-02, 3.90e+00]    [0.00e+00, 3.20e+00, 3.90e+00]    [3.55e+00, 3.20e+00, 1.81e+00, 2.54e+00]    
24000     [2.76e+00, 2.02e-02, 3.88e+00]    [0.00e+00, 3.25e+00, 3.88e+00]    [3.58e+00, 3.25e+00, 1.88e+00, 2.45e+00]    
25000     [2.75e+00, 3.69e-02, 3.85e+00]    [0.00e+00, 3.25e+00, 3.85e+00]    [3.58e+00, 3.25e+00, 1.96e+00, 2.37e+00]    
26000     [2.79e+00, 6.85e-02, 3.83e+00]    [0.00e+00, 3.30e+00, 3.83e+00]    [3.62e+00, 3.30e+00, 2.07e+00, 2.29e+00]    
27000     [2.67e+00, 2.65e-02, 3.81e+00]    [0.00e+00, 3.20e+00, 3.81e+00]    [3.51e+00, 3.20e+00, 1.96e+00, 2.23e+00]    
28000     [2.65e+00, 8.78e-03, 3.78e+00]    [0.00e+00, 3.18e+00, 3.78e+00]    [3.49e+00, 3.18e+00, 2.00e+00, 2.16e+00]    
29000     [2.66e+00, 2.97e-02, 3.76e+00]    [0.00e+00, 3.21e+00, 3.76e+00]    [3.51e+00, 3.21e+00, 2.09e+00, 2.09e+00]    
30000     [2.61e+00, 2.75e-02, 3.73e+00]    [0.00e+00, 3.14e+00, 3.73e+00]    [3.44e+00, 3.14e+00, 2.05e+00, 2.09e+00]    

Best model at step 30000:
  train loss: 6.37e+00
  test loss: 6.87e+00
  test metric: [3.44e+00, 3.14e+00, 2.05e+00, 2.09e+00]

'train' took 81.529939 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.482710 s

'compile' took 2.328772 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.01e+02, 2.73e+00]    [0.00e+00, 1.02e+02, 2.73e+00]    [1.02e+02, 1.02e+02, 3.03e+00, 3.03e+00]    
1000      [7.84e+01, 5.19e+01, 3.00e+00]    [0.00e+00, 7.78e+01, 3.00e+00]    [7.79e+01, 7.78e+01, 2.45e+01, 2.46e+01]    
2000      [4.90e+01, 6.25e+00, 3.66e+00]    [0.00e+00, 5.67e+01, 3.66e+00]    [5.70e+01, 5.67e+01, 3.11e+01, 3.00e+01]    
3000      [3.63e+01, 2.88e+00, 3.81e+00]    [0.00e+00, 4.38e+01, 3.81e+00]    [4.46e+01, 4.38e+01, 2.48e+01, 2.45e+01]    
4000      [2.56e+01, 1.07e-01, 3.94e+00]    [0.00e+00, 3.24e+01, 3.94e+00]    [3.27e+01, 3.24e+01, 1.93e+01, 1.91e+01]    
5000      [1.69e+01, 4.36e-01, 4.01e+00]    [0.00e+00, 2.22e+01, 4.01e+00]    [2.21e+01, 2.22e+01, 1.55e+01, 1.55e+01]    
6000      [9.35e+00, 9.50e-01, 4.07e+00]    [0.00e+00, 1.29e+01, 4.07e+00]    [1.25e+01, 1.29e+01, 1.28e+01, 1.31e+01]    
7000      [6.23e+00, 8.12e-02, 4.04e+00]    [0.00e+00, 9.01e+00, 4.04e+00]    [8.55e+00, 9.01e+00, 1.07e+01, 1.12e+01]    
8000      [5.06e+00, 2.75e-02, 4.01e+00]    [0.00e+00, 7.02e+00, 4.01e+00]    [7.12e+00, 7.02e+00, 9.34e+00, 1.02e+01]    
9000      [4.62e+00, 5.06e-02, 3.95e+00]    [0.00e+00, 6.20e+00, 3.95e+00]    [6.37e+00, 6.20e+00, 8.67e+00, 9.48e+00]    
10000     [4.37e+00, 5.80e-02, 3.91e+00]    [0.00e+00, 5.67e+00, 3.91e+00]    [5.89e+00, 5.67e+00, 8.18e+00, 8.96e+00]    
11000     [4.25e+00, 1.44e-01, 3.86e+00]    [0.00e+00, 5.44e+00, 3.86e+00]    [5.71e+00, 5.44e+00, 7.73e+00, 8.54e+00]    
12000     [4.11e+00, 2.35e-01, 3.83e+00]    [0.00e+00, 5.40e+00, 3.83e+00]    [5.44e+00, 5.40e+00, 7.71e+00, 8.39e+00]    
13000     [3.97e+00, 1.70e-01, 3.80e+00]    [0.00e+00, 5.14e+00, 3.80e+00]    [5.10e+00, 5.14e+00, 7.50e+00, 8.14e+00]    
14000     [3.87e+00, 1.60e-01, 3.77e+00]    [0.00e+00, 5.03e+00, 3.77e+00]    [4.95e+00, 5.03e+00, 7.24e+00, 7.89e+00]    
15000     [3.77e+00, 7.72e-02, 3.73e+00]    [0.00e+00, 4.92e+00, 3.73e+00]    [4.93e+00, 4.92e+00, 6.85e+00, 7.57e+00]    
16000     [3.71e+00, 3.49e-02, 3.70e+00]    [0.00e+00, 5.03e+00, 3.70e+00]    [4.82e+00, 5.03e+00, 6.82e+00, 7.50e+00]    
17000     [3.72e+00, 2.77e-01, 3.67e+00]    [0.00e+00, 5.24e+00, 3.67e+00]    [4.84e+00, 5.24e+00, 6.93e+00, 7.56e+00]    
18000     [3.63e+00, 4.05e-03, 3.64e+00]    [0.00e+00, 5.10e+00, 3.64e+00]    [4.80e+00, 5.10e+00, 6.59e+00, 7.32e+00]    
19000     [3.64e+00, 9.50e-02, 3.62e+00]    [0.00e+00, 5.02e+00, 3.62e+00]    [4.85e+00, 5.02e+00, 6.33e+00, 7.18e+00]    
20000     [3.54e+00, 4.44e-02, 3.59e+00]    [0.00e+00, 5.20e+00, 3.59e+00]    [4.70e+00, 5.20e+00, 6.38e+00, 7.11e+00]    
21000     [3.50e+00, 9.22e-02, 3.57e+00]    [0.00e+00, 5.25e+00, 3.57e+00]    [4.66e+00, 5.25e+00, 6.36e+00, 7.08e+00]    
22000     [3.48e+00, 4.19e-02, 3.54e+00]    [0.00e+00, 5.22e+00, 3.54e+00]    [4.59e+00, 5.22e+00, 6.16e+00, 6.90e+00]    
23000     [3.47e+00, 1.74e-01, 3.52e+00]    [0.00e+00, 5.16e+00, 3.52e+00]    [4.57e+00, 5.16e+00, 5.97e+00, 6.77e+00]    
24000     [3.40e+00, 2.57e-02, 3.50e+00]    [0.00e+00, 5.26e+00, 3.50e+00]    [4.54e+00, 5.26e+00, 6.01e+00, 6.78e+00]    
25000     [3.39e+00, 1.34e-01, 3.48e+00]    [0.00e+00, 5.19e+00, 3.48e+00]    [4.51e+00, 5.19e+00, 5.76e+00, 6.60e+00]    
26000     [3.37e+00, 9.96e-02, 3.46e+00]    [0.00e+00, 5.34e+00, 3.46e+00]    [4.47e+00, 5.34e+00, 5.87e+00, 6.66e+00]    
27000     [3.31e+00, 2.28e-02, 3.44e+00]    [0.00e+00, 5.30e+00, 3.44e+00]    [4.44e+00, 5.30e+00, 5.65e+00, 6.50e+00]    
28000     [3.28e+00, 3.95e-02, 3.42e+00]    [0.00e+00, 5.33e+00, 3.42e+00]    [4.40e+00, 5.33e+00, 5.62e+00, 6.46e+00]    
29000     [3.25e+00, 1.08e-01, 3.40e+00]    [0.00e+00, 5.31e+00, 3.40e+00]    [4.38e+00, 5.31e+00, 5.59e+00, 6.45e+00]    
30000     [3.31e+00, 1.62e-01, 3.38e+00]    [0.00e+00, 5.18e+00, 3.38e+00]    [4.31e+00, 5.18e+00, 5.37e+00, 6.26e+00]    

Best model at step 28000:
  train loss: 6.74e+00
  test loss: 8.75e+00
  test metric: [4.40e+00, 5.33e+00, 5.62e+00, 6.46e+00]

'train' took 81.351414 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.487694 s

'compile' took 2.248983 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 9.97e+01, 2.74e+00]    [0.00e+00, 1.02e+02, 2.74e+00]    [1.02e+02, 1.02e+02, 2.85e+00, 2.85e+00]    
1000      [5.42e+01, 3.80e+01, 3.84e+00]    [0.00e+00, 4.71e+01, 3.84e+00]    [4.79e+01, 4.71e+01, 3.15e+01, 3.18e+01]    
2000      [2.29e+01, 2.81e-02, 4.55e+00]    [0.00e+00, 2.98e+01, 4.55e+00]    [3.03e+01, 2.98e+01, 2.45e+01, 2.43e+01]    
3000      [1.23e+01, 1.54e-02, 4.63e+00]    [0.00e+00, 1.70e+01, 4.63e+00]    [1.72e+01, 1.70e+01, 1.40e+01, 1.39e+01]    
4000      [5.72e+00, 1.37e-02, 4.63e+00]    [0.00e+00, 6.89e+00, 4.63e+00]    [6.87e+00, 6.89e+00, 6.84e+00, 6.86e+00]    
5000      [4.05e+00, 8.92e-03, 4.53e+00]    [0.00e+00, 3.88e+00, 4.53e+00]    [3.86e+00, 3.88e+00, 3.79e+00, 3.86e+00]    
6000      [3.58e+00, 1.20e-02, 4.44e+00]    [0.00e+00, 3.81e+00, 4.44e+00]    [3.75e+00, 3.81e+00, 2.56e+00, 2.64e+00]    
7000      [3.38e+00, 1.55e-02, 4.36e+00]    [0.00e+00, 3.63e+00, 4.36e+00]    [3.57e+00, 3.63e+00, 1.97e+00, 2.04e+00]    
8000      [3.24e+00, 1.48e-02, 4.29e+00]    [0.00e+00, 3.59e+00, 4.29e+00]    [3.53e+00, 3.59e+00, 1.77e+00, 1.84e+00]    
9000      [3.16e+00, 1.95e-02, 4.23e+00]    [0.00e+00, 3.55e+00, 4.23e+00]    [3.51e+00, 3.55e+00, 1.68e+00, 1.73e+00]    
10000     [3.08e+00, 2.10e-02, 4.17e+00]    [0.00e+00, 3.48e+00, 4.17e+00]    [3.45e+00, 3.48e+00, 1.60e+00, 1.64e+00]    
11000     [3.02e+00, 1.61e-02, 4.11e+00]    [0.00e+00, 3.50e+00, 4.11e+00]    [3.47e+00, 3.50e+00, 1.59e+00, 1.63e+00]    
12000     [2.97e+00, 2.85e-02, 4.06e+00]    [0.00e+00, 3.48e+00, 4.06e+00]    [3.46e+00, 3.48e+00, 1.61e+00, 1.63e+00]    
13000     [2.92e+00, 1.72e-02, 4.01e+00]    [0.00e+00, 3.49e+00, 4.01e+00]    [3.48e+00, 3.49e+00, 1.59e+00, 1.61e+00]    
14000     [2.85e+00, 1.77e-02, 3.96e+00]    [0.00e+00, 3.45e+00, 3.96e+00]    [3.44e+00, 3.45e+00, 1.53e+00, 1.55e+00]    
15000     [2.82e+00, 1.00e-02, 3.92e+00]    [0.00e+00, 3.35e+00, 3.92e+00]    [3.34e+00, 3.35e+00, 1.40e+00, 1.41e+00]    
16000     [2.79e+00, 8.00e-03, 3.88e+00]    [0.00e+00, 3.45e+00, 3.88e+00]    [3.44e+00, 3.45e+00, 1.46e+00, 1.47e+00]    
17000     [2.73e+00, 5.27e-03, 3.84e+00]    [0.00e+00, 3.39e+00, 3.84e+00]    [3.39e+00, 3.39e+00, 1.39e+00, 1.40e+00]    
18000     [2.70e+00, 5.77e-03, 3.80e+00]    [0.00e+00, 3.38e+00, 3.80e+00]    [3.38e+00, 3.38e+00, 1.37e+00, 1.38e+00]    
19000     [2.68e+00, 2.56e-02, 3.77e+00]    [0.00e+00, 3.34e+00, 3.77e+00]    [3.33e+00, 3.34e+00, 1.37e+00, 1.37e+00]    
20000     [2.65e+00, 9.56e-03, 3.74e+00]    [0.00e+00, 3.32e+00, 3.74e+00]    [3.32e+00, 3.32e+00, 1.34e+00, 1.34e+00]    
21000     [2.62e+00, 2.05e-02, 3.70e+00]    [0.00e+00, 3.30e+00, 3.70e+00]    [3.31e+00, 3.30e+00, 1.27e+00, 1.26e+00]    
22000     [2.60e+00, 2.64e-02, 3.67e+00]    [0.00e+00, 3.29e+00, 3.67e+00]    [3.29e+00, 3.29e+00, 1.25e+00, 1.24e+00]    
23000     [2.56e+00, 1.43e-02, 3.65e+00]    [0.00e+00, 3.32e+00, 3.65e+00]    [3.33e+00, 3.32e+00, 1.36e+00, 1.35e+00]    
24000     [2.54e+00, 2.21e-02, 3.62e+00]    [0.00e+00, 3.32e+00, 3.62e+00]    [3.32e+00, 3.32e+00, 1.37e+00, 1.35e+00]    
25000     [2.52e+00, 1.29e-02, 3.59e+00]    [0.00e+00, 3.28e+00, 3.59e+00]    [3.29e+00, 3.28e+00, 1.33e+00, 1.30e+00]    
26000     [2.53e+00, 2.25e-02, 3.57e+00]    [0.00e+00, 3.34e+00, 3.57e+00]    [3.36e+00, 3.34e+00, 1.40e+00, 1.37e+00]    
27000     [2.47e+00, 2.78e-02, 3.54e+00]    [0.00e+00, 3.25e+00, 3.54e+00]    [3.27e+00, 3.25e+00, 1.32e+00, 1.28e+00]    
28000     [2.44e+00, 1.19e-02, 3.52e+00]    [0.00e+00, 3.24e+00, 3.52e+00]    [3.26e+00, 3.24e+00, 1.36e+00, 1.31e+00]    
29000     [2.44e+00, 1.03e-02, 3.49e+00]    [0.00e+00, 3.20e+00, 3.49e+00]    [3.22e+00, 3.20e+00, 1.33e+00, 1.27e+00]    
30000     [2.50e+00, 1.38e-02, 3.47e+00]    [0.00e+00, 3.32e+00, 3.47e+00]    [3.35e+00, 3.32e+00, 1.56e+00, 1.49e+00]    

Best model at step 29000:
  train loss: 5.94e+00
  test loss: 6.69e+00
  test metric: [3.22e+00, 3.20e+00, 1.33e+00, 1.27e+00]

'train' took 81.331468 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.488695 s

'compile' took 2.231033 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 1.00e+02, 2.71e+00]    [0.00e+00, 9.94e+01, 2.71e+00]    [9.94e+01, 9.94e+01, 1.28e+00, 1.28e+00]    
1000      [5.54e+01, 5.58e+01, 3.31e+00]    [0.00e+00, 5.65e+01, 3.31e+00]    [5.68e+01, 5.65e+01, 3.42e+01, 3.41e+01]    
2000      [1.90e+01, 6.14e+00, 4.03e+00]    [0.00e+00, 2.19e+01, 4.03e+00]    [2.26e+01, 2.19e+01, 2.05e+01, 1.98e+01]    
3000      [8.54e+00, 1.75e-02, 4.13e+00]    [0.00e+00, 9.86e+00, 4.13e+00]    [1.02e+01, 9.86e+00, 1.19e+01, 1.16e+01]    
4000      [6.73e+00, 2.71e-02, 4.09e+00]    [0.00e+00, 7.96e+00, 4.09e+00]    [7.87e+00, 7.96e+00, 9.92e+00, 1.01e+01]    
5000      [5.27e+00, 2.55e-02, 4.06e+00]    [0.00e+00, 6.64e+00, 4.06e+00]    [6.39e+00, 6.64e+00, 7.25e+00, 8.17e+00]    
6000      [4.23e+00, 2.53e-02, 4.03e+00]    [0.00e+00, 5.48e+00, 4.03e+00]    [5.16e+00, 5.48e+00, 4.48e+00, 6.16e+00]    
7000      [3.41e+00, 1.60e-02, 4.01e+00]    [0.00e+00, 4.54e+00, 4.01e+00]    [4.07e+00, 4.54e+00, 2.21e+00, 4.20e+00]    
8000      [2.91e+00, 2.61e-02, 3.98e+00]    [0.00e+00, 4.05e+00, 3.98e+00]    [3.29e+00, 4.05e+00, 1.43e+00, 2.98e+00]    
9000      [2.73e+00, 1.96e-02, 3.94e+00]    [0.00e+00, 3.75e+00, 3.94e+00]    [2.88e+00, 3.75e+00, 1.58e+00, 2.42e+00]    
10000     [2.66e+00, 1.60e-02, 3.89e+00]    [0.00e+00, 3.54e+00, 3.89e+00]    [2.74e+00, 3.54e+00, 1.66e+00, 2.30e+00]    
11000     [2.63e+00, 5.83e-03, 3.83e+00]    [0.00e+00, 3.49e+00, 3.83e+00]    [2.65e+00, 3.49e+00, 1.65e+00, 2.22e+00]    
12000     [2.60e+00, 3.93e-03, 3.78e+00]    [0.00e+00, 3.43e+00, 3.78e+00]    [2.69e+00, 3.43e+00, 1.63e+00, 2.24e+00]    
13000     [2.57e+00, 2.00e-02, 3.74e+00]    [0.00e+00, 3.43e+00, 3.74e+00]    [2.73e+00, 3.43e+00, 1.58e+00, 2.27e+00]    
14000     [2.54e+00, 6.36e-03, 3.70e+00]    [0.00e+00, 3.41e+00, 3.70e+00]    [2.71e+00, 3.41e+00, 1.57e+00, 2.24e+00]    
15000     [2.55e+00, 2.28e-02, 3.66e+00]    [0.00e+00, 3.30e+00, 3.66e+00]    [2.77e+00, 3.30e+00, 1.58e+00, 2.30e+00]    
16000     [2.52e+00, 1.48e-02, 3.62e+00]    [0.00e+00, 3.28e+00, 3.62e+00]    [2.75e+00, 3.28e+00, 1.56e+00, 2.28e+00]    
17000     [2.53e+00, 6.59e-03, 3.58e+00]    [0.00e+00, 3.34e+00, 3.58e+00]    [2.70e+00, 3.34e+00, 1.50e+00, 2.21e+00]    
18000     [2.55e+00, 1.17e-02, 3.55e+00]    [0.00e+00, 3.34e+00, 3.55e+00]    [2.69e+00, 3.34e+00, 1.47e+00, 2.20e+00]    
19000     [2.49e+00, 5.16e-03, 3.52e+00]    [0.00e+00, 3.25e+00, 3.52e+00]    [2.76e+00, 3.25e+00, 1.46e+00, 2.26e+00]    
20000     [2.46e+00, 6.00e-03, 3.49e+00]    [0.00e+00, 3.16e+00, 3.49e+00]    [2.81e+00, 3.16e+00, 1.48e+00, 2.28e+00]    
21000     [2.51e+00, 2.18e-02, 3.45e+00]    [0.00e+00, 3.08e+00, 3.45e+00]    [2.87e+00, 3.08e+00, 1.58e+00, 2.27e+00]    
22000     [2.44e+00, 1.80e-02, 3.42e+00]    [0.00e+00, 3.14e+00, 3.42e+00]    [2.84e+00, 3.14e+00, 1.45e+00, 2.29e+00]    
23000     [2.44e+00, 6.09e-03, 3.40e+00]    [0.00e+00, 3.11e+00, 3.40e+00]    [2.87e+00, 3.11e+00, 1.47e+00, 2.29e+00]    
24000     [2.43e+00, 1.61e-02, 3.37e+00]    [0.00e+00, 3.14e+00, 3.37e+00]    [2.82e+00, 3.14e+00, 1.47e+00, 2.24e+00]    
25000     [2.44e+00, 8.87e-03, 3.34e+00]    [0.00e+00, 3.09e+00, 3.34e+00]    [2.88e+00, 3.09e+00, 1.49e+00, 2.28e+00]    
26000     [2.41e+00, 4.78e-03, 3.31e+00]    [0.00e+00, 3.12e+00, 3.31e+00]    [2.87e+00, 3.12e+00, 1.45e+00, 2.28e+00]    
27000     [2.41e+00, 1.75e-02, 3.29e+00]    [0.00e+00, 3.11e+00, 3.29e+00]    [2.87e+00, 3.11e+00, 1.47e+00, 2.26e+00]    
28000     [2.46e+00, 8.23e-03, 3.26e+00]    [0.00e+00, 3.20e+00, 3.26e+00]    [2.78e+00, 3.20e+00, 1.39e+00, 2.26e+00]    
29000     [2.38e+00, 1.52e-02, 3.24e+00]    [0.00e+00, 3.12e+00, 3.24e+00]    [2.87e+00, 3.12e+00, 1.45e+00, 2.28e+00]    
30000     [2.41e+00, 1.28e-02, 3.22e+00]    [0.00e+00, 3.13e+00, 3.22e+00]    [2.88e+00, 3.13e+00, 1.44e+00, 2.30e+00]    

Best model at step 29000:
  train loss: 5.64e+00
  test loss: 6.36e+00
  test metric: [2.87e+00, 3.12e+00, 1.45e+00, 2.28e+00]

'train' took 81.049222 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.514622 s

'compile' took 2.270925 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.96e+01, 2.76e+00]    [0.00e+00, 1.00e+02, 2.76e+00]    [1.00e+02, 1.00e+02, 1.07e+00, 1.07e+00]    
1000      [8.97e+01, 5.43e+00, 2.78e+00]    [0.00e+00, 9.62e+01, 2.78e+00]    [9.63e+01, 9.62e+01, 7.38e+00, 7.41e+00]    
2000      [8.00e+01, 5.19e+00, 2.80e+00]    [0.00e+00, 9.03e+01, 2.80e+00]    [9.03e+01, 9.03e+01, 7.71e+00, 7.75e+00]    
3000      [7.68e+01, 4.25e+00, 2.81e+00]    [0.00e+00, 8.69e+01, 2.81e+00]    [8.70e+01, 8.69e+01, 7.47e+00, 7.51e+00]    
4000      [7.31e+01, 2.82e+00, 2.86e+00]    [0.00e+00, 8.24e+01, 2.86e+00]    [8.25e+01, 8.24e+01, 7.33e+00, 7.38e+00]    
5000      [6.76e+01, 8.55e-01, 2.95e+00]    [0.00e+00, 7.58e+01, 2.95e+00]    [7.60e+01, 7.58e+01, 7.18e+00, 7.23e+00]    
6000      [6.02e+01, 2.61e-01, 3.08e+00]    [0.00e+00, 6.75e+01, 3.08e+00]    [6.78e+01, 6.75e+01, 6.82e+00, 6.87e+00]    
7000      [5.15e+01, 1.89e-01, 3.25e+00]    [0.00e+00, 5.80e+01, 3.25e+00]    [5.83e+01, 5.80e+01, 6.52e+00, 6.57e+00]    
8000      [4.05e+01, 1.58e-01, 3.50e+00]    [0.00e+00, 4.57e+01, 3.50e+00]    [4.61e+01, 4.57e+01, 6.75e+00, 6.80e+00]    
9000      [2.64e+01, 5.42e-02, 3.83e+00]    [0.00e+00, 2.97e+01, 3.83e+00]    [3.02e+01, 2.97e+01, 8.95e+00, 9.02e+00]    
10000     [1.12e+01, 7.76e-01, 4.19e+00]    [0.00e+00, 1.17e+01, 4.19e+00]    [1.21e+01, 1.17e+01, 1.23e+01, 1.21e+01]    
11000     [7.73e+00, 3.35e-01, 4.27e+00]    [0.00e+00, 9.53e+00, 4.27e+00]    [9.40e+00, 9.53e+00, 8.82e+00, 8.42e+00]    
12000     [6.78e+00, 2.21e-01, 4.25e+00]    [0.00e+00, 8.17e+00, 4.25e+00]    [8.05e+00, 8.17e+00, 7.00e+00, 6.68e+00]    
13000     [6.18e+00, 2.21e-01, 4.21e+00]    [0.00e+00, 6.98e+00, 4.21e+00]    [6.94e+00, 6.98e+00, 6.06e+00, 5.88e+00]    
14000     [5.84e+00, 9.00e-02, 4.16e+00]    [0.00e+00, 6.24e+00, 4.16e+00]    [6.25e+00, 6.24e+00, 5.51e+00, 5.43e+00]    
15000     [5.53e+00, 4.42e-01, 4.12e+00]    [0.00e+00, 5.54e+00, 4.12e+00]    [5.56e+00, 5.54e+00, 4.97e+00, 4.94e+00]    
16000     [5.28e+00, 4.84e-01, 4.08e+00]    [0.00e+00, 5.02e+00, 4.08e+00]    [5.05e+00, 5.02e+00, 4.61e+00, 4.62e+00]    
17000     [5.09e+00, 3.30e-01, 4.05e+00]    [0.00e+00, 4.83e+00, 4.05e+00]    [4.76e+00, 4.83e+00, 4.29e+00, 4.23e+00]    
18000     [4.89e+00, 3.56e-01, 4.02e+00]    [0.00e+00, 4.72e+00, 4.02e+00]    [4.65e+00, 4.72e+00, 3.90e+00, 3.91e+00]    
19000     [4.69e+00, 3.53e-01, 3.99e+00]    [0.00e+00, 4.49e+00, 3.99e+00]    [4.42e+00, 4.49e+00, 3.56e+00, 3.62e+00]    
20000     [4.52e+00, 1.67e-01, 3.96e+00]    [0.00e+00, 4.30e+00, 3.96e+00]    [4.25e+00, 4.30e+00, 3.35e+00, 3.47e+00]    
21000     [4.47e+00, 1.09e+00, 3.93e+00]    [0.00e+00, 4.22e+00, 3.93e+00]    [4.16e+00, 4.22e+00, 3.25e+00, 3.41e+00]    
22000     [4.33e+00, 2.31e-01, 3.91e+00]    [0.00e+00, 4.08e+00, 3.91e+00]    [4.02e+00, 4.08e+00, 3.09e+00, 3.27e+00]    
23000     [4.26e+00, 2.09e-01, 3.89e+00]    [0.00e+00, 3.98e+00, 3.89e+00]    [3.91e+00, 3.98e+00, 3.00e+00, 3.20e+00]    
24000     [4.17e+00, 1.33e-01, 3.86e+00]    [0.00e+00, 3.88e+00, 3.86e+00]    [3.79e+00, 3.88e+00, 2.87e+00, 3.09e+00]    
25000     [4.09e+00, 4.04e-02, 3.85e+00]    [0.00e+00, 3.80e+00, 3.85e+00]    [3.65e+00, 3.80e+00, 2.78e+00, 2.96e+00]    
26000     [4.04e+00, 3.17e-01, 3.83e+00]    [0.00e+00, 3.81e+00, 3.83e+00]    [3.56e+00, 3.81e+00, 2.69e+00, 2.79e+00]    
27000     [3.96e+00, 8.28e-02, 3.81e+00]    [0.00e+00, 3.76e+00, 3.81e+00]    [3.50e+00, 3.76e+00, 2.60e+00, 2.71e+00]    
28000     [3.89e+00, 9.42e-02, 3.80e+00]    [0.00e+00, 3.73e+00, 3.80e+00]    [3.45e+00, 3.73e+00, 2.52e+00, 2.63e+00]    
29000     [3.87e+00, 9.39e-01, 3.78e+00]    [0.00e+00, 3.70e+00, 3.78e+00]    [3.41e+00, 3.70e+00, 2.42e+00, 2.57e+00]    
30000     [3.78e+00, 1.59e-01, 3.76e+00]    [0.00e+00, 3.65e+00, 3.76e+00]    [3.35e+00, 3.65e+00, 2.31e+00, 2.48e+00]    

Best model at step 30000:
  train loss: 7.70e+00
  test loss: 7.42e+00
  test metric: [3.35e+00, 3.65e+00, 2.31e+00, 2.48e+00]

'train' took 79.981082 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.443813 s

'compile' took 2.098389 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.85e+01, 9.69e+01, 2.74e+00]    [0.00e+00, 9.80e+01, 2.74e+00]    [9.80e+01, 9.80e+01, 4.80e+00, 4.80e+00]    
1000      [8.11e+01, 5.24e+01, 2.84e+00]    [0.00e+00, 8.04e+01, 2.84e+00]    [8.05e+01, 8.04e+01, 3.07e+01, 3.07e+01]    
2000      [5.80e+01, 1.74e+01, 3.49e+00]    [0.00e+00, 6.28e+01, 3.49e+00]    [6.31e+01, 6.28e+01, 3.57e+01, 3.58e+01]    
3000      [4.81e+01, 1.07e+01, 3.55e+00]    [0.00e+00, 5.37e+01, 3.55e+00]    [5.39e+01, 5.37e+01, 3.15e+01, 3.12e+01]    
4000      [3.76e+01, 6.35e+00, 3.62e+00]    [0.00e+00, 4.26e+01, 3.62e+00]    [4.27e+01, 4.26e+01, 2.66e+01, 2.64e+01]    
5000      [2.07e+01, 4.51e-01, 3.82e+00]    [0.00e+00, 2.40e+01, 3.82e+00]    [2.43e+01, 2.40e+01, 1.84e+01, 1.82e+01]    
6000      [1.07e+01, 2.22e+00, 3.90e+00]    [0.00e+00, 1.15e+01, 3.90e+00]    [1.16e+01, 1.15e+01, 9.59e+00, 9.36e+00]    
7000      [5.31e+00, 2.52e+00, 3.91e+00]    [0.00e+00, 4.62e+00, 3.91e+00]    [4.57e+00, 4.62e+00, 3.47e+00, 3.43e+00]    
8000      [4.42e+00, 1.68e+00, 3.85e+00]    [0.00e+00, 4.90e+00, 3.85e+00]    [4.84e+00, 4.90e+00, 3.33e+00, 3.33e+00]    
9000      [4.36e+00, 1.22e+00, 3.80e+00]    [0.00e+00, 4.93e+00, 3.80e+00]    [4.94e+00, 4.93e+00, 3.41e+00, 3.41e+00]    
10000     [4.31e+00, 8.49e-01, 3.77e+00]    [0.00e+00, 4.44e+00, 3.77e+00]    [4.53e+00, 4.44e+00, 3.12e+00, 3.10e+00]    
11000     [4.24e+00, 7.27e-02, 3.74e+00]    [0.00e+00, 4.61e+00, 3.74e+00]    [4.77e+00, 4.61e+00, 2.98e+00, 2.89e+00]    
12000     [3.98e+00, 8.10e-02, 3.72e+00]    [0.00e+00, 4.52e+00, 3.72e+00]    [4.73e+00, 4.52e+00, 2.87e+00, 2.63e+00]    
13000     [3.87e+00, 1.06e-01, 3.68e+00]    [0.00e+00, 4.32e+00, 3.68e+00]    [4.55e+00, 4.32e+00, 2.77e+00, 2.42e+00]    
14000     [3.73e+00, 2.07e-01, 3.64e+00]    [0.00e+00, 3.90e+00, 3.64e+00]    [4.27e+00, 3.90e+00, 2.39e+00, 2.20e+00]    
15000     [3.57e+00, 1.44e-01, 3.62e+00]    [0.00e+00, 4.01e+00, 3.62e+00]    [4.43e+00, 4.01e+00, 2.19e+00, 1.89e+00]    
16000     [3.49e+00, 1.06e-01, 3.59e+00]    [0.00e+00, 4.20e+00, 3.59e+00]    [4.63e+00, 4.20e+00, 2.17e+00, 1.74e+00]    
17000     [3.41e+00, 9.68e-02, 3.56e+00]    [0.00e+00, 3.98e+00, 3.56e+00]    [4.41e+00, 3.98e+00, 2.01e+00, 1.57e+00]    
18000     [3.40e+00, 3.73e-01, 3.53e+00]    [0.00e+00, 3.72e+00, 3.53e+00]    [4.14e+00, 3.72e+00, 1.92e+00, 1.51e+00]    
19000     [3.30e+00, 2.65e-02, 3.50e+00]    [0.00e+00, 3.95e+00, 3.50e+00]    [4.37e+00, 3.95e+00, 1.94e+00, 1.40e+00]    
20000     [3.27e+00, 5.02e-02, 3.48e+00]    [0.00e+00, 3.94e+00, 3.48e+00]    [4.36e+00, 3.94e+00, 1.94e+00, 1.38e+00]    
21000     [3.24e+00, 8.85e-02, 3.46e+00]    [0.00e+00, 3.94e+00, 3.46e+00]    [4.35e+00, 3.94e+00, 1.98e+00, 1.40e+00]    
22000     [3.21e+00, 3.14e-02, 3.43e+00]    [0.00e+00, 3.89e+00, 3.43e+00]    [4.29e+00, 3.89e+00, 1.99e+00, 1.40e+00]    
23000     [3.15e+00, 2.47e-02, 3.41e+00]    [0.00e+00, 3.84e+00, 3.41e+00]    [4.23e+00, 3.84e+00, 1.99e+00, 1.41e+00]    
24000     [3.14e+00, 7.22e-02, 3.39e+00]    [0.00e+00, 3.72e+00, 3.39e+00]    [4.10e+00, 3.72e+00, 1.95e+00, 1.41e+00]    
25000     [3.09e+00, 2.74e-02, 3.37e+00]    [0.00e+00, 3.81e+00, 3.37e+00]    [4.17e+00, 3.81e+00, 2.01e+00, 1.47e+00]    
26000     [3.12e+00, 7.91e-02, 3.35e+00]    [0.00e+00, 3.70e+00, 3.35e+00]    [4.04e+00, 3.70e+00, 1.95e+00, 1.45e+00]    
27000     [3.07e+00, 1.33e-01, 3.33e+00]    [0.00e+00, 3.76e+00, 3.33e+00]    [4.09e+00, 3.76e+00, 1.97e+00, 1.49e+00]    
28000     [3.06e+00, 3.88e-02, 3.31e+00]    [0.00e+00, 3.90e+00, 3.31e+00]    [4.21e+00, 3.90e+00, 2.05e+00, 1.58e+00]    
29000     [3.08e+00, 1.06e-01, 3.29e+00]    [0.00e+00, 3.97e+00, 3.29e+00]    [4.27e+00, 3.97e+00, 2.11e+00, 1.66e+00]    
30000     [3.02e+00, 1.49e-01, 3.27e+00]    [0.00e+00, 3.78e+00, 3.27e+00]    [4.06e+00, 3.78e+00, 1.98e+00, 1.57e+00]    

Best model at step 28000:
  train loss: 6.41e+00
  test loss: 7.20e+00
  test metric: [4.21e+00, 3.90e+00, 2.05e+00, 1.58e+00]

'train' took 79.874363 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.484703 s

'compile' took 2.190142 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 1.01e+02, 2.76e+00]    [0.00e+00, 1.01e+02, 2.76e+00]    [1.01e+02, 1.01e+02, 1.94e+00, 1.94e+00]    
1000      [8.23e+01, 3.97e+01, 2.78e+00]    [0.00e+00, 9.15e+01, 2.78e+00]    [9.15e+01, 9.15e+01, 1.55e+01, 1.55e+01]    
2000      [7.66e+01, 1.11e-01, 3.27e+00]    [0.00e+00, 7.67e+01, 3.27e+00]    [7.67e+01, 7.67e+01, 2.18e+01, 2.17e+01]    
3000      [6.91e+01, 5.60e-02, 3.35e+00]    [0.00e+00, 7.11e+01, 3.35e+00]    [7.11e+01, 7.11e+01, 2.28e+01, 2.27e+01]    
4000      [6.27e+01, 4.67e-02, 3.44e+00]    [0.00e+00, 6.52e+01, 3.44e+00]    [6.52e+01, 6.52e+01, 2.34e+01, 2.33e+01]    
5000      [5.54e+01, 6.39e-02, 3.57e+00]    [0.00e+00, 5.87e+01, 3.57e+00]    [5.88e+01, 5.87e+01, 2.43e+01, 2.42e+01]    
6000      [5.17e+01, 1.08e-01, 3.59e+00]    [0.00e+00, 5.44e+01, 3.59e+00]    [5.44e+01, 5.44e+01, 2.31e+01, 2.31e+01]    
7000      [4.80e+01, 1.18e-01, 3.62e+00]    [0.00e+00, 5.01e+01, 3.62e+00]    [5.01e+01, 5.01e+01, 2.17e+01, 2.17e+01]    
8000      [4.37e+01, 4.52e-02, 3.70e+00]    [0.00e+00, 4.50e+01, 3.70e+00]    [4.50e+01, 4.50e+01, 2.03e+01, 2.03e+01]    
9000      [3.85e+01, 3.86e-02, 3.81e+00]    [0.00e+00, 3.90e+01, 3.81e+00]    [3.90e+01, 3.90e+01, 1.90e+01, 1.90e+01]    
10000     [3.28e+01, 3.23e-01, 3.94e+00]    [0.00e+00, 3.36e+01, 3.94e+00]    [3.36e+01, 3.36e+01, 1.62e+01, 1.62e+01]    
11000     [2.66e+01, 3.25e-01, 4.09e+00]    [0.00e+00, 2.78e+01, 4.09e+00]    [2.77e+01, 2.78e+01, 1.64e+01, 1.64e+01]    
12000     [2.02e+01, 1.36e-01, 4.24e+00]    [0.00e+00, 2.12e+01, 4.24e+00]    [2.12e+01, 2.12e+01, 1.90e+01, 1.90e+01]    
13000     [1.36e+01, 7.29e-02, 4.40e+00]    [0.00e+00, 1.49e+01, 4.40e+00]    [1.48e+01, 1.49e+01, 2.27e+01, 2.27e+01]    
14000     [1.14e+01, 1.89e-01, 4.43e+00]    [0.00e+00, 1.23e+01, 4.43e+00]    [1.23e+01, 1.23e+01, 2.02e+01, 2.02e+01]    
15000     [1.06e+01, 1.03e-01, 4.39e+00]    [0.00e+00, 1.09e+01, 4.39e+00]    [1.08e+01, 1.09e+01, 1.80e+01, 1.81e+01]    
16000     [9.83e+00, 2.72e-01, 4.36e+00]    [0.00e+00, 9.83e+00, 4.36e+00]    [9.81e+00, 9.83e+00, 1.64e+01, 1.65e+01]    
17000     [9.13e+00, 5.58e-02, 4.33e+00]    [0.00e+00, 9.03e+00, 4.33e+00]    [9.00e+00, 9.03e+00, 1.46e+01, 1.46e+01]    
18000     [8.62e+00, 3.73e-01, 4.29e+00]    [0.00e+00, 8.49e+00, 4.29e+00]    [8.45e+00, 8.49e+00, 1.34e+01, 1.35e+01]    
19000     [8.05e+00, 8.07e-02, 4.24e+00]    [0.00e+00, 7.97e+00, 4.24e+00]    [7.92e+00, 7.97e+00, 1.27e+01, 1.28e+01]    
20000     [7.62e+00, 9.92e-02, 4.21e+00]    [0.00e+00, 7.49e+00, 4.21e+00]    [7.43e+00, 7.49e+00, 1.17e+01, 1.18e+01]    
21000     [7.19e+00, 2.05e-01, 4.17e+00]    [0.00e+00, 7.02e+00, 4.17e+00]    [6.93e+00, 7.02e+00, 1.05e+01, 1.06e+01]    
22000     [6.97e+00, 2.95e-01, 4.14e+00]    [0.00e+00, 6.76e+00, 4.14e+00]    [6.66e+00, 6.76e+00, 9.72e+00, 9.83e+00]    
23000     [6.67e+00, 1.92e-01, 4.11e+00]    [0.00e+00, 6.45e+00, 4.11e+00]    [6.34e+00, 6.45e+00, 8.68e+00, 8.79e+00]    
24000     [6.45e+00, 5.43e-01, 4.08e+00]    [0.00e+00, 6.23e+00, 4.08e+00]    [6.12e+00, 6.23e+00, 7.95e+00, 8.05e+00]    
25000     [6.18e+00, 8.82e-02, 4.05e+00]    [0.00e+00, 5.92e+00, 4.05e+00]    [5.81e+00, 5.92e+00, 6.96e+00, 7.06e+00]    
26000     [5.90e+00, 1.41e-01, 4.03e+00]    [0.00e+00, 5.63e+00, 4.03e+00]    [5.52e+00, 5.63e+00, 5.89e+00, 5.99e+00]    
27000     [5.69e+00, 2.52e-01, 4.01e+00]    [0.00e+00, 5.40e+00, 4.01e+00]    [5.29e+00, 5.40e+00, 5.03e+00, 5.12e+00]    
28000     [5.40e+00, 8.56e-02, 4.00e+00]    [0.00e+00, 5.09e+00, 4.00e+00]    [4.98e+00, 5.09e+00, 4.08e+00, 4.16e+00]    
29000     [5.20e+00, 1.43e-01, 3.99e+00]    [0.00e+00, 4.86e+00, 3.99e+00]    [4.75e+00, 4.86e+00, 3.33e+00, 3.41e+00]    
30000     [4.99e+00, 2.99e-01, 3.98e+00]    [0.00e+00, 4.66e+00, 3.98e+00]    [4.55e+00, 4.66e+00, 2.72e+00, 2.79e+00]    

Best model at step 30000:
  train loss: 9.27e+00
  test loss: 8.64e+00
  test metric: [4.55e+00, 4.66e+00, 2.72e+00, 2.79e+00]

'train' took 81.761318 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.505647 s

'compile' took 2.211086 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.82e+01, 9.81e+01, 2.79e+00]    [0.00e+00, 9.86e+01, 2.79e+00]    [9.86e+01, 9.86e+01, 2.85e+00, 2.85e+00]    
1000      [8.84e+01, 4.91e+01, 2.82e+00]    [0.00e+00, 8.90e+01, 2.82e+00]    [8.89e+01, 8.90e+01, 2.94e+01, 2.99e+01]    
2000      [7.72e+01, 3.04e+01, 3.32e+00]    [0.00e+00, 8.03e+01, 3.32e+00]    [8.04e+01, 8.03e+01, 3.90e+01, 4.09e+01]    
3000      [5.00e+01, 2.30e+01, 3.74e+00]    [0.00e+00, 4.69e+01, 3.74e+00]    [4.80e+01, 4.69e+01, 2.51e+01, 2.67e+01]    
4000      [2.06e+01, 9.30e+00, 4.27e+00]    [0.00e+00, 1.84e+01, 4.27e+00]    [2.06e+01, 1.84e+01, 1.48e+01, 1.51e+01]    
5000      [7.43e+00, 1.36e-01, 4.47e+00]    [0.00e+00, 1.24e+01, 4.47e+00]    [8.52e+00, 1.24e+01, 9.30e+00, 7.74e+00]    
6000      [5.94e+00, 1.86e-01, 4.41e+00]    [0.00e+00, 1.07e+01, 4.41e+00]    [7.41e+00, 1.07e+01, 6.26e+00, 5.41e+00]    
7000      [5.21e+00, 7.39e-02, 4.36e+00]    [0.00e+00, 9.11e+00, 4.36e+00]    [6.29e+00, 9.11e+00, 4.26e+00, 3.67e+00]    
8000      [4.52e+00, 1.19e-01, 4.31e+00]    [0.00e+00, 7.30e+00, 4.31e+00]    [4.85e+00, 7.30e+00, 2.83e+00, 2.35e+00]    
9000      [4.02e+00, 7.50e-02, 4.27e+00]    [0.00e+00, 6.06e+00, 4.27e+00]    [3.90e+00, 6.06e+00, 2.53e+00, 2.58e+00]    
10000     [3.69e+00, 3.58e-02, 4.23e+00]    [0.00e+00, 5.50e+00, 4.23e+00]    [3.58e+00, 5.50e+00, 2.21e+00, 2.75e+00]    
11000     [3.45e+00, 8.27e-02, 4.20e+00]    [0.00e+00, 5.03e+00, 4.20e+00]    [3.33e+00, 5.03e+00, 2.07e+00, 2.89e+00]    
12000     [3.26e+00, 1.01e-01, 4.17e+00]    [0.00e+00, 4.93e+00, 4.17e+00]    [3.36e+00, 4.93e+00, 2.05e+00, 2.86e+00]    
13000     [3.19e+00, 2.36e-01, 4.13e+00]    [0.00e+00, 4.70e+00, 4.13e+00]    [3.25e+00, 4.70e+00, 2.01e+00, 2.78e+00]    
14000     [3.09e+00, 9.47e-03, 4.09e+00]    [0.00e+00, 4.33e+00, 4.09e+00]    [3.02e+00, 4.33e+00, 1.90e+00, 2.62e+00]    
15000     [3.18e+00, 3.55e-01, 4.05e+00]    [0.00e+00, 3.99e+00, 4.05e+00]    [2.92e+00, 3.99e+00, 1.88e+00, 2.62e+00]    
16000     [3.03e+00, 6.91e-02, 4.02e+00]    [0.00e+00, 4.20e+00, 4.02e+00]    [3.08e+00, 4.20e+00, 1.91e+00, 2.52e+00]    
17000     [3.04e+00, 1.57e-01, 3.99e+00]    [0.00e+00, 4.13e+00, 3.99e+00]    [3.05e+00, 4.13e+00, 1.91e+00, 2.41e+00]    
18000     [3.05e+00, 2.30e-01, 3.96e+00]    [0.00e+00, 4.13e+00, 3.96e+00]    [3.12e+00, 4.13e+00, 1.92e+00, 2.39e+00]    
19000     [2.97e+00, 7.40e-02, 3.93e+00]    [0.00e+00, 3.94e+00, 3.93e+00]    [3.04e+00, 3.94e+00, 1.89e+00, 2.34e+00]    
20000     [3.07e+00, 3.14e-01, 3.90e+00]    [0.00e+00, 3.62e+00, 3.90e+00]    [2.91e+00, 3.62e+00, 1.85e+00, 2.37e+00]    
21000     [2.94e+00, 1.37e-01, 3.87e+00]    [0.00e+00, 3.63e+00, 3.87e+00]    [2.92e+00, 3.63e+00, 1.81e+00, 2.26e+00]    
22000     [2.89e+00, 2.73e-02, 3.84e+00]    [0.00e+00, 3.60e+00, 3.84e+00]    [2.90e+00, 3.60e+00, 1.82e+00, 2.20e+00]    
23000     [2.88e+00, 5.91e-02, 3.82e+00]    [0.00e+00, 3.50e+00, 3.82e+00]    [2.88e+00, 3.50e+00, 1.78e+00, 2.18e+00]    
24000     [2.90e+00, 6.74e-02, 3.79e+00]    [0.00e+00, 3.50e+00, 3.79e+00]    [2.91e+00, 3.50e+00, 1.77e+00, 2.14e+00]    
25000     [2.90e+00, 1.83e-01, 3.77e+00]    [0.00e+00, 3.25e+00, 3.77e+00]    [2.75e+00, 3.25e+00, 1.76e+00, 2.11e+00]    
26000     [2.84e+00, 7.36e-02, 3.74e+00]    [0.00e+00, 3.33e+00, 3.74e+00]    [2.77e+00, 3.33e+00, 1.80e+00, 2.01e+00]    
27000     [2.85e+00, 1.16e-01, 3.72e+00]    [0.00e+00, 3.18e+00, 3.72e+00]    [2.78e+00, 3.18e+00, 1.71e+00, 2.06e+00]    
28000     [2.82e+00, 9.26e-02, 3.70e+00]    [0.00e+00, 3.16e+00, 3.70e+00]    [2.76e+00, 3.16e+00, 1.72e+00, 2.00e+00]    
29000     [2.87e+00, 2.67e-01, 3.68e+00]    [0.00e+00, 3.36e+00, 3.68e+00]    [2.89e+00, 3.36e+00, 1.80e+00, 1.95e+00]    
30000     [2.81e+00, 1.26e-01, 3.66e+00]    [0.00e+00, 3.10e+00, 3.66e+00]    [2.80e+00, 3.10e+00, 1.67e+00, 1.95e+00]    

Best model at step 30000:
  train loss: 6.59e+00
  test loss: 6.76e+00
  test metric: [2.80e+00, 3.10e+00, 1.67e+00, 1.95e+00]

'train' took 80.698161 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[2.9657399488049823, 3.03477141325766, 3.1393744358983144, 5.330633729005638, 3.196279545442391, 3.118459896494629, 3.6549914290417576, 3.8963044481819082, 4.658579173335608, 3.1037808793553725]
E* 3 3.609891489881826 0.7603008949273711
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.459770 s

'compile' took 2.184160 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.05e+02, 2.70e+00]    [0.00e+00, 1.00e+02, 2.70e+00]    [1.00e+02, 1.00e+02, 6.46e-01, 6.46e-01]    
1000      [8.16e+01, 2.92e+01, 2.82e+00]    [0.00e+00, 9.10e+01, 2.82e+00]    [9.10e+01, 9.10e+01, 6.22e+00, 6.22e+00]    
2000      [7.58e+01, 2.56e+01, 2.90e+00]    [0.00e+00, 8.52e+01, 2.90e+00]    [8.52e+01, 8.52e+01, 6.29e+00, 6.30e+00]    
3000      [6.37e+01, 1.78e+01, 3.20e+00]    [0.00e+00, 7.20e+01, 3.20e+00]    [7.21e+01, 7.20e+01, 1.50e+01, 1.50e+01]    
4000      [3.92e+01, 1.97e+00, 3.78e+00]    [0.00e+00, 5.15e+01, 3.78e+00]    [5.16e+01, 5.15e+01, 2.46e+01, 2.43e+01]    
5000      [3.17e+01, 2.36e-01, 3.81e+00]    [0.00e+00, 4.10e+01, 3.81e+00]    [4.11e+01, 4.10e+01, 2.28e+01, 2.27e+01]    
6000      [2.52e+01, 2.85e-01, 3.82e+00]    [0.00e+00, 3.09e+01, 3.82e+00]    [3.10e+01, 3.09e+01, 2.12e+01, 2.12e+01]    
7000      [1.82e+01, 4.42e-01, 3.85e+00]    [0.00e+00, 2.06e+01, 3.85e+00]    [2.07e+01, 2.06e+01, 1.96e+01, 1.96e+01]    
8000      [1.29e+01, 4.44e-01, 3.88e+00]    [0.00e+00, 1.29e+01, 3.88e+00]    [1.29e+01, 1.29e+01, 1.66e+01, 1.66e+01]    
9000      [9.23e+00, 1.42e-01, 3.90e+00]    [0.00e+00, 9.77e+00, 3.90e+00]    [9.76e+00, 9.77e+00, 1.17e+01, 1.17e+01]    
10000     [7.30e+00, 1.56e-01, 3.90e+00]    [0.00e+00, 8.54e+00, 3.90e+00]    [8.56e+00, 8.54e+00, 8.03e+00, 8.07e+00]    
11000     [6.47e+00, 1.21e-01, 3.88e+00]    [0.00e+00, 7.61e+00, 3.88e+00]    [7.62e+00, 7.61e+00, 6.33e+00, 6.34e+00]    
12000     [6.02e+00, 2.52e-01, 3.86e+00]    [0.00e+00, 6.98e+00, 3.86e+00]    [7.02e+00, 6.98e+00, 5.41e+00, 5.44e+00]    
13000     [5.60e+00, 3.08e-01, 3.84e+00]    [0.00e+00, 6.44e+00, 3.84e+00]    [6.48e+00, 6.44e+00, 4.80e+00, 4.82e+00]    
14000     [5.21e+00, 7.40e-02, 3.82e+00]    [0.00e+00, 5.95e+00, 3.82e+00]    [5.99e+00, 5.95e+00, 4.28e+00, 4.28e+00]    
15000     [4.90e+00, 5.19e-02, 3.81e+00]    [0.00e+00, 5.42e+00, 3.81e+00]    [5.46e+00, 5.42e+00, 3.86e+00, 3.84e+00]    
16000     [4.64e+00, 1.22e-01, 3.80e+00]    [0.00e+00, 4.95e+00, 3.80e+00]    [5.00e+00, 4.95e+00, 3.64e+00, 3.59e+00]    
17000     [4.45e+00, 3.83e-01, 3.79e+00]    [0.00e+00, 4.61e+00, 3.79e+00]    [4.66e+00, 4.61e+00, 3.53e+00, 3.47e+00]    
18000     [4.31e+00, 1.65e-01, 3.77e+00]    [0.00e+00, 4.32e+00, 3.77e+00]    [4.33e+00, 4.32e+00, 3.53e+00, 3.40e+00]    
19000     [4.16e+00, 1.92e-01, 3.76e+00]    [0.00e+00, 4.04e+00, 3.76e+00]    [4.05e+00, 4.04e+00, 3.48e+00, 3.34e+00]    
20000     [4.06e+00, 4.34e-01, 3.74e+00]    [0.00e+00, 3.81e+00, 3.74e+00]    [3.81e+00, 3.81e+00, 3.42e+00, 3.27e+00]    
21000     [3.96e+00, 4.35e-02, 3.73e+00]    [0.00e+00, 3.67e+00, 3.73e+00]    [3.67e+00, 3.67e+00, 3.36e+00, 3.20e+00]    
22000     [3.92e+00, 1.77e-01, 3.71e+00]    [0.00e+00, 3.55e+00, 3.71e+00]    [3.60e+00, 3.55e+00, 3.32e+00, 3.18e+00]    
23000     [3.88e+00, 8.87e-02, 3.69e+00]    [0.00e+00, 3.54e+00, 3.69e+00]    [3.60e+00, 3.54e+00, 3.23e+00, 3.10e+00]    
24000     [3.80e+00, 1.77e-01, 3.67e+00]    [0.00e+00, 3.58e+00, 3.67e+00]    [3.65e+00, 3.58e+00, 3.13e+00, 2.97e+00]    
25000     [3.73e+00, 1.20e-01, 3.66e+00]    [0.00e+00, 3.60e+00, 3.66e+00]    [3.67e+00, 3.60e+00, 3.03e+00, 2.87e+00]    
26000     [3.69e+00, 1.07e-01, 3.64e+00]    [0.00e+00, 3.64e+00, 3.64e+00]    [3.72e+00, 3.64e+00, 2.91e+00, 2.73e+00]    
27000     [3.66e+00, 2.14e-01, 3.62e+00]    [0.00e+00, 3.66e+00, 3.62e+00]    [3.74e+00, 3.66e+00, 2.84e+00, 2.64e+00]    
28000     [3.62e+00, 3.07e-01, 3.61e+00]    [0.00e+00, 3.67e+00, 3.61e+00]    [3.76e+00, 3.67e+00, 2.77e+00, 2.55e+00]    
29000     [3.61e+00, 4.15e-01, 3.59e+00]    [0.00e+00, 3.66e+00, 3.59e+00]    [3.76e+00, 3.66e+00, 2.72e+00, 2.48e+00]    
30000     [3.52e+00, 7.44e-02, 3.58e+00]    [0.00e+00, 3.65e+00, 3.58e+00]    [3.75e+00, 3.65e+00, 2.70e+00, 2.44e+00]    

Best model at step 30000:
  train loss: 7.17e+00
  test loss: 7.22e+00
  test metric: [3.75e+00, 3.65e+00, 2.70e+00, 2.44e+00]

'train' took 80.954473 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.476727 s

'compile' took 2.184158 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.93e+01, 9.96e+01, 2.66e+00]    [0.00e+00, 1.00e+02, 2.66e+00]    [1.00e+02, 1.00e+02, 1.01e+00, 1.01e+00]    
1000      [5.95e+01, 5.22e+01, 3.25e+00]    [0.00e+00, 6.27e+01, 3.25e+00]    [6.34e+01, 6.27e+01, 3.29e+01, 3.31e+01]    
2000      [2.61e+01, 1.08e+01, 3.96e+00]    [0.00e+00, 3.43e+01, 3.96e+00]    [3.48e+01, 3.43e+01, 2.86e+01, 2.74e+01]    
3000      [1.27e+01, 2.72e-02, 4.11e+00]    [0.00e+00, 1.53e+01, 4.11e+00]    [1.59e+01, 1.53e+01, 1.70e+01, 1.61e+01]    
4000      [9.74e+00, 2.02e-01, 4.06e+00]    [0.00e+00, 1.11e+01, 4.06e+00]    [1.14e+01, 1.11e+01, 1.26e+01, 1.23e+01]    
5000      [6.87e+00, 7.71e-01, 4.05e+00]    [0.00e+00, 7.39e+00, 4.05e+00]    [7.37e+00, 7.39e+00, 7.58e+00, 7.37e+00]    
6000      [4.70e+00, 9.60e-01, 4.07e+00]    [0.00e+00, 4.76e+00, 4.07e+00]    [4.76e+00, 4.76e+00, 3.67e+00, 3.70e+00]    
7000      [4.07e+00, 4.54e-01, 4.06e+00]    [0.00e+00, 4.06e+00, 4.06e+00]    [3.98e+00, 4.06e+00, 2.91e+00, 3.04e+00]    
8000      [3.52e+00, 3.71e-02, 4.05e+00]    [0.00e+00, 4.07e+00, 4.05e+00]    [3.81e+00, 4.07e+00, 2.98e+00, 3.50e+00]    
9000      [3.19e+00, 1.38e-02, 4.01e+00]    [0.00e+00, 3.48e+00, 4.01e+00]    [3.39e+00, 3.48e+00, 2.37e+00, 3.26e+00]    
10000     [3.09e+00, 3.49e-02, 3.96e+00]    [0.00e+00, 3.53e+00, 3.96e+00]    [3.13e+00, 3.53e+00, 2.28e+00, 3.04e+00]    
11000     [3.01e+00, 4.99e-02, 3.91e+00]    [0.00e+00, 3.52e+00, 3.91e+00]    [2.93e+00, 3.52e+00, 2.14e+00, 2.82e+00]    
12000     [2.92e+00, 7.75e-03, 3.86e+00]    [0.00e+00, 3.59e+00, 3.86e+00]    [2.80e+00, 3.59e+00, 2.05e+00, 2.68e+00]    
13000     [2.89e+00, 1.49e-02, 3.81e+00]    [0.00e+00, 3.57e+00, 3.81e+00]    [2.75e+00, 3.57e+00, 2.00e+00, 2.62e+00]    
14000     [2.89e+00, 3.78e-02, 3.77e+00]    [0.00e+00, 3.52e+00, 3.77e+00]    [2.68e+00, 3.52e+00, 1.91e+00, 2.52e+00]    
15000     [2.79e+00, 1.57e-02, 3.73e+00]    [0.00e+00, 3.63e+00, 3.73e+00]    [2.59e+00, 3.63e+00, 1.85e+00, 2.41e+00]    
16000     [2.77e+00, 2.29e-02, 3.68e+00]    [0.00e+00, 3.58e+00, 3.68e+00]    [2.54e+00, 3.58e+00, 1.82e+00, 2.37e+00]    
17000     [2.72e+00, 2.12e-02, 3.64e+00]    [0.00e+00, 3.56e+00, 3.64e+00]    [2.46e+00, 3.56e+00, 1.79e+00, 2.29e+00]    
18000     [2.69e+00, 1.12e-02, 3.61e+00]    [0.00e+00, 3.55e+00, 3.61e+00]    [2.37e+00, 3.55e+00, 1.74e+00, 2.17e+00]    
19000     [2.66e+00, 1.42e-02, 3.57e+00]    [0.00e+00, 3.47e+00, 3.57e+00]    [2.32e+00, 3.47e+00, 1.73e+00, 2.12e+00]    
20000     [2.65e+00, 3.85e-02, 3.53e+00]    [0.00e+00, 3.53e+00, 3.53e+00]    [2.25e+00, 3.53e+00, 1.69e+00, 2.03e+00]    
21000     [2.62e+00, 1.50e-02, 3.50e+00]    [0.00e+00, 3.37e+00, 3.50e+00]    [2.20e+00, 3.37e+00, 1.72e+00, 2.02e+00]    
22000     [2.58e+00, 2.76e-02, 3.47e+00]    [0.00e+00, 3.43e+00, 3.47e+00]    [2.15e+00, 3.43e+00, 1.69e+00, 1.96e+00]    
23000     [2.60e+00, 6.38e-02, 3.43e+00]    [0.00e+00, 3.36e+00, 3.43e+00]    [2.11e+00, 3.36e+00, 1.69e+00, 1.94e+00]    
24000     [2.59e+00, 4.11e-02, 3.40e+00]    [0.00e+00, 3.50e+00, 3.40e+00]    [2.06e+00, 3.50e+00, 1.66e+00, 1.86e+00]    
25000     [2.54e+00, 3.29e-02, 3.37e+00]    [0.00e+00, 3.48e+00, 3.37e+00]    [2.08e+00, 3.48e+00, 1.68e+00, 1.88e+00]    
26000     [2.53e+00, 1.95e-02, 3.34e+00]    [0.00e+00, 3.45e+00, 3.34e+00]    [2.03e+00, 3.45e+00, 1.70e+00, 1.86e+00]    
27000     [2.52e+00, 9.15e-03, 3.32e+00]    [0.00e+00, 3.48e+00, 3.32e+00]    [1.99e+00, 3.48e+00, 1.70e+00, 1.83e+00]    
28000     [2.50e+00, 2.27e-02, 3.29e+00]    [0.00e+00, 3.44e+00, 3.29e+00]    [1.99e+00, 3.44e+00, 1.72e+00, 1.85e+00]    
29000     [2.47e+00, 6.61e-03, 3.26e+00]    [0.00e+00, 3.33e+00, 3.26e+00]    [1.92e+00, 3.33e+00, 1.81e+00, 1.86e+00]    
30000     [2.48e+00, 5.28e-02, 3.24e+00]    [0.00e+00, 3.23e+00, 3.24e+00]    [1.93e+00, 3.23e+00, 1.80e+00, 1.86e+00]    

Best model at step 29000:
  train loss: 5.74e+00
  test loss: 6.59e+00
  test metric: [1.92e+00, 3.33e+00, 1.81e+00, 1.86e+00]

'train' took 78.659827 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.491684 s

'compile' took 2.223052 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.93e+01, 2.82e+00]    [0.00e+00, 1.01e+02, 2.82e+00]    [1.01e+02, 1.01e+02, 1.87e+00, 1.87e+00]    
1000      [8.36e+01, 5.76e+01, 2.86e+00]    [0.00e+00, 8.37e+01, 2.86e+00]    [8.37e+01, 8.37e+01, 2.80e+01, 2.81e+01]    
2000      [5.48e+01, 2.80e+01, 3.53e+00]    [0.00e+00, 5.73e+01, 3.53e+00]    [5.76e+01, 5.73e+01, 3.00e+01, 2.98e+01]    
3000      [3.02e+01, 3.90e+00, 4.01e+00]    [0.00e+00, 3.11e+01, 4.01e+00]    [3.16e+01, 3.11e+01, 2.28e+01, 2.24e+01]    
4000      [1.41e+01, 9.56e-01, 4.11e+00]    [0.00e+00, 1.27e+01, 4.11e+00]    [1.36e+01, 1.27e+01, 1.24e+01, 1.23e+01]    
5000      [6.11e+00, 2.52e+00, 4.17e+00]    [0.00e+00, 5.22e+00, 4.17e+00]    [5.40e+00, 5.22e+00, 4.77e+00, 4.80e+00]    
6000      [4.26e+00, 2.28e+00, 4.17e+00]    [0.00e+00, 3.40e+00, 4.17e+00]    [3.31e+00, 3.40e+00, 4.22e+00, 4.29e+00]    
7000      [3.72e+00, 1.76e+00, 4.14e+00]    [0.00e+00, 3.38e+00, 4.14e+00]    [3.37e+00, 3.38e+00, 4.02e+00, 4.03e+00]    
8000      [3.46e+00, 1.16e+00, 4.12e+00]    [0.00e+00, 3.78e+00, 4.12e+00]    [4.00e+00, 3.78e+00, 3.00e+00, 3.01e+00]    
9000      [3.42e+00, 6.30e-01, 4.10e+00]    [0.00e+00, 4.13e+00, 4.10e+00]    [4.58e+00, 4.13e+00, 2.78e+00, 2.79e+00]    
10000     [3.46e+00, 8.69e-02, 4.11e+00]    [0.00e+00, 4.19e+00, 4.11e+00]    [4.88e+00, 4.19e+00, 2.59e+00, 2.61e+00]    
11000     [3.34e+00, 8.64e-02, 4.08e+00]    [0.00e+00, 4.32e+00, 4.08e+00]    [5.05e+00, 4.32e+00, 2.18e+00, 2.06e+00]    
12000     [3.24e+00, 4.12e-02, 4.05e+00]    [0.00e+00, 4.26e+00, 4.05e+00]    [5.00e+00, 4.26e+00, 1.86e+00, 1.64e+00]    
13000     [3.23e+00, 1.44e-01, 4.02e+00]    [0.00e+00, 4.04e+00, 4.02e+00]    [4.77e+00, 4.04e+00, 1.79e+00, 1.56e+00]    
14000     [3.15e+00, 4.03e-02, 3.99e+00]    [0.00e+00, 4.19e+00, 3.99e+00]    [4.89e+00, 4.19e+00, 1.63e+00, 1.35e+00]    
15000     [3.10e+00, 4.70e-02, 3.96e+00]    [0.00e+00, 4.17e+00, 3.96e+00]    [4.84e+00, 4.17e+00, 1.57e+00, 1.28e+00]    
16000     [3.04e+00, 4.88e-02, 3.93e+00]    [0.00e+00, 4.10e+00, 3.93e+00]    [4.76e+00, 4.10e+00, 1.54e+00, 1.26e+00]    
17000     [3.03e+00, 4.36e-02, 3.90e+00]    [0.00e+00, 4.10e+00, 3.90e+00]    [4.76e+00, 4.10e+00, 1.48e+00, 1.19e+00]    
18000     [3.06e+00, 8.12e-02, 3.87e+00]    [0.00e+00, 4.18e+00, 3.87e+00]    [4.81e+00, 4.18e+00, 1.49e+00, 1.18e+00]    
19000     [3.02e+00, 7.94e-02, 3.85e+00]    [0.00e+00, 4.00e+00, 3.85e+00]    [4.62e+00, 4.00e+00, 1.51e+00, 1.23e+00]    
20000     [2.96e+00, 5.82e-02, 3.82e+00]    [0.00e+00, 4.03e+00, 3.82e+00]    [4.65e+00, 4.03e+00, 1.53e+00, 1.23e+00]    
21000     [2.93e+00, 1.60e-02, 3.80e+00]    [0.00e+00, 4.07e+00, 3.80e+00]    [4.68e+00, 4.07e+00, 1.55e+00, 1.24e+00]    
22000     [2.90e+00, 1.00e-02, 3.77e+00]    [0.00e+00, 4.05e+00, 3.77e+00]    [4.65e+00, 4.05e+00, 1.59e+00, 1.28e+00]    
23000     [2.89e+00, 6.45e-02, 3.75e+00]    [0.00e+00, 4.03e+00, 3.75e+00]    [4.60e+00, 4.03e+00, 1.63e+00, 1.34e+00]    
24000     [2.89e+00, 2.42e-02, 3.73e+00]    [0.00e+00, 4.04e+00, 3.73e+00]    [4.60e+00, 4.04e+00, 1.67e+00, 1.38e+00]    
25000     [2.85e+00, 3.41e-02, 3.70e+00]    [0.00e+00, 4.05e+00, 3.70e+00]    [4.60e+00, 4.05e+00, 1.70e+00, 1.41e+00]    
26000     [2.85e+00, 4.21e-02, 3.68e+00]    [0.00e+00, 4.12e+00, 3.68e+00]    [4.65e+00, 4.12e+00, 1.75e+00, 1.46e+00]    
27000     [2.84e+00, 6.22e-02, 3.66e+00]    [0.00e+00, 4.00e+00, 3.66e+00]    [4.52e+00, 4.00e+00, 1.78e+00, 1.50e+00]    
28000     [2.89e+00, 1.13e-01, 3.64e+00]    [0.00e+00, 3.93e+00, 3.64e+00]    [4.44e+00, 3.93e+00, 1.81e+00, 1.54e+00]    
29000     [2.83e+00, 9.57e-02, 3.62e+00]    [0.00e+00, 4.09e+00, 3.62e+00]    [4.59e+00, 4.09e+00, 1.84e+00, 1.56e+00]    
30000     [2.86e+00, 9.22e-02, 3.60e+00]    [0.00e+00, 4.14e+00, 3.60e+00]    [4.64e+00, 4.14e+00, 1.89e+00, 1.60e+00]    

Best model at step 29000:
  train loss: 6.54e+00
  test loss: 7.71e+00
  test metric: [4.59e+00, 4.09e+00, 1.84e+00, 1.56e+00]

'train' took 79.297463 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.459772 s

'compile' took 2.160224 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 9.89e+01, 2.70e+00]    [0.00e+00, 9.99e+01, 2.70e+00]    [9.99e+01, 9.99e+01, 9.68e-01, 9.68e-01]    
1000      [7.27e+01, 5.21e+01, 3.12e+00]    [0.00e+00, 7.09e+01, 3.12e+00]    [7.10e+01, 7.09e+01, 2.83e+01, 2.83e+01]    
2000      [4.44e+01, 1.35e+01, 3.70e+00]    [0.00e+00, 5.26e+01, 3.70e+00]    [5.30e+01, 5.26e+01, 3.11e+01, 3.07e+01]    
3000      [2.92e+01, 5.07e+00, 3.92e+00]    [0.00e+00, 3.70e+01, 3.92e+00]    [3.77e+01, 3.70e+01, 1.87e+01, 1.89e+01]    
4000      [1.27e+01, 1.87e+00, 4.13e+00]    [0.00e+00, 1.94e+01, 4.13e+00]    [1.95e+01, 1.94e+01, 1.08e+01, 1.08e+01]    
5000      [7.23e+00, 4.18e-01, 4.13e+00]    [0.00e+00, 1.08e+01, 4.13e+00]    [1.07e+01, 1.08e+01, 7.55e+00, 7.68e+00]    
6000      [5.43e+00, 4.52e-02, 4.08e+00]    [0.00e+00, 7.23e+00, 4.08e+00]    [7.15e+00, 7.23e+00, 6.63e+00, 6.75e+00]    
7000      [4.71e+00, 3.08e-02, 4.03e+00]    [0.00e+00, 6.01e+00, 4.03e+00]    [6.21e+00, 6.01e+00, 5.77e+00, 6.12e+00]    
8000      [4.36e+00, 7.64e-02, 3.98e+00]    [0.00e+00, 5.33e+00, 3.98e+00]    [5.54e+00, 5.33e+00, 5.44e+00, 5.73e+00]    
9000      [4.14e+00, 5.93e-02, 3.93e+00]    [0.00e+00, 5.04e+00, 3.93e+00]    [5.25e+00, 5.04e+00, 4.90e+00, 5.14e+00]    
10000     [3.96e+00, 4.17e-02, 3.88e+00]    [0.00e+00, 4.82e+00, 3.88e+00]    [5.02e+00, 4.82e+00, 4.62e+00, 4.85e+00]    
11000     [3.85e+00, 5.26e-02, 3.84e+00]    [0.00e+00, 4.68e+00, 3.84e+00]    [4.87e+00, 4.68e+00, 4.18e+00, 4.41e+00]    
12000     [3.74e+00, 3.03e-02, 3.80e+00]    [0.00e+00, 4.56e+00, 3.80e+00]    [4.74e+00, 4.56e+00, 4.02e+00, 4.25e+00]    
13000     [3.69e+00, 2.31e-02, 3.77e+00]    [0.00e+00, 4.49e+00, 3.77e+00]    [4.67e+00, 4.49e+00, 3.89e+00, 4.12e+00]    
14000     [3.63e+00, 3.62e-02, 3.73e+00]    [0.00e+00, 4.42e+00, 3.73e+00]    [4.60e+00, 4.42e+00, 3.78e+00, 4.01e+00]    
15000     [3.59e+00, 1.52e-02, 3.70e+00]    [0.00e+00, 4.46e+00, 3.70e+00]    [4.64e+00, 4.46e+00, 3.61e+00, 3.81e+00]    
16000     [3.53e+00, 1.29e-02, 3.67e+00]    [0.00e+00, 4.36e+00, 3.67e+00]    [4.54e+00, 4.36e+00, 3.54e+00, 3.74e+00]    
17000     [3.50e+00, 1.32e-02, 3.64e+00]    [0.00e+00, 4.26e+00, 3.64e+00]    [4.44e+00, 4.26e+00, 3.54e+00, 3.73e+00]    
18000     [3.51e+00, 2.97e-01, 3.61e+00]    [0.00e+00, 4.23e+00, 3.61e+00]    [4.41e+00, 4.23e+00, 3.76e+00, 3.96e+00]    
19000     [3.40e+00, 1.36e-02, 3.59e+00]    [0.00e+00, 4.16e+00, 3.59e+00]    [4.33e+00, 4.16e+00, 3.44e+00, 3.61e+00]    
20000     [3.36e+00, 6.19e-02, 3.56e+00]    [0.00e+00, 4.15e+00, 3.56e+00]    [4.33e+00, 4.15e+00, 3.40e+00, 3.56e+00]    
21000     [3.35e+00, 4.60e-02, 3.53e+00]    [0.00e+00, 4.22e+00, 3.53e+00]    [4.39e+00, 4.22e+00, 3.23e+00, 3.38e+00]    
22000     [3.31e+00, 3.05e-02, 3.51e+00]    [0.00e+00, 4.11e+00, 3.51e+00]    [4.28e+00, 4.11e+00, 3.17e+00, 3.32e+00]    
23000     [3.30e+00, 4.49e-02, 3.48e+00]    [0.00e+00, 4.09e+00, 3.48e+00]    [4.25e+00, 4.09e+00, 3.07e+00, 3.22e+00]    
24000     [3.27e+00, 3.71e-02, 3.46e+00]    [0.00e+00, 4.04e+00, 3.46e+00]    [4.21e+00, 4.04e+00, 3.06e+00, 3.21e+00]    
25000     [3.27e+00, 1.40e-01, 3.44e+00]    [0.00e+00, 3.95e+00, 3.44e+00]    [4.12e+00, 3.95e+00, 3.18e+00, 3.35e+00]    
26000     [3.33e+00, 7.25e-02, 3.41e+00]    [0.00e+00, 4.10e+00, 3.41e+00]    [4.26e+00, 4.10e+00, 2.83e+00, 2.96e+00]    
27000     [3.22e+00, 1.81e-02, 3.39e+00]    [0.00e+00, 3.99e+00, 3.39e+00]    [4.16e+00, 3.99e+00, 2.87e+00, 3.01e+00]    
28000     [3.22e+00, 8.63e-02, 3.37e+00]    [0.00e+00, 3.99e+00, 3.37e+00]    [4.15e+00, 3.99e+00, 2.76e+00, 2.90e+00]    
29000     [3.18e+00, 5.01e-02, 3.35e+00]    [0.00e+00, 4.00e+00, 3.35e+00]    [4.16e+00, 4.00e+00, 2.78e+00, 2.92e+00]    
30000     [3.18e+00, 3.96e-02, 3.33e+00]    [0.00e+00, 3.90e+00, 3.33e+00]    [4.06e+00, 3.90e+00, 2.79e+00, 2.95e+00]    

Best model at step 30000:
  train loss: 6.55e+00
  test loss: 7.23e+00
  test metric: [4.06e+00, 3.90e+00, 2.79e+00, 2.95e+00]

'train' took 78.893985 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.470739 s

'compile' took 2.076445 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.82e+01, 9.82e+01, 2.75e+00]    [0.00e+00, 9.91e+01, 2.75e+00]    [9.91e+01, 9.91e+01, 1.38e+00, 1.38e+00]    
1000      [8.08e+01, 7.31e+01, 2.70e+00]    [0.00e+00, 8.57e+01, 2.70e+00]    [8.57e+01, 8.57e+01, 1.44e+01, 1.44e+01]    
2000      [7.12e+01, 6.00e+01, 3.00e+00]    [0.00e+00, 7.65e+01, 3.00e+00]    [7.66e+01, 7.65e+01, 1.29e+01, 1.30e+01]    
3000      [4.38e+01, 2.26e+01, 3.88e+00]    [0.00e+00, 5.05e+01, 3.88e+00]    [5.13e+01, 5.05e+01, 1.80e+01, 1.83e+01]    
4000      [2.42e+01, 2.41e-01, 4.31e+00]    [0.00e+00, 3.13e+01, 4.31e+00]    [3.20e+01, 3.13e+01, 2.22e+01, 2.22e+01]    
5000      [1.72e+01, 2.36e-01, 4.35e+00]    [0.00e+00, 2.20e+01, 4.35e+00]    [2.21e+01, 2.20e+01, 1.80e+01, 1.79e+01]    
6000      [1.18e+01, 3.73e-02, 4.36e+00]    [0.00e+00, 1.68e+01, 4.36e+00]    [1.66e+01, 1.68e+01, 1.27e+01, 1.27e+01]    
7000      [8.22e+00, 1.13e-01, 4.35e+00]    [0.00e+00, 1.30e+01, 4.35e+00]    [1.27e+01, 1.30e+01, 9.51e+00, 9.72e+00]    
8000      [6.48e+00, 1.20e-01, 4.32e+00]    [0.00e+00, 1.00e+01, 4.32e+00]    [9.67e+00, 1.00e+01, 8.64e+00, 8.42e+00]    
9000      [5.53e+00, 2.16e-01, 4.26e+00]    [0.00e+00, 7.99e+00, 4.26e+00]    [7.64e+00, 7.99e+00, 8.63e+00, 8.09e+00]    
10000     [5.02e+00, 5.86e-02, 4.20e+00]    [0.00e+00, 6.48e+00, 4.20e+00]    [6.44e+00, 6.48e+00, 7.84e+00, 7.46e+00]    
11000     [4.56e+00, 1.24e-01, 4.15e+00]    [0.00e+00, 5.55e+00, 4.15e+00]    [5.75e+00, 5.55e+00, 6.58e+00, 6.34e+00]    
12000     [4.19e+00, 1.14e-01, 4.11e+00]    [0.00e+00, 5.09e+00, 4.11e+00]    [5.21e+00, 5.09e+00, 5.68e+00, 5.35e+00]    
13000     [3.95e+00, 1.71e-01, 4.06e+00]    [0.00e+00, 4.52e+00, 4.06e+00]    [4.58e+00, 4.52e+00, 4.85e+00, 4.49e+00]    
14000     [3.80e+00, 6.17e-02, 4.02e+00]    [0.00e+00, 4.26e+00, 4.02e+00]    [4.30e+00, 4.26e+00, 4.15e+00, 3.81e+00]    
15000     [3.67e+00, 1.55e-01, 3.99e+00]    [0.00e+00, 3.88e+00, 3.99e+00]    [3.91e+00, 3.88e+00, 3.65e+00, 3.24e+00]    
16000     [3.56e+00, 4.39e-02, 3.95e+00]    [0.00e+00, 3.73e+00, 3.95e+00]    [3.76e+00, 3.73e+00, 3.39e+00, 2.94e+00]    
17000     [3.49e+00, 5.62e-02, 3.92e+00]    [0.00e+00, 3.60e+00, 3.92e+00]    [3.62e+00, 3.60e+00, 3.16e+00, 2.69e+00]    
18000     [3.51e+00, 2.26e-01, 3.89e+00]    [0.00e+00, 3.54e+00, 3.89e+00]    [3.56e+00, 3.54e+00, 2.88e+00, 2.46e+00]    
19000     [3.39e+00, 5.20e-02, 3.86e+00]    [0.00e+00, 3.32e+00, 3.86e+00]    [3.34e+00, 3.32e+00, 2.69e+00, 2.25e+00]    
20000     [3.33e+00, 6.33e-02, 3.84e+00]    [0.00e+00, 3.26e+00, 3.84e+00]    [3.28e+00, 3.26e+00, 2.47e+00, 2.06e+00]    
21000     [3.46e+00, 4.44e-01, 3.81e+00]    [0.00e+00, 3.21e+00, 3.81e+00]    [3.23e+00, 3.21e+00, 2.24e+00, 1.89e+00]    
22000     [3.35e+00, 3.07e-01, 3.79e+00]    [0.00e+00, 3.08e+00, 3.79e+00]    [3.09e+00, 3.08e+00, 2.13e+00, 1.76e+00]    
23000     [3.17e+00, 2.23e-02, 3.76e+00]    [0.00e+00, 2.88e+00, 3.76e+00]    [2.89e+00, 2.88e+00, 2.13e+00, 1.70e+00]    
24000     [3.20e+00, 3.55e-01, 3.74e+00]    [0.00e+00, 2.67e+00, 3.74e+00]    [2.68e+00, 2.67e+00, 2.20e+00, 1.74e+00]    
25000     [3.25e+00, 5.36e-01, 3.72e+00]    [0.00e+00, 2.57e+00, 3.72e+00]    [2.67e+00, 2.57e+00, 2.19e+00, 1.81e+00]    
26000     [3.06e+00, 2.61e-02, 3.70e+00]    [0.00e+00, 2.66e+00, 3.70e+00]    [2.67e+00, 2.66e+00, 2.12e+00, 1.66e+00]    
27000     [3.06e+00, 1.06e-01, 3.68e+00]    [0.00e+00, 2.66e+00, 3.68e+00]    [2.69e+00, 2.66e+00, 2.05e+00, 1.63e+00]    
28000     [3.01e+00, 6.48e-02, 3.66e+00]    [0.00e+00, 2.59e+00, 3.66e+00]    [2.73e+00, 2.59e+00, 1.96e+00, 1.67e+00]    
29000     [2.97e+00, 2.01e-02, 3.64e+00]    [0.00e+00, 2.59e+00, 3.64e+00]    [2.77e+00, 2.59e+00, 1.87e+00, 1.63e+00]    
30000     [3.00e+00, 3.23e-01, 3.62e+00]    [0.00e+00, 2.64e+00, 3.62e+00]    [2.83e+00, 2.64e+00, 1.85e+00, 1.51e+00]    

Best model at step 29000:
  train loss: 6.63e+00
  test loss: 6.23e+00
  test metric: [2.77e+00, 2.59e+00, 1.87e+00, 1.63e+00]

'train' took 78.540742 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.461765 s

'compile' took 2.068466 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 1.00e+02, 2.71e+00]    [0.00e+00, 9.97e+01, 2.71e+00]    [9.97e+01, 9.97e+01, 9.26e-01, 9.26e-01]    
1000      [5.75e+01, 4.48e+01, 3.38e+00]    [0.00e+00, 6.09e+01, 3.38e+00]    [6.12e+01, 6.09e+01, 3.03e+01, 3.05e+01]    
2000      [2.92e+01, 1.57e+01, 3.91e+00]    [0.00e+00, 3.21e+01, 3.91e+00]    [3.23e+01, 3.21e+01, 2.34e+01, 2.33e+01]    
3000      [1.03e+01, 4.49e-01, 4.22e+00]    [0.00e+00, 1.03e+01, 4.22e+00]    [1.04e+01, 1.03e+01, 1.12e+01, 1.11e+01]    
4000      [7.48e+00, 8.78e-01, 4.18e+00]    [0.00e+00, 7.31e+00, 4.18e+00]    [7.25e+00, 7.31e+00, 6.67e+00, 6.73e+00]    
5000      [5.32e+00, 9.86e-01, 4.16e+00]    [0.00e+00, 5.26e+00, 4.16e+00]    [4.81e+00, 5.26e+00, 6.51e+00, 6.70e+00]    
6000      [4.91e+00, 2.00e-01, 4.12e+00]    [0.00e+00, 5.35e+00, 4.12e+00]    [4.67e+00, 5.35e+00, 5.28e+00, 6.05e+00]    
7000      [4.38e+00, 6.72e-02, 4.08e+00]    [0.00e+00, 4.91e+00, 4.08e+00]    [4.35e+00, 4.91e+00, 4.44e+00, 5.68e+00]    
8000      [4.01e+00, 1.30e-02, 4.03e+00]    [0.00e+00, 4.72e+00, 4.03e+00]    [4.12e+00, 4.72e+00, 3.84e+00, 5.29e+00]    
9000      [3.80e+00, 1.85e-02, 3.98e+00]    [0.00e+00, 4.43e+00, 3.98e+00]    [3.80e+00, 4.43e+00, 3.26e+00, 4.75e+00]    
10000     [3.67e+00, 5.58e-02, 3.94e+00]    [0.00e+00, 4.31e+00, 3.94e+00]    [3.53e+00, 4.31e+00, 2.78e+00, 4.29e+00]    
11000     [3.47e+00, 1.89e-02, 3.89e+00]    [0.00e+00, 4.32e+00, 3.89e+00]    [3.16e+00, 4.32e+00, 2.34e+00, 3.75e+00]    
12000     [3.33e+00, 1.43e-02, 3.86e+00]    [0.00e+00, 4.28e+00, 3.86e+00]    [2.89e+00, 4.28e+00, 1.96e+00, 3.33e+00]    
13000     [3.24e+00, 3.48e-02, 3.82e+00]    [0.00e+00, 4.19e+00, 3.82e+00]    [2.72e+00, 4.19e+00, 1.73e+00, 3.08e+00]    
14000     [3.17e+00, 2.49e-02, 3.78e+00]    [0.00e+00, 4.04e+00, 3.78e+00]    [2.57e+00, 4.04e+00, 1.52e+00, 2.86e+00]    
15000     [3.09e+00, 1.88e-02, 3.75e+00]    [0.00e+00, 4.01e+00, 3.75e+00]    [2.46e+00, 4.01e+00, 1.36e+00, 2.70e+00]    
16000     [3.06e+00, 3.14e-02, 3.72e+00]    [0.00e+00, 3.87e+00, 3.72e+00]    [2.35e+00, 3.87e+00, 1.22e+00, 2.53e+00]    
17000     [3.00e+00, 3.84e-02, 3.69e+00]    [0.00e+00, 3.95e+00, 3.69e+00]    [2.26e+00, 3.95e+00, 1.13e+00, 2.42e+00]    
18000     [2.93e+00, 1.48e-02, 3.66e+00]    [0.00e+00, 3.70e+00, 3.66e+00]    [2.17e+00, 3.70e+00, 1.06e+00, 2.29e+00]    
19000     [2.90e+00, 2.97e-02, 3.62e+00]    [0.00e+00, 3.58e+00, 3.62e+00]    [2.16e+00, 3.58e+00, 1.05e+00, 2.26e+00]    
20000     [2.85e+00, 2.25e-02, 3.59e+00]    [0.00e+00, 3.55e+00, 3.59e+00]    [2.13e+00, 3.55e+00, 1.04e+00, 2.24e+00]    
21000     [2.85e+00, 4.67e-02, 3.56e+00]    [0.00e+00, 3.42e+00, 3.56e+00]    [2.13e+00, 3.42e+00, 1.05e+00, 2.23e+00]    
22000     [2.79e+00, 1.07e-02, 3.53e+00]    [0.00e+00, 3.37e+00, 3.53e+00]    [2.02e+00, 3.37e+00, 1.06e+00, 2.14e+00]    
23000     [2.80e+00, 3.10e-02, 3.50e+00]    [0.00e+00, 3.23e+00, 3.50e+00]    [2.04e+00, 3.23e+00, 1.14e+00, 2.17e+00]    
24000     [2.76e+00, 3.41e-02, 3.47e+00]    [0.00e+00, 3.12e+00, 3.47e+00]    [1.96e+00, 3.12e+00, 1.26e+00, 2.13e+00]    
25000     [2.72e+00, 1.18e-02, 3.45e+00]    [0.00e+00, 3.07e+00, 3.45e+00]    [1.89e+00, 3.07e+00, 1.32e+00, 2.09e+00]    
26000     [2.71e+00, 4.56e-02, 3.42e+00]    [0.00e+00, 3.12e+00, 3.42e+00]    [1.91e+00, 3.12e+00, 1.30e+00, 2.11e+00]    
27000     [2.71e+00, 5.48e-02, 3.39e+00]    [0.00e+00, 3.11e+00, 3.39e+00]    [1.86e+00, 3.11e+00, 1.27e+00, 2.04e+00]    
28000     [2.63e+00, 2.07e-02, 3.37e+00]    [0.00e+00, 3.05e+00, 3.37e+00]    [1.94e+00, 3.05e+00, 1.28e+00, 2.10e+00]    
29000     [2.62e+00, 1.56e-02, 3.34e+00]    [0.00e+00, 3.05e+00, 3.34e+00]    [1.94e+00, 3.05e+00, 1.28e+00, 2.10e+00]    
30000     [2.61e+00, 2.07e-02, 3.32e+00]    [0.00e+00, 3.03e+00, 3.32e+00]    [1.95e+00, 3.03e+00, 1.29e+00, 2.10e+00]    

Best model at step 30000:
  train loss: 5.94e+00
  test loss: 6.35e+00
  test metric: [1.95e+00, 3.03e+00, 1.29e+00, 2.10e+00]

'train' took 78.389185 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.452789 s

'compile' took 2.141274 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.02e+02, 2.65e+00]    [0.00e+00, 1.00e+02, 2.65e+00]    [1.00e+02, 1.00e+02, 6.64e-01, 6.64e-01]    
1000      [8.11e+01, 2.29e+01, 2.80e+00]    [0.00e+00, 9.28e+01, 2.80e+00]    [9.28e+01, 9.28e+01, 7.65e+00, 7.70e+00]    
2000      [7.51e+01, 1.95e+01, 2.89e+00]    [0.00e+00, 8.59e+01, 2.89e+00]    [8.61e+01, 8.59e+01, 6.35e+00, 6.40e+00]    
3000      [6.63e+01, 1.26e+01, 3.12e+00]    [0.00e+00, 7.44e+01, 3.12e+00]    [7.47e+01, 7.44e+01, 8.48e+00, 8.60e+00]    
4000      [5.21e+01, 1.77e+00, 3.51e+00]    [0.00e+00, 5.53e+01, 3.51e+00]    [5.64e+01, 5.53e+01, 1.50e+01, 1.53e+01]    
5000      [3.78e+01, 1.92e-01, 3.73e+00]    [0.00e+00, 4.01e+01, 3.73e+00]    [4.13e+01, 4.01e+01, 1.81e+01, 1.79e+01]    
6000      [2.46e+01, 9.92e-02, 3.96e+00]    [0.00e+00, 2.82e+01, 3.96e+00]    [2.95e+01, 2.82e+01, 1.32e+01, 1.32e+01]    
7000      [1.48e+01, 4.46e-01, 4.14e+00]    [0.00e+00, 1.66e+01, 4.14e+00]    [1.71e+01, 1.66e+01, 1.07e+01, 9.85e+00]    
8000      [6.62e+00, 5.56e-02, 4.26e+00]    [0.00e+00, 7.18e+00, 4.26e+00]    [7.30e+00, 7.18e+00, 4.88e+00, 4.27e+00]    
9000      [4.51e+00, 4.39e-01, 4.29e+00]    [0.00e+00, 4.29e+00, 4.29e+00]    [4.05e+00, 4.29e+00, 2.38e+00, 3.13e+00]    
10000     [4.19e+00, 3.05e-01, 4.26e+00]    [0.00e+00, 4.00e+00, 4.26e+00]    [3.57e+00, 4.00e+00, 2.28e+00, 2.94e+00]    
11000     [4.00e+00, 4.86e-01, 4.22e+00]    [0.00e+00, 3.88e+00, 4.22e+00]    [3.35e+00, 3.88e+00, 2.21e+00, 2.72e+00]    
12000     [3.87e+00, 2.85e-01, 4.19e+00]    [0.00e+00, 3.79e+00, 4.19e+00]    [3.30e+00, 3.79e+00, 2.07e+00, 2.61e+00]    
13000     [3.77e+00, 2.30e-01, 4.16e+00]    [0.00e+00, 3.70e+00, 4.16e+00]    [3.30e+00, 3.70e+00, 1.96e+00, 2.52e+00]    
14000     [3.63e+00, 1.23e-01, 4.14e+00]    [0.00e+00, 3.72e+00, 4.14e+00]    [3.33e+00, 3.72e+00, 1.80e+00, 2.40e+00]    
15000     [3.51e+00, 2.37e-01, 4.11e+00]    [0.00e+00, 3.69e+00, 4.11e+00]    [3.33e+00, 3.69e+00, 1.63e+00, 2.30e+00]    
16000     [3.39e+00, 1.24e-01, 4.10e+00]    [0.00e+00, 3.71e+00, 4.10e+00]    [3.36e+00, 3.71e+00, 1.51e+00, 2.24e+00]    
17000     [3.37e+00, 4.05e-01, 4.08e+00]    [0.00e+00, 3.63e+00, 4.08e+00]    [3.29e+00, 3.63e+00, 1.50e+00, 2.28e+00]    
18000     [3.31e+00, 1.74e-01, 4.06e+00]    [0.00e+00, 3.63e+00, 4.06e+00]    [3.31e+00, 3.63e+00, 1.43e+00, 2.25e+00]    
19000     [3.24e+00, 1.60e-01, 4.04e+00]    [0.00e+00, 3.60e+00, 4.04e+00]    [3.28e+00, 3.60e+00, 1.39e+00, 2.22e+00]    
20000     [3.18e+00, 5.46e-02, 4.02e+00]    [0.00e+00, 3.56e+00, 4.02e+00]    [3.25e+00, 3.56e+00, 1.36e+00, 2.17e+00]    
21000     [3.16e+00, 2.82e-01, 4.00e+00]    [0.00e+00, 3.54e+00, 4.00e+00]    [3.24e+00, 3.54e+00, 1.32e+00, 2.12e+00]    
22000     [3.12e+00, 3.33e-01, 3.99e+00]    [0.00e+00, 3.54e+00, 3.99e+00]    [3.25e+00, 3.54e+00, 1.25e+00, 2.03e+00]    
23000     [3.12e+00, 3.31e-01, 3.97e+00]    [0.00e+00, 3.55e+00, 3.97e+00]    [3.26e+00, 3.55e+00, 1.25e+00, 2.01e+00]    
24000     [3.11e+00, 3.46e-01, 3.95e+00]    [0.00e+00, 3.54e+00, 3.95e+00]    [3.27e+00, 3.54e+00, 1.24e+00, 2.00e+00]    
25000     [3.06e+00, 7.34e-02, 3.93e+00]    [0.00e+00, 3.52e+00, 3.93e+00]    [3.25e+00, 3.52e+00, 1.23e+00, 1.96e+00]    
26000     [3.04e+00, 1.11e-01, 3.91e+00]    [0.00e+00, 3.52e+00, 3.91e+00]    [3.25e+00, 3.52e+00, 1.23e+00, 1.96e+00]    
27000     [3.02e+00, 1.33e-01, 3.89e+00]    [0.00e+00, 3.52e+00, 3.89e+00]    [3.25e+00, 3.52e+00, 1.23e+00, 1.93e+00]    
28000     [3.00e+00, 1.35e-01, 3.87e+00]    [0.00e+00, 3.49e+00, 3.87e+00]    [3.23e+00, 3.49e+00, 1.24e+00, 1.93e+00]    
29000     [3.06e+00, 6.18e-01, 3.85e+00]    [0.00e+00, 3.49e+00, 3.85e+00]    [3.23e+00, 3.49e+00, 1.25e+00, 1.94e+00]    
30000     [3.03e+00, 5.43e-01, 3.83e+00]    [0.00e+00, 3.50e+00, 3.83e+00]    [3.25e+00, 3.50e+00, 1.26e+00, 1.92e+00]    

Best model at step 28000:
  train loss: 7.01e+00
  test loss: 7.36e+00
  test metric: [3.23e+00, 3.49e+00, 1.24e+00, 1.93e+00]

'train' took 77.838049 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.493674 s

'compile' took 2.059248 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.85e+01, 2.61e+00]    [0.00e+00, 1.00e+02, 2.61e+00]    [1.00e+02, 1.00e+02, 1.85e+00, 1.85e+00]    
1000      [7.90e+01, 6.07e+01, 2.75e+00]    [0.00e+00, 7.70e+01, 2.75e+00]    [7.70e+01, 7.70e+01, 3.13e+01, 3.12e+01]    
2000      [4.72e+01, 2.09e+01, 3.57e+00]    [0.00e+00, 5.07e+01, 3.57e+00]    [5.10e+01, 5.07e+01, 3.20e+01, 3.14e+01]    
3000      [2.71e+01, 5.62e+00, 3.85e+00]    [0.00e+00, 3.11e+01, 3.85e+00]    [3.13e+01, 3.11e+01, 2.32e+01, 2.21e+01]    
4000      [1.52e+01, 1.03e+00, 3.86e+00]    [0.00e+00, 1.66e+01, 3.86e+00]    [1.72e+01, 1.66e+01, 1.37e+01, 1.33e+01]    
5000      [8.10e+00, 2.45e+00, 3.91e+00]    [0.00e+00, 8.54e+00, 3.91e+00]    [8.78e+00, 8.54e+00, 5.75e+00, 5.53e+00]    
6000      [4.54e+00, 2.15e+00, 3.91e+00]    [0.00e+00, 5.10e+00, 3.91e+00]    [5.18e+00, 5.10e+00, 3.62e+00, 3.87e+00]    
7000      [4.00e+00, 1.61e+00, 3.86e+00]    [0.00e+00, 5.14e+00, 3.86e+00]    [5.03e+00, 5.14e+00, 3.36e+00, 3.43e+00]    
8000      [3.88e+00, 1.28e+00, 3.80e+00]    [0.00e+00, 4.78e+00, 3.80e+00]    [4.72e+00, 4.78e+00, 2.63e+00, 2.69e+00]    
9000      [3.77e+00, 1.13e+00, 3.75e+00]    [0.00e+00, 4.35e+00, 3.75e+00]    [4.33e+00, 4.35e+00, 2.20e+00, 2.22e+00]    
10000     [3.64e+00, 7.52e-01, 3.71e+00]    [0.00e+00, 4.06e+00, 3.71e+00]    [4.07e+00, 4.06e+00, 1.84e+00, 1.82e+00]    
11000     [3.60e+00, 5.33e-01, 3.67e+00]    [0.00e+00, 3.91e+00, 3.67e+00]    [3.99e+00, 3.91e+00, 1.92e+00, 1.93e+00]    
12000     [3.54e+00, 3.18e-01, 3.64e+00]    [0.00e+00, 3.97e+00, 3.64e+00]    [4.11e+00, 3.97e+00, 1.67e+00, 1.66e+00]    
13000     [3.51e+00, 1.05e-01, 3.62e+00]    [0.00e+00, 3.99e+00, 3.62e+00]    [4.18e+00, 3.99e+00, 1.42e+00, 1.37e+00]    
14000     [3.34e+00, 2.05e-02, 3.61e+00]    [0.00e+00, 4.11e+00, 3.61e+00]    [4.35e+00, 4.11e+00, 1.42e+00, 1.16e+00]    
15000     [3.24e+00, 1.33e-01, 3.59e+00]    [0.00e+00, 3.93e+00, 3.59e+00]    [4.19e+00, 3.93e+00, 1.44e+00, 1.11e+00]    
16000     [3.17e+00, 9.25e-02, 3.56e+00]    [0.00e+00, 3.90e+00, 3.56e+00]    [4.18e+00, 3.90e+00, 1.57e+00, 1.19e+00]    
17000     [3.09e+00, 2.43e-02, 3.53e+00]    [0.00e+00, 4.00e+00, 3.53e+00]    [4.29e+00, 4.00e+00, 1.81e+00, 1.37e+00]    
18000     [3.08e+00, 1.10e-01, 3.51e+00]    [0.00e+00, 3.85e+00, 3.51e+00]    [4.15e+00, 3.85e+00, 1.82e+00, 1.38e+00]    
19000     [3.07e+00, 1.33e-01, 3.48e+00]    [0.00e+00, 3.78e+00, 3.48e+00]    [4.12e+00, 3.78e+00, 1.82e+00, 1.45e+00]    
20000     [3.02e+00, 4.65e-02, 3.45e+00]    [0.00e+00, 3.99e+00, 3.45e+00]    [4.39e+00, 3.99e+00, 1.95e+00, 1.65e+00]    
21000     [2.98e+00, 1.26e-02, 3.42e+00]    [0.00e+00, 3.88e+00, 3.42e+00]    [4.30e+00, 3.88e+00, 1.86e+00, 1.62e+00]    
22000     [3.01e+00, 8.38e-02, 3.39e+00]    [0.00e+00, 3.99e+00, 3.39e+00]    [4.45e+00, 3.99e+00, 1.96e+00, 1.78e+00]    
23000     [2.95e+00, 3.19e-02, 3.37e+00]    [0.00e+00, 3.96e+00, 3.37e+00]    [4.43e+00, 3.96e+00, 1.88e+00, 1.70e+00]    
24000     [2.94e+00, 1.20e-01, 3.34e+00]    [0.00e+00, 3.87e+00, 3.34e+00]    [4.34e+00, 3.87e+00, 1.75e+00, 1.59e+00]    
25000     [2.93e+00, 2.70e-02, 3.31e+00]    [0.00e+00, 3.96e+00, 3.31e+00]    [4.43e+00, 3.96e+00, 1.80e+00, 1.62e+00]    
26000     [2.90e+00, 2.03e-02, 3.29e+00]    [0.00e+00, 3.93e+00, 3.29e+00]    [4.40e+00, 3.93e+00, 1.78e+00, 1.60e+00]    
27000     [2.97e+00, 4.71e-02, 3.27e+00]    [0.00e+00, 4.06e+00, 3.27e+00]    [4.54e+00, 4.06e+00, 1.82e+00, 1.62e+00]    
28000     [2.90e+00, 2.19e-02, 3.24e+00]    [0.00e+00, 3.97e+00, 3.24e+00]    [4.45e+00, 3.97e+00, 1.76e+00, 1.57e+00]    
29000     [2.87e+00, 1.78e-02, 3.22e+00]    [0.00e+00, 3.98e+00, 3.22e+00]    [4.47e+00, 3.98e+00, 1.73e+00, 1.53e+00]    
30000     [2.86e+00, 3.97e-02, 3.20e+00]    [0.00e+00, 3.93e+00, 3.20e+00]    [4.42e+00, 3.93e+00, 1.67e+00, 1.47e+00]    

Best model at step 30000:
  train loss: 6.10e+00
  test loss: 7.13e+00
  test metric: [4.42e+00, 3.93e+00, 1.67e+00, 1.47e+00]

'train' took 78.862840 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.483119 s

'compile' took 2.201521 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.03e+02, 2.91e+00]    [0.00e+00, 1.01e+02, 2.91e+00]    [1.01e+02, 1.01e+02, 1.36e+00, 1.36e+00]    
1000      [8.24e+01, 5.30e+01, 2.91e+00]    [0.00e+00, 8.99e+01, 2.91e+00]    [8.99e+01, 8.99e+01, 1.71e+01, 1.72e+01]    
2000      [7.79e+01, 2.01e+01, 3.30e+00]    [0.00e+00, 7.66e+01, 3.30e+00]    [7.66e+01, 7.66e+01, 1.86e+01, 1.80e+01]    
3000      [6.61e+01, 9.98e+00, 3.55e+00]    [0.00e+00, 6.77e+01, 3.55e+00]    [6.80e+01, 6.77e+01, 1.63e+01, 1.56e+01]    
4000      [5.26e+01, 2.12e+00, 3.84e+00]    [0.00e+00, 5.60e+01, 3.84e+00]    [5.67e+01, 5.60e+01, 1.70e+01, 1.67e+01]    
5000      [4.06e+01, 2.01e-01, 4.04e+00]    [0.00e+00, 4.47e+01, 4.04e+00]    [4.57e+01, 4.47e+01, 1.80e+01, 1.80e+01]    
6000      [2.94e+01, 4.30e-01, 4.22e+00]    [0.00e+00, 3.34e+01, 4.22e+00]    [3.40e+01, 3.34e+01, 1.99e+01, 1.94e+01]    
7000      [2.10e+01, 2.99e-01, 4.38e+00]    [0.00e+00, 2.82e+01, 4.38e+00]    [2.84e+01, 2.82e+01, 1.79e+01, 1.76e+01]    
8000      [1.67e+01, 1.38e-01, 4.43e+00]    [0.00e+00, 2.44e+01, 4.43e+00]    [2.44e+01, 2.44e+01, 1.55e+01, 1.52e+01]    
9000      [1.34e+01, 2.87e-01, 4.40e+00]    [0.00e+00, 1.96e+01, 4.40e+00]    [1.96e+01, 1.96e+01, 1.30e+01, 1.28e+01]    
10000     [1.05e+01, 6.46e-01, 4.37e+00]    [0.00e+00, 1.46e+01, 4.37e+00]    [1.47e+01, 1.46e+01, 1.08e+01, 1.07e+01]    
11000     [7.81e+00, 8.00e-02, 4.34e+00]    [0.00e+00, 1.07e+01, 4.34e+00]    [1.08e+01, 1.07e+01, 8.52e+00, 8.47e+00]    
12000     [6.18e+00, 4.56e-01, 4.31e+00]    [0.00e+00, 8.65e+00, 4.31e+00]    [8.65e+00, 8.65e+00, 6.14e+00, 6.15e+00]    
13000     [5.29e+00, 4.81e-01, 4.27e+00]    [0.00e+00, 7.00e+00, 4.27e+00]    [6.91e+00, 7.00e+00, 4.68e+00, 4.64e+00]    
14000     [4.39e+00, 7.52e-02, 4.23e+00]    [0.00e+00, 5.32e+00, 4.23e+00]    [5.14e+00, 5.32e+00, 4.02e+00, 3.88e+00]    
15000     [3.88e+00, 4.41e-02, 4.20e+00]    [0.00e+00, 4.47e+00, 4.20e+00]    [4.23e+00, 4.47e+00, 4.11e+00, 3.86e+00]    
16000     [3.57e+00, 1.22e-01, 4.16e+00]    [0.00e+00, 4.19e+00, 4.16e+00]    [4.17e+00, 4.19e+00, 3.95e+00, 3.84e+00]    
17000     [3.44e+00, 5.67e-01, 4.13e+00]    [0.00e+00, 4.30e+00, 4.13e+00]    [4.35e+00, 4.30e+00, 3.63e+00, 3.47e+00]    
18000     [3.37e+00, 6.58e-01, 4.10e+00]    [0.00e+00, 4.41e+00, 4.10e+00]    [4.46e+00, 4.41e+00, 3.48e+00, 3.26e+00]    
19000     [3.25e+00, 2.10e-01, 4.07e+00]    [0.00e+00, 4.44e+00, 4.07e+00]    [4.54e+00, 4.44e+00, 3.38e+00, 3.20e+00]    
20000     [3.18e+00, 1.59e-01, 4.05e+00]    [0.00e+00, 4.48e+00, 4.05e+00]    [4.65e+00, 4.48e+00, 3.26e+00, 3.09e+00]    
21000     [3.16e+00, 3.63e-01, 4.02e+00]    [0.00e+00, 4.44e+00, 4.02e+00]    [4.64e+00, 4.44e+00, 3.12e+00, 2.93e+00]    
22000     [3.10e+00, 5.93e-02, 4.00e+00]    [0.00e+00, 4.46e+00, 4.00e+00]    [4.68e+00, 4.46e+00, 3.07e+00, 2.91e+00]    
23000     [3.14e+00, 3.77e-01, 3.98e+00]    [0.00e+00, 4.51e+00, 3.98e+00]    [4.73e+00, 4.51e+00, 3.04e+00, 2.90e+00]    
24000     [3.09e+00, 2.79e-01, 3.96e+00]    [0.00e+00, 4.45e+00, 3.96e+00]    [4.68e+00, 4.45e+00, 2.95e+00, 2.81e+00]    
25000     [3.02e+00, 3.28e-02, 3.94e+00]    [0.00e+00, 4.39e+00, 3.94e+00]    [4.64e+00, 4.39e+00, 2.87e+00, 2.72e+00]    
26000     [3.06e+00, 5.42e-01, 3.92e+00]    [0.00e+00, 4.31e+00, 3.92e+00]    [4.58e+00, 4.31e+00, 2.78e+00, 2.58e+00]    
27000     [3.08e+00, 6.25e-01, 3.91e+00]    [0.00e+00, 4.32e+00, 3.91e+00]    [4.61e+00, 4.32e+00, 2.73e+00, 2.52e+00]    
28000     [2.96e+00, 4.52e-02, 3.89e+00]    [0.00e+00, 4.31e+00, 3.89e+00]    [4.60e+00, 4.31e+00, 2.75e+00, 2.56e+00]    
29000     [2.95e+00, 8.92e-02, 3.87e+00]    [0.00e+00, 4.26e+00, 3.87e+00]    [4.55e+00, 4.26e+00, 2.70e+00, 2.47e+00]    
30000     [2.93e+00, 6.79e-02, 3.86e+00]    [0.00e+00, 4.26e+00, 3.86e+00]    [4.56e+00, 4.26e+00, 2.66e+00, 2.43e+00]    

Best model at step 30000:
  train loss: 6.85e+00
  test loss: 8.12e+00
  test metric: [4.56e+00, 4.26e+00, 2.66e+00, 2.43e+00]

'train' took 79.267259 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.478719 s

'compile' took 2.166206 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.05e+02, 2.87e+00]    [0.00e+00, 1.00e+02, 2.87e+00]    [1.00e+02, 1.00e+02, 1.81e+00, 1.81e+00]    
1000      [8.19e+01, 5.60e+01, 2.78e+00]    [0.00e+00, 9.12e+01, 2.78e+00]    [9.12e+01, 9.12e+01, 1.55e+01, 1.56e+01]    
2000      [7.47e+01, 3.87e+01, 3.00e+00]    [0.00e+00, 7.82e+01, 3.00e+00]    [7.82e+01, 7.82e+01, 2.16e+01, 2.13e+01]    
3000      [6.16e+01, 2.83e+01, 3.24e+00]    [0.00e+00, 6.39e+01, 3.24e+00]    [6.42e+01, 6.39e+01, 1.94e+01, 1.89e+01]    
4000      [4.25e+01, 1.41e+01, 3.63e+00]    [0.00e+00, 4.32e+01, 3.63e+00]    [4.43e+01, 4.32e+01, 2.02e+01, 2.01e+01]    
5000      [2.41e+01, 7.09e+00, 3.94e+00]    [0.00e+00, 2.48e+01, 3.94e+00]    [2.61e+01, 2.48e+01, 1.55e+01, 1.48e+01]    
6000      [9.64e+00, 3.23e-01, 4.21e+00]    [0.00e+00, 1.09e+01, 4.21e+00]    [1.09e+01, 1.09e+01, 1.00e+01, 9.07e+00]    
7000      [7.81e+00, 2.44e-01, 4.19e+00]    [0.00e+00, 9.80e+00, 4.19e+00]    [9.55e+00, 9.80e+00, 8.39e+00, 7.68e+00]    
8000      [6.86e+00, 9.01e-01, 4.15e+00]    [0.00e+00, 8.21e+00, 4.15e+00]    [8.05e+00, 8.21e+00, 7.25e+00, 6.75e+00]    
9000      [6.00e+00, 1.95e-01, 4.12e+00]    [0.00e+00, 7.46e+00, 4.12e+00]    [7.16e+00, 7.46e+00, 6.17e+00, 5.68e+00]    
10000     [5.28e+00, 3.10e-01, 4.10e+00]    [0.00e+00, 6.48e+00, 4.10e+00]    [6.20e+00, 6.48e+00, 5.20e+00, 4.80e+00]    
11000     [4.74e+00, 8.29e-02, 4.09e+00]    [0.00e+00, 5.66e+00, 4.09e+00]    [5.32e+00, 5.66e+00, 4.29e+00, 3.85e+00]    
12000     [4.27e+00, 6.89e-02, 4.08e+00]    [0.00e+00, 4.87e+00, 4.08e+00]    [4.64e+00, 4.87e+00, 3.11e+00, 2.82e+00]    
13000     [3.89e+00, 3.18e-01, 4.07e+00]    [0.00e+00, 4.22e+00, 4.07e+00]    [4.06e+00, 4.22e+00, 2.33e+00, 2.18e+00]    
14000     [3.56e+00, 4.06e-01, 4.06e+00]    [0.00e+00, 3.95e+00, 4.06e+00]    [3.89e+00, 3.95e+00, 1.55e+00, 1.59e+00]    
15000     [3.41e+00, 1.55e-01, 4.05e+00]    [0.00e+00, 3.72e+00, 4.05e+00]    [3.69e+00, 3.72e+00, 1.18e+00, 1.23e+00]    
16000     [3.22e+00, 3.35e-01, 4.03e+00]    [0.00e+00, 3.65e+00, 4.03e+00]    [3.64e+00, 3.65e+00, 1.13e+00, 1.16e+00]    
17000     [3.10e+00, 2.15e-01, 4.01e+00]    [0.00e+00, 3.65e+00, 4.01e+00]    [3.65e+00, 3.65e+00, 1.03e+00, 1.01e+00]    
18000     [2.94e+00, 2.87e-01, 3.99e+00]    [0.00e+00, 3.71e+00, 3.99e+00]    [3.73e+00, 3.71e+00, 1.17e+00, 1.11e+00]    
19000     [2.93e+00, 2.20e-01, 3.96e+00]    [0.00e+00, 3.82e+00, 3.96e+00]    [3.85e+00, 3.82e+00, 1.31e+00, 1.25e+00]    
20000     [2.87e+00, 1.36e-01, 3.94e+00]    [0.00e+00, 3.93e+00, 3.94e+00]    [3.97e+00, 3.93e+00, 1.44e+00, 1.37e+00]    
21000     [2.85e+00, 1.62e-01, 3.91e+00]    [0.00e+00, 3.91e+00, 3.91e+00]    [3.96e+00, 3.91e+00, 1.53e+00, 1.46e+00]    
22000     [2.82e+00, 2.94e-01, 3.89e+00]    [0.00e+00, 3.97e+00, 3.89e+00]    [4.03e+00, 3.97e+00, 1.62e+00, 1.55e+00]    
23000     [2.89e+00, 4.70e-01, 3.87e+00]    [0.00e+00, 3.87e+00, 3.87e+00]    [3.94e+00, 3.87e+00, 1.61e+00, 1.53e+00]    
24000     [2.75e+00, 1.60e-01, 3.85e+00]    [0.00e+00, 3.85e+00, 3.85e+00]    [3.93e+00, 3.85e+00, 1.59e+00, 1.50e+00]    
25000     [2.71e+00, 2.99e-01, 3.83e+00]    [0.00e+00, 3.80e+00, 3.83e+00]    [3.89e+00, 3.80e+00, 1.56e+00, 1.45e+00]    
26000     [2.71e+00, 1.68e-01, 3.81e+00]    [0.00e+00, 3.71e+00, 3.81e+00]    [3.81e+00, 3.71e+00, 1.54e+00, 1.41e+00]    
27000     [2.71e+00, 8.30e-02, 3.79e+00]    [0.00e+00, 3.68e+00, 3.79e+00]    [3.78e+00, 3.68e+00, 1.54e+00, 1.40e+00]    
28000     [2.78e+00, 6.78e-01, 3.77e+00]    [0.00e+00, 3.68e+00, 3.77e+00]    [3.79e+00, 3.68e+00, 1.49e+00, 1.29e+00]    
29000     [2.68e+00, 7.21e-02, 3.75e+00]    [0.00e+00, 3.69e+00, 3.75e+00]    [3.81e+00, 3.69e+00, 1.52e+00, 1.34e+00]    
30000     [2.65e+00, 1.66e-01, 3.74e+00]    [0.00e+00, 3.66e+00, 3.74e+00]    [3.78e+00, 3.66e+00, 1.53e+00, 1.35e+00]    

Best model at step 29000:
  train loss: 6.50e+00
  test loss: 7.45e+00
  test metric: [3.81e+00, 3.69e+00, 1.52e+00, 1.34e+00]

'train' took 78.532951 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[3.645388490356909, 3.329443281261103, 4.089817817153617, 3.8999571045187933, 2.5916077510248163, 3.033056791635512, 3.489656328739735, 3.9349612937250926, 4.262736355655355, 3.6928887556054364]
E* 4 3.596951396967637 0.481203282747518
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.457775 s

'compile' took 2.258960 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.06e+02, 2.82e+00]    [0.00e+00, 9.97e+01, 2.82e+00]    [9.97e+01, 9.97e+01, 2.11e+00, 2.11e+00]    
1000      [7.94e+01, 4.06e+01, 2.94e+00]    [0.00e+00, 8.85e+01, 2.94e+00]    [8.86e+01, 8.85e+01, 6.95e+00, 6.97e+00]    
2000      [6.71e+01, 3.20e+01, 3.13e+00]    [0.00e+00, 7.47e+01, 3.13e+00]    [7.49e+01, 7.47e+01, 1.35e+01, 1.36e+01]    
3000      [4.29e+01, 1.42e+01, 3.60e+00]    [0.00e+00, 4.72e+01, 3.60e+00]    [4.81e+01, 4.72e+01, 3.02e+01, 3.03e+01]    
4000      [2.84e+01, 2.18e+00, 3.83e+00]    [0.00e+00, 3.51e+01, 3.83e+00]    [3.41e+01, 3.51e+01, 2.85e+01, 2.61e+01]    
5000      [2.15e+01, 2.33e-01, 3.82e+00]    [0.00e+00, 2.61e+01, 3.82e+00]    [2.56e+01, 2.61e+01, 2.55e+01, 2.36e+01]    
6000      [1.80e+01, 2.28e-01, 3.80e+00]    [0.00e+00, 2.09e+01, 3.80e+00]    [2.12e+01, 2.09e+01, 2.15e+01, 2.08e+01]    
7000      [1.46e+01, 4.12e-01, 3.81e+00]    [0.00e+00, 1.63e+01, 3.81e+00]    [1.65e+01, 1.63e+01, 1.74e+01, 1.70e+01]    
8000      [1.11e+01, 5.45e-01, 3.83e+00]    [0.00e+00, 1.12e+01, 3.83e+00]    [1.13e+01, 1.12e+01, 1.26e+01, 1.26e+01]    
9000      [7.97e+00, 5.20e-01, 3.87e+00]    [0.00e+00, 7.05e+00, 3.87e+00]    [6.95e+00, 7.05e+00, 7.54e+00, 7.54e+00]    
10000     [5.60e+00, 2.32e-01, 3.90e+00]    [0.00e+00, 4.29e+00, 3.90e+00]    [4.06e+00, 4.29e+00, 2.96e+00, 3.10e+00]    
11000     [4.83e+00, 6.59e-02, 3.89e+00]    [0.00e+00, 3.07e+00, 3.89e+00]    [2.79e+00, 3.07e+00, 1.73e+00, 1.75e+00]    
12000     [4.47e+00, 9.89e-02, 3.87e+00]    [0.00e+00, 2.60e+00, 3.87e+00]    [2.30e+00, 2.60e+00, 1.50e+00, 1.43e+00]    
13000     [4.25e+00, 3.75e-02, 3.84e+00]    [0.00e+00, 2.44e+00, 3.84e+00]    [2.20e+00, 2.44e+00, 1.35e+00, 1.38e+00]    
14000     [4.06e+00, 8.84e-02, 3.82e+00]    [0.00e+00, 2.34e+00, 3.82e+00]    [2.06e+00, 2.34e+00, 1.42e+00, 1.32e+00]    
15000     [3.96e+00, 8.00e-02, 3.79e+00]    [0.00e+00, 2.28e+00, 3.79e+00]    [2.13e+00, 2.28e+00, 1.36e+00, 1.40e+00]    
16000     [3.85e+00, 5.85e-02, 3.77e+00]    [0.00e+00, 2.23e+00, 3.77e+00]    [2.21e+00, 2.23e+00, 1.27e+00, 1.47e+00]    
17000     [3.77e+00, 1.05e-01, 3.75e+00]    [0.00e+00, 2.23e+00, 3.75e+00]    [2.30e+00, 2.23e+00, 1.24e+00, 1.49e+00]    
18000     [3.70e+00, 8.08e-02, 3.74e+00]    [0.00e+00, 2.34e+00, 3.74e+00]    [2.43e+00, 2.34e+00, 1.24e+00, 1.43e+00]    
19000     [3.66e+00, 2.90e-01, 3.72e+00]    [0.00e+00, 2.37e+00, 3.72e+00]    [2.50e+00, 2.37e+00, 1.23e+00, 1.47e+00]    
20000     [3.56e+00, 4.00e-02, 3.70e+00]    [0.00e+00, 2.41e+00, 3.70e+00]    [2.53e+00, 2.41e+00, 1.25e+00, 1.49e+00]    
21000     [3.51e+00, 4.20e-02, 3.68e+00]    [0.00e+00, 2.41e+00, 3.68e+00]    [2.58e+00, 2.41e+00, 1.25e+00, 1.53e+00]    
22000     [3.47e+00, 5.12e-02, 3.66e+00]    [0.00e+00, 2.37e+00, 3.66e+00]    [2.59e+00, 2.37e+00, 1.25e+00, 1.58e+00]    
23000     [3.48e+00, 2.45e-01, 3.65e+00]    [0.00e+00, 2.35e+00, 3.65e+00]    [2.58e+00, 2.35e+00, 1.27e+00, 1.62e+00]    
24000     [3.43e+00, 1.41e-01, 3.63e+00]    [0.00e+00, 2.37e+00, 3.63e+00]    [2.65e+00, 2.37e+00, 1.25e+00, 1.61e+00]    
25000     [3.45e+00, 2.79e-01, 3.61e+00]    [0.00e+00, 2.41e+00, 3.61e+00]    [2.69e+00, 2.41e+00, 1.27e+00, 1.57e+00]    
26000     [3.37e+00, 1.35e-01, 3.59e+00]    [0.00e+00, 2.40e+00, 3.59e+00]    [2.69e+00, 2.40e+00, 1.29e+00, 1.56e+00]    
27000     [3.30e+00, 1.33e-01, 3.58e+00]    [0.00e+00, 2.35e+00, 3.58e+00]    [2.65e+00, 2.35e+00, 1.30e+00, 1.62e+00]    
28000     [3.33e+00, 3.76e-01, 3.56e+00]    [0.00e+00, 2.37e+00, 3.56e+00]    [2.67e+00, 2.37e+00, 1.31e+00, 1.62e+00]    
29000     [3.22e+00, 3.48e-02, 3.55e+00]    [0.00e+00, 2.38e+00, 3.55e+00]    [2.67e+00, 2.38e+00, 1.31e+00, 1.64e+00]    
30000     [3.20e+00, 1.02e-01, 3.53e+00]    [0.00e+00, 2.37e+00, 3.53e+00]    [2.67e+00, 2.37e+00, 1.32e+00, 1.64e+00]    

Best model at step 29000:
  train loss: 6.80e+00
  test loss: 5.92e+00
  test metric: [2.67e+00, 2.38e+00, 1.31e+00, 1.64e+00]

'train' took 81.421225 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.479718 s

'compile' took 2.315806 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+02, 1.01e+02, 2.75e+00]    [0.00e+00, 1.04e+02, 2.75e+00]    [1.04e+02, 1.04e+02, 6.02e+00, 6.02e+00]    
1000      [6.07e+01, 5.97e+01, 3.35e+00]    [0.00e+00, 6.23e+01, 3.35e+00]    [6.28e+01, 6.23e+01, 3.45e+01, 3.45e+01]    
2000      [1.84e+01, 5.15e+00, 4.37e+00]    [0.00e+00, 2.52e+01, 4.37e+00]    [2.54e+01, 2.52e+01, 2.65e+01, 2.46e+01]    
3000      [1.02e+01, 2.30e+00, 4.36e+00]    [0.00e+00, 1.08e+01, 4.36e+00]    [1.11e+01, 1.08e+01, 1.65e+01, 1.57e+01]    
4000      [5.86e+00, 1.94e+00, 4.36e+00]    [0.00e+00, 6.39e+00, 4.36e+00]    [6.43e+00, 6.39e+00, 7.46e+00, 7.02e+00]    
5000      [4.54e+00, 1.45e+00, 4.33e+00]    [0.00e+00, 5.05e+00, 4.33e+00]    [5.08e+00, 5.05e+00, 3.04e+00, 3.00e+00]    
6000      [4.04e+00, 9.15e-01, 4.29e+00]    [0.00e+00, 4.37e+00, 4.29e+00]    [4.18e+00, 4.37e+00, 1.56e+00, 1.92e+00]    
7000      [3.64e+00, 5.33e-01, 4.26e+00]    [0.00e+00, 3.93e+00, 4.26e+00]    [3.36e+00, 3.93e+00, 1.20e+00, 2.21e+00]    
8000      [3.38e+00, 8.07e-02, 4.24e+00]    [0.00e+00, 4.14e+00, 4.24e+00]    [3.06e+00, 4.14e+00, 1.55e+00, 2.60e+00]    
9000      [3.15e+00, 4.19e-02, 4.19e+00]    [0.00e+00, 4.30e+00, 4.19e+00]    [2.78e+00, 4.30e+00, 1.61e+00, 2.55e+00]    
10000     [3.01e+00, 2.40e-02, 4.13e+00]    [0.00e+00, 4.04e+00, 4.13e+00]    [2.70e+00, 4.04e+00, 1.45e+00, 2.58e+00]    
11000     [2.91e+00, 2.48e-02, 4.08e+00]    [0.00e+00, 4.04e+00, 4.08e+00]    [2.67e+00, 4.04e+00, 1.58e+00, 2.65e+00]    
12000     [2.82e+00, 2.50e-02, 4.03e+00]    [0.00e+00, 3.95e+00, 4.03e+00]    [2.63e+00, 3.95e+00, 1.64e+00, 2.67e+00]    
13000     [2.77e+00, 1.06e-02, 3.98e+00]    [0.00e+00, 3.81e+00, 3.98e+00]    [2.68e+00, 3.81e+00, 1.63e+00, 2.70e+00]    
14000     [2.71e+00, 2.50e-02, 3.93e+00]    [0.00e+00, 3.63e+00, 3.93e+00]    [2.69e+00, 3.63e+00, 1.61e+00, 2.67e+00]    
15000     [2.67e+00, 3.11e-02, 3.88e+00]    [0.00e+00, 3.66e+00, 3.88e+00]    [2.72e+00, 3.66e+00, 1.70e+00, 2.72e+00]    
16000     [2.60e+00, 6.65e-03, 3.84e+00]    [0.00e+00, 3.55e+00, 3.84e+00]    [2.70e+00, 3.55e+00, 1.70e+00, 2.72e+00]    
17000     [2.57e+00, 2.85e-02, 3.80e+00]    [0.00e+00, 3.54e+00, 3.80e+00]    [2.67e+00, 3.54e+00, 1.67e+00, 2.69e+00]    
18000     [2.56e+00, 2.02e-02, 3.76e+00]    [0.00e+00, 3.45e+00, 3.76e+00]    [2.72e+00, 3.45e+00, 1.64e+00, 2.71e+00]    
19000     [2.51e+00, 7.63e-03, 3.72e+00]    [0.00e+00, 3.40e+00, 3.72e+00]    [2.73e+00, 3.40e+00, 1.63e+00, 2.72e+00]    
20000     [2.51e+00, 2.17e-02, 3.68e+00]    [0.00e+00, 3.33e+00, 3.68e+00]    [2.77e+00, 3.33e+00, 1.64e+00, 2.71e+00]    
21000     [2.50e+00, 1.77e-02, 3.65e+00]    [0.00e+00, 3.32e+00, 3.65e+00]    [2.77e+00, 3.32e+00, 1.63e+00, 2.74e+00]    
22000     [2.45e+00, 1.51e-02, 3.62e+00]    [0.00e+00, 3.45e+00, 3.62e+00]    [2.67e+00, 3.45e+00, 1.64e+00, 2.69e+00]    
23000     [2.44e+00, 2.02e-02, 3.58e+00]    [0.00e+00, 3.40e+00, 3.58e+00]    [2.68e+00, 3.40e+00, 1.65e+00, 2.71e+00]    
24000     [2.42e+00, 1.93e-02, 3.55e+00]    [0.00e+00, 3.34e+00, 3.55e+00]    [2.71e+00, 3.34e+00, 1.66e+00, 2.76e+00]    
25000     [2.40e+00, 1.86e-02, 3.52e+00]    [0.00e+00, 3.34e+00, 3.52e+00]    [2.68e+00, 3.34e+00, 1.67e+00, 2.73e+00]    
26000     [2.40e+00, 1.90e-02, 3.50e+00]    [0.00e+00, 3.27e+00, 3.50e+00]    [2.73e+00, 3.27e+00, 1.68e+00, 2.70e+00]    
27000     [2.37e+00, 1.25e-02, 3.47e+00]    [0.00e+00, 3.29e+00, 3.47e+00]    [2.73e+00, 3.29e+00, 1.68e+00, 2.74e+00]    
28000     [2.37e+00, 1.58e-02, 3.44e+00]    [0.00e+00, 3.29e+00, 3.44e+00]    [2.68e+00, 3.29e+00, 1.67e+00, 2.69e+00]    
29000     [2.36e+00, 3.10e-02, 3.42e+00]    [0.00e+00, 3.27e+00, 3.42e+00]    [2.65e+00, 3.27e+00, 1.67e+00, 2.63e+00]    
30000     [2.35e+00, 1.96e-02, 3.40e+00]    [0.00e+00, 3.21e+00, 3.40e+00]    [2.69e+00, 3.21e+00, 1.68e+00, 2.64e+00]    

Best model at step 30000:
  train loss: 5.77e+00
  test loss: 6.61e+00
  test metric: [2.69e+00, 3.21e+00, 1.68e+00, 2.64e+00]

'train' took 92.448732 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.582443 s

'compile' took 2.685823 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.98e+01, 1.00e+02, 2.68e+00]    [0.00e+00, 9.99e+01, 2.68e+00]    [9.99e+01, 9.99e+01, 2.69e-01, 2.69e-01]    
1000      [8.14e+01, 5.80e+01, 2.68e+00]    [0.00e+00, 9.01e+01, 2.68e+00]    [9.01e+01, 9.01e+01, 1.87e+01, 1.87e+01]    
2000      [6.94e+01, 3.89e+01, 3.05e+00]    [0.00e+00, 7.54e+01, 3.05e+00]    [7.57e+01, 7.54e+01, 2.17e+01, 2.21e+01]    
3000      [5.03e+01, 1.84e+01, 3.60e+00]    [0.00e+00, 5.55e+01, 3.60e+00]    [5.72e+01, 5.55e+01, 2.48e+01, 2.58e+01]    
4000      [3.58e+01, 5.81e+00, 3.88e+00]    [0.00e+00, 3.71e+01, 3.88e+00]    [3.92e+01, 3.71e+01, 2.16e+01, 2.13e+01]    
5000      [2.10e+01, 1.07e+00, 3.99e+00]    [0.00e+00, 1.99e+01, 3.99e+00]    [2.25e+01, 1.99e+01, 1.32e+01, 1.26e+01]    
6000      [8.83e+00, 1.62e+00, 4.12e+00]    [0.00e+00, 5.24e+00, 4.12e+00]    [7.19e+00, 5.24e+00, 4.71e+00, 4.11e+00]    
7000      [4.48e+00, 2.27e+00, 4.16e+00]    [0.00e+00, 4.05e+00, 4.16e+00]    [2.73e+00, 4.05e+00, 2.48e+00, 1.97e+00]    
8000      [3.95e+00, 1.91e+00, 4.12e+00]    [0.00e+00, 3.83e+00, 4.12e+00]    [3.14e+00, 3.83e+00, 1.92e+00, 2.43e+00]    
9000      [3.83e+00, 1.74e+00, 4.09e+00]    [0.00e+00, 4.13e+00, 4.09e+00]    [3.56e+00, 4.13e+00, 1.70e+00, 2.36e+00]    
10000     [3.71e+00, 1.55e+00, 4.06e+00]    [0.00e+00, 4.23e+00, 4.06e+00]    [3.78e+00, 4.23e+00, 1.71e+00, 2.36e+00]    
11000     [3.59e+00, 1.12e+00, 4.03e+00]    [0.00e+00, 4.19e+00, 4.03e+00]    [3.86e+00, 4.19e+00, 1.76e+00, 2.29e+00]    
12000     [3.49e+00, 9.24e-01, 4.02e+00]    [0.00e+00, 4.28e+00, 4.02e+00]    [4.06e+00, 4.28e+00, 2.06e+00, 2.47e+00]    
13000     [3.45e+00, 8.32e-01, 4.02e+00]    [0.00e+00, 4.52e+00, 4.02e+00]    [4.31e+00, 4.52e+00, 2.43e+00, 2.57e+00]    
14000     [3.43e+00, 6.69e-01, 4.02e+00]    [0.00e+00, 4.69e+00, 4.02e+00]    [4.57e+00, 4.69e+00, 2.50e+00, 2.58e+00]    
15000     [3.41e+00, 3.09e-01, 4.02e+00]    [0.00e+00, 4.96e+00, 4.02e+00]    [4.91e+00, 4.96e+00, 2.61e+00, 2.64e+00]    
16000     [3.40e+00, 6.62e-02, 4.01e+00]    [0.00e+00, 5.02e+00, 4.01e+00]    [5.04e+00, 5.02e+00, 2.51e+00, 2.49e+00]    
17000     [3.35e+00, 3.07e-01, 3.99e+00]    [0.00e+00, 4.93e+00, 3.99e+00]    [4.99e+00, 4.93e+00, 2.36e+00, 2.31e+00]    
18000     [3.26e+00, 2.00e-01, 3.96e+00]    [0.00e+00, 4.95e+00, 3.96e+00]    [5.01e+00, 4.95e+00, 2.19e+00, 2.14e+00]    
19000     [3.18e+00, 6.23e-02, 3.94e+00]    [0.00e+00, 4.84e+00, 3.94e+00]    [4.90e+00, 4.84e+00, 2.07e+00, 2.02e+00]    
20000     [3.11e+00, 1.53e-01, 3.91e+00]    [0.00e+00, 4.80e+00, 3.91e+00]    [4.86e+00, 4.80e+00, 1.99e+00, 1.93e+00]    
21000     [3.07e+00, 5.15e-02, 3.89e+00]    [0.00e+00, 4.77e+00, 3.89e+00]    [4.83e+00, 4.77e+00, 1.94e+00, 1.88e+00]    
22000     [3.03e+00, 7.58e-02, 3.87e+00]    [0.00e+00, 4.69e+00, 3.87e+00]    [4.75e+00, 4.69e+00, 1.87e+00, 1.81e+00]    
23000     [3.01e+00, 1.17e-01, 3.84e+00]    [0.00e+00, 4.69e+00, 3.84e+00]    [4.75e+00, 4.69e+00, 1.85e+00, 1.80e+00]    
24000     [2.99e+00, 1.29e-01, 3.82e+00]    [0.00e+00, 4.63e+00, 3.82e+00]    [4.68e+00, 4.63e+00, 1.81e+00, 1.76e+00]    
25000     [2.94e+00, 8.03e-02, 3.80e+00]    [0.00e+00, 4.61e+00, 3.80e+00]    [4.66e+00, 4.61e+00, 1.76e+00, 1.72e+00]    
26000     [2.95e+00, 6.97e-02, 3.78e+00]    [0.00e+00, 4.58e+00, 3.78e+00]    [4.63e+00, 4.58e+00, 1.77e+00, 1.72e+00]    
27000     [2.92e+00, 1.24e-01, 3.76e+00]    [0.00e+00, 4.56e+00, 3.76e+00]    [4.61e+00, 4.56e+00, 1.75e+00, 1.71e+00]    
28000     [2.88e+00, 9.73e-02, 3.74e+00]    [0.00e+00, 4.50e+00, 3.74e+00]    [4.56e+00, 4.50e+00, 1.69e+00, 1.63e+00]    
29000     [2.87e+00, 9.15e-02, 3.72e+00]    [0.00e+00, 4.51e+00, 3.72e+00]    [4.58e+00, 4.51e+00, 1.71e+00, 1.65e+00]    
30000     [2.86e+00, 1.15e-01, 3.70e+00]    [0.00e+00, 4.41e+00, 3.70e+00]    [4.48e+00, 4.41e+00, 1.65e+00, 1.59e+00]    

Best model at step 30000:
  train loss: 6.67e+00
  test loss: 8.11e+00
  test metric: [4.48e+00, 4.41e+00, 1.65e+00, 1.59e+00]

'train' took 85.890269 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.483706 s

'compile' took 2.241009 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.82e+01, 9.83e+01, 2.71e+00]    [0.00e+00, 9.91e+01, 2.71e+00]    [9.91e+01, 9.91e+01, 1.08e+00, 1.08e+00]    
1000      [7.95e+01, 6.02e+01, 2.84e+00]    [0.00e+00, 8.43e+01, 2.84e+00]    [8.43e+01, 8.43e+01, 1.94e+01, 1.94e+01]    
2000      [5.44e+01, 1.75e+01, 3.62e+00]    [0.00e+00, 6.50e+01, 3.62e+00]    [6.51e+01, 6.50e+01, 3.08e+01, 3.07e+01]    
3000      [4.09e+01, 1.04e+01, 3.81e+00]    [0.00e+00, 5.44e+01, 3.81e+00]    [5.45e+01, 5.44e+01, 2.53e+01, 2.52e+01]    
4000      [3.01e+01, 5.18e+00, 3.97e+00]    [0.00e+00, 4.31e+01, 3.97e+00]    [4.32e+01, 4.31e+01, 1.77e+01, 1.77e+01]    
5000      [1.72e+01, 2.99e+00, 4.21e+00]    [0.00e+00, 2.86e+01, 4.21e+00]    [2.86e+01, 2.86e+01, 1.25e+01, 1.25e+01]    
6000      [1.16e+01, 1.87e+00, 4.26e+00]    [0.00e+00, 2.02e+01, 4.26e+00]    [2.01e+01, 2.02e+01, 1.33e+01, 1.33e+01]    
7000      [8.51e+00, 9.54e-01, 4.26e+00]    [0.00e+00, 1.44e+01, 4.26e+00]    [1.41e+01, 1.44e+01, 1.29e+01, 1.30e+01]    
8000      [7.15e+00, 1.52e-01, 4.24e+00]    [0.00e+00, 1.14e+01, 4.24e+00]    [1.12e+01, 1.14e+01, 1.23e+01, 1.26e+01]    
9000      [6.21e+00, 2.17e-01, 4.19e+00]    [0.00e+00, 9.65e+00, 4.19e+00]    [9.26e+00, 9.65e+00, 1.16e+01, 1.19e+01]    
10000     [5.67e+00, 1.90e-01, 4.16e+00]    [0.00e+00, 8.81e+00, 4.16e+00]    [8.40e+00, 8.81e+00, 1.13e+01, 1.16e+01]    
11000     [5.39e+00, 1.65e-01, 4.12e+00]    [0.00e+00, 8.01e+00, 4.12e+00]    [8.05e+00, 8.01e+00, 1.07e+01, 1.13e+01]    
12000     [5.18e+00, 2.54e-02, 4.08e+00]    [0.00e+00, 7.76e+00, 4.08e+00]    [7.95e+00, 7.76e+00, 1.04e+01, 1.11e+01]    
13000     [5.05e+00, 1.37e-01, 4.04e+00]    [0.00e+00, 7.62e+00, 4.04e+00]    [7.87e+00, 7.62e+00, 1.02e+01, 1.09e+01]    
14000     [4.94e+00, 1.12e-01, 4.01e+00]    [0.00e+00, 7.37e+00, 4.01e+00]    [7.75e+00, 7.37e+00, 9.93e+00, 1.07e+01]    
15000     [4.83e+00, 1.61e-01, 3.97e+00]    [0.00e+00, 7.23e+00, 3.97e+00]    [7.63e+00, 7.23e+00, 9.86e+00, 1.06e+01]    
16000     [4.77e+00, 1.97e-01, 3.94e+00]    [0.00e+00, 7.11e+00, 3.94e+00]    [7.51e+00, 7.11e+00, 9.48e+00, 1.02e+01]    
17000     [4.65e+00, 2.02e-01, 3.91e+00]    [0.00e+00, 6.95e+00, 3.91e+00]    [7.35e+00, 6.95e+00, 9.48e+00, 1.02e+01]    
18000     [4.55e+00, 3.47e-02, 3.88e+00]    [0.00e+00, 6.75e+00, 3.88e+00]    [7.14e+00, 6.75e+00, 9.22e+00, 9.92e+00]    
19000     [4.50e+00, 5.03e-02, 3.85e+00]    [0.00e+00, 6.59e+00, 3.85e+00]    [6.98e+00, 6.59e+00, 9.00e+00, 9.70e+00]    
20000     [4.38e+00, 1.68e-01, 3.82e+00]    [0.00e+00, 6.56e+00, 3.82e+00]    [6.94e+00, 6.56e+00, 8.89e+00, 9.57e+00]    
21000     [4.30e+00, 2.52e-02, 3.79e+00]    [0.00e+00, 6.46e+00, 3.79e+00]    [6.83e+00, 6.46e+00, 8.60e+00, 9.26e+00]    
22000     [4.24e+00, 8.33e-02, 3.77e+00]    [0.00e+00, 6.37e+00, 3.77e+00]    [6.74e+00, 6.37e+00, 8.42e+00, 9.07e+00]    
23000     [4.16e+00, 1.02e-01, 3.74e+00]    [0.00e+00, 6.32e+00, 3.74e+00]    [6.68e+00, 6.32e+00, 8.23e+00, 8.86e+00]    
24000     [4.13e+00, 6.61e-02, 3.72e+00]    [0.00e+00, 6.26e+00, 3.72e+00]    [6.61e+00, 6.26e+00, 8.01e+00, 8.60e+00]    
25000     [4.06e+00, 7.26e-02, 3.69e+00]    [0.00e+00, 6.13e+00, 3.69e+00]    [6.48e+00, 6.13e+00, 7.76e+00, 8.34e+00]    
26000     [4.02e+00, 3.42e-01, 3.67e+00]    [0.00e+00, 6.09e+00, 3.67e+00]    [6.44e+00, 6.09e+00, 7.78e+00, 8.34e+00]    
27000     [3.96e+00, 3.56e-02, 3.65e+00]    [0.00e+00, 5.98e+00, 3.65e+00]    [6.32e+00, 5.98e+00, 7.51e+00, 8.06e+00]    
28000     [3.94e+00, 1.70e-01, 3.63e+00]    [0.00e+00, 5.92e+00, 3.63e+00]    [6.25e+00, 5.92e+00, 7.30e+00, 7.84e+00]    
29000     [3.91e+00, 1.64e-01, 3.60e+00]    [0.00e+00, 5.85e+00, 3.60e+00]    [6.18e+00, 5.85e+00, 7.32e+00, 7.86e+00]    
30000     [3.89e+00, 1.58e-01, 3.58e+00]    [0.00e+00, 5.77e+00, 3.58e+00]    [6.10e+00, 5.77e+00, 7.19e+00, 7.72e+00]    

Best model at step 30000:
  train loss: 7.63e+00
  test loss: 9.35e+00
  test metric: [6.10e+00, 5.77e+00, 7.19e+00, 7.72e+00]

'train' took 86.707088 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.481709 s

'compile' took 2.238014 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.00e+02, 2.74e+00]    [0.00e+00, 1.00e+02, 2.74e+00]    [1.00e+02, 1.00e+02, 1.29e+00, 1.29e+00]    
1000      [8.02e+01, 7.62e+01, 2.72e+00]    [0.00e+00, 8.39e+01, 2.72e+00]    [8.39e+01, 8.39e+01, 1.50e+01, 1.50e+01]    
2000      [6.64e+01, 5.96e+01, 3.12e+00]    [0.00e+00, 6.56e+01, 3.12e+00]    [6.57e+01, 6.56e+01, 1.80e+01, 1.81e+01]    
3000      [2.95e+01, 1.61e+01, 4.12e+00]    [0.00e+00, 2.77e+01, 4.12e+00]    [2.83e+01, 2.77e+01, 1.57e+01, 1.53e+01]    
4000      [6.85e+00, 2.94e+00, 4.46e+00]    [0.00e+00, 9.10e+00, 4.46e+00]    [9.50e+00, 9.10e+00, 6.96e+00, 6.95e+00]    
5000      [5.41e+00, 7.43e-02, 4.38e+00]    [0.00e+00, 7.17e+00, 4.38e+00]    [7.34e+00, 7.17e+00, 5.94e+00, 5.89e+00]    
6000      [4.95e+00, 1.96e-01, 4.31e+00]    [0.00e+00, 6.88e+00, 4.31e+00]    [6.91e+00, 6.88e+00, 5.10e+00, 5.00e+00]    
7000      [4.63e+00, 9.48e-02, 4.25e+00]    [0.00e+00, 6.58e+00, 4.25e+00]    [6.60e+00, 6.58e+00, 4.57e+00, 4.52e+00]    
8000      [4.37e+00, 1.50e-01, 4.19e+00]    [0.00e+00, 6.03e+00, 4.19e+00]    [6.04e+00, 6.03e+00, 3.91e+00, 3.89e+00]    
9000      [4.16e+00, 1.44e-01, 4.14e+00]    [0.00e+00, 5.64e+00, 4.14e+00]    [5.65e+00, 5.64e+00, 3.55e+00, 3.55e+00]    
10000     [3.93e+00, 7.30e-02, 4.09e+00]    [0.00e+00, 5.22e+00, 4.09e+00]    [5.22e+00, 5.22e+00, 3.20e+00, 3.21e+00]    
11000     [3.78e+00, 4.76e-02, 4.05e+00]    [0.00e+00, 5.04e+00, 4.05e+00]    [5.04e+00, 5.04e+00, 2.85e+00, 2.87e+00]    
12000     [3.64e+00, 1.67e-01, 4.01e+00]    [0.00e+00, 4.73e+00, 4.01e+00]    [4.72e+00, 4.73e+00, 2.46e+00, 2.46e+00]    
13000     [3.62e+00, 2.21e-01, 3.97e+00]    [0.00e+00, 4.59e+00, 3.97e+00]    [4.58e+00, 4.59e+00, 2.27e+00, 2.30e+00]    
14000     [3.46e+00, 5.14e-02, 3.93e+00]    [0.00e+00, 4.26e+00, 3.93e+00]    [4.24e+00, 4.26e+00, 2.03e+00, 2.03e+00]    
15000     [3.42e+00, 1.13e-01, 3.89e+00]    [0.00e+00, 4.09e+00, 3.89e+00]    [4.07e+00, 4.09e+00, 1.85e+00, 1.89e+00]    
16000     [3.38e+00, 1.46e-01, 3.86e+00]    [0.00e+00, 3.99e+00, 3.86e+00]    [3.95e+00, 3.99e+00, 1.77e+00, 1.80e+00]    
17000     [3.28e+00, 7.69e-02, 3.83e+00]    [0.00e+00, 3.84e+00, 3.83e+00]    [3.80e+00, 3.84e+00, 1.77e+00, 1.70e+00]    
18000     [3.25e+00, 1.36e-01, 3.80e+00]    [0.00e+00, 3.74e+00, 3.80e+00]    [3.70e+00, 3.74e+00, 1.77e+00, 1.62e+00]    
19000     [3.18e+00, 2.13e-02, 3.77e+00]    [0.00e+00, 3.70e+00, 3.77e+00]    [3.65e+00, 3.70e+00, 1.68e+00, 1.52e+00]    
20000     [3.15e+00, 6.02e-02, 3.74e+00]    [0.00e+00, 3.59e+00, 3.74e+00]    [3.54e+00, 3.59e+00, 1.66e+00, 1.47e+00]    
21000     [3.11e+00, 2.36e-02, 3.72e+00]    [0.00e+00, 3.50e+00, 3.72e+00]    [3.44e+00, 3.50e+00, 1.68e+00, 1.44e+00]    
22000     [3.11e+00, 1.29e-01, 3.69e+00]    [0.00e+00, 3.44e+00, 3.69e+00]    [3.38e+00, 3.44e+00, 1.65e+00, 1.40e+00]    
23000     [3.07e+00, 6.25e-02, 3.67e+00]    [0.00e+00, 3.31e+00, 3.67e+00]    [3.25e+00, 3.31e+00, 1.79e+00, 1.46e+00]    
24000     [3.04e+00, 4.53e-02, 3.64e+00]    [0.00e+00, 3.26e+00, 3.64e+00]    [3.19e+00, 3.26e+00, 1.83e+00, 1.47e+00]    
25000     [3.02e+00, 6.96e-02, 3.62e+00]    [0.00e+00, 3.22e+00, 3.62e+00]    [3.15e+00, 3.22e+00, 1.87e+00, 1.49e+00]    
26000     [2.98e+00, 2.41e-02, 3.60e+00]    [0.00e+00, 3.15e+00, 3.60e+00]    [3.08e+00, 3.15e+00, 1.92e+00, 1.51e+00]    
27000     [2.97e+00, 9.18e-02, 3.58e+00]    [0.00e+00, 3.15e+00, 3.58e+00]    [3.07e+00, 3.15e+00, 1.89e+00, 1.49e+00]    
28000     [2.92e+00, 1.88e-02, 3.55e+00]    [0.00e+00, 3.02e+00, 3.55e+00]    [2.99e+00, 3.02e+00, 1.99e+00, 1.62e+00]    
29000     [2.90e+00, 6.49e-02, 3.53e+00]    [0.00e+00, 2.97e+00, 3.53e+00]    [3.01e+00, 2.97e+00, 1.93e+00, 1.65e+00]    
30000     [2.89e+00, 1.08e-01, 3.51e+00]    [0.00e+00, 2.92e+00, 3.51e+00]    [3.01e+00, 2.92e+00, 1.88e+00, 1.68e+00]    

Best model at step 29000:
  train loss: 6.50e+00
  test loss: 6.50e+00
  test metric: [3.01e+00, 2.97e+00, 1.93e+00, 1.65e+00]

'train' took 87.098559 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.523600 s

'compile' took 2.461418 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.01e+02, 2.66e+00]    [0.00e+00, 1.02e+02, 2.66e+00]    [1.02e+02, 1.02e+02, 3.15e+00, 3.15e+00]    
1000      [5.58e+01, 4.15e+01, 3.38e+00]    [0.00e+00, 6.56e+01, 3.38e+00]    [6.60e+01, 6.56e+01, 3.22e+01, 3.25e+01]    
2000      [2.81e+01, 1.36e+01, 3.82e+00]    [0.00e+00, 4.12e+01, 3.82e+00]    [4.17e+01, 4.12e+01, 3.11e+01, 3.11e+01]    
3000      [1.28e+01, 4.33e-01, 3.97e+00]    [0.00e+00, 1.88e+01, 3.97e+00]    [1.91e+01, 1.88e+01, 1.85e+01, 1.84e+01]    
4000      [9.44e+00, 4.55e-01, 3.89e+00]    [0.00e+00, 1.26e+01, 3.89e+00]    [1.27e+01, 1.26e+01, 1.31e+01, 1.31e+01]    
5000      [6.15e+00, 1.29e+00, 3.89e+00]    [0.00e+00, 7.04e+00, 3.89e+00]    [6.98e+00, 7.04e+00, 7.50e+00, 7.52e+00]    
6000      [4.58e+00, 1.30e+00, 3.89e+00]    [0.00e+00, 5.71e+00, 3.89e+00]    [5.37e+00, 5.71e+00, 4.09e+00, 4.26e+00]    
7000      [3.94e+00, 1.04e+00, 3.87e+00]    [0.00e+00, 4.71e+00, 3.87e+00]    [3.88e+00, 4.71e+00, 2.71e+00, 3.26e+00]    
8000      [3.64e+00, 7.07e-01, 3.84e+00]    [0.00e+00, 4.83e+00, 3.84e+00]    [3.39e+00, 4.83e+00, 2.23e+00, 3.29e+00]    
9000      [3.54e+00, 3.33e-01, 3.81e+00]    [0.00e+00, 5.39e+00, 3.81e+00]    [3.24e+00, 5.39e+00, 2.11e+00, 3.21e+00]    
10000     [3.39e+00, 2.24e-02, 3.78e+00]    [0.00e+00, 5.55e+00, 3.78e+00]    [2.90e+00, 5.55e+00, 1.73e+00, 2.87e+00]    
11000     [3.10e+00, 1.17e-02, 3.75e+00]    [0.00e+00, 5.29e+00, 3.75e+00]    [2.12e+00, 5.29e+00, 1.18e+00, 2.16e+00]    
12000     [3.01e+00, 1.59e-02, 3.71e+00]    [0.00e+00, 5.28e+00, 3.71e+00]    [2.08e+00, 5.28e+00, 1.16e+00, 2.17e+00]    
13000     [2.94e+00, 1.23e-02, 3.66e+00]    [0.00e+00, 5.14e+00, 3.66e+00]    [2.00e+00, 5.14e+00, 1.10e+00, 2.12e+00]    
14000     [2.94e+00, 4.17e-02, 3.62e+00]    [0.00e+00, 5.24e+00, 3.62e+00]    [1.88e+00, 5.24e+00, 9.74e-01, 1.92e+00]    
15000     [2.87e+00, 6.02e-03, 3.58e+00]    [0.00e+00, 4.97e+00, 3.58e+00]    [1.81e+00, 4.97e+00, 1.19e+00, 2.08e+00]    
16000     [2.84e+00, 5.91e-02, 3.55e+00]    [0.00e+00, 5.03e+00, 3.55e+00]    [1.67e+00, 5.03e+00, 1.14e+00, 1.93e+00]    
17000     [2.77e+00, 4.15e-02, 3.51e+00]    [0.00e+00, 4.94e+00, 3.51e+00]    [1.70e+00, 4.94e+00, 1.29e+00, 2.07e+00]    
18000     [2.71e+00, 2.09e-02, 3.48e+00]    [0.00e+00, 4.88e+00, 3.48e+00]    [1.73e+00, 4.88e+00, 1.26e+00, 2.09e+00]    
19000     [2.68e+00, 1.23e-02, 3.44e+00]    [0.00e+00, 4.94e+00, 3.44e+00]    [1.71e+00, 4.94e+00, 1.19e+00, 2.03e+00]    
20000     [2.63e+00, 1.71e-02, 3.41e+00]    [0.00e+00, 4.92e+00, 3.41e+00]    [1.74e+00, 4.92e+00, 1.17e+00, 2.05e+00]    
21000     [2.64e+00, 3.22e-02, 3.38e+00]    [0.00e+00, 5.03e+00, 3.38e+00]    [1.65e+00, 5.03e+00, 1.07e+00, 1.92e+00]    
22000     [2.63e+00, 3.53e-02, 3.35e+00]    [0.00e+00, 5.00e+00, 3.35e+00]    [1.66e+00, 5.00e+00, 1.06e+00, 1.92e+00]    
23000     [2.58e+00, 1.95e-02, 3.32e+00]    [0.00e+00, 4.91e+00, 3.32e+00]    [1.75e+00, 4.91e+00, 1.11e+00, 2.03e+00]    
24000     [2.56e+00, 9.07e-03, 3.29e+00]    [0.00e+00, 4.88e+00, 3.29e+00]    [1.78e+00, 4.88e+00, 1.11e+00, 2.05e+00]    
25000     [2.53e+00, 2.25e-02, 3.27e+00]    [0.00e+00, 4.89e+00, 3.27e+00]    [1.77e+00, 4.89e+00, 1.08e+00, 2.03e+00]    
26000     [2.52e+00, 1.58e-02, 3.24e+00]    [0.00e+00, 4.96e+00, 3.24e+00]    [1.77e+00, 4.96e+00, 1.06e+00, 2.02e+00]    
27000     [2.51e+00, 3.94e-02, 3.22e+00]    [0.00e+00, 4.87e+00, 3.22e+00]    [1.81e+00, 4.87e+00, 1.10e+00, 2.08e+00]    
28000     [2.54e+00, 3.08e-02, 3.19e+00]    [0.00e+00, 4.67e+00, 3.19e+00]    [1.92e+00, 4.67e+00, 1.23e+00, 2.23e+00]    
29000     [2.48e+00, 1.76e-02, 3.17e+00]    [0.00e+00, 4.83e+00, 3.17e+00]    [1.84e+00, 4.83e+00, 1.12e+00, 2.11e+00]    
30000     [2.48e+00, 2.53e-02, 3.15e+00]    [0.00e+00, 4.83e+00, 3.15e+00]    [1.83e+00, 4.83e+00, 1.11e+00, 2.10e+00]    

Best model at step 30000:
  train loss: 5.65e+00
  test loss: 7.98e+00
  test metric: [1.83e+00, 4.83e+00, 1.11e+00, 2.10e+00]

'train' took 83.773934 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.481711 s

'compile' took 2.312814 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 1.01e+02, 2.63e+00]    [0.00e+00, 9.98e+01, 2.63e+00]    [9.98e+01, 9.98e+01, 5.07e-01, 5.07e-01]    
1000      [8.00e+01, 3.41e+01, 2.76e+00]    [0.00e+00, 9.34e+01, 2.76e+00]    [9.35e+01, 9.34e+01, 8.40e+00, 8.48e+00]    
2000      [6.97e+01, 2.86e+01, 2.98e+00]    [0.00e+00, 8.19e+01, 2.98e+00]    [8.22e+01, 8.19e+01, 1.17e+01, 1.19e+01]    
3000      [4.48e+01, 1.28e+01, 3.74e+00]    [0.00e+00, 5.27e+01, 3.74e+00]    [5.38e+01, 5.27e+01, 2.36e+01, 2.25e+01]    
4000      [3.07e+01, 3.04e+00, 4.00e+00]    [0.00e+00, 3.19e+01, 4.00e+00]    [3.39e+01, 3.19e+01, 1.64e+01, 1.56e+01]    
5000      [1.78e+01, 6.09e-01, 4.16e+00]    [0.00e+00, 1.53e+01, 4.16e+00]    [1.79e+01, 1.53e+01, 9.96e+00, 1.03e+01]    
6000      [7.75e+00, 1.43e-01, 4.25e+00]    [0.00e+00, 5.37e+00, 4.25e+00]    [6.22e+00, 5.37e+00, 3.91e+00, 3.38e+00]    
7000      [5.13e+00, 2.00e-01, 4.26e+00]    [0.00e+00, 4.32e+00, 4.26e+00]    [4.25e+00, 4.32e+00, 2.68e+00, 3.67e+00]    
8000      [4.68e+00, 2.35e-01, 4.21e+00]    [0.00e+00, 4.29e+00, 4.21e+00]    [3.84e+00, 4.29e+00, 3.22e+00, 3.84e+00]    
9000      [4.38e+00, 1.15e-01, 4.18e+00]    [0.00e+00, 4.64e+00, 4.18e+00]    [3.88e+00, 4.64e+00, 3.34e+00, 3.65e+00]    
10000     [4.19e+00, 6.54e-02, 4.14e+00]    [0.00e+00, 4.92e+00, 4.14e+00]    [4.15e+00, 4.92e+00, 3.22e+00, 3.51e+00]    
11000     [4.07e+00, 1.28e-01, 4.10e+00]    [0.00e+00, 5.00e+00, 4.10e+00]    [4.26e+00, 5.00e+00, 3.08e+00, 3.38e+00]    
12000     [3.98e+00, 1.43e-01, 4.07e+00]    [0.00e+00, 4.83e+00, 4.07e+00]    [4.13e+00, 4.83e+00, 2.98e+00, 3.26e+00]    
13000     [3.92e+00, 3.43e-01, 4.03e+00]    [0.00e+00, 4.64e+00, 4.03e+00]    [3.98e+00, 4.64e+00, 2.88e+00, 3.12e+00]    
14000     [3.81e+00, 1.04e-01, 4.00e+00]    [0.00e+00, 4.46e+00, 4.00e+00]    [3.83e+00, 4.46e+00, 2.82e+00, 3.03e+00]    
15000     [3.75e+00, 1.57e-01, 3.96e+00]    [0.00e+00, 4.45e+00, 3.96e+00]    [3.86e+00, 4.45e+00, 2.73e+00, 2.94e+00]    
16000     [3.68e+00, 4.21e-02, 3.94e+00]    [0.00e+00, 4.44e+00, 3.94e+00]    [3.87e+00, 4.44e+00, 2.67e+00, 2.89e+00]    
17000     [3.62e+00, 1.42e-01, 3.91e+00]    [0.00e+00, 4.42e+00, 3.91e+00]    [3.88e+00, 4.42e+00, 2.61e+00, 2.84e+00]    
18000     [3.64e+00, 3.65e-01, 3.88e+00]    [0.00e+00, 4.37e+00, 3.88e+00]    [3.85e+00, 4.37e+00, 2.55e+00, 2.79e+00]    
19000     [3.52e+00, 1.17e-01, 3.85e+00]    [0.00e+00, 4.37e+00, 3.85e+00]    [3.86e+00, 4.37e+00, 2.50e+00, 2.76e+00]    
20000     [3.48e+00, 1.08e-01, 3.83e+00]    [0.00e+00, 4.35e+00, 3.83e+00]    [3.85e+00, 4.35e+00, 2.46e+00, 2.73e+00]    
21000     [3.43e+00, 1.13e-01, 3.80e+00]    [0.00e+00, 4.31e+00, 3.80e+00]    [3.82e+00, 4.31e+00, 2.45e+00, 2.73e+00]    
22000     [3.38e+00, 1.54e-01, 3.78e+00]    [0.00e+00, 4.27e+00, 3.78e+00]    [3.79e+00, 4.27e+00, 2.44e+00, 2.74e+00]    
23000     [3.36e+00, 3.42e-01, 3.76e+00]    [0.00e+00, 4.24e+00, 3.76e+00]    [3.76e+00, 4.24e+00, 2.42e+00, 2.73e+00]    
24000     [3.31e+00, 1.49e-01, 3.74e+00]    [0.00e+00, 4.21e+00, 3.74e+00]    [3.73e+00, 4.21e+00, 2.39e+00, 2.74e+00]    
25000     [3.27e+00, 7.44e-02, 3.71e+00]    [0.00e+00, 4.20e+00, 3.71e+00]    [3.72e+00, 4.20e+00, 2.38e+00, 2.75e+00]    
26000     [3.27e+00, 1.61e-01, 3.69e+00]    [0.00e+00, 4.18e+00, 3.69e+00]    [3.69e+00, 4.18e+00, 2.38e+00, 2.76e+00]    
27000     [3.23e+00, 9.24e-02, 3.67e+00]    [0.00e+00, 4.21e+00, 3.67e+00]    [3.72e+00, 4.21e+00, 2.35e+00, 2.76e+00]    
28000     [3.20e+00, 1.01e-01, 3.65e+00]    [0.00e+00, 4.18e+00, 3.65e+00]    [3.69e+00, 4.18e+00, 2.32e+00, 2.75e+00]    
29000     [3.19e+00, 1.88e-01, 3.63e+00]    [0.00e+00, 4.19e+00, 3.63e+00]    [3.70e+00, 4.19e+00, 2.29e+00, 2.75e+00]    
30000     [3.18e+00, 7.18e-02, 3.61e+00]    [0.00e+00, 4.15e+00, 3.61e+00]    [3.66e+00, 4.15e+00, 2.29e+00, 2.77e+00]    

Best model at step 30000:
  train loss: 6.86e+00
  test loss: 7.76e+00
  test metric: [3.66e+00, 4.15e+00, 2.29e+00, 2.77e+00]

'train' took 87.683481 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.514624 s

'compile' took 2.348720 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.89e+01, 9.88e+01, 2.66e+00]    [0.00e+00, 9.84e+01, 2.66e+00]    [9.84e+01, 9.84e+01, 3.70e+00, 3.70e+00]    
1000      [7.43e+01, 6.18e+01, 2.92e+00]    [0.00e+00, 7.14e+01, 2.92e+00]    [7.17e+01, 7.14e+01, 3.62e+01, 3.63e+01]    
2000      [4.32e+01, 2.17e+01, 3.72e+00]    [0.00e+00, 5.18e+01, 3.72e+00]    [5.21e+01, 5.18e+01, 3.23e+01, 3.08e+01]    
3000      [2.54e+01, 7.88e+00, 3.87e+00]    [0.00e+00, 3.04e+01, 3.87e+00]    [3.28e+01, 3.04e+01, 2.29e+01, 2.23e+01]    
4000      [1.10e+01, 2.30e+00, 4.00e+00]    [0.00e+00, 1.32e+01, 4.00e+00]    [1.45e+01, 1.32e+01, 9.02e+00, 8.37e+00]    
5000      [4.62e+00, 1.61e+00, 4.05e+00]    [0.00e+00, 6.13e+00, 4.05e+00]    [5.87e+00, 6.13e+00, 2.40e+00, 2.14e+00]    
6000      [3.87e+00, 1.30e+00, 4.00e+00]    [0.00e+00, 5.96e+00, 4.00e+00]    [5.76e+00, 5.96e+00, 2.28e+00, 2.83e+00]    
7000      [3.55e+00, 1.05e+00, 3.95e+00]    [0.00e+00, 5.44e+00, 3.95e+00]    [5.28e+00, 5.44e+00, 2.72e+00, 3.29e+00]    
8000      [3.42e+00, 9.43e-01, 3.90e+00]    [0.00e+00, 5.34e+00, 3.90e+00]    [5.04e+00, 5.34e+00, 2.62e+00, 2.87e+00]    
9000      [3.38e+00, 8.46e-01, 3.85e+00]    [0.00e+00, 5.35e+00, 3.85e+00]    [5.00e+00, 5.35e+00, 2.49e+00, 2.49e+00]    
10000     [3.32e+00, 6.43e-01, 3.81e+00]    [0.00e+00, 5.20e+00, 3.81e+00]    [4.95e+00, 5.20e+00, 2.05e+00, 2.01e+00]    
11000     [3.30e+00, 4.71e-01, 3.78e+00]    [0.00e+00, 5.40e+00, 3.78e+00]    [5.25e+00, 5.40e+00, 1.77e+00, 1.76e+00]    
12000     [3.29e+00, 3.28e-01, 3.76e+00]    [0.00e+00, 5.42e+00, 3.76e+00]    [5.35e+00, 5.42e+00, 1.62e+00, 1.61e+00]    
13000     [3.27e+00, 1.17e-01, 3.73e+00]    [0.00e+00, 5.23e+00, 3.73e+00]    [5.20e+00, 5.23e+00, 1.48e+00, 1.48e+00]    
14000     [3.21e+00, 1.71e-02, 3.70e+00]    [0.00e+00, 5.00e+00, 3.70e+00]    [5.03e+00, 5.00e+00, 1.35e+00, 1.36e+00]    
15000     [3.13e+00, 4.78e-02, 3.66e+00]    [0.00e+00, 4.75e+00, 3.66e+00]    [4.83e+00, 4.75e+00, 1.28e+00, 1.30e+00]    
16000     [3.07e+00, 4.09e-02, 3.63e+00]    [0.00e+00, 4.62e+00, 3.63e+00]    [4.75e+00, 4.62e+00, 1.26e+00, 1.28e+00]    
17000     [3.04e+00, 3.17e-02, 3.59e+00]    [0.00e+00, 4.56e+00, 3.59e+00]    [4.71e+00, 4.56e+00, 1.26e+00, 1.27e+00]    
18000     [3.00e+00, 4.69e-02, 3.56e+00]    [0.00e+00, 4.54e+00, 3.56e+00]    [4.72e+00, 4.54e+00, 1.30e+00, 1.30e+00]    
19000     [2.98e+00, 2.51e-02, 3.53e+00]    [0.00e+00, 4.41e+00, 3.53e+00]    [4.61e+00, 4.41e+00, 1.30e+00, 1.29e+00]    
20000     [2.94e+00, 4.15e-02, 3.50e+00]    [0.00e+00, 4.27e+00, 3.50e+00]    [4.50e+00, 4.27e+00, 1.28e+00, 1.26e+00]    
21000     [2.90e+00, 1.11e-02, 3.48e+00]    [0.00e+00, 4.33e+00, 3.48e+00]    [4.57e+00, 4.33e+00, 1.32e+00, 1.28e+00]    
22000     [2.87e+00, 1.30e-02, 3.45e+00]    [0.00e+00, 4.24e+00, 3.45e+00]    [4.50e+00, 4.24e+00, 1.34e+00, 1.30e+00]    
23000     [2.87e+00, 2.48e-02, 3.42e+00]    [0.00e+00, 4.31e+00, 3.42e+00]    [4.59e+00, 4.31e+00, 1.39e+00, 1.32e+00]    
24000     [2.83e+00, 2.81e-02, 3.40e+00]    [0.00e+00, 4.23e+00, 3.40e+00]    [4.52e+00, 4.23e+00, 1.42e+00, 1.35e+00]    
25000     [2.82e+00, 3.33e-02, 3.37e+00]    [0.00e+00, 4.06e+00, 3.37e+00]    [4.36e+00, 4.06e+00, 1.40e+00, 1.35e+00]    
26000     [2.79e+00, 5.25e-02, 3.35e+00]    [0.00e+00, 4.09e+00, 3.35e+00]    [4.40e+00, 4.09e+00, 1.41e+00, 1.33e+00]    
27000     [2.86e+00, 1.50e-01, 3.33e+00]    [0.00e+00, 3.86e+00, 3.33e+00]    [4.19e+00, 3.86e+00, 1.44e+00, 1.38e+00]    
28000     [2.79e+00, 2.65e-02, 3.31e+00]    [0.00e+00, 4.14e+00, 3.31e+00]    [4.47e+00, 4.14e+00, 1.50e+00, 1.40e+00]    
29000     [2.75e+00, 1.11e-02, 3.29e+00]    [0.00e+00, 4.07e+00, 3.29e+00]    [4.41e+00, 4.07e+00, 1.51e+00, 1.41e+00]    
30000     [2.82e+00, 1.06e-01, 3.27e+00]    [0.00e+00, 4.24e+00, 3.27e+00]    [4.59e+00, 4.24e+00, 1.67e+00, 1.55e+00]    

Best model at step 29000:
  train loss: 6.05e+00
  test loss: 7.36e+00
  test metric: [4.41e+00, 4.07e+00, 1.51e+00, 1.41e+00]

'train' took 80.160597 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.494677 s

'compile' took 2.278903 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.71e+01, 9.41e+01, 2.76e+00]    [0.00e+00, 9.86e+01, 2.76e+00]    [9.86e+01, 9.86e+01, 3.23e+00, 3.23e+00]    
1000      [8.10e+01, 5.80e+01, 2.76e+00]    [0.00e+00, 8.80e+01, 2.76e+00]    [8.80e+01, 8.80e+01, 2.02e+01, 2.02e+01]    
2000      [6.74e+01, 2.49e+01, 3.29e+00]    [0.00e+00, 7.04e+01, 3.29e+00]    [7.04e+01, 7.04e+01, 1.73e+01, 1.70e+01]    
3000      [4.32e+01, 2.36e+00, 3.91e+00]    [0.00e+00, 4.94e+01, 3.91e+00]    [4.98e+01, 4.94e+01, 1.09e+01, 1.05e+01]    
4000      [2.78e+01, 2.05e+00, 4.10e+00]    [0.00e+00, 3.34e+01, 4.10e+00]    [3.39e+01, 3.34e+01, 9.34e+00, 9.12e+00]    
5000      [1.59e+01, 1.74e+00, 4.24e+00]    [0.00e+00, 1.86e+01, 4.24e+00]    [1.89e+01, 1.86e+01, 1.25e+01, 1.23e+01]    
6000      [8.58e+00, 1.39e+00, 4.31e+00]    [0.00e+00, 1.23e+01, 4.31e+00]    [1.25e+01, 1.23e+01, 9.77e+00, 9.77e+00]    
7000      [6.37e+00, 8.69e-01, 4.26e+00]    [0.00e+00, 9.39e+00, 4.26e+00]    [9.48e+00, 9.39e+00, 6.29e+00, 6.27e+00]    
8000      [5.11e+00, 4.16e-01, 4.21e+00]    [0.00e+00, 6.87e+00, 4.21e+00]    [6.95e+00, 6.87e+00, 4.37e+00, 4.44e+00]    
9000      [4.33e+00, 4.38e-01, 4.16e+00]    [0.00e+00, 5.78e+00, 4.16e+00]    [5.78e+00, 5.78e+00, 3.26e+00, 3.30e+00]    
10000     [3.99e+00, 3.86e-01, 4.11e+00]    [0.00e+00, 5.54e+00, 4.11e+00]    [5.49e+00, 5.54e+00, 3.05e+00, 3.06e+00]    
11000     [3.68e+00, 1.50e-01, 4.07e+00]    [0.00e+00, 5.28e+00, 4.07e+00]    [5.24e+00, 5.28e+00, 2.87e+00, 2.92e+00]    
12000     [3.54e+00, 1.31e-01, 4.03e+00]    [0.00e+00, 5.18e+00, 4.03e+00]    [5.14e+00, 5.18e+00, 2.87e+00, 2.93e+00]    
13000     [3.45e+00, 4.03e-01, 4.00e+00]    [0.00e+00, 5.04e+00, 4.00e+00]    [5.00e+00, 5.04e+00, 2.96e+00, 3.03e+00]    
14000     [3.38e+00, 1.97e-01, 3.96e+00]    [0.00e+00, 5.07e+00, 3.96e+00]    [5.03e+00, 5.07e+00, 3.10e+00, 3.17e+00]    
15000     [3.28e+00, 1.11e-01, 3.93e+00]    [0.00e+00, 5.02e+00, 3.93e+00]    [4.95e+00, 5.02e+00, 3.06e+00, 3.08e+00]    
16000     [3.25e+00, 9.56e-02, 3.90e+00]    [0.00e+00, 5.07e+00, 3.90e+00]    [5.00e+00, 5.07e+00, 2.99e+00, 3.01e+00]    
17000     [3.20e+00, 5.28e-02, 3.88e+00]    [0.00e+00, 5.13e+00, 3.88e+00]    [5.06e+00, 5.13e+00, 2.88e+00, 2.92e+00]    
18000     [3.24e+00, 3.32e-01, 3.85e+00]    [0.00e+00, 5.23e+00, 3.85e+00]    [5.15e+00, 5.23e+00, 2.87e+00, 2.91e+00]    
19000     [3.13e+00, 3.67e-02, 3.83e+00]    [0.00e+00, 5.23e+00, 3.83e+00]    [5.16e+00, 5.23e+00, 2.73e+00, 2.79e+00]    
20000     [3.08e+00, 3.50e-02, 3.80e+00]    [0.00e+00, 5.27e+00, 3.80e+00]    [5.19e+00, 5.27e+00, 2.71e+00, 2.78e+00]    
21000     [3.07e+00, 7.10e-02, 3.78e+00]    [0.00e+00, 5.24e+00, 3.78e+00]    [5.16e+00, 5.24e+00, 2.74e+00, 2.80e+00]    
22000     [3.05e+00, 1.50e-01, 3.76e+00]    [0.00e+00, 5.21e+00, 3.76e+00]    [5.13e+00, 5.21e+00, 2.72e+00, 2.80e+00]    
23000     [3.03e+00, 1.39e-01, 3.74e+00]    [0.00e+00, 5.19e+00, 3.74e+00]    [5.11e+00, 5.19e+00, 2.73e+00, 2.81e+00]    
24000     [3.09e+00, 6.58e-01, 3.72e+00]    [0.00e+00, 5.10e+00, 3.72e+00]    [5.02e+00, 5.10e+00, 2.65e+00, 2.74e+00]    
25000     [3.01e+00, 9.78e-02, 3.70e+00]    [0.00e+00, 5.20e+00, 3.70e+00]    [5.12e+00, 5.20e+00, 2.78e+00, 2.87e+00]    
26000     [3.02e+00, 3.16e-01, 3.68e+00]    [0.00e+00, 5.17e+00, 3.68e+00]    [5.08e+00, 5.17e+00, 2.72e+00, 2.82e+00]    
27000     [3.02e+00, 2.05e-01, 3.66e+00]    [0.00e+00, 5.19e+00, 3.66e+00]    [5.11e+00, 5.19e+00, 2.83e+00, 2.92e+00]    
28000     [3.01e+00, 4.05e-01, 3.64e+00]    [0.00e+00, 5.13e+00, 3.64e+00]    [5.04e+00, 5.13e+00, 2.73e+00, 2.84e+00]    
29000     [2.97e+00, 2.21e-01, 3.63e+00]    [0.00e+00, 5.18e+00, 3.63e+00]    [5.09e+00, 5.18e+00, 2.80e+00, 2.90e+00]    
30000     [2.93e+00, 3.85e-02, 3.61e+00]    [0.00e+00, 5.13e+00, 3.61e+00]    [5.04e+00, 5.13e+00, 2.81e+00, 2.91e+00]    

Best model at step 30000:
  train loss: 6.58e+00
  test loss: 8.74e+00
  test metric: [5.04e+00, 5.13e+00, 2.81e+00, 2.91e+00]

'train' took 80.344108 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.465756 s

'compile' took 2.240009 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 9.88e+01, 2.65e+00]    [0.00e+00, 9.98e+01, 2.65e+00]    [9.98e+01, 9.98e+01, 1.34e+00, 1.34e+00]    
1000      [8.16e+01, 6.24e+01, 2.59e+00]    [0.00e+00, 9.00e+01, 2.59e+00]    [9.00e+01, 9.00e+01, 1.75e+01, 1.75e+01]    
2000      [7.07e+01, 4.11e+01, 2.92e+00]    [0.00e+00, 7.57e+01, 2.92e+00]    [7.57e+01, 7.57e+01, 2.21e+01, 2.14e+01]    
3000      [4.91e+01, 1.76e+01, 3.38e+00]    [0.00e+00, 5.68e+01, 3.38e+00]    [5.81e+01, 5.68e+01, 2.23e+01, 2.31e+01]    
4000      [3.37e+01, 8.14e+00, 3.63e+00]    [0.00e+00, 3.60e+01, 3.63e+00]    [3.77e+01, 3.60e+01, 1.92e+01, 1.67e+01]    
5000      [1.90e+01, 1.69e+00, 3.78e+00]    [0.00e+00, 1.77e+01, 3.78e+00]    [2.11e+01, 1.77e+01, 1.33e+01, 1.07e+01]    
6000      [8.82e+00, 9.18e-01, 3.87e+00]    [0.00e+00, 8.24e+00, 3.87e+00]    [9.35e+00, 8.24e+00, 7.79e+00, 6.21e+00]    
7000      [5.01e+00, 3.98e-01, 3.90e+00]    [0.00e+00, 6.16e+00, 3.90e+00]    [5.96e+00, 6.16e+00, 3.99e+00, 3.93e+00]    
8000      [4.14e+00, 4.19e-02, 3.87e+00]    [0.00e+00, 4.60e+00, 3.87e+00]    [4.46e+00, 4.60e+00, 2.29e+00, 3.39e+00]    
9000      [3.81e+00, 4.30e-02, 3.85e+00]    [0.00e+00, 4.16e+00, 3.85e+00]    [3.89e+00, 4.16e+00, 2.25e+00, 3.49e+00]    
10000     [3.63e+00, 1.05e-01, 3.82e+00]    [0.00e+00, 4.11e+00, 3.82e+00]    [3.81e+00, 4.11e+00, 2.35e+00, 3.52e+00]    
11000     [3.50e+00, 5.92e-02, 3.79e+00]    [0.00e+00, 4.13e+00, 3.79e+00]    [3.80e+00, 4.13e+00, 2.30e+00, 3.37e+00]    
12000     [3.48e+00, 4.61e-01, 3.76e+00]    [0.00e+00, 4.33e+00, 3.76e+00]    [3.93e+00, 4.33e+00, 2.10e+00, 3.06e+00]    
13000     [3.28e+00, 5.09e-02, 3.73e+00]    [0.00e+00, 4.36e+00, 3.73e+00]    [3.79e+00, 4.36e+00, 2.09e+00, 2.91e+00]    
14000     [3.16e+00, 1.97e-01, 3.71e+00]    [0.00e+00, 4.37e+00, 3.71e+00]    [3.78e+00, 4.37e+00, 2.03e+00, 2.77e+00]    
15000     [3.11e+00, 4.79e-02, 3.69e+00]    [0.00e+00, 4.45e+00, 3.69e+00]    [3.91e+00, 4.45e+00, 1.83e+00, 2.58e+00]    
16000     [3.04e+00, 1.25e-01, 3.67e+00]    [0.00e+00, 4.48e+00, 3.67e+00]    [3.97e+00, 4.48e+00, 1.70e+00, 2.47e+00]    
17000     [2.96e+00, 1.34e-01, 3.65e+00]    [0.00e+00, 4.48e+00, 3.65e+00]    [4.01e+00, 4.48e+00, 1.66e+00, 2.43e+00]    
18000     [2.93e+00, 3.62e-01, 3.63e+00]    [0.00e+00, 4.40e+00, 3.63e+00]    [3.96e+00, 4.40e+00, 1.67e+00, 2.40e+00]    
19000     [2.89e+00, 1.40e-01, 3.61e+00]    [0.00e+00, 4.43e+00, 3.61e+00]    [4.01e+00, 4.43e+00, 1.56e+00, 2.22e+00]    
20000     [2.85e+00, 1.05e-01, 3.59e+00]    [0.00e+00, 4.43e+00, 3.59e+00]    [4.03e+00, 4.43e+00, 1.55e+00, 2.18e+00]    
21000     [2.83e+00, 9.13e-02, 3.57e+00]    [0.00e+00, 4.40e+00, 3.57e+00]    [4.03e+00, 4.40e+00, 1.53e+00, 2.10e+00]    
22000     [2.82e+00, 8.18e-02, 3.55e+00]    [0.00e+00, 4.42e+00, 3.55e+00]    [4.08e+00, 4.42e+00, 1.49e+00, 2.00e+00]    
23000     [2.78e+00, 2.62e-02, 3.53e+00]    [0.00e+00, 4.33e+00, 3.53e+00]    [4.02e+00, 4.33e+00, 1.53e+00, 1.99e+00]    
24000     [2.80e+00, 2.16e-01, 3.52e+00]    [0.00e+00, 4.38e+00, 3.52e+00]    [4.09e+00, 4.38e+00, 1.49e+00, 1.89e+00]    
25000     [2.78e+00, 3.62e-01, 3.50e+00]    [0.00e+00, 4.23e+00, 3.50e+00]    [3.97e+00, 4.23e+00, 1.58e+00, 1.98e+00]    
26000     [2.75e+00, 9.96e-02, 3.48e+00]    [0.00e+00, 4.23e+00, 3.48e+00]    [3.99e+00, 4.23e+00, 1.57e+00, 1.92e+00]    
27000     [2.73e+00, 6.20e-02, 3.46e+00]    [0.00e+00, 4.26e+00, 3.46e+00]    [4.04e+00, 4.26e+00, 1.53e+00, 1.83e+00]    
28000     [2.77e+00, 3.05e-01, 3.45e+00]    [0.00e+00, 4.29e+00, 3.45e+00]    [4.09e+00, 4.29e+00, 1.49e+00, 1.75e+00]    
29000     [2.73e+00, 1.89e-01, 3.43e+00]    [0.00e+00, 4.25e+00, 3.43e+00]    [4.06e+00, 4.25e+00, 1.50e+00, 1.73e+00]    
30000     [2.72e+00, 3.68e-01, 3.42e+00]    [0.00e+00, 4.13e+00, 3.42e+00]    [3.97e+00, 4.13e+00, 1.60e+00, 1.83e+00]    

Best model at step 27000:
  train loss: 6.25e+00
  test loss: 7.72e+00
  test metric: [4.04e+00, 4.26e+00, 1.53e+00, 1.83e+00]

'train' took 80.115720 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[2.37666424996068, 3.2110494689736417, 4.41065001811728, 5.770374128158645, 2.966340674624768, 4.833259129371289, 4.1511670680617385, 4.070209963891985, 5.131411273882379, 4.258234614941837]
E* 5 4.117936058998424 0.9776891446062537
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.498666 s

'compile' took 2.309834 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.00e+02, 2.72e+00]    [0.00e+00, 1.00e+02, 2.72e+00]    [1.00e+02, 1.00e+02, 6.56e-01, 6.56e-01]    
1000      [7.95e+01, 5.01e+01, 2.80e+00]    [0.00e+00, 8.84e+01, 2.80e+00]    [8.84e+01, 8.84e+01, 6.30e+00, 6.30e+00]    
2000      [6.59e+01, 4.10e+01, 3.04e+00]    [0.00e+00, 7.34e+01, 3.04e+00]    [7.36e+01, 7.34e+01, 6.23e+00, 6.25e+00]    
3000      [2.83e+01, 1.29e+01, 3.84e+00]    [0.00e+00, 3.04e+01, 3.84e+00]    [3.23e+01, 3.04e+01, 1.87e+01, 1.92e+01]    
4000      [9.29e+00, 2.51e+00, 4.21e+00]    [0.00e+00, 9.74e+00, 4.21e+00]    [1.05e+01, 9.74e+00, 1.01e+01, 9.32e+00]    
5000      [7.00e+00, 4.59e-01, 4.22e+00]    [0.00e+00, 6.96e+00, 4.22e+00]    [7.39e+00, 6.96e+00, 4.20e+00, 3.76e+00]    
6000      [5.69e+00, 1.98e-01, 4.23e+00]    [0.00e+00, 5.53e+00, 4.23e+00]    [5.72e+00, 5.53e+00, 2.07e+00, 1.98e+00]    
7000      [4.95e+00, 1.02e-01, 4.22e+00]    [0.00e+00, 4.30e+00, 4.22e+00]    [4.37e+00, 4.30e+00, 1.76e+00, 1.77e+00]    
8000      [4.53e+00, 1.45e-01, 4.20e+00]    [0.00e+00, 3.52e+00, 4.20e+00]    [3.53e+00, 3.52e+00, 1.76e+00, 1.76e+00]    
9000      [4.27e+00, 1.86e-02, 4.17e+00]    [0.00e+00, 3.06e+00, 4.17e+00]    [3.04e+00, 3.06e+00, 1.69e+00, 1.69e+00]    
10000     [4.13e+00, 1.41e-01, 4.14e+00]    [0.00e+00, 2.87e+00, 4.14e+00]    [2.84e+00, 2.87e+00, 1.66e+00, 1.64e+00]    
11000     [3.96e+00, 1.88e-01, 4.10e+00]    [0.00e+00, 2.82e+00, 4.10e+00]    [2.83e+00, 2.82e+00, 1.44e+00, 1.49e+00]    
12000     [3.88e+00, 1.96e-01, 4.07e+00]    [0.00e+00, 2.89e+00, 4.07e+00]    [2.90e+00, 2.89e+00, 1.23e+00, 1.25e+00]    
13000     [3.73e+00, 1.36e-01, 4.04e+00]    [0.00e+00, 2.76e+00, 4.04e+00]    [2.78e+00, 2.76e+00, 1.21e+00, 1.20e+00]    
14000     [3.63e+00, 2.96e-02, 4.01e+00]    [0.00e+00, 2.69e+00, 4.01e+00]    [2.70e+00, 2.69e+00, 1.28e+00, 1.23e+00]    
15000     [3.56e+00, 1.19e-01, 3.98e+00]    [0.00e+00, 2.57e+00, 3.98e+00]    [2.59e+00, 2.57e+00, 1.36e+00, 1.30e+00]    
16000     [3.46e+00, 8.62e-02, 3.95e+00]    [0.00e+00, 2.47e+00, 3.95e+00]    [2.51e+00, 2.47e+00, 1.45e+00, 1.40e+00]    
17000     [3.39e+00, 6.79e-02, 3.92e+00]    [0.00e+00, 2.34e+00, 3.92e+00]    [2.51e+00, 2.34e+00, 1.37e+00, 1.54e+00]    
18000     [3.32e+00, 4.34e-02, 3.89e+00]    [0.00e+00, 2.33e+00, 3.89e+00]    [2.52e+00, 2.33e+00, 1.31e+00, 1.51e+00]    
19000     [3.28e+00, 1.04e-01, 3.86e+00]    [0.00e+00, 2.40e+00, 3.86e+00]    [2.60e+00, 2.40e+00, 1.25e+00, 1.44e+00]    
20000     [3.24e+00, 6.65e-02, 3.83e+00]    [0.00e+00, 2.43e+00, 3.83e+00]    [2.65e+00, 2.43e+00, 1.16e+00, 1.36e+00]    
21000     [3.21e+00, 1.20e-01, 3.81e+00]    [0.00e+00, 2.44e+00, 3.81e+00]    [2.67e+00, 2.44e+00, 1.10e+00, 1.31e+00]    
22000     [3.18e+00, 6.89e-02, 3.78e+00]    [0.00e+00, 2.48e+00, 3.78e+00]    [2.72e+00, 2.48e+00, 1.04e+00, 1.25e+00]    
23000     [3.16e+00, 2.01e-01, 3.76e+00]    [0.00e+00, 2.53e+00, 3.76e+00]    [2.78e+00, 2.53e+00, 9.47e-01, 1.15e+00]    
24000     [3.10e+00, 6.95e-02, 3.73e+00]    [0.00e+00, 2.49e+00, 3.73e+00]    [2.75e+00, 2.49e+00, 9.43e-01, 1.16e+00]    
25000     [3.07e+00, 2.85e-02, 3.71e+00]    [0.00e+00, 2.54e+00, 3.71e+00]    [2.80e+00, 2.54e+00, 9.04e-01, 1.10e+00]    
26000     [3.06e+00, 3.51e-02, 3.69e+00]    [0.00e+00, 2.54e+00, 3.69e+00]    [2.81e+00, 2.54e+00, 8.70e-01, 1.07e+00]    
27000     [3.03e+00, 4.83e-02, 3.66e+00]    [0.00e+00, 2.57e+00, 3.66e+00]    [2.84e+00, 2.57e+00, 8.43e-01, 1.04e+00]    
28000     [3.04e+00, 1.54e-01, 3.64e+00]    [0.00e+00, 2.57e+00, 3.64e+00]    [2.85e+00, 2.57e+00, 8.43e-01, 1.04e+00]    
29000     [3.01e+00, 8.34e-02, 3.62e+00]    [0.00e+00, 2.59e+00, 3.62e+00]    [2.87e+00, 2.59e+00, 8.23e-01, 1.03e+00]    
30000     [2.99e+00, 3.49e-02, 3.60e+00]    [0.00e+00, 2.60e+00, 3.60e+00]    [2.88e+00, 2.60e+00, 8.10e-01, 1.03e+00]    

Best model at step 30000:
  train loss: 6.63e+00
  test loss: 6.19e+00
  test metric: [2.88e+00, 2.60e+00, 8.10e-01, 1.03e+00]

'train' took 81.891958 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.497670 s

'compile' took 2.422521 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.98e+01, 9.99e+01, 2.81e+00]    [0.00e+00, 1.00e+02, 2.81e+00]    [1.00e+02, 1.00e+02, 3.66e-01, 3.66e-01]    
1000      [5.86e+01, 6.27e+01, 3.44e+00]    [0.00e+00, 5.63e+01, 3.44e+00]    [5.68e+01, 5.63e+01, 3.29e+01, 3.31e+01]    
2000      [1.26e+01, 3.84e+00, 4.59e+00]    [0.00e+00, 1.88e+01, 4.59e+00]    [2.09e+01, 1.88e+01, 1.71e+01, 1.59e+01]    
3000      [5.57e+00, 1.79e+00, 4.61e+00]    [0.00e+00, 6.84e+00, 4.61e+00]    [7.23e+00, 6.84e+00, 7.35e+00, 6.47e+00]    
4000      [4.15e+00, 1.28e+00, 4.56e+00]    [0.00e+00, 4.83e+00, 4.56e+00]    [5.05e+00, 4.83e+00, 3.43e+00, 2.87e+00]    
5000      [3.65e+00, 9.24e-01, 4.50e+00]    [0.00e+00, 4.01e+00, 4.50e+00]    [4.10e+00, 4.01e+00, 2.30e+00, 2.11e+00]    
6000      [3.43e+00, 5.77e-01, 4.44e+00]    [0.00e+00, 3.86e+00, 4.44e+00]    [3.84e+00, 3.86e+00, 2.19e+00, 2.21e+00]    
7000      [3.29e+00, 2.88e-01, 4.39e+00]    [0.00e+00, 3.91e+00, 4.39e+00]    [3.83e+00, 3.91e+00, 2.21e+00, 2.54e+00]    
8000      [3.19e+00, 1.75e-02, 4.35e+00]    [0.00e+00, 4.06e+00, 4.35e+00]    [3.88e+00, 4.06e+00, 2.35e+00, 3.04e+00]    
9000      [3.05e+00, 2.04e-02, 4.30e+00]    [0.00e+00, 3.74e+00, 4.30e+00]    [3.45e+00, 3.74e+00, 2.18e+00, 3.18e+00]    
10000     [2.96e+00, 1.13e-02, 4.25e+00]    [0.00e+00, 3.34e+00, 4.25e+00]    [3.19e+00, 3.34e+00, 1.88e+00, 3.04e+00]    
11000     [2.89e+00, 2.08e-02, 4.20e+00]    [0.00e+00, 3.18e+00, 4.20e+00]    [3.16e+00, 3.18e+00, 1.68e+00, 2.98e+00]    
12000     [2.86e+00, 3.51e-02, 4.15e+00]    [0.00e+00, 3.26e+00, 4.15e+00]    [2.98e+00, 3.26e+00, 1.75e+00, 2.94e+00]    
13000     [2.81e+00, 2.91e-02, 4.11e+00]    [0.00e+00, 3.03e+00, 4.11e+00]    [3.12e+00, 3.03e+00, 1.58e+00, 2.85e+00]    
14000     [2.71e+00, 1.92e-02, 4.07e+00]    [0.00e+00, 3.12e+00, 4.07e+00]    [2.90e+00, 3.12e+00, 1.70e+00, 2.89e+00]    
15000     [2.66e+00, 2.57e-02, 4.02e+00]    [0.00e+00, 2.88e+00, 4.02e+00]    [2.95e+00, 2.88e+00, 1.65e+00, 2.92e+00]    
16000     [2.64e+00, 2.65e-02, 3.98e+00]    [0.00e+00, 2.82e+00, 3.98e+00]    [2.97e+00, 2.82e+00, 1.67e+00, 2.88e+00]    
17000     [2.59e+00, 2.96e-02, 3.94e+00]    [0.00e+00, 2.76e+00, 3.94e+00]    [2.92e+00, 2.76e+00, 1.66e+00, 2.80e+00]    
18000     [2.57e+00, 2.26e-02, 3.91e+00]    [0.00e+00, 2.73e+00, 3.91e+00]    [2.91e+00, 2.73e+00, 1.68e+00, 2.73e+00]    
19000     [2.52e+00, 1.57e-02, 3.87e+00]    [0.00e+00, 2.74e+00, 3.87e+00]    [2.88e+00, 2.74e+00, 1.66e+00, 2.72e+00]    
20000     [2.51e+00, 1.61e-02, 3.84e+00]    [0.00e+00, 2.71e+00, 3.84e+00]    [2.88e+00, 2.71e+00, 1.60e+00, 2.63e+00]    
21000     [2.51e+00, 2.81e-02, 3.81e+00]    [0.00e+00, 2.66e+00, 3.81e+00]    [3.00e+00, 2.66e+00, 1.55e+00, 2.58e+00]    
22000     [2.45e+00, 1.40e-02, 3.78e+00]    [0.00e+00, 2.60e+00, 3.78e+00]    [2.95e+00, 2.60e+00, 1.51e+00, 2.59e+00]    
23000     [2.43e+00, 1.74e-02, 3.75e+00]    [0.00e+00, 2.61e+00, 3.75e+00]    [2.93e+00, 2.61e+00, 1.49e+00, 2.54e+00]    
24000     [2.45e+00, 2.94e-02, 3.72e+00]    [0.00e+00, 2.60e+00, 3.72e+00]    [2.89e+00, 2.60e+00, 1.46e+00, 2.53e+00]    
25000     [2.39e+00, 2.02e-02, 3.69e+00]    [0.00e+00, 2.53e+00, 3.69e+00]    [2.92e+00, 2.53e+00, 1.43e+00, 2.46e+00]    
26000     [2.38e+00, 1.37e-02, 3.67e+00]    [0.00e+00, 2.52e+00, 3.67e+00]    [2.90e+00, 2.52e+00, 1.40e+00, 2.42e+00]    
27000     [2.38e+00, 1.61e-02, 3.64e+00]    [0.00e+00, 2.48e+00, 3.64e+00]    [2.95e+00, 2.48e+00, 1.39e+00, 2.38e+00]    
28000     [2.40e+00, 1.72e-02, 3.62e+00]    [0.00e+00, 2.53e+00, 3.62e+00]    [2.87e+00, 2.53e+00, 1.38e+00, 2.39e+00]    
29000     [2.36e+00, 2.41e-02, 3.59e+00]    [0.00e+00, 2.50e+00, 3.59e+00]    [2.92e+00, 2.50e+00, 1.37e+00, 2.46e+00]    
30000     [2.34e+00, 2.49e-02, 3.57e+00]    [0.00e+00, 2.42e+00, 3.57e+00]    [2.94e+00, 2.42e+00, 1.36e+00, 2.40e+00]    

Best model at step 30000:
  train loss: 5.93e+00
  test loss: 5.99e+00
  test metric: [2.94e+00, 2.42e+00, 1.36e+00, 2.40e+00]

'train' took 80.857735 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.534571 s

'compile' took 2.357694 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 1.01e+02, 2.79e+00]    [0.00e+00, 1.00e+02, 2.79e+00]    [1.00e+02, 1.00e+02, 1.30e+00, 1.30e+00]    
1000      [8.11e+01, 5.92e+01, 2.82e+00]    [0.00e+00, 8.76e+01, 2.82e+00]    [8.77e+01, 8.76e+01, 2.43e+01, 2.43e+01]    
2000      [6.56e+01, 4.33e+01, 3.16e+00]    [0.00e+00, 7.26e+01, 3.16e+00]    [7.30e+01, 7.26e+01, 2.00e+01, 2.02e+01]    
3000      [3.63e+01, 1.48e+01, 3.83e+00]    [0.00e+00, 4.31e+01, 3.83e+00]    [4.41e+01, 4.31e+01, 2.41e+01, 2.35e+01]    
4000      [2.03e+01, 4.22e+00, 4.02e+00]    [0.00e+00, 2.19e+01, 4.02e+00]    [2.34e+01, 2.19e+01, 1.58e+01, 1.53e+01]    
5000      [9.65e+00, 2.11e+00, 4.09e+00]    [0.00e+00, 8.50e+00, 4.09e+00]    [9.98e+00, 8.50e+00, 7.44e+00, 7.56e+00]    
6000      [5.65e+00, 1.79e+00, 4.12e+00]    [0.00e+00, 3.90e+00, 4.12e+00]    [4.29e+00, 3.90e+00, 2.69e+00, 2.42e+00]    
7000      [4.79e+00, 1.63e+00, 4.09e+00]    [0.00e+00, 3.21e+00, 4.09e+00]    [3.32e+00, 3.21e+00, 1.75e+00, 1.54e+00]    
8000      [4.03e+00, 1.55e+00, 4.06e+00]    [0.00e+00, 2.63e+00, 4.06e+00]    [2.72e+00, 2.63e+00, 2.01e+00, 2.05e+00]    
9000      [3.65e+00, 1.38e+00, 4.04e+00]    [0.00e+00, 2.92e+00, 4.04e+00]    [2.94e+00, 2.92e+00, 2.61e+00, 2.64e+00]    
10000     [3.53e+00, 1.06e+00, 4.02e+00]    [0.00e+00, 3.14e+00, 4.02e+00]    [3.15e+00, 3.14e+00, 2.18e+00, 2.16e+00]    
11000     [3.40e+00, 1.07e+00, 4.00e+00]    [0.00e+00, 3.29e+00, 4.00e+00]    [3.30e+00, 3.29e+00, 2.14e+00, 2.04e+00]    
12000     [3.34e+00, 7.15e-01, 3.99e+00]    [0.00e+00, 3.50e+00, 3.99e+00]    [3.51e+00, 3.50e+00, 2.17e+00, 1.98e+00]    
13000     [3.31e+00, 6.24e-01, 3.98e+00]    [0.00e+00, 3.52e+00, 3.98e+00]    [3.54e+00, 3.52e+00, 2.40e+00, 2.12e+00]    
14000     [3.27e+00, 4.64e-01, 3.98e+00]    [0.00e+00, 3.69e+00, 3.98e+00]    [3.72e+00, 3.69e+00, 2.63e+00, 2.27e+00]    
15000     [3.33e+00, 5.64e-01, 3.97e+00]    [0.00e+00, 3.64e+00, 3.97e+00]    [3.81e+00, 3.64e+00, 2.73e+00, 2.46e+00]    
16000     [3.25e+00, 1.87e-01, 3.96e+00]    [0.00e+00, 3.70e+00, 3.96e+00]    [3.85e+00, 3.70e+00, 2.72e+00, 2.37e+00]    
17000     [3.24e+00, 8.47e-02, 3.94e+00]    [0.00e+00, 3.66e+00, 3.94e+00]    [3.91e+00, 3.66e+00, 2.68e+00, 2.36e+00]    
18000     [3.25e+00, 3.27e-01, 3.92e+00]    [0.00e+00, 3.54e+00, 3.92e+00]    [4.03e+00, 3.54e+00, 2.52e+00, 2.40e+00]    
19000     [3.18e+00, 6.12e-02, 3.90e+00]    [0.00e+00, 3.53e+00, 3.90e+00]    [4.04e+00, 3.53e+00, 2.56e+00, 2.36e+00]    
20000     [3.15e+00, 9.93e-02, 3.87e+00]    [0.00e+00, 3.47e+00, 3.87e+00]    [3.95e+00, 3.47e+00, 2.52e+00, 2.27e+00]    
21000     [3.14e+00, 1.87e-01, 3.84e+00]    [0.00e+00, 3.40e+00, 3.84e+00]    [4.02e+00, 3.40e+00, 2.38e+00, 2.29e+00]    
22000     [3.08e+00, 1.96e-02, 3.82e+00]    [0.00e+00, 3.37e+00, 3.82e+00]    [3.97e+00, 3.37e+00, 2.40e+00, 2.26e+00]    
23000     [3.06e+00, 2.89e-02, 3.79e+00]    [0.00e+00, 3.31e+00, 3.79e+00]    [3.96e+00, 3.31e+00, 2.34e+00, 2.26e+00]    
24000     [3.06e+00, 1.18e-01, 3.77e+00]    [0.00e+00, 3.29e+00, 3.77e+00]    [3.91e+00, 3.29e+00, 2.33e+00, 2.22e+00]    
25000     [3.05e+00, 1.28e-01, 3.75e+00]    [0.00e+00, 3.26e+00, 3.75e+00]    [3.89e+00, 3.26e+00, 2.30e+00, 2.20e+00]    
26000     [3.01e+00, 6.96e-02, 3.73e+00]    [0.00e+00, 3.25e+00, 3.73e+00]    [3.92e+00, 3.25e+00, 2.18e+00, 2.13e+00]    
27000     [2.98e+00, 6.51e-02, 3.71e+00]    [0.00e+00, 3.23e+00, 3.71e+00]    [3.91e+00, 3.23e+00, 2.14e+00, 2.09e+00]    
28000     [2.97e+00, 2.96e-02, 3.69e+00]    [0.00e+00, 3.20e+00, 3.69e+00]    [3.88e+00, 3.20e+00, 2.14e+00, 2.10e+00]    
29000     [2.97e+00, 6.99e-02, 3.67e+00]    [0.00e+00, 3.18e+00, 3.67e+00]    [3.87e+00, 3.18e+00, 2.10e+00, 2.07e+00]    
30000     [2.95e+00, 4.53e-02, 3.65e+00]    [0.00e+00, 3.15e+00, 3.65e+00]    [3.85e+00, 3.15e+00, 2.07e+00, 2.04e+00]    

Best model at step 30000:
  train loss: 6.64e+00
  test loss: 6.80e+00
  test metric: [3.85e+00, 3.15e+00, 2.07e+00, 2.04e+00]

'train' took 81.595761 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.473738 s

'compile' took 2.248985 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.02e+02, 2.85e+00]    [0.00e+00, 1.00e+02, 2.85e+00]    [1.00e+02, 1.00e+02, 1.68e+00, 1.68e+00]    
1000      [7.77e+01, 5.81e+01, 3.00e+00]    [0.00e+00, 8.76e+01, 3.00e+00]    [8.76e+01, 8.76e+01, 1.84e+01, 1.84e+01]    
2000      [5.52e+01, 1.69e+01, 3.69e+00]    [0.00e+00, 6.92e+01, 3.69e+00]    [6.98e+01, 6.92e+01, 2.45e+01, 2.49e+01]    
3000      [3.97e+01, 1.16e+01, 3.95e+00]    [0.00e+00, 5.63e+01, 3.95e+00]    [5.68e+01, 5.63e+01, 2.08e+01, 2.11e+01]    
4000      [2.99e+01, 6.61e+00, 4.09e+00]    [0.00e+00, 4.47e+01, 4.09e+00]    [4.50e+01, 4.47e+01, 1.53e+01, 1.54e+01]    
5000      [1.69e+01, 1.78e+00, 4.36e+00]    [0.00e+00, 2.66e+01, 4.36e+00]    [2.68e+01, 2.66e+01, 1.19e+01, 1.19e+01]    
6000      [1.09e+01, 3.30e-01, 4.42e+00]    [0.00e+00, 1.65e+01, 4.42e+00]    [1.64e+01, 1.65e+01, 1.41e+01, 1.41e+01]    
7000      [8.04e+00, 2.59e-01, 4.40e+00]    [0.00e+00, 1.26e+01, 4.40e+00]    [1.26e+01, 1.26e+01, 1.32e+01, 1.34e+01]    
8000      [6.63e+00, 8.53e-02, 4.35e+00]    [0.00e+00, 1.07e+01, 4.35e+00]    [1.06e+01, 1.07e+01, 1.32e+01, 1.33e+01]    
9000      [6.01e+00, 1.26e-01, 4.30e+00]    [0.00e+00, 9.70e+00, 4.30e+00]    [9.56e+00, 9.70e+00, 1.30e+01, 1.32e+01]    
10000     [5.71e+00, 4.95e-02, 4.24e+00]    [0.00e+00, 9.21e+00, 4.24e+00]    [9.22e+00, 9.21e+00, 1.26e+01, 1.29e+01]    
11000     [5.50e+00, 1.07e-01, 4.19e+00]    [0.00e+00, 8.86e+00, 4.19e+00]    [8.96e+00, 8.86e+00, 1.22e+01, 1.25e+01]    
12000     [5.31e+00, 8.46e-02, 4.13e+00]    [0.00e+00, 8.69e+00, 4.13e+00]    [8.79e+00, 8.69e+00, 1.20e+01, 1.24e+01]    
13000     [5.17e+00, 7.44e-02, 4.09e+00]    [0.00e+00, 8.46e+00, 4.09e+00]    [8.57e+00, 8.46e+00, 1.15e+01, 1.20e+01]    
14000     [5.00e+00, 1.08e-01, 4.05e+00]    [0.00e+00, 8.28e+00, 4.05e+00]    [8.40e+00, 8.28e+00, 1.12e+01, 1.17e+01]    
15000     [4.87e+00, 1.26e-02, 4.01e+00]    [0.00e+00, 8.09e+00, 4.01e+00]    [8.20e+00, 8.09e+00, 1.10e+01, 1.15e+01]    
16000     [4.80e+00, 1.35e-01, 3.97e+00]    [0.00e+00, 7.93e+00, 3.97e+00]    [8.05e+00, 7.93e+00, 1.07e+01, 1.12e+01]    
17000     [4.73e+00, 2.06e-01, 3.94e+00]    [0.00e+00, 7.79e+00, 3.94e+00]    [7.91e+00, 7.79e+00, 1.06e+01, 1.11e+01]    
18000     [4.57e+00, 8.12e-02, 3.91e+00]    [0.00e+00, 7.66e+00, 3.91e+00]    [7.77e+00, 7.66e+00, 1.02e+01, 1.07e+01]    
19000     [4.52e+00, 9.58e-02, 3.88e+00]    [0.00e+00, 7.47e+00, 3.88e+00]    [7.59e+00, 7.47e+00, 9.96e+00, 1.05e+01]    
20000     [4.43e+00, 1.52e-01, 3.85e+00]    [0.00e+00, 7.32e+00, 3.85e+00]    [7.43e+00, 7.32e+00, 9.57e+00, 1.01e+01]    
21000     [4.34e+00, 9.91e-02, 3.82e+00]    [0.00e+00, 7.20e+00, 3.82e+00]    [7.32e+00, 7.20e+00, 9.31e+00, 9.82e+00]    
22000     [4.31e+00, 2.44e-01, 3.79e+00]    [0.00e+00, 7.11e+00, 3.79e+00]    [7.22e+00, 7.11e+00, 8.92e+00, 9.41e+00]    
23000     [4.20e+00, 9.44e-02, 3.77e+00]    [0.00e+00, 7.03e+00, 3.77e+00]    [7.14e+00, 7.03e+00, 8.81e+00, 9.28e+00]    
24000     [4.09e+00, 8.81e-02, 3.74e+00]    [0.00e+00, 6.93e+00, 3.74e+00]    [7.04e+00, 6.93e+00, 8.42e+00, 8.86e+00]    
25000     [4.01e+00, 1.08e-01, 3.72e+00]    [0.00e+00, 6.87e+00, 3.72e+00]    [6.98e+00, 6.87e+00, 8.22e+00, 8.64e+00]    
26000     [3.96e+00, 4.12e-02, 3.70e+00]    [0.00e+00, 6.74e+00, 3.70e+00]    [6.84e+00, 6.74e+00, 7.91e+00, 8.29e+00]    
27000     [3.88e+00, 6.05e-02, 3.68e+00]    [0.00e+00, 6.65e+00, 3.68e+00]    [6.75e+00, 6.65e+00, 7.66e+00, 8.02e+00]    
28000     [3.84e+00, 1.29e-01, 3.66e+00]    [0.00e+00, 6.60e+00, 3.66e+00]    [6.69e+00, 6.60e+00, 7.52e+00, 7.86e+00]    
29000     [3.79e+00, 6.33e-02, 3.64e+00]    [0.00e+00, 6.49e+00, 3.64e+00]    [6.59e+00, 6.49e+00, 7.24e+00, 7.57e+00]    
30000     [3.71e+00, 5.03e-02, 3.62e+00]    [0.00e+00, 6.39e+00, 3.62e+00]    [6.48e+00, 6.39e+00, 7.09e+00, 7.41e+00]    

Best model at step 30000:
  train loss: 7.38e+00
  test loss: 1.00e+01
  test metric: [6.48e+00, 6.39e+00, 7.09e+00, 7.41e+00]

'train' took 81.406267 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.478719 s

'compile' took 2.073455 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.99e+01, 1.00e+02, 2.76e+00]    [0.00e+00, 1.00e+02, 2.76e+00]    [1.00e+02, 1.00e+02, 5.40e-01, 5.40e-01]    
1000      [7.89e+01, 7.75e+01, 2.74e+00]    [0.00e+00, 8.07e+01, 2.74e+00]    [8.07e+01, 8.07e+01, 1.63e+01, 1.63e+01]    
2000      [6.11e+01, 5.53e+01, 3.29e+00]    [0.00e+00, 5.75e+01, 3.29e+00]    [5.79e+01, 5.75e+01, 2.08e+01, 2.10e+01]    
3000      [2.37e+01, 1.12e+01, 4.31e+00]    [0.00e+00, 2.48e+01, 4.31e+00]    [2.60e+01, 2.48e+01, 1.65e+01, 1.49e+01]    
4000      [7.28e+00, 3.98e+00, 4.47e+00]    [0.00e+00, 1.13e+01, 4.47e+00]    [1.10e+01, 1.13e+01, 8.06e+00, 7.01e+00]    
5000      [5.29e+00, 1.21e+00, 4.40e+00]    [0.00e+00, 7.49e+00, 4.40e+00]    [7.63e+00, 7.49e+00, 5.84e+00, 5.95e+00]    
6000      [4.86e+00, 3.63e-01, 4.36e+00]    [0.00e+00, 6.41e+00, 4.36e+00]    [6.34e+00, 6.41e+00, 5.47e+00, 5.38e+00]    
7000      [4.49e+00, 9.03e-02, 4.33e+00]    [0.00e+00, 6.36e+00, 4.33e+00]    [6.14e+00, 6.36e+00, 5.49e+00, 5.09e+00]    
8000      [4.24e+00, 7.05e-02, 4.28e+00]    [0.00e+00, 6.23e+00, 4.28e+00]    [5.93e+00, 6.23e+00, 5.43e+00, 4.83e+00]    
9000      [4.08e+00, 8.65e-02, 4.24e+00]    [0.00e+00, 5.98e+00, 4.24e+00]    [5.73e+00, 5.98e+00, 5.22e+00, 4.61e+00]    
10000     [3.95e+00, 1.37e-01, 4.20e+00]    [0.00e+00, 5.48e+00, 4.20e+00]    [5.48e+00, 5.48e+00, 4.76e+00, 4.30e+00]    
11000     [3.81e+00, 2.41e-02, 4.15e+00]    [0.00e+00, 5.25e+00, 4.15e+00]    [5.34e+00, 5.25e+00, 4.15e+00, 3.81e+00]    
12000     [3.75e+00, 1.34e-01, 4.11e+00]    [0.00e+00, 5.05e+00, 4.11e+00]    [5.14e+00, 5.05e+00, 3.47e+00, 3.26e+00]    
13000     [3.72e+00, 2.54e-01, 4.08e+00]    [0.00e+00, 4.99e+00, 4.08e+00]    [5.08e+00, 4.99e+00, 2.91e+00, 2.88e+00]    
14000     [3.58e+00, 1.23e-01, 4.04e+00]    [0.00e+00, 4.61e+00, 4.04e+00]    [4.69e+00, 4.61e+00, 2.57e+00, 2.55e+00]    
15000     [3.55e+00, 1.77e-01, 4.01e+00]    [0.00e+00, 4.46e+00, 4.01e+00]    [4.54e+00, 4.46e+00, 2.23e+00, 2.35e+00]    
16000     [3.44e+00, 3.50e-02, 3.97e+00]    [0.00e+00, 4.08e+00, 3.97e+00]    [4.15e+00, 4.08e+00, 2.13e+00, 2.14e+00]    
17000     [3.36e+00, 6.92e-02, 3.94e+00]    [0.00e+00, 3.96e+00, 3.94e+00]    [4.03e+00, 3.96e+00, 1.92e+00, 2.00e+00]    
18000     [3.30e+00, 6.03e-02, 3.91e+00]    [0.00e+00, 3.82e+00, 3.91e+00]    [3.89e+00, 3.82e+00, 1.82e+00, 1.83e+00]    
19000     [3.25e+00, 7.58e-02, 3.88e+00]    [0.00e+00, 3.66e+00, 3.88e+00]    [3.71e+00, 3.66e+00, 1.80e+00, 1.62e+00]    
20000     [3.20e+00, 6.77e-03, 3.84e+00]    [0.00e+00, 3.61e+00, 3.84e+00]    [3.66e+00, 3.61e+00, 1.69e+00, 1.49e+00]    
21000     [3.23e+00, 1.41e-01, 3.82e+00]    [0.00e+00, 3.55e+00, 3.82e+00]    [3.60e+00, 3.55e+00, 1.62e+00, 1.36e+00]    
22000     [3.14e+00, 9.46e-02, 3.79e+00]    [0.00e+00, 3.39e+00, 3.79e+00]    [3.44e+00, 3.39e+00, 1.69e+00, 1.25e+00]    
23000     [3.12e+00, 1.09e-02, 3.76e+00]    [0.00e+00, 3.35e+00, 3.76e+00]    [3.39e+00, 3.35e+00, 1.61e+00, 1.21e+00]    
24000     [3.17e+00, 1.55e-01, 3.73e+00]    [0.00e+00, 3.39e+00, 3.73e+00]    [3.42e+00, 3.39e+00, 1.48e+00, 1.16e+00]    
25000     [3.13e+00, 1.01e-01, 3.71e+00]    [0.00e+00, 3.19e+00, 3.71e+00]    [3.22e+00, 3.19e+00, 1.59e+00, 1.18e+00]    
26000     [3.03e+00, 1.05e-01, 3.68e+00]    [0.00e+00, 3.06e+00, 3.68e+00]    [3.09e+00, 3.06e+00, 1.70e+00, 1.17e+00]    
27000     [3.02e+00, 1.10e-02, 3.66e+00]    [0.00e+00, 3.03e+00, 3.66e+00]    [3.06e+00, 3.03e+00, 1.64e+00, 1.16e+00]    
28000     [3.02e+00, 1.58e-01, 3.63e+00]    [0.00e+00, 2.85e+00, 3.63e+00]    [2.88e+00, 2.85e+00, 1.83e+00, 1.28e+00]    
29000     [3.02e+00, 1.59e-01, 3.61e+00]    [0.00e+00, 3.02e+00, 3.61e+00]    [3.04e+00, 3.02e+00, 1.58e+00, 1.12e+00]    
30000     [2.95e+00, 3.95e-02, 3.59e+00]    [0.00e+00, 2.93e+00, 3.59e+00]    [2.95e+00, 2.93e+00, 1.68e+00, 1.15e+00]    

Best model at step 30000:
  train loss: 6.58e+00
  test loss: 6.52e+00
  test metric: [2.95e+00, 2.93e+00, 1.68e+00, 1.15e+00]

'train' took 81.541903 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.480715 s

'compile' took 2.401578 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.61e+01, 1.01e+02, 2.74e+00]    [0.00e+00, 9.17e+01, 2.74e+00]    [9.17e+01, 9.17e+01, 1.03e+01, 1.03e+01]    
1000      [5.54e+01, 4.44e+01, 3.51e+00]    [0.00e+00, 7.13e+01, 3.51e+00]    [7.16e+01, 7.13e+01, 3.44e+01, 3.44e+01]    
2000      [3.39e+01, 2.36e+01, 3.90e+00]    [0.00e+00, 5.21e+01, 3.90e+00]    [5.27e+01, 5.21e+01, 2.94e+01, 2.96e+01]    
3000      [9.37e+00, 8.94e-01, 4.39e+00]    [0.00e+00, 1.81e+01, 4.39e+00]    [1.84e+01, 1.81e+01, 1.49e+01, 1.46e+01]    
4000      [6.46e+00, 7.37e-01, 4.32e+00]    [0.00e+00, 1.21e+01, 4.32e+00]    [1.21e+01, 1.21e+01, 9.59e+00, 9.61e+00]    
5000      [4.60e+00, 6.43e-01, 4.29e+00]    [0.00e+00, 7.60e+00, 4.29e+00]    [7.34e+00, 7.60e+00, 5.68e+00, 5.93e+00]    
6000      [3.53e+00, 5.00e-01, 4.26e+00]    [0.00e+00, 4.73e+00, 4.26e+00]    [4.36e+00, 4.73e+00, 3.13e+00, 3.75e+00]    
7000      [3.10e+00, 3.26e-01, 4.24e+00]    [0.00e+00, 3.64e+00, 4.24e+00]    [3.28e+00, 3.64e+00, 1.55e+00, 2.58e+00]    
8000      [2.97e+00, 9.64e-02, 4.20e+00]    [0.00e+00, 3.55e+00, 4.20e+00]    [3.07e+00, 3.55e+00, 1.45e+00, 2.61e+00]    
9000      [2.91e+00, 1.19e-02, 4.15e+00]    [0.00e+00, 3.55e+00, 4.15e+00]    [2.81e+00, 3.55e+00, 1.45e+00, 2.73e+00]    
10000     [2.81e+00, 1.44e-02, 4.10e+00]    [0.00e+00, 3.40e+00, 4.10e+00]    [2.74e+00, 3.40e+00, 1.33e+00, 2.65e+00]    
11000     [2.74e+00, 1.75e-02, 4.06e+00]    [0.00e+00, 3.34e+00, 4.06e+00]    [2.61e+00, 3.34e+00, 1.32e+00, 2.51e+00]    
12000     [2.74e+00, 3.07e-02, 4.01e+00]    [0.00e+00, 3.33e+00, 4.01e+00]    [2.67e+00, 3.33e+00, 1.38e+00, 2.50e+00]    
13000     [2.66e+00, 5.89e-03, 3.97e+00]    [0.00e+00, 3.46e+00, 3.97e+00]    [2.51e+00, 3.46e+00, 1.33e+00, 2.41e+00]    
14000     [2.62e+00, 1.89e-02, 3.93e+00]    [0.00e+00, 3.50e+00, 3.93e+00]    [2.51e+00, 3.50e+00, 1.31e+00, 2.42e+00]    
15000     [2.60e+00, 2.29e-02, 3.90e+00]    [0.00e+00, 3.48e+00, 3.90e+00]    [2.58e+00, 3.48e+00, 1.34e+00, 2.44e+00]    
16000     [2.65e+00, 3.77e-02, 3.86e+00]    [0.00e+00, 3.62e+00, 3.86e+00]    [2.51e+00, 3.62e+00, 1.32e+00, 2.40e+00]    
17000     [2.58e+00, 9.48e-03, 3.83e+00]    [0.00e+00, 3.61e+00, 3.83e+00]    [2.55e+00, 3.61e+00, 1.31e+00, 2.47e+00]    
18000     [2.55e+00, 9.94e-03, 3.79e+00]    [0.00e+00, 3.55e+00, 3.79e+00]    [2.58e+00, 3.55e+00, 1.31e+00, 2.45e+00]    
19000     [2.53e+00, 1.32e-02, 3.76e+00]    [0.00e+00, 3.48e+00, 3.76e+00]    [2.62e+00, 3.48e+00, 1.33e+00, 2.47e+00]    
20000     [2.55e+00, 2.80e-02, 3.73e+00]    [0.00e+00, 3.50e+00, 3.73e+00]    [2.54e+00, 3.50e+00, 1.33e+00, 2.36e+00]    
21000     [2.52e+00, 1.31e-02, 3.70e+00]    [0.00e+00, 3.42e+00, 3.70e+00]    [2.58e+00, 3.42e+00, 1.35e+00, 2.41e+00]    
22000     [2.50e+00, 1.78e-02, 3.67e+00]    [0.00e+00, 3.52e+00, 3.67e+00]    [2.46e+00, 3.52e+00, 1.29e+00, 2.38e+00]    
23000     [2.51e+00, 2.38e-02, 3.64e+00]    [0.00e+00, 3.36e+00, 3.64e+00]    [2.57e+00, 3.36e+00, 1.39e+00, 2.39e+00]    
24000     [2.48e+00, 1.14e-02, 3.61e+00]    [0.00e+00, 3.50e+00, 3.61e+00]    [2.43e+00, 3.50e+00, 1.31e+00, 2.32e+00]    
25000     [2.46e+00, 2.10e-02, 3.59e+00]    [0.00e+00, 3.43e+00, 3.59e+00]    [2.49e+00, 3.43e+00, 1.35e+00, 2.33e+00]    
26000     [2.50e+00, 3.32e-02, 3.56e+00]    [0.00e+00, 3.47e+00, 3.56e+00]    [2.42e+00, 3.47e+00, 1.34e+00, 2.26e+00]    
27000     [2.44e+00, 2.12e-02, 3.54e+00]    [0.00e+00, 3.40e+00, 3.54e+00]    [2.50e+00, 3.40e+00, 1.38e+00, 2.35e+00]    
28000     [2.44e+00, 1.46e-02, 3.51e+00]    [0.00e+00, 3.46e+00, 3.51e+00]    [2.45e+00, 3.46e+00, 1.36e+00, 2.34e+00]    
29000     [2.42e+00, 9.71e-03, 3.49e+00]    [0.00e+00, 3.40e+00, 3.49e+00]    [2.49e+00, 3.40e+00, 1.41e+00, 2.33e+00]    
30000     [2.43e+00, 1.86e-02, 3.46e+00]    [0.00e+00, 3.34e+00, 3.46e+00]    [2.53e+00, 3.34e+00, 1.44e+00, 2.36e+00]    

Best model at step 29000:
  train loss: 5.91e+00
  test loss: 6.89e+00
  test metric: [2.49e+00, 3.40e+00, 1.41e+00, 2.33e+00]

'train' took 81.665572 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.501658 s

'compile' took 2.294862 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.03e+02, 2.75e+00]    [0.00e+00, 1.00e+02, 2.75e+00]    [1.00e+02, 1.00e+02, 4.48e-01, 4.48e-01]    
1000      [8.00e+01, 4.33e+01, 2.85e+00]    [0.00e+00, 9.36e+01, 2.85e+00]    [9.36e+01, 9.36e+01, 9.18e+00, 9.25e+00]    
2000      [6.59e+01, 3.64e+01, 3.08e+00]    [0.00e+00, 7.64e+01, 3.08e+00]    [7.67e+01, 7.64e+01, 9.00e+00, 9.15e+00]    
3000      [3.38e+01, 1.39e+01, 3.79e+00]    [0.00e+00, 3.43e+01, 3.79e+00]    [3.78e+01, 3.43e+01, 1.61e+01, 1.70e+01]    
4000      [1.04e+01, 7.89e-01, 4.11e+00]    [0.00e+00, 1.02e+01, 4.11e+00]    [1.15e+01, 1.02e+01, 7.10e+00, 5.51e+00]    
5000      [6.34e+00, 9.23e-01, 4.10e+00]    [0.00e+00, 6.05e+00, 4.10e+00]    [7.20e+00, 6.05e+00, 3.51e+00, 5.08e+00]    
6000      [5.21e+00, 1.07e+00, 4.09e+00]    [0.00e+00, 4.98e+00, 4.09e+00]    [5.33e+00, 4.98e+00, 4.11e+00, 5.21e+00]    
7000      [4.87e+00, 5.08e-01, 4.06e+00]    [0.00e+00, 5.04e+00, 4.06e+00]    [4.98e+00, 5.04e+00, 3.53e+00, 4.08e+00]    
8000      [4.75e+00, 2.38e-02, 4.03e+00]    [0.00e+00, 5.04e+00, 4.03e+00]    [5.00e+00, 5.04e+00, 2.99e+00, 3.35e+00]    
9000      [4.55e+00, 5.35e-02, 4.01e+00]    [0.00e+00, 4.88e+00, 4.01e+00]    [4.85e+00, 4.88e+00, 2.88e+00, 3.18e+00]    
10000     [4.40e+00, 6.87e-02, 3.98e+00]    [0.00e+00, 4.67e+00, 3.98e+00]    [4.65e+00, 4.67e+00, 2.73e+00, 2.95e+00]    
11000     [4.29e+00, 1.94e-01, 3.96e+00]    [0.00e+00, 4.50e+00, 3.96e+00]    [4.48e+00, 4.50e+00, 2.69e+00, 2.85e+00]    
12000     [4.24e+00, 2.17e-01, 3.93e+00]    [0.00e+00, 4.40e+00, 3.93e+00]    [4.39e+00, 4.40e+00, 2.67e+00, 2.76e+00]    
13000     [4.13e+00, 1.76e-02, 3.91e+00]    [0.00e+00, 4.28e+00, 3.91e+00]    [4.28e+00, 4.28e+00, 2.70e+00, 2.74e+00]    
14000     [4.06e+00, 3.26e-02, 3.89e+00]    [0.00e+00, 4.15e+00, 3.89e+00]    [4.15e+00, 4.15e+00, 2.72e+00, 2.71e+00]    
15000     [3.99e+00, 1.17e-01, 3.86e+00]    [0.00e+00, 4.03e+00, 3.86e+00]    [4.03e+00, 4.03e+00, 2.72e+00, 2.67e+00]    
16000     [3.97e+00, 2.40e-01, 3.84e+00]    [0.00e+00, 3.91e+00, 3.84e+00]    [3.91e+00, 3.91e+00, 2.70e+00, 2.62e+00]    
17000     [3.87e+00, 3.88e-02, 3.82e+00]    [0.00e+00, 3.78e+00, 3.82e+00]    [3.83e+00, 3.78e+00, 2.57e+00, 2.52e+00]    
18000     [3.84e+00, 4.25e-02, 3.80e+00]    [0.00e+00, 3.71e+00, 3.80e+00]    [3.77e+00, 3.71e+00, 2.50e+00, 2.45e+00]    
19000     [3.79e+00, 7.45e-02, 3.77e+00]    [0.00e+00, 3.62e+00, 3.77e+00]    [3.69e+00, 3.62e+00, 2.40e+00, 2.35e+00]    
20000     [3.77e+00, 1.69e-01, 3.75e+00]    [0.00e+00, 3.57e+00, 3.75e+00]    [3.65e+00, 3.57e+00, 2.37e+00, 2.31e+00]    
21000     [3.74e+00, 1.50e-01, 3.73e+00]    [0.00e+00, 3.54e+00, 3.73e+00]    [3.62e+00, 3.54e+00, 2.31e+00, 2.24e+00]    
22000     [3.68e+00, 1.72e-01, 3.71e+00]    [0.00e+00, 3.45e+00, 3.71e+00]    [3.54e+00, 3.45e+00, 2.24e+00, 2.17e+00]    
23000     [3.64e+00, 5.15e-02, 3.69e+00]    [0.00e+00, 3.38e+00, 3.69e+00]    [3.48e+00, 3.38e+00, 2.18e+00, 2.11e+00]    
24000     [3.60e+00, 2.97e-02, 3.67e+00]    [0.00e+00, 3.30e+00, 3.67e+00]    [3.40e+00, 3.30e+00, 2.15e+00, 2.07e+00]    
25000     [3.55e+00, 5.20e-02, 3.65e+00]    [0.00e+00, 3.25e+00, 3.65e+00]    [3.36e+00, 3.25e+00, 2.11e+00, 2.03e+00]    
26000     [3.52e+00, 4.12e-02, 3.63e+00]    [0.00e+00, 3.19e+00, 3.63e+00]    [3.30e+00, 3.19e+00, 2.07e+00, 1.99e+00]    
27000     [3.50e+00, 1.60e-01, 3.61e+00]    [0.00e+00, 3.13e+00, 3.61e+00]    [3.24e+00, 3.13e+00, 2.06e+00, 1.98e+00]    
28000     [3.47e+00, 1.50e-01, 3.60e+00]    [0.00e+00, 3.08e+00, 3.60e+00]    [3.19e+00, 3.08e+00, 2.03e+00, 1.95e+00]    
29000     [3.43e+00, 9.57e-02, 3.58e+00]    [0.00e+00, 3.07e+00, 3.58e+00]    [3.17e+00, 3.07e+00, 2.01e+00, 1.93e+00]    
30000     [3.39e+00, 8.70e-02, 3.56e+00]    [0.00e+00, 3.02e+00, 3.56e+00]    [3.13e+00, 3.02e+00, 2.00e+00, 1.93e+00]    

Best model at step 30000:
  train loss: 7.04e+00
  test loss: 6.58e+00
  test metric: [3.13e+00, 3.02e+00, 2.00e+00, 1.93e+00]

'train' took 81.647621 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.523599 s

'compile' took 2.262947 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.02e+02, 2.79e+00]    [0.00e+00, 1.01e+02, 2.79e+00]    [1.01e+02, 1.01e+02, 3.10e+00, 3.10e+00]    
1000      [8.04e+01, 5.96e+01, 2.83e+00]    [0.00e+00, 8.42e+01, 2.83e+00]    [8.41e+01, 8.42e+01, 2.21e+01, 2.16e+01]    
2000      [4.29e+01, 1.90e+01, 3.76e+00]    [0.00e+00, 5.30e+01, 3.76e+00]    [5.28e+01, 5.30e+01, 3.01e+01, 2.67e+01]    
3000      [2.55e+01, 5.24e+00, 3.98e+00]    [0.00e+00, 3.19e+01, 3.98e+00]    [3.33e+01, 3.19e+01, 2.30e+01, 1.97e+01]    
4000      [1.19e+01, 1.17e+00, 4.09e+00]    [0.00e+00, 1.27e+01, 4.09e+00]    [1.47e+01, 1.27e+01, 1.28e+01, 1.24e+01]    
5000      [6.85e+00, 1.01e+00, 4.13e+00]    [0.00e+00, 6.99e+00, 4.13e+00]    [7.61e+00, 6.99e+00, 7.62e+00, 7.43e+00]    
6000      [4.86e+00, 7.67e-01, 4.14e+00]    [0.00e+00, 4.93e+00, 4.14e+00]    [5.04e+00, 4.93e+00, 4.62e+00, 4.57e+00]    
7000      [4.10e+00, 7.36e-01, 4.12e+00]    [0.00e+00, 3.61e+00, 4.12e+00]    [3.62e+00, 3.61e+00, 3.21e+00, 3.22e+00]    
8000      [3.79e+00, 5.87e-01, 4.10e+00]    [0.00e+00, 2.87e+00, 4.10e+00]    [2.87e+00, 2.87e+00, 2.35e+00, 2.34e+00]    
9000      [3.58e+00, 4.87e-01, 4.07e+00]    [0.00e+00, 2.49e+00, 4.07e+00]    [2.54e+00, 2.49e+00, 1.79e+00, 1.79e+00]    
10000     [3.49e+00, 3.91e-01, 4.04e+00]    [0.00e+00, 2.52e+00, 4.04e+00]    [2.63e+00, 2.52e+00, 1.56e+00, 1.54e+00]    
11000     [3.38e+00, 2.77e-01, 4.01e+00]    [0.00e+00, 2.48e+00, 4.01e+00]    [2.60e+00, 2.48e+00, 1.67e+00, 1.64e+00]    
12000     [3.32e+00, 1.96e-01, 3.98e+00]    [0.00e+00, 2.66e+00, 3.98e+00]    [2.80e+00, 2.66e+00, 1.59e+00, 1.63e+00]    
13000     [3.30e+00, 1.89e-01, 3.95e+00]    [0.00e+00, 3.13e+00, 3.95e+00]    [3.26e+00, 3.13e+00, 1.45e+00, 1.49e+00]    
14000     [3.27e+00, 9.76e-02, 3.92e+00]    [0.00e+00, 3.31e+00, 3.92e+00]    [3.40e+00, 3.31e+00, 1.37e+00, 1.39e+00]    
15000     [3.23e+00, 8.86e-02, 3.89e+00]    [0.00e+00, 3.55e+00, 3.89e+00]    [3.62e+00, 3.55e+00, 1.35e+00, 1.36e+00]    
16000     [3.15e+00, 1.01e-02, 3.86e+00]    [0.00e+00, 3.52e+00, 3.86e+00]    [3.59e+00, 3.52e+00, 1.38e+00, 1.39e+00]    
17000     [3.12e+00, 3.14e-02, 3.83e+00]    [0.00e+00, 3.68e+00, 3.83e+00]    [3.76e+00, 3.68e+00, 1.32e+00, 1.33e+00]    
18000     [3.12e+00, 4.34e-02, 3.80e+00]    [0.00e+00, 3.57e+00, 3.80e+00]    [3.64e+00, 3.57e+00, 1.32e+00, 1.34e+00]    
19000     [3.05e+00, 8.69e-03, 3.77e+00]    [0.00e+00, 3.65e+00, 3.77e+00]    [3.73e+00, 3.65e+00, 1.27e+00, 1.29e+00]    
20000     [3.04e+00, 3.91e-02, 3.75e+00]    [0.00e+00, 3.66e+00, 3.75e+00]    [3.73e+00, 3.66e+00, 1.26e+00, 1.27e+00]    
21000     [3.02e+00, 6.31e-02, 3.72e+00]    [0.00e+00, 3.73e+00, 3.72e+00]    [3.79e+00, 3.73e+00, 1.24e+00, 1.25e+00]    
22000     [3.01e+00, 5.24e-02, 3.69e+00]    [0.00e+00, 3.80e+00, 3.69e+00]    [3.84e+00, 3.80e+00, 1.27e+00, 1.28e+00]    
23000     [2.96e+00, 3.20e-02, 3.67e+00]    [0.00e+00, 3.64e+00, 3.67e+00]    [3.67e+00, 3.64e+00, 1.31e+00, 1.32e+00]    
24000     [2.96e+00, 8.22e-02, 3.65e+00]    [0.00e+00, 3.60e+00, 3.65e+00]    [3.62e+00, 3.60e+00, 1.35e+00, 1.35e+00]    
25000     [2.94e+00, 2.37e-02, 3.62e+00]    [0.00e+00, 3.64e+00, 3.62e+00]    [3.66e+00, 3.64e+00, 1.36e+00, 1.36e+00]    
26000     [2.93e+00, 1.13e-01, 3.60e+00]    [0.00e+00, 3.69e+00, 3.60e+00]    [3.70e+00, 3.69e+00, 1.35e+00, 1.36e+00]    
27000     [2.92e+00, 8.77e-02, 3.58e+00]    [0.00e+00, 3.56e+00, 3.58e+00]    [3.59e+00, 3.56e+00, 1.38e+00, 1.39e+00]    
28000     [2.89e+00, 2.40e-02, 3.55e+00]    [0.00e+00, 3.58e+00, 3.55e+00]    [3.62e+00, 3.58e+00, 1.35e+00, 1.36e+00]    
29000     [2.88e+00, 7.22e-02, 3.53e+00]    [0.00e+00, 3.58e+00, 3.53e+00]    [3.64e+00, 3.58e+00, 1.34e+00, 1.36e+00]    
30000     [2.89e+00, 3.64e-02, 3.51e+00]    [0.00e+00, 3.57e+00, 3.51e+00]    [3.63e+00, 3.57e+00, 1.33e+00, 1.35e+00]    

Best model at step 30000:
  train loss: 6.44e+00
  test loss: 7.08e+00
  test metric: [3.63e+00, 3.57e+00, 1.33e+00, 1.35e+00]

'train' took 81.543899 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.535569 s

'compile' took 2.293865 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.01e+02, 2.79e+00]    [0.00e+00, 1.00e+02, 2.79e+00]    [1.00e+02, 1.00e+02, 1.08e+00, 1.08e+00]    
1000      [8.08e+01, 6.49e+01, 2.80e+00]    [0.00e+00, 8.81e+01, 2.80e+00]    [8.81e+01, 8.81e+01, 2.03e+01, 2.04e+01]    
2000      [6.43e+01, 3.41e+01, 3.44e+00]    [0.00e+00, 6.66e+01, 3.44e+00]    [6.67e+01, 6.66e+01, 2.01e+01, 1.96e+01]    
3000      [3.42e+01, 6.37e+00, 4.20e+00]    [0.00e+00, 3.85e+01, 4.20e+00]    [3.94e+01, 3.85e+01, 1.43e+01, 1.35e+01]    
4000      [1.75e+01, 1.38e+00, 4.39e+00]    [0.00e+00, 1.79e+01, 4.39e+00]    [1.93e+01, 1.79e+01, 1.33e+01, 1.36e+01]    
5000      [9.09e+00, 6.97e-01, 4.45e+00]    [0.00e+00, 8.16e+00, 4.45e+00]    [8.83e+00, 8.16e+00, 7.17e+00, 7.05e+00]    
6000      [4.33e+00, 5.09e-01, 4.50e+00]    [0.00e+00, 4.32e+00, 4.50e+00]    [4.50e+00, 4.32e+00, 1.58e+00, 1.67e+00]    
7000      [3.85e+00, 2.56e-01, 4.46e+00]    [0.00e+00, 4.36e+00, 4.46e+00]    [4.39e+00, 4.36e+00, 2.32e+00, 2.33e+00]    
8000      [3.61e+00, 1.36e-01, 4.40e+00]    [0.00e+00, 4.14e+00, 4.40e+00]    [4.13e+00, 4.14e+00, 2.49e+00, 2.48e+00]    
9000      [3.45e+00, 9.42e-02, 4.35e+00]    [0.00e+00, 3.98e+00, 4.35e+00]    [3.99e+00, 3.98e+00, 2.48e+00, 2.50e+00]    
10000     [3.32e+00, 1.92e-01, 4.30e+00]    [0.00e+00, 4.02e+00, 4.30e+00]    [4.03e+00, 4.02e+00, 2.31e+00, 2.30e+00]    
11000     [3.25e+00, 1.53e-01, 4.26e+00]    [0.00e+00, 4.02e+00, 4.26e+00]    [4.03e+00, 4.02e+00, 2.37e+00, 2.37e+00]    
12000     [3.15e+00, 1.97e-01, 4.22e+00]    [0.00e+00, 3.94e+00, 4.22e+00]    [3.96e+00, 3.94e+00, 2.30e+00, 2.27e+00]    
13000     [3.09e+00, 6.24e-02, 4.18e+00]    [0.00e+00, 3.97e+00, 4.18e+00]    [3.99e+00, 3.97e+00, 2.38e+00, 2.36e+00]    
14000     [3.05e+00, 9.04e-02, 4.15e+00]    [0.00e+00, 3.93e+00, 4.15e+00]    [3.95e+00, 3.93e+00, 2.41e+00, 2.37e+00]    
15000     [3.09e+00, 2.91e-01, 4.11e+00]    [0.00e+00, 3.96e+00, 4.11e+00]    [3.98e+00, 3.96e+00, 2.52e+00, 2.49e+00]    
16000     [2.97e+00, 2.51e-01, 4.08e+00]    [0.00e+00, 3.85e+00, 4.08e+00]    [3.87e+00, 3.85e+00, 2.38e+00, 2.27e+00]    
17000     [2.93e+00, 1.17e-01, 4.05e+00]    [0.00e+00, 3.87e+00, 4.05e+00]    [3.89e+00, 3.87e+00, 2.42e+00, 2.32e+00]    
18000     [2.93e+00, 2.99e-01, 4.02e+00]    [0.00e+00, 3.81e+00, 4.02e+00]    [3.83e+00, 3.81e+00, 2.38e+00, 2.24e+00]    
19000     [2.89e+00, 1.58e-01, 3.99e+00]    [0.00e+00, 3.84e+00, 3.99e+00]    [3.86e+00, 3.84e+00, 2.42e+00, 2.28e+00]    
20000     [2.93e+00, 1.94e-01, 3.97e+00]    [0.00e+00, 3.93e+00, 3.97e+00]    [3.95e+00, 3.93e+00, 2.54e+00, 2.45e+00]    
21000     [2.85e+00, 5.01e-02, 3.94e+00]    [0.00e+00, 3.88e+00, 3.94e+00]    [3.90e+00, 3.88e+00, 2.49e+00, 2.33e+00]    
22000     [2.87e+00, 1.26e-01, 3.91e+00]    [0.00e+00, 3.91e+00, 3.91e+00]    [3.93e+00, 3.91e+00, 2.56e+00, 2.40e+00]    
23000     [2.88e+00, 1.08e-01, 3.89e+00]    [0.00e+00, 3.87e+00, 3.89e+00]    [3.90e+00, 3.87e+00, 2.55e+00, 2.38e+00]    
24000     [2.85e+00, 1.68e-01, 3.87e+00]    [0.00e+00, 3.84e+00, 3.87e+00]    [3.90e+00, 3.84e+00, 2.46e+00, 2.27e+00]    
25000     [2.81e+00, 8.25e-02, 3.84e+00]    [0.00e+00, 3.85e+00, 3.84e+00]    [3.92e+00, 3.85e+00, 2.46e+00, 2.29e+00]    
26000     [2.85e+00, 2.60e-01, 3.82e+00]    [0.00e+00, 3.78e+00, 3.82e+00]    [3.86e+00, 3.78e+00, 2.40e+00, 2.21e+00]    
27000     [2.82e+00, 1.96e-01, 3.80e+00]    [0.00e+00, 3.80e+00, 3.80e+00]    [3.90e+00, 3.80e+00, 2.41e+00, 2.23e+00]    
28000     [2.84e+00, 2.27e-01, 3.78e+00]    [0.00e+00, 3.91e+00, 3.78e+00]    [4.03e+00, 3.91e+00, 2.51e+00, 2.42e+00]    
29000     [2.80e+00, 2.26e-01, 3.76e+00]    [0.00e+00, 3.81e+00, 3.76e+00]    [3.94e+00, 3.81e+00, 2.39e+00, 2.21e+00]    
30000     [2.79e+00, 6.01e-02, 3.74e+00]    [0.00e+00, 3.85e+00, 3.74e+00]    [3.99e+00, 3.85e+00, 2.44e+00, 2.31e+00]    

Best model at step 30000:
  train loss: 6.58e+00
  test loss: 7.59e+00
  test metric: [3.99e+00, 3.85e+00, 2.44e+00, 2.31e+00]

'train' took 80.086799 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.471739 s

'compile' took 2.059491 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.03e+02, 2.70e+00]    [0.00e+00, 1.01e+02, 2.70e+00]    [1.01e+02, 1.01e+02, 1.65e+00, 1.65e+00]    
1000      [7.95e+01, 6.72e+01, 2.71e+00]    [0.00e+00, 8.53e+01, 2.71e+00]    [8.54e+01, 8.53e+01, 1.74e+01, 1.75e+01]    
2000      [6.06e+01, 4.08e+01, 3.31e+00]    [0.00e+00, 5.90e+01, 3.31e+00]    [5.96e+01, 5.90e+01, 2.02e+01, 1.91e+01]    
3000      [3.33e+01, 1.24e+01, 4.05e+00]    [0.00e+00, 3.28e+01, 4.05e+00]    [3.47e+01, 3.28e+01, 1.15e+01, 9.15e+00]    
4000      [2.03e+01, 4.70e+00, 4.24e+00]    [0.00e+00, 1.90e+01, 4.24e+00]    [2.18e+01, 1.90e+01, 6.56e+00, 1.03e+01]    
5000      [9.51e+00, 1.15e+00, 4.38e+00]    [0.00e+00, 7.61e+00, 4.38e+00]    [8.28e+00, 7.61e+00, 4.37e+00, 6.74e+00]    
6000      [7.12e+00, 3.38e-02, 4.33e+00]    [0.00e+00, 5.11e+00, 4.33e+00]    [4.71e+00, 5.11e+00, 2.90e+00, 3.47e+00]    
7000      [5.75e+00, 1.04e-01, 4.28e+00]    [0.00e+00, 3.85e+00, 4.28e+00]    [3.18e+00, 3.85e+00, 2.30e+00, 2.68e+00]    
8000      [5.15e+00, 2.19e-01, 4.23e+00]    [0.00e+00, 3.37e+00, 4.23e+00]    [2.88e+00, 3.37e+00, 2.12e+00, 2.68e+00]    
9000      [4.73e+00, 8.24e-02, 4.18e+00]    [0.00e+00, 3.08e+00, 4.18e+00]    [2.75e+00, 3.08e+00, 2.26e+00, 2.80e+00]    
10000     [4.47e+00, 5.57e-02, 4.14e+00]    [0.00e+00, 3.15e+00, 4.14e+00]    [2.80e+00, 3.15e+00, 2.43e+00, 2.80e+00]    
11000     [4.22e+00, 1.19e-01, 4.10e+00]    [0.00e+00, 3.20e+00, 4.10e+00]    [2.85e+00, 3.20e+00, 2.63e+00, 2.90e+00]    
12000     [4.06e+00, 8.86e-02, 4.07e+00]    [0.00e+00, 3.16e+00, 4.07e+00]    [2.89e+00, 3.16e+00, 2.61e+00, 2.95e+00]    
13000     [3.93e+00, 8.12e-02, 4.04e+00]    [0.00e+00, 3.19e+00, 4.04e+00]    [3.01e+00, 3.19e+00, 2.49e+00, 2.90e+00]    
14000     [3.81e+00, 9.37e-02, 4.01e+00]    [0.00e+00, 3.27e+00, 4.01e+00]    [3.11e+00, 3.27e+00, 2.42e+00, 2.78e+00]    
15000     [3.68e+00, 6.98e-02, 3.98e+00]    [0.00e+00, 3.32e+00, 3.98e+00]    [3.20e+00, 3.32e+00, 2.42e+00, 2.71e+00]    
16000     [3.57e+00, 4.30e-02, 3.96e+00]    [0.00e+00, 3.41e+00, 3.96e+00]    [3.32e+00, 3.41e+00, 2.42e+00, 2.62e+00]    
17000     [3.49e+00, 6.88e-02, 3.93e+00]    [0.00e+00, 3.57e+00, 3.93e+00]    [3.51e+00, 3.57e+00, 2.35e+00, 2.48e+00]    
18000     [3.39e+00, 1.23e-01, 3.91e+00]    [0.00e+00, 3.74e+00, 3.91e+00]    [3.70e+00, 3.74e+00, 2.25e+00, 2.32e+00]    
19000     [3.32e+00, 1.17e-01, 3.89e+00]    [0.00e+00, 3.88e+00, 3.89e+00]    [3.86e+00, 3.88e+00, 2.20e+00, 2.23e+00]    
20000     [3.22e+00, 1.03e-01, 3.86e+00]    [0.00e+00, 3.88e+00, 3.86e+00]    [3.88e+00, 3.88e+00, 2.14e+00, 2.13e+00]    
21000     [3.25e+00, 2.70e-01, 3.84e+00]    [0.00e+00, 3.92e+00, 3.84e+00]    [3.95e+00, 3.92e+00, 2.03e+00, 1.98e+00]    
22000     [3.18e+00, 1.48e-01, 3.82e+00]    [0.00e+00, 3.87e+00, 3.82e+00]    [3.93e+00, 3.87e+00, 1.94e+00, 1.86e+00]    
23000     [3.14e+00, 1.61e-01, 3.79e+00]    [0.00e+00, 3.80e+00, 3.79e+00]    [3.88e+00, 3.80e+00, 1.84e+00, 1.72e+00]    
24000     [3.08e+00, 7.15e-02, 3.77e+00]    [0.00e+00, 3.81e+00, 3.77e+00]    [3.91e+00, 3.81e+00, 1.79e+00, 1.65e+00]    
25000     [3.07e+00, 4.54e-02, 3.75e+00]    [0.00e+00, 3.79e+00, 3.75e+00]    [3.91e+00, 3.79e+00, 1.74e+00, 1.60e+00]    
26000     [3.04e+00, 7.24e-02, 3.73e+00]    [0.00e+00, 3.77e+00, 3.73e+00]    [3.90e+00, 3.77e+00, 1.68e+00, 1.53e+00]    
27000     [3.11e+00, 2.91e-01, 3.71e+00]    [0.00e+00, 3.78e+00, 3.71e+00]    [3.93e+00, 3.78e+00, 1.64e+00, 1.51e+00]    
28000     [3.01e+00, 7.03e-02, 3.69e+00]    [0.00e+00, 3.71e+00, 3.69e+00]    [3.86e+00, 3.71e+00, 1.55e+00, 1.42e+00]    
29000     [3.00e+00, 8.13e-02, 3.67e+00]    [0.00e+00, 3.72e+00, 3.67e+00]    [3.88e+00, 3.72e+00, 1.52e+00, 1.41e+00]    
30000     [3.04e+00, 2.26e-01, 3.65e+00]    [0.00e+00, 3.67e+00, 3.65e+00]    [3.84e+00, 3.67e+00, 1.48e+00, 1.38e+00]    

Best model at step 29000:
  train loss: 6.75e+00
  test loss: 7.39e+00
  test metric: [3.88e+00, 3.72e+00, 1.52e+00, 1.41e+00]

'train' took 79.074413 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[2.5972664059509825, 2.4234453826274436, 3.1501456343239305, 6.387998001033118, 2.9286676685339774, 3.3999017399496645, 3.0194878018507474, 3.5679991368779485, 3.8514838369605036, 3.7214722003963767]
E* 6 3.504786780850469 1.0574093889066043
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.456779 s

'compile' took 2.024585 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.02e+02, 2.63e+00]    [0.00e+00, 1.00e+02, 2.63e+00]    [1.00e+02, 1.00e+02, 6.94e-01, 6.94e-01]    
1000      [7.89e+01, 5.47e+01, 2.77e+00]    [0.00e+00, 8.93e+01, 2.77e+00]    [8.94e+01, 8.93e+01, 7.06e+00, 7.09e+00]    
2000      [5.05e+01, 2.54e+01, 3.40e+00]    [0.00e+00, 6.00e+01, 3.40e+00]    [6.08e+01, 6.00e+01, 2.44e+01, 2.50e+01]    
3000      [2.47e+01, 8.76e+00, 3.83e+00]    [0.00e+00, 3.25e+01, 3.83e+00]    [3.34e+01, 3.25e+01, 2.66e+01, 2.66e+01]    
4000      [1.20e+01, 3.19e+00, 3.98e+00]    [0.00e+00, 1.50e+01, 3.98e+00]    [1.57e+01, 1.50e+01, 1.83e+01, 1.82e+01]    
5000      [6.97e+00, 6.34e-01, 4.02e+00]    [0.00e+00, 9.15e+00, 4.02e+00]    [9.30e+00, 9.15e+00, 8.20e+00, 8.07e+00]    
6000      [5.60e+00, 1.84e-01, 3.98e+00]    [0.00e+00, 7.31e+00, 3.98e+00]    [7.32e+00, 7.31e+00, 5.43e+00, 5.43e+00]    
7000      [4.97e+00, 6.88e-02, 3.94e+00]    [0.00e+00, 6.10e+00, 3.94e+00]    [6.05e+00, 6.10e+00, 3.94e+00, 3.95e+00]    
8000      [4.45e+00, 1.38e-01, 3.92e+00]    [0.00e+00, 4.99e+00, 3.92e+00]    [4.97e+00, 4.99e+00, 2.92e+00, 2.98e+00]    
9000      [4.10e+00, 9.63e-02, 3.90e+00]    [0.00e+00, 4.43e+00, 3.90e+00]    [4.39e+00, 4.43e+00, 2.28e+00, 2.24e+00]    
10000     [3.87e+00, 4.98e-02, 3.88e+00]    [0.00e+00, 3.95e+00, 3.88e+00]    [3.91e+00, 3.95e+00, 2.28e+00, 2.09e+00]    
11000     [3.76e+00, 1.72e-02, 3.85e+00]    [0.00e+00, 3.71e+00, 3.85e+00]    [3.66e+00, 3.71e+00, 2.41e+00, 2.15e+00]    
12000     [3.67e+00, 6.02e-02, 3.82e+00]    [0.00e+00, 3.50e+00, 3.82e+00]    [3.45e+00, 3.50e+00, 2.56e+00, 2.27e+00]    
13000     [3.60e+00, 8.40e-02, 3.80e+00]    [0.00e+00, 3.46e+00, 3.80e+00]    [3.42e+00, 3.46e+00, 2.58e+00, 2.27e+00]    
14000     [3.55e+00, 1.77e-01, 3.77e+00]    [0.00e+00, 3.41e+00, 3.77e+00]    [3.36e+00, 3.41e+00, 2.59e+00, 2.28e+00]    
15000     [3.49e+00, 4.96e-02, 3.74e+00]    [0.00e+00, 3.37e+00, 3.74e+00]    [3.32e+00, 3.37e+00, 2.59e+00, 2.27e+00]    
16000     [3.48e+00, 1.71e-01, 3.72e+00]    [0.00e+00, 3.33e+00, 3.72e+00]    [3.27e+00, 3.33e+00, 2.59e+00, 2.26e+00]    
17000     [3.40e+00, 6.20e-02, 3.69e+00]    [0.00e+00, 3.26e+00, 3.69e+00]    [3.21e+00, 3.26e+00, 2.65e+00, 2.30e+00]    
18000     [3.36e+00, 6.75e-02, 3.67e+00]    [0.00e+00, 3.22e+00, 3.67e+00]    [3.18e+00, 3.22e+00, 2.66e+00, 2.29e+00]    
19000     [3.33e+00, 3.38e-02, 3.65e+00]    [0.00e+00, 3.17e+00, 3.65e+00]    [3.16e+00, 3.17e+00, 2.63e+00, 2.27e+00]    
20000     [3.32e+00, 1.06e-01, 3.63e+00]    [0.00e+00, 3.14e+00, 3.63e+00]    [3.16e+00, 3.14e+00, 2.62e+00, 2.26e+00]    
21000     [3.30e+00, 1.19e-01, 3.61e+00]    [0.00e+00, 3.12e+00, 3.61e+00]    [3.19e+00, 3.12e+00, 2.59e+00, 2.27e+00]    
22000     [3.27e+00, 8.45e-02, 3.58e+00]    [0.00e+00, 3.13e+00, 3.58e+00]    [3.15e+00, 3.13e+00, 2.63e+00, 2.22e+00]    
23000     [3.25e+00, 1.02e-01, 3.56e+00]    [0.00e+00, 3.09e+00, 3.56e+00]    [3.20e+00, 3.09e+00, 2.57e+00, 2.24e+00]    
24000     [3.20e+00, 3.43e-02, 3.54e+00]    [0.00e+00, 3.10e+00, 3.54e+00]    [3.18e+00, 3.10e+00, 2.58e+00, 2.20e+00]    
25000     [3.22e+00, 1.42e-01, 3.52e+00]    [0.00e+00, 3.11e+00, 3.52e+00]    [3.17e+00, 3.11e+00, 2.56e+00, 2.16e+00]    
26000     [3.18e+00, 5.20e-02, 3.51e+00]    [0.00e+00, 3.09e+00, 3.51e+00]    [3.20e+00, 3.09e+00, 2.54e+00, 2.17e+00]    
27000     [3.14e+00, 5.44e-02, 3.49e+00]    [0.00e+00, 3.08e+00, 3.49e+00]    [3.24e+00, 3.08e+00, 2.50e+00, 2.17e+00]    
28000     [3.12e+00, 6.03e-02, 3.47e+00]    [0.00e+00, 3.09e+00, 3.47e+00]    [3.26e+00, 3.09e+00, 2.48e+00, 2.14e+00]    
29000     [3.12e+00, 7.98e-02, 3.45e+00]    [0.00e+00, 3.07e+00, 3.45e+00]    [3.25e+00, 3.07e+00, 2.48e+00, 2.13e+00]    
30000     [3.12e+00, 1.96e-01, 3.43e+00]    [0.00e+00, 3.10e+00, 3.43e+00]    [3.28e+00, 3.10e+00, 2.46e+00, 2.09e+00]    

Best model at step 28000:
  train loss: 6.65e+00
  test loss: 6.56e+00
  test metric: [3.26e+00, 3.09e+00, 2.48e+00, 2.14e+00]

'train' took 79.323422 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.417882 s

'compile' took 2.067472 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.79e+01, 1.00e+02, 2.76e+00]    [0.00e+00, 9.53e+01, 2.76e+00]    [9.53e+01, 9.53e+01, 4.80e+00, 4.80e+00]    
1000      [5.68e+01, 5.49e+01, 3.43e+00]    [0.00e+00, 6.07e+01, 3.43e+00]    [6.12e+01, 6.07e+01, 3.33e+01, 3.38e+01]    
2000      [1.32e+01, 3.52e+00, 4.38e+00]    [0.00e+00, 1.92e+01, 4.38e+00]    [2.07e+01, 1.92e+01, 2.04e+01, 2.06e+01]    
3000      [6.14e+00, 2.27e+00, 4.37e+00]    [0.00e+00, 7.31e+00, 4.37e+00]    [7.69e+00, 7.31e+00, 8.56e+00, 8.28e+00]    
4000      [4.10e+00, 1.82e+00, 4.33e+00]    [0.00e+00, 4.10e+00, 4.33e+00]    [4.17e+00, 4.10e+00, 3.61e+00, 3.59e+00]    
5000      [3.58e+00, 1.50e+00, 4.27e+00]    [0.00e+00, 3.41e+00, 4.27e+00]    [3.12e+00, 3.41e+00, 2.14e+00, 2.23e+00]    
6000      [3.29e+00, 1.23e+00, 4.22e+00]    [0.00e+00, 3.20e+00, 4.22e+00]    [2.76e+00, 3.20e+00, 1.58e+00, 2.13e+00]    
7000      [3.15e+00, 9.86e-01, 4.18e+00]    [0.00e+00, 3.14e+00, 4.18e+00]    [2.60e+00, 3.14e+00, 1.54e+00, 2.30e+00]    
8000      [3.04e+00, 7.36e-01, 4.14e+00]    [0.00e+00, 3.43e+00, 4.14e+00]    [2.73e+00, 3.43e+00, 1.67e+00, 2.66e+00]    
9000      [2.95e+00, 5.11e-01, 4.11e+00]    [0.00e+00, 3.62e+00, 4.11e+00]    [2.80e+00, 3.62e+00, 1.68e+00, 2.77e+00]    
10000     [2.91e+00, 3.16e-01, 4.08e+00]    [0.00e+00, 3.65e+00, 4.08e+00]    [2.76e+00, 3.65e+00, 1.70e+00, 2.87e+00]    
11000     [2.89e+00, 1.37e-01, 4.06e+00]    [0.00e+00, 3.55e+00, 4.06e+00]    [2.54e+00, 3.55e+00, 1.60e+00, 2.78e+00]    
12000     [2.82e+00, 2.08e-02, 4.02e+00]    [0.00e+00, 3.52e+00, 4.02e+00]    [2.56e+00, 3.52e+00, 1.60e+00, 2.84e+00]    
13000     [2.78e+00, 1.30e-02, 3.97e+00]    [0.00e+00, 3.39e+00, 3.97e+00]    [2.55e+00, 3.39e+00, 1.57e+00, 2.83e+00]    
14000     [2.75e+00, 2.05e-02, 3.93e+00]    [0.00e+00, 3.35e+00, 3.93e+00]    [2.53e+00, 3.35e+00, 1.62e+00, 2.85e+00]    
15000     [2.70e+00, 2.39e-02, 3.88e+00]    [0.00e+00, 3.39e+00, 3.88e+00]    [2.44e+00, 3.39e+00, 1.68e+00, 2.81e+00]    
16000     [2.69e+00, 1.23e-02, 3.84e+00]    [0.00e+00, 3.17e+00, 3.84e+00]    [2.51e+00, 3.17e+00, 1.64e+00, 2.87e+00]    
17000     [2.64e+00, 1.59e-02, 3.80e+00]    [0.00e+00, 3.22e+00, 3.80e+00]    [2.45e+00, 3.22e+00, 1.70e+00, 2.84e+00]    
18000     [2.61e+00, 2.11e-02, 3.76e+00]    [0.00e+00, 3.14e+00, 3.76e+00]    [2.47e+00, 3.14e+00, 1.73e+00, 2.89e+00]    
19000     [2.58e+00, 5.58e-03, 3.72e+00]    [0.00e+00, 3.09e+00, 3.72e+00]    [2.43e+00, 3.09e+00, 1.72e+00, 2.85e+00]    
20000     [2.56e+00, 1.62e-02, 3.69e+00]    [0.00e+00, 3.06e+00, 3.69e+00]    [2.47e+00, 3.06e+00, 1.72e+00, 2.85e+00]    
21000     [2.55e+00, 1.88e-02, 3.65e+00]    [0.00e+00, 3.11e+00, 3.65e+00]    [2.41e+00, 3.11e+00, 1.72e+00, 2.78e+00]    
22000     [2.56e+00, 2.84e-02, 3.62e+00]    [0.00e+00, 2.97e+00, 3.62e+00]    [2.55e+00, 2.97e+00, 1.63e+00, 2.69e+00]    
23000     [2.51e+00, 1.94e-02, 3.59e+00]    [0.00e+00, 3.07e+00, 3.59e+00]    [2.48e+00, 3.07e+00, 1.60e+00, 2.72e+00]    
24000     [2.53e+00, 3.55e-02, 3.56e+00]    [0.00e+00, 3.00e+00, 3.56e+00]    [2.56e+00, 3.00e+00, 1.54e+00, 2.77e+00]    
25000     [2.47e+00, 4.75e-03, 3.53e+00]    [0.00e+00, 2.99e+00, 3.53e+00]    [2.49e+00, 2.99e+00, 1.50e+00, 2.64e+00]    
26000     [2.49e+00, 3.19e-02, 3.50e+00]    [0.00e+00, 2.97e+00, 3.50e+00]    [2.48e+00, 2.97e+00, 1.46e+00, 2.65e+00]    
27000     [2.45e+00, 1.77e-02, 3.47e+00]    [0.00e+00, 2.97e+00, 3.47e+00]    [2.46e+00, 2.97e+00, 1.43e+00, 2.61e+00]    
28000     [2.44e+00, 1.33e-02, 3.45e+00]    [0.00e+00, 2.82e+00, 3.45e+00]    [2.57e+00, 2.82e+00, 1.38e+00, 2.60e+00]    
29000     [2.43e+00, 2.65e-02, 3.42e+00]    [0.00e+00, 2.81e+00, 3.42e+00]    [2.54e+00, 2.81e+00, 1.36e+00, 2.54e+00]    
30000     [2.43e+00, 1.16e-02, 3.40e+00]    [0.00e+00, 2.78e+00, 3.40e+00]    [2.61e+00, 2.78e+00, 1.32e+00, 2.60e+00]    

Best model at step 30000:
  train loss: 5.84e+00
  test loss: 6.18e+00
  test metric: [2.61e+00, 2.78e+00, 1.32e+00, 2.60e+00]

'train' took 80.080558 s


Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.421057 s

'compile' took 2.050957 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.89e+01, 9.80e+01, 2.74e+00]    [0.00e+00, 1.00e+02, 2.74e+00]    [1.00e+02, 1.00e+02, 7.06e-01, 7.06e-01]    
1000      [7.83e+01, 4.88e+01, 2.90e+00]    [0.00e+00, 9.29e+01, 2.90e+00]    [9.30e+01, 9.29e+01, 9.32e+00, 9.40e+00]    
2000      [5.54e+01, 3.15e+01, 3.47e+00]    [0.00e+00, 6.58e+01, 3.47e+00]    [6.69e+01, 6.58e+01, 1.28e+01, 1.33e+01]    
3000      [2.14e+01, 2.48e+00, 4.27e+00]    [0.00e+00, 2.17e+01, 4.27e+00]    [2.65e+01, 2.17e+01, 1.57e+01, 1.49e+01]    
4000      [9.41e+00, 1.11e+00, 4.38e+00]    [0.00e+00, 9.16e+00, 4.38e+00]    [1.18e+01, 9.16e+00, 6.26e+00, 6.41e+00]    
5000      [5.23e+00, 1.74e+00, 4.41e+00]    [0.00e+00, 4.44e+00, 4.41e+00]    [5.73e+00, 4.44e+00, 3.66e+00, 4.99e+00]    
6000      [4.64e+00, 1.15e+00, 4.39e+00]    [0.00e+00, 3.39e+00, 4.39e+00]    [4.15e+00, 3.39e+00, 3.30e+00, 4.05e+00]    
7000      [4.29e+00, 8.29e-01, 4.36e+00]    [0.00e+00, 3.22e+00, 4.36e+00]    [3.32e+00, 3.22e+00, 3.26e+00, 3.39e+00]    
8000      [3.99e+00, 8.08e-01, 4.34e+00]    [0.00e+00, 3.25e+00, 4.34e+00]    [3.16e+00, 3.25e+00, 2.56e+00, 2.47e+00]    
9000      [3.76e+00, 6.47e-01, 4.31e+00]    [0.00e+00, 3.38e+00, 4.31e+00]    [3.30e+00, 3.38e+00, 2.17e+00, 2.07e+00]    
10000     [3.60e+00, 5.57e-01, 4.28e+00]    [0.00e+00, 3.45e+00, 4.28e+00]    [3.49e+00, 3.45e+00, 1.79e+00, 1.84e+00]    
11000     [3.51e+00, 4.38e-01, 4.25e+00]    [0.00e+00, 3.32e+00, 4.25e+00]    [3.52e+00, 3.32e+00, 1.50e+00, 1.81e+00]    
12000     [3.43e+00, 3.61e-01, 4.23e+00]    [0.00e+00, 3.27e+00, 4.23e+00]    [3.61e+00, 3.27e+00, 1.28e+00, 1.86e+00]    
13000     [3.37e+00, 2.79e-01, 4.20e+00]    [0.00e+00, 3.27e+00, 4.20e+00]    [3.76e+00, 3.27e+00, 1.11e+00, 1.96e+00]    
14000     [3.34e+00, 1.73e-01, 4.19e+00]    [0.00e+00, 3.36e+00, 4.19e+00]    [4.01e+00, 3.36e+00, 8.97e-01, 2.00e+00]    
15000     [3.32e+00, 1.77e-01, 4.17e+00]    [0.00e+00, 3.33e+00, 4.17e+00]    [4.10e+00, 3.33e+00, 8.90e-01, 2.16e+00]    
16000     [3.26e+00, 1.31e-01, 4.14e+00]    [0.00e+00, 3.38e+00, 4.14e+00]    [4.18e+00, 3.38e+00, 8.63e-01, 2.07e+00]    
17000     [3.22e+00, 8.24e-02, 4.11e+00]    [0.00e+00, 3.39e+00, 4.11e+00]    [4.20e+00, 3.39e+00, 8.94e-01, 2.03e+00]    
18000     [3.20e+00, 9.31e-02, 4.08e+00]    [0.00e+00, 3.44e+00, 4.08e+00]    [4.25e+00, 3.44e+00, 9.39e-01, 1.97e+00]    
19000     [3.19e+00, 1.24e-01, 4.06e+00]    [0.00e+00, 3.48e+00, 4.06e+00]    [4.30e+00, 3.48e+00, 9.92e-01, 1.91e+00]    
20000     [3.14e+00, 3.61e-02, 4.03e+00]    [0.00e+00, 3.51e+00, 4.03e+00]    [4.32e+00, 3.51e+00, 1.12e+00, 1.91e+00]    
21000     [3.13e+00, 1.10e-01, 4.01e+00]    [0.00e+00, 3.59e+00, 4.01e+00]    [4.39e+00, 3.59e+00, 1.21e+00, 1.88e+00]    
22000     [3.08e+00, 6.02e-02, 3.99e+00]    [0.00e+00, 3.65e+00, 3.99e+00]    [4.43e+00, 3.65e+00, 1.27e+00, 1.86e+00]    
23000     [3.07e+00, 5.33e-02, 3.96e+00]    [0.00e+00, 3.71e+00, 3.96e+00]    [4.46e+00, 3.71e+00, 1.31e+00, 1.84e+00]    
24000     [3.06e+00, 6.91e-02, 3.94e+00]    [0.00e+00, 3.76e+00, 3.94e+00]    [4.50e+00, 3.76e+00, 1.40e+00, 1.86e+00]    
25000     [3.06e+00, 1.87e-01, 3.92e+00]    [0.00e+00, 3.78e+00, 3.92e+00]    [4.52e+00, 3.78e+00, 1.46e+00, 1.86e+00]    
26000     [3.07e+00, 2.11e-01, 3.89e+00]    [0.00e+00, 3.81e+00, 3.89e+00]    [4.54e+00, 3.81e+00, 1.53e+00, 1.91e+00]    
27000     [3.04e+00, 7.99e-02, 3.87e+00]    [0.00e+00, 3.79e+00, 3.87e+00]    [4.52e+00, 3.79e+00, 1.52e+00, 1.91e+00]    
28000     [3.02e+00, 1.22e-01, 3.84e+00]    [0.00e+00, 3.80e+00, 3.84e+00]    [4.52e+00, 3.80e+00, 1.49e+00, 1.88e+00]    
29000     [3.06e+00, 2.95e-01, 3.82e+00]    [0.00e+00, 3.78e+00, 3.82e+00]    [4.50e+00, 3.78e+00, 1.49e+00, 1.87e+00]    
30000     [3.00e+00, 1.62e-01, 3.80e+00]    [0.00e+00, 3.77e+00, 3.80e+00]    [4.48e+00, 3.77e+00, 1.49e+00, 1.88e+00]    

Best model at step 30000:
  train loss: 6.96e+00
  test loss: 7.57e+00
  test metric: [4.48e+00, 3.77e+00, 1.49e+00, 1.88e+00]

'train' took 78.969471 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.463176 s

'compile' took 2.180116 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.01e+02, 2.83e+00]    [0.00e+00, 1.01e+02, 2.83e+00]    [1.01e+02, 1.01e+02, 1.47e+00, 1.47e+00]    
1000      [7.76e+01, 6.76e+01, 2.94e+00]    [0.00e+00, 8.66e+01, 2.94e+00]    [8.67e+01, 8.66e+01, 1.76e+01, 1.77e+01]    
2000      [5.06e+01, 2.34e+01, 3.73e+00]    [0.00e+00, 6.19e+01, 3.73e+00]    [6.36e+01, 6.19e+01, 2.74e+01, 2.87e+01]    
3000      [3.33e+01, 1.36e+01, 3.99e+00]    [0.00e+00, 4.85e+01, 3.99e+00]    [4.99e+01, 4.85e+01, 1.98e+01, 2.03e+01]    
4000      [1.62e+01, 3.00e+00, 4.30e+00]    [0.00e+00, 2.92e+01, 4.30e+00]    [3.04e+01, 2.92e+01, 9.94e+00, 1.01e+01]    
5000      [1.03e+01, 8.92e-01, 4.35e+00]    [0.00e+00, 2.01e+01, 4.35e+00]    [2.01e+01, 2.01e+01, 1.20e+01, 1.20e+01]    
6000      [7.75e+00, 6.24e-01, 4.35e+00]    [0.00e+00, 1.52e+01, 4.35e+00]    [1.44e+01, 1.52e+01, 1.23e+01, 1.21e+01]    
7000      [6.42e+00, 6.83e-01, 4.32e+00]    [0.00e+00, 1.26e+01, 4.32e+00]    [1.14e+01, 1.26e+01, 1.18e+01, 1.18e+01]    
8000      [5.48e+00, 5.79e-01, 4.29e+00]    [0.00e+00, 1.07e+01, 4.29e+00]    [9.43e+00, 1.07e+01, 1.15e+01, 1.17e+01]    
9000      [5.13e+00, 6.00e-01, 4.25e+00]    [0.00e+00, 1.00e+01, 4.25e+00]    [8.39e+00, 1.00e+01, 1.12e+01, 1.13e+01]    
10000     [4.83e+00, 4.35e-01, 4.21e+00]    [0.00e+00, 9.14e+00, 4.21e+00]    [7.39e+00, 9.14e+00, 1.06e+01, 1.07e+01]    
11000     [4.58e+00, 3.04e-01, 4.18e+00]    [0.00e+00, 8.79e+00, 4.18e+00]    [7.01e+00, 8.79e+00, 1.01e+01, 1.03e+01]    
12000     [4.40e+00, 2.45e-01, 4.14e+00]    [0.00e+00, 8.42e+00, 4.14e+00]    [6.87e+00, 8.42e+00, 9.52e+00, 9.96e+00]    
13000     [4.27e+00, 1.94e-01, 4.11e+00]    [0.00e+00, 7.99e+00, 4.11e+00]    [6.68e+00, 7.99e+00, 8.78e+00, 9.45e+00]    
14000     [4.13e+00, 2.03e-01, 4.08e+00]    [0.00e+00, 7.59e+00, 4.08e+00]    [6.56e+00, 7.59e+00, 8.19e+00, 9.09e+00]    
15000     [4.02e+00, 1.29e-01, 4.05e+00]    [0.00e+00, 7.40e+00, 4.05e+00]    [6.40e+00, 7.40e+00, 7.79e+00, 8.75e+00]    
16000     [3.93e+00, 1.29e-01, 4.02e+00]    [0.00e+00, 7.14e+00, 4.02e+00]    [6.27e+00, 7.14e+00, 7.45e+00, 8.50e+00]    
17000     [3.87e+00, 5.46e-02, 3.99e+00]    [0.00e+00, 6.88e+00, 3.99e+00]    [6.11e+00, 6.88e+00, 7.11e+00, 8.22e+00]    
18000     [3.78e+00, 2.88e-02, 3.97e+00]    [0.00e+00, 6.58e+00, 3.97e+00]    [5.95e+00, 6.58e+00, 6.67e+00, 7.87e+00]    
19000     [3.72e+00, 5.64e-02, 3.94e+00]    [0.00e+00, 6.35e+00, 3.94e+00]    [5.78e+00, 6.35e+00, 6.26e+00, 7.50e+00]    
20000     [3.68e+00, 1.35e-01, 3.91e+00]    [0.00e+00, 6.16e+00, 3.91e+00]    [5.68e+00, 6.16e+00, 5.87e+00, 7.14e+00]    
21000     [3.58e+00, 9.07e-02, 3.88e+00]    [0.00e+00, 6.03e+00, 3.88e+00]    [5.61e+00, 6.03e+00, 5.76e+00, 7.05e+00]    
22000     [3.52e+00, 3.19e-02, 3.86e+00]    [0.00e+00, 5.89e+00, 3.86e+00]    [5.49e+00, 5.89e+00, 5.37e+00, 6.67e+00]    
23000     [3.46e+00, 6.20e-02, 3.83e+00]    [0.00e+00, 5.76e+00, 3.83e+00]    [5.37e+00, 5.76e+00, 5.05e+00, 6.36e+00]    
24000     [3.40e+00, 3.83e-02, 3.81e+00]    [0.00e+00, 5.58e+00, 3.81e+00]    [5.21e+00, 5.58e+00, 4.63e+00, 5.96e+00]    
25000     [3.36e+00, 1.16e-01, 3.78e+00]    [0.00e+00, 5.47e+00, 3.78e+00]    [5.12e+00, 5.47e+00, 4.47e+00, 5.81e+00]    
26000     [3.30e+00, 5.99e-02, 3.76e+00]    [0.00e+00, 5.33e+00, 3.76e+00]    [5.00e+00, 5.33e+00, 4.17e+00, 5.52e+00]    
27000     [3.29e+00, 4.68e-02, 3.73e+00]    [0.00e+00, 5.19e+00, 3.73e+00]    [4.85e+00, 5.19e+00, 3.90e+00, 5.24e+00]    
28000     [3.28e+00, 1.19e-01, 3.71e+00]    [0.00e+00, 5.13e+00, 3.71e+00]    [4.77e+00, 5.13e+00, 3.89e+00, 5.22e+00]    
29000     [3.21e+00, 4.27e-02, 3.69e+00]    [0.00e+00, 5.04e+00, 3.69e+00]    [4.68e+00, 5.04e+00, 3.65e+00, 4.99e+00]    
30000     [3.22e+00, 2.41e-01, 3.66e+00]    [0.00e+00, 5.00e+00, 3.66e+00]    [4.65e+00, 5.00e+00, 3.72e+00, 5.07e+00]    

Best model at step 29000:
  train loss: 6.94e+00
  test loss: 8.72e+00
  test metric: [4.68e+00, 5.04e+00, 3.65e+00, 4.99e+00]

'train' took 79.607135 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.459991 s

'compile' took 2.061804 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.82e+01, 9.78e+01, 2.83e+00]    [0.00e+00, 9.89e+01, 2.83e+00]    [9.89e+01, 9.89e+01, 1.39e+00, 1.39e+00]    
1000      [7.95e+01, 7.12e+01, 2.84e+00]    [0.00e+00, 8.21e+01, 2.84e+00]    [8.21e+01, 8.21e+01, 2.07e+01, 2.08e+01]    
2000      [5.72e+01, 3.78e+01, 3.48e+00]    [0.00e+00, 6.38e+01, 3.48e+00]    [6.39e+01, 6.38e+01, 1.19e+01, 1.14e+01]    
3000      [2.75e+01, 1.33e+01, 4.07e+00]    [0.00e+00, 3.15e+01, 4.07e+00]    [3.26e+01, 3.15e+01, 1.08e+01, 1.20e+01]    
4000      [6.24e+00, 8.64e-01, 4.38e+00]    [0.00e+00, 9.88e+00, 4.38e+00]    [9.32e+00, 9.88e+00, 5.62e+00, 6.66e+00]    
5000      [5.38e+00, 1.61e-01, 4.32e+00]    [0.00e+00, 7.20e+00, 4.32e+00]    [6.92e+00, 7.20e+00, 3.85e+00, 4.41e+00]    
6000      [4.49e+00, 3.68e-01, 4.28e+00]    [0.00e+00, 5.54e+00, 4.28e+00]    [5.37e+00, 5.54e+00, 3.32e+00, 3.75e+00]    
7000      [4.16e+00, 1.64e-01, 4.24e+00]    [0.00e+00, 4.84e+00, 4.24e+00]    [4.75e+00, 4.84e+00, 3.29e+00, 3.50e+00]    
8000      [3.87e+00, 1.20e-01, 4.19e+00]    [0.00e+00, 4.26e+00, 4.19e+00]    [4.28e+00, 4.26e+00, 3.46e+00, 3.60e+00]    
9000      [3.72e+00, 3.58e-02, 4.15e+00]    [0.00e+00, 4.10e+00, 4.15e+00]    [4.10e+00, 4.10e+00, 3.42e+00, 3.45e+00]    
10000     [3.53e+00, 4.22e-02, 4.10e+00]    [0.00e+00, 3.97e+00, 4.10e+00]    [3.98e+00, 3.97e+00, 3.31e+00, 3.29e+00]    
11000     [3.42e+00, 1.10e-01, 4.06e+00]    [0.00e+00, 3.85e+00, 4.06e+00]    [3.87e+00, 3.85e+00, 3.19e+00, 3.13e+00]    
12000     [3.49e+00, 2.73e-01, 4.02e+00]    [0.00e+00, 3.83e+00, 4.02e+00]    [3.85e+00, 3.83e+00, 2.95e+00, 2.86e+00]    
13000     [3.42e+00, 1.92e-01, 3.98e+00]    [0.00e+00, 3.65e+00, 3.98e+00]    [3.69e+00, 3.65e+00, 2.82e+00, 2.69e+00]    
14000     [3.27e+00, 1.24e-01, 3.94e+00]    [0.00e+00, 3.54e+00, 3.94e+00]    [3.59e+00, 3.54e+00, 2.72e+00, 2.57e+00]    
15000     [3.27e+00, 2.33e-01, 3.91e+00]    [0.00e+00, 3.44e+00, 3.91e+00]    [3.52e+00, 3.44e+00, 2.59e+00, 2.45e+00]    
16000     [3.19e+00, 2.51e-02, 3.87e+00]    [0.00e+00, 3.40e+00, 3.87e+00]    [3.51e+00, 3.40e+00, 2.41e+00, 2.29e+00]    
17000     [3.17e+00, 1.11e-01, 3.84e+00]    [0.00e+00, 3.30e+00, 3.84e+00]    [3.43e+00, 3.30e+00, 2.32e+00, 2.20e+00]    
18000     [3.15e+00, 1.10e-01, 3.81e+00]    [0.00e+00, 3.21e+00, 3.81e+00]    [3.38e+00, 3.21e+00, 2.24e+00, 2.13e+00]    
19000     [3.12e+00, 4.19e-02, 3.78e+00]    [0.00e+00, 3.16e+00, 3.78e+00]    [3.34e+00, 3.16e+00, 2.14e+00, 2.04e+00]    
20000     [3.09e+00, 9.56e-02, 3.75e+00]    [0.00e+00, 3.15e+00, 3.75e+00]    [3.35e+00, 3.15e+00, 2.12e+00, 2.01e+00]    
21000     [3.08e+00, 1.11e-01, 3.72e+00]    [0.00e+00, 3.18e+00, 3.72e+00]    [3.38e+00, 3.18e+00, 2.10e+00, 1.98e+00]    
22000     [3.06e+00, 1.80e-02, 3.70e+00]    [0.00e+00, 3.17e+00, 3.70e+00]    [3.38e+00, 3.17e+00, 2.10e+00, 1.98e+00]    
23000     [3.04e+00, 1.79e-02, 3.67e+00]    [0.00e+00, 3.17e+00, 3.67e+00]    [3.39e+00, 3.17e+00, 2.08e+00, 1.96e+00]    
24000     [3.02e+00, 4.73e-02, 3.65e+00]    [0.00e+00, 3.17e+00, 3.65e+00]    [3.39e+00, 3.17e+00, 2.09e+00, 1.96e+00]    
25000     [3.06e+00, 2.27e-01, 3.62e+00]    [0.00e+00, 3.12e+00, 3.62e+00]    [3.35e+00, 3.12e+00, 2.15e+00, 1.99e+00]    
26000     [3.04e+00, 1.14e-01, 3.60e+00]    [0.00e+00, 3.23e+00, 3.60e+00]    [3.46e+00, 3.23e+00, 2.02e+00, 1.90e+00]    
27000     [3.03e+00, 1.28e-01, 3.58e+00]    [0.00e+00, 3.19e+00, 3.58e+00]    [3.43e+00, 3.19e+00, 2.09e+00, 1.92e+00]    
28000     [3.02e+00, 2.02e-01, 3.56e+00]    [0.00e+00, 3.11e+00, 3.56e+00]    [3.35e+00, 3.11e+00, 2.14e+00, 1.96e+00]    
29000     [2.96e+00, 3.13e-02, 3.53e+00]    [0.00e+00, 3.16e+00, 3.53e+00]    [3.41e+00, 3.16e+00, 2.05e+00, 1.89e+00]    
30000     [2.98e+00, 1.02e-01, 3.51e+00]    [0.00e+00, 3.19e+00, 3.51e+00]    [3.44e+00, 3.19e+00, 2.03e+00, 1.87e+00]    

Best model at step 29000:
  train loss: 6.53e+00
  test loss: 6.70e+00
  test metric: [3.41e+00, 3.16e+00, 2.05e+00, 1.89e+00]

'train' took 78.758116 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.459770 s

'compile' took 2.044343 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 9.94e+01, 2.55e+00]    [0.00e+00, 1.04e+02, 2.55e+00]    [1.04e+02, 1.04e+02, 4.05e+00, 4.05e+00]    
1000      [5.76e+01, 5.05e+01, 3.23e+00]    [0.00e+00, 6.87e+01, 3.23e+00]    [6.90e+01, 6.87e+01, 3.84e+01, 3.83e+01]    
2000      [2.53e+01, 1.15e+01, 3.85e+00]    [0.00e+00, 4.52e+01, 3.85e+00]    [4.56e+01, 4.52e+01, 3.41e+01, 3.36e+01]    
3000      [1.26e+01, 1.02e+00, 3.94e+00]    [0.00e+00, 2.61e+01, 3.94e+00]    [2.65e+01, 2.61e+01, 2.16e+01, 2.07e+01]    
4000      [8.56e+00, 7.47e-01, 3.91e+00]    [0.00e+00, 1.58e+01, 3.91e+00]    [1.60e+01, 1.58e+01, 1.41e+01, 1.37e+01]    
5000      [5.42e+00, 1.02e+00, 3.90e+00]    [0.00e+00, 8.30e+00, 3.90e+00]    [8.34e+00, 8.30e+00, 8.37e+00, 8.33e+00]    
6000      [3.74e+00, 1.11e+00, 3.92e+00]    [0.00e+00, 4.93e+00, 3.92e+00]    [4.90e+00, 4.93e+00, 3.52e+00, 3.84e+00]    
7000      [3.22e+00, 9.27e-01, 3.91e+00]    [0.00e+00, 3.82e+00, 3.91e+00]    [3.74e+00, 3.82e+00, 1.77e+00, 2.44e+00]    
8000      [3.03e+00, 7.11e-01, 3.89e+00]    [0.00e+00, 3.51e+00, 3.89e+00]    [3.38e+00, 3.51e+00, 1.54e+00, 2.47e+00]    
9000      [2.92e+00, 4.83e-01, 3.87e+00]    [0.00e+00, 3.66e+00, 3.87e+00]    [3.43e+00, 3.66e+00, 1.73e+00, 2.93e+00]    
10000     [2.88e+00, 3.19e-01, 3.84e+00]    [0.00e+00, 4.00e+00, 3.84e+00]    [3.65e+00, 4.00e+00, 1.88e+00, 3.25e+00]    
11000     [2.80e+00, 1.10e-01, 3.83e+00]    [0.00e+00, 4.33e+00, 3.83e+00]    [3.78e+00, 4.33e+00, 2.09e+00, 3.53e+00]    
12000     [2.76e+00, 1.85e-02, 3.79e+00]    [0.00e+00, 4.62e+00, 3.79e+00]    [3.84e+00, 4.62e+00, 2.26e+00, 3.72e+00]    
13000     [2.74e+00, 2.19e-02, 3.74e+00]    [0.00e+00, 4.54e+00, 3.74e+00]    [3.74e+00, 4.54e+00, 2.20e+00, 3.64e+00]    
14000     [2.69e+00, 3.62e-02, 3.70e+00]    [0.00e+00, 4.50e+00, 3.70e+00]    [3.60e+00, 4.50e+00, 2.18e+00, 3.56e+00]    
15000     [2.68e+00, 1.90e-02, 3.66e+00]    [0.00e+00, 4.52e+00, 3.66e+00]    [3.42e+00, 4.52e+00, 2.19e+00, 3.46e+00]    
16000     [2.66e+00, 5.38e-02, 3.61e+00]    [0.00e+00, 4.43e+00, 3.61e+00]    [3.47e+00, 4.43e+00, 2.14e+00, 3.45e+00]    
17000     [2.64e+00, 3.97e-02, 3.57e+00]    [0.00e+00, 4.45e+00, 3.57e+00]    [3.42e+00, 4.45e+00, 2.14e+00, 3.40e+00]    
18000     [2.60e+00, 7.85e-03, 3.54e+00]    [0.00e+00, 4.37e+00, 3.54e+00]    [3.49e+00, 4.37e+00, 2.08e+00, 3.40e+00]    
19000     [2.60e+00, 3.53e-02, 3.50e+00]    [0.00e+00, 4.36e+00, 3.50e+00]    [3.45e+00, 4.36e+00, 2.06e+00, 3.36e+00]    
20000     [2.58e+00, 1.56e-02, 3.47e+00]    [0.00e+00, 4.29e+00, 3.47e+00]    [3.51e+00, 4.29e+00, 2.03e+00, 3.37e+00]    
21000     [2.60e+00, 3.42e-02, 3.43e+00]    [0.00e+00, 4.27e+00, 3.43e+00]    [3.49e+00, 4.27e+00, 2.02e+00, 3.30e+00]    
22000     [2.57e+00, 2.36e-02, 3.40e+00]    [0.00e+00, 4.23e+00, 3.40e+00]    [3.52e+00, 4.23e+00, 2.00e+00, 3.30e+00]    
23000     [2.54e+00, 2.49e-02, 3.37e+00]    [0.00e+00, 4.27e+00, 3.37e+00]    [3.44e+00, 4.27e+00, 1.98e+00, 3.25e+00]    
24000     [2.52e+00, 1.45e-02, 3.34e+00]    [0.00e+00, 4.28e+00, 3.34e+00]    [3.45e+00, 4.28e+00, 1.96e+00, 3.25e+00]    
25000     [2.52e+00, 1.86e-02, 3.31e+00]    [0.00e+00, 4.21e+00, 3.31e+00]    [3.49e+00, 4.21e+00, 1.94e+00, 3.24e+00]    
26000     [2.50e+00, 8.48e-03, 3.28e+00]    [0.00e+00, 4.25e+00, 3.28e+00]    [3.45e+00, 4.25e+00, 1.92e+00, 3.21e+00]    
27000     [2.53e+00, 3.81e-02, 3.25e+00]    [0.00e+00, 4.17e+00, 3.25e+00]    [3.52e+00, 4.17e+00, 1.92e+00, 3.22e+00]    
28000     [2.49e+00, 2.26e-02, 3.23e+00]    [0.00e+00, 4.25e+00, 3.23e+00]    [3.45e+00, 4.25e+00, 1.91e+00, 3.13e+00]    
29000     [2.49e+00, 2.38e-02, 3.20e+00]    [0.00e+00, 4.29e+00, 3.20e+00]    [3.45e+00, 4.29e+00, 1.88e+00, 3.14e+00]    
30000     [2.47e+00, 1.21e-02, 3.18e+00]    [0.00e+00, 4.20e+00, 3.18e+00]    [3.49e+00, 4.20e+00, 1.88e+00, 3.13e+00]    

Best model at step 30000:
  train loss: 5.66e+00
  test loss: 7.38e+00
  test metric: [3.49e+00, 4.20e+00, 1.88e+00, 3.13e+00]

'train' took 78.933883 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.421066 s

'compile' took 2.106718 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.04e+02, 2.89e+00]    [0.00e+00, 1.00e+02, 2.89e+00]    [1.00e+02, 1.00e+02, 1.23e+00, 1.23e+00]    
1000      [8.01e+01, 4.92e+01, 2.95e+00]    [0.00e+00, 9.46e+01, 2.95e+00]    [9.47e+01, 9.46e+01, 1.02e+01, 1.03e+01]    
2000      [6.23e+01, 3.79e+01, 3.25e+00]    [0.00e+00, 7.37e+01, 3.25e+00]    [7.44e+01, 7.37e+01, 1.40e+01, 1.43e+01]    
3000      [2.88e+01, 7.16e+00, 3.94e+00]    [0.00e+00, 3.45e+01, 3.94e+00]    [3.74e+01, 3.45e+01, 2.02e+01, 1.87e+01]    
4000      [1.36e+01, 4.87e-01, 4.10e+00]    [0.00e+00, 1.44e+01, 4.10e+00]    [1.71e+01, 1.44e+01, 1.27e+01, 1.19e+01]    
5000      [6.34e+00, 7.98e-01, 4.17e+00]    [0.00e+00, 7.10e+00, 4.17e+00]    [7.24e+00, 7.10e+00, 5.99e+00, 5.23e+00]    
6000      [4.57e+00, 6.79e-01, 4.18e+00]    [0.00e+00, 4.53e+00, 4.18e+00]    [4.63e+00, 4.53e+00, 3.77e+00, 4.21e+00]    
7000      [4.27e+00, 6.51e-01, 4.15e+00]    [0.00e+00, 3.95e+00, 4.15e+00]    [3.82e+00, 3.95e+00, 3.42e+00, 3.60e+00]    
8000      [3.98e+00, 5.16e-01, 4.12e+00]    [0.00e+00, 3.69e+00, 4.12e+00]    [3.57e+00, 3.69e+00, 2.97e+00, 3.21e+00]    
9000      [3.79e+00, 5.77e-01, 4.09e+00]    [0.00e+00, 3.38e+00, 4.09e+00]    [3.32e+00, 3.38e+00, 2.59e+00, 2.78e+00]    
10000     [3.66e+00, 4.84e-01, 4.06e+00]    [0.00e+00, 3.19e+00, 4.06e+00]    [3.16e+00, 3.19e+00, 2.32e+00, 2.47e+00]    
11000     [3.62e+00, 4.75e-01, 4.03e+00]    [0.00e+00, 3.01e+00, 4.03e+00]    [2.98e+00, 3.01e+00, 2.11e+00, 2.25e+00]    
12000     [3.53e+00, 3.84e-01, 4.01e+00]    [0.00e+00, 2.84e+00, 4.01e+00]    [2.81e+00, 2.84e+00, 2.03e+00, 2.17e+00]    
13000     [3.47e+00, 5.57e-01, 3.98e+00]    [0.00e+00, 2.64e+00, 3.98e+00]    [2.62e+00, 2.64e+00, 1.98e+00, 2.11e+00]    
14000     [3.41e+00, 4.95e-01, 3.96e+00]    [0.00e+00, 2.51e+00, 3.96e+00]    [2.48e+00, 2.51e+00, 2.11e+00, 2.22e+00]    
15000     [3.30e+00, 3.84e-01, 3.94e+00]    [0.00e+00, 2.57e+00, 3.94e+00]    [2.51e+00, 2.57e+00, 2.16e+00, 2.23e+00]    
16000     [3.26e+00, 4.65e-01, 3.92e+00]    [0.00e+00, 2.64e+00, 3.92e+00]    [2.58e+00, 2.64e+00, 2.09e+00, 2.17e+00]    
17000     [3.17e+00, 2.80e-01, 3.90e+00]    [0.00e+00, 2.75e+00, 3.90e+00]    [2.69e+00, 2.75e+00, 2.10e+00, 2.18e+00]    
18000     [3.15e+00, 3.18e-01, 3.88e+00]    [0.00e+00, 2.81e+00, 3.88e+00]    [2.76e+00, 2.81e+00, 2.08e+00, 2.15e+00]    
19000     [3.12e+00, 2.63e-01, 3.86e+00]    [0.00e+00, 2.86e+00, 3.86e+00]    [2.81e+00, 2.86e+00, 2.08e+00, 2.13e+00]    
20000     [3.07e+00, 2.92e-01, 3.84e+00]    [0.00e+00, 2.83e+00, 3.84e+00]    [2.81e+00, 2.83e+00, 2.07e+00, 2.09e+00]    
21000     [3.06e+00, 1.93e-01, 3.82e+00]    [0.00e+00, 2.83e+00, 3.82e+00]    [2.84e+00, 2.83e+00, 2.06e+00, 2.06e+00]    
22000     [3.04e+00, 2.05e-01, 3.79e+00]    [0.00e+00, 2.85e+00, 3.79e+00]    [2.89e+00, 2.85e+00, 2.04e+00, 2.03e+00]    
23000     [3.06e+00, 3.13e-01, 3.77e+00]    [0.00e+00, 2.82e+00, 3.77e+00]    [2.89e+00, 2.82e+00, 2.03e+00, 2.01e+00]    
24000     [3.00e+00, 1.14e-01, 3.76e+00]    [0.00e+00, 2.79e+00, 3.76e+00]    [2.88e+00, 2.79e+00, 2.01e+00, 1.98e+00]    
25000     [3.03e+00, 2.82e-01, 3.74e+00]    [0.00e+00, 2.76e+00, 3.74e+00]    [2.88e+00, 2.76e+00, 1.99e+00, 1.95e+00]    
26000     [2.99e+00, 1.12e-01, 3.72e+00]    [0.00e+00, 2.73e+00, 3.72e+00]    [2.87e+00, 2.73e+00, 1.99e+00, 1.94e+00]    
27000     [2.98e+00, 7.13e-02, 3.70e+00]    [0.00e+00, 2.70e+00, 3.70e+00]    [2.87e+00, 2.70e+00, 1.96e+00, 1.90e+00]    
28000     [2.96e+00, 1.65e-01, 3.68e+00]    [0.00e+00, 2.74e+00, 3.68e+00]    [2.92e+00, 2.74e+00, 1.91e+00, 1.83e+00]    
29000     [2.92e+00, 9.36e-02, 3.66e+00]    [0.00e+00, 2.73e+00, 3.66e+00]    [2.94e+00, 2.73e+00, 1.87e+00, 1.78e+00]    
30000     [2.94e+00, 7.29e-02, 3.65e+00]    [0.00e+00, 2.70e+00, 3.65e+00]    [2.93e+00, 2.70e+00, 1.84e+00, 1.74e+00]    

Best model at step 30000:
  train loss: 6.66e+00
  test loss: 6.35e+00
  test metric: [2.93e+00, 2.70e+00, 1.84e+00, 1.74e+00]

'train' took 78.944067 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.423870 s

'compile' took 2.055502 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.97e+01, 9.91e+01, 2.73e+00]    [0.00e+00, 9.95e+01, 2.73e+00]    [9.95e+01, 9.95e+01, 1.48e+00, 1.48e+00]    
1000      [7.64e+01, 6.34e+01, 2.90e+00]    [0.00e+00, 7.53e+01, 2.90e+00]    [7.52e+01, 7.53e+01, 2.28e+01, 2.20e+01]    
2000      [3.12e+01, 1.78e+01, 4.02e+00]    [0.00e+00, 3.02e+01, 4.02e+00]    [3.15e+01, 3.02e+01, 2.08e+01, 1.44e+01]    
3000      [8.27e+00, 1.18e+00, 4.33e+00]    [0.00e+00, 8.52e+00, 4.33e+00]    [1.10e+01, 8.52e+00, 5.41e+00, 9.43e+00]    
4000      [5.47e+00, 9.55e-01, 4.30e+00]    [0.00e+00, 6.63e+00, 4.30e+00]    [7.28e+00, 6.63e+00, 5.73e+00, 7.24e+00]    
5000      [4.53e+00, 8.35e-01, 4.26e+00]    [0.00e+00, 5.43e+00, 4.26e+00]    [5.57e+00, 5.43e+00, 4.97e+00, 5.63e+00]    
6000      [4.14e+00, 7.62e-01, 4.22e+00]    [0.00e+00, 4.75e+00, 4.22e+00]    [4.86e+00, 4.75e+00, 4.08e+00, 4.57e+00]    
7000      [3.89e+00, 7.32e-01, 4.17e+00]    [0.00e+00, 4.28e+00, 4.17e+00]    [4.20e+00, 4.28e+00, 3.75e+00, 3.96e+00]    
8000      [3.72e+00, 7.18e-01, 4.14e+00]    [0.00e+00, 4.47e+00, 4.14e+00]    [4.38e+00, 4.47e+00, 3.18e+00, 3.30e+00]    
9000      [3.53e+00, 5.76e-01, 4.10e+00]    [0.00e+00, 4.23e+00, 4.10e+00]    [4.20e+00, 4.23e+00, 3.01e+00, 3.05e+00]    
10000     [3.42e+00, 5.05e-01, 4.06e+00]    [0.00e+00, 4.18e+00, 4.06e+00]    [4.16e+00, 4.18e+00, 2.77e+00, 2.77e+00]    
11000     [3.36e+00, 4.27e-01, 4.03e+00]    [0.00e+00, 4.04e+00, 4.03e+00]    [4.03e+00, 4.04e+00, 2.63e+00, 2.61e+00]    
12000     [3.36e+00, 4.59e-01, 4.00e+00]    [0.00e+00, 4.05e+00, 4.00e+00]    [4.05e+00, 4.05e+00, 2.48e+00, 2.46e+00]    
13000     [3.28e+00, 3.21e-01, 3.97e+00]    [0.00e+00, 3.90e+00, 3.97e+00]    [3.89e+00, 3.90e+00, 2.52e+00, 2.49e+00]    
14000     [3.21e+00, 2.37e-01, 3.94e+00]    [0.00e+00, 3.69e+00, 3.94e+00]    [3.70e+00, 3.69e+00, 2.52e+00, 2.47e+00]    
15000     [3.20e+00, 2.25e-01, 3.91e+00]    [0.00e+00, 3.65e+00, 3.91e+00]    [3.68e+00, 3.65e+00, 2.42e+00, 2.41e+00]    
16000     [3.17e+00, 1.79e-01, 3.89e+00]    [0.00e+00, 3.55e+00, 3.89e+00]    [3.60e+00, 3.55e+00, 2.24e+00, 2.21e+00]    
17000     [3.14e+00, 1.24e-01, 3.86e+00]    [0.00e+00, 3.45e+00, 3.86e+00]    [3.52e+00, 3.45e+00, 2.04e+00, 2.01e+00]    
18000     [3.13e+00, 1.31e-01, 3.83e+00]    [0.00e+00, 3.26e+00, 3.83e+00]    [3.36e+00, 3.26e+00, 1.92e+00, 1.88e+00]    
19000     [3.10e+00, 1.58e-02, 3.81e+00]    [0.00e+00, 3.24e+00, 3.81e+00]    [3.37e+00, 3.24e+00, 1.71e+00, 1.65e+00]    
20000     [3.13e+00, 1.39e-01, 3.78e+00]    [0.00e+00, 2.98e+00, 3.78e+00]    [3.12e+00, 2.98e+00, 1.65e+00, 1.59e+00]    
21000     [3.04e+00, 1.94e-02, 3.75e+00]    [0.00e+00, 2.98e+00, 3.75e+00]    [3.14e+00, 2.98e+00, 1.48e+00, 1.39e+00]    
22000     [3.04e+00, 9.25e-02, 3.73e+00]    [0.00e+00, 2.79e+00, 3.73e+00]    [2.95e+00, 2.79e+00, 1.47e+00, 1.40e+00]    
23000     [3.03e+00, 7.57e-02, 3.70e+00]    [0.00e+00, 2.88e+00, 3.70e+00]    [3.05e+00, 2.88e+00, 1.43e+00, 1.33e+00]    
24000     [2.97e+00, 1.78e-02, 3.68e+00]    [0.00e+00, 2.78e+00, 3.68e+00]    [2.94e+00, 2.78e+00, 1.45e+00, 1.36e+00]    
25000     [2.96e+00, 4.37e-02, 3.65e+00]    [0.00e+00, 2.77e+00, 3.65e+00]    [2.95e+00, 2.77e+00, 1.49e+00, 1.38e+00]    
26000     [2.94e+00, 3.10e-02, 3.63e+00]    [0.00e+00, 2.65e+00, 3.63e+00]    [2.83e+00, 2.65e+00, 1.50e+00, 1.40e+00]    
27000     [2.94e+00, 3.12e-02, 3.61e+00]    [0.00e+00, 2.69e+00, 3.61e+00]    [2.92e+00, 2.69e+00, 1.46e+00, 1.42e+00]    
28000     [2.94e+00, 9.73e-02, 3.59e+00]    [0.00e+00, 2.52e+00, 3.59e+00]    [2.80e+00, 2.52e+00, 1.46e+00, 1.48e+00]    
29000     [2.92e+00, 4.79e-02, 3.57e+00]    [0.00e+00, 2.60e+00, 3.57e+00]    [2.93e+00, 2.60e+00, 1.40e+00, 1.46e+00]    
30000     [2.90e+00, 3.26e-02, 3.54e+00]    [0.00e+00, 2.59e+00, 3.54e+00]    [2.95e+00, 2.59e+00, 1.39e+00, 1.49e+00]    

Best model at step 30000:
  train loss: 6.48e+00
  test loss: 6.13e+00
  test metric: [2.95e+00, 2.59e+00, 1.39e+00, 1.49e+00]

'train' took 78.550613 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.448799 s

'compile' took 2.077444 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.91e+01, 9.88e+01, 2.71e+00]    [0.00e+00, 9.96e+01, 2.71e+00]    [9.96e+01, 9.96e+01, 5.48e-01, 5.48e-01]    
1000      [7.94e+01, 6.75e+01, 2.78e+00]    [0.00e+00, 8.31e+01, 2.78e+00]    [8.31e+01, 8.31e+01, 2.09e+01, 2.09e+01]    
2000      [5.84e+01, 3.12e+01, 3.54e+00]    [0.00e+00, 6.06e+01, 3.54e+00]    [6.07e+01, 6.06e+01, 1.47e+01, 1.41e+01]    
3000      [2.72e+01, 1.06e+01, 4.19e+00]    [0.00e+00, 3.14e+01, 4.19e+00]    [3.22e+01, 3.14e+01, 6.98e+00, 6.69e+00]    
4000      [7.11e+00, 1.40e+00, 4.47e+00]    [0.00e+00, 6.34e+00, 4.47e+00]    [6.68e+00, 6.34e+00, 3.48e+00, 3.75e+00]    
5000      [4.89e+00, 9.28e-01, 4.41e+00]    [0.00e+00, 4.24e+00, 4.41e+00]    [4.25e+00, 4.24e+00, 2.15e+00, 2.05e+00]    
6000      [4.16e+00, 3.02e-01, 4.36e+00]    [0.00e+00, 4.28e+00, 4.36e+00]    [3.96e+00, 4.28e+00, 1.73e+00, 1.53e+00]    
7000      [3.53e+00, 4.09e-01, 4.33e+00]    [0.00e+00, 3.83e+00, 4.33e+00]    [3.62e+00, 3.83e+00, 2.20e+00, 1.85e+00]    
8000      [3.35e+00, 1.50e-01, 4.27e+00]    [0.00e+00, 3.94e+00, 4.27e+00]    [3.70e+00, 3.94e+00, 2.31e+00, 1.92e+00]    
9000      [3.32e+00, 1.66e-01, 4.22e+00]    [0.00e+00, 3.85e+00, 4.22e+00]    [3.59e+00, 3.85e+00, 2.42e+00, 1.96e+00]    
10000     [3.27e+00, 1.62e-01, 4.17e+00]    [0.00e+00, 3.85e+00, 4.17e+00]    [3.60e+00, 3.85e+00, 2.38e+00, 1.96e+00]    
11000     [3.21e+00, 8.85e-02, 4.13e+00]    [0.00e+00, 3.86e+00, 4.13e+00]    [3.60e+00, 3.86e+00, 2.34e+00, 2.05e+00]    
12000     [3.19e+00, 1.92e-01, 4.09e+00]    [0.00e+00, 3.84e+00, 4.09e+00]    [3.58e+00, 3.84e+00, 2.33e+00, 2.09e+00]    
13000     [3.10e+00, 4.29e-02, 4.05e+00]    [0.00e+00, 3.76e+00, 4.05e+00]    [3.49e+00, 3.76e+00, 2.35e+00, 2.03e+00]    
14000     [3.05e+00, 1.87e-02, 4.01e+00]    [0.00e+00, 3.75e+00, 4.01e+00]    [3.49e+00, 3.75e+00, 2.29e+00, 1.97e+00]    
15000     [3.03e+00, 6.25e-02, 3.98e+00]    [0.00e+00, 3.74e+00, 3.98e+00]    [3.48e+00, 3.74e+00, 2.30e+00, 1.98e+00]    
16000     [3.03e+00, 9.08e-02, 3.94e+00]    [0.00e+00, 3.70e+00, 3.94e+00]    [3.43e+00, 3.70e+00, 2.28e+00, 1.97e+00]    
17000     [2.98e+00, 2.85e-02, 3.91e+00]    [0.00e+00, 3.65e+00, 3.91e+00]    [3.39e+00, 3.65e+00, 2.28e+00, 1.92e+00]    
18000     [2.98e+00, 1.40e-01, 3.88e+00]    [0.00e+00, 3.59e+00, 3.88e+00]    [3.33e+00, 3.59e+00, 2.29e+00, 1.86e+00]    
19000     [2.93e+00, 4.33e-02, 3.85e+00]    [0.00e+00, 3.61e+00, 3.85e+00]    [3.35e+00, 3.61e+00, 2.26e+00, 1.87e+00]    
20000     [2.96e+00, 1.57e-01, 3.82e+00]    [0.00e+00, 3.55e+00, 3.82e+00]    [3.29e+00, 3.55e+00, 2.28e+00, 1.81e+00]    
21000     [2.93e+00, 6.98e-02, 3.80e+00]    [0.00e+00, 3.55e+00, 3.80e+00]    [3.29e+00, 3.55e+00, 2.25e+00, 1.81e+00]    
22000     [2.93e+00, 7.31e-02, 3.77e+00]    [0.00e+00, 3.54e+00, 3.77e+00]    [3.27e+00, 3.54e+00, 2.24e+00, 1.86e+00]    
23000     [2.91e+00, 3.32e-02, 3.74e+00]    [0.00e+00, 3.52e+00, 3.74e+00]    [3.26e+00, 3.52e+00, 2.25e+00, 1.80e+00]    
24000     [2.92e+00, 1.25e-01, 3.72e+00]    [0.00e+00, 3.52e+00, 3.72e+00]    [3.25e+00, 3.52e+00, 2.23e+00, 1.85e+00]    
25000     [2.87e+00, 6.16e-02, 3.70e+00]    [0.00e+00, 3.46e+00, 3.70e+00]    [3.20e+00, 3.46e+00, 2.24e+00, 1.80e+00]    
26000     [2.91e+00, 1.70e-01, 3.68e+00]    [0.00e+00, 3.49e+00, 3.68e+00]    [3.25e+00, 3.49e+00, 2.19e+00, 1.82e+00]    
27000     [2.90e+00, 1.79e-01, 3.65e+00]    [0.00e+00, 3.36e+00, 3.65e+00]    [3.15e+00, 3.36e+00, 2.22e+00, 1.68e+00]    
28000     [2.83e+00, 2.76e-02, 3.63e+00]    [0.00e+00, 3.37e+00, 3.63e+00]    [3.16e+00, 3.37e+00, 2.17e+00, 1.74e+00]    
29000     [2.86e+00, 1.99e-01, 3.61e+00]    [0.00e+00, 3.30e+00, 3.61e+00]    [3.12e+00, 3.30e+00, 2.17e+00, 1.65e+00]    
30000     [2.92e+00, 2.36e-01, 3.59e+00]    [0.00e+00, 3.37e+00, 3.59e+00]    [3.19e+00, 3.37e+00, 2.10e+00, 1.82e+00]    

Best model at step 28000:
  train loss: 6.49e+00
  test loss: 7.01e+00
  test metric: [3.16e+00, 3.37e+00, 2.17e+00, 1.74e+00]

'train' took 79.967591 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.482709 s

'compile' took 2.294863 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 1.00e+02, 2.69e+00]    [0.00e+00, 1.00e+02, 2.69e+00]    [1.00e+02, 1.00e+02, 2.51e-01, 2.51e-01]    
1000      [7.79e+01, 5.18e+01, 2.86e+00]    [0.00e+00, 8.89e+01, 2.86e+00]    [8.89e+01, 8.89e+01, 5.92e+00, 5.94e+00]    
2000      [5.85e+01, 3.74e+01, 3.34e+00]    [0.00e+00, 6.57e+01, 3.34e+00]    [6.62e+01, 6.57e+01, 8.06e+00, 8.17e+00]    
3000      [1.98e+01, 6.69e+00, 4.34e+00]    [0.00e+00, 1.74e+01, 4.34e+00]    [2.04e+01, 1.74e+01, 9.56e+00, 7.71e+00]    
4000      [5.68e+00, 1.86e-01, 4.55e+00]    [0.00e+00, 5.77e+00, 4.55e+00]    [6.10e+00, 5.77e+00, 3.16e+00, 2.63e+00]    
5000      [4.40e+00, 1.78e-01, 4.51e+00]    [0.00e+00, 4.14e+00, 4.51e+00]    [4.31e+00, 4.14e+00, 1.35e+00, 2.67e+00]    
6000      [4.00e+00, 1.78e-01, 4.48e+00]    [0.00e+00, 3.70e+00, 4.48e+00]    [3.41e+00, 3.70e+00, 1.78e+00, 2.69e+00]    
7000      [3.83e+00, 9.75e-02, 4.44e+00]    [0.00e+00, 3.73e+00, 4.44e+00]    [3.20e+00, 3.73e+00, 1.93e+00, 2.55e+00]    
8000      [3.71e+00, 1.55e-01, 4.40e+00]    [0.00e+00, 3.73e+00, 4.40e+00]    [3.16e+00, 3.73e+00, 1.92e+00, 2.47e+00]    
9000      [3.58e+00, 1.58e-01, 4.36e+00]    [0.00e+00, 3.72e+00, 4.36e+00]    [3.08e+00, 3.72e+00, 1.97e+00, 2.41e+00]    
10000     [3.46e+00, 2.99e-02, 4.33e+00]    [0.00e+00, 3.75e+00, 4.33e+00]    [3.15e+00, 3.75e+00, 1.89e+00, 2.35e+00]    
11000     [3.42e+00, 4.03e-01, 4.29e+00]    [0.00e+00, 3.73e+00, 4.29e+00]    [3.18e+00, 3.73e+00, 1.80e+00, 2.27e+00]    
12000     [3.34e+00, 1.63e-01, 4.26e+00]    [0.00e+00, 3.71e+00, 4.26e+00]    [3.18e+00, 3.71e+00, 1.77e+00, 2.16e+00]    
13000     [3.29e+00, 7.51e-02, 4.22e+00]    [0.00e+00, 3.65e+00, 4.22e+00]    [3.14e+00, 3.65e+00, 1.72e+00, 2.07e+00]    
14000     [3.24e+00, 2.27e-01, 4.19e+00]    [0.00e+00, 3.64e+00, 4.19e+00]    [3.15e+00, 3.64e+00, 1.65e+00, 1.96e+00]    
15000     [3.18e+00, 5.72e-02, 4.16e+00]    [0.00e+00, 3.62e+00, 4.16e+00]    [3.15e+00, 3.62e+00, 1.61e+00, 1.89e+00]    
16000     [3.14e+00, 5.18e-02, 4.14e+00]    [0.00e+00, 3.58e+00, 4.14e+00]    [3.13e+00, 3.58e+00, 1.58e+00, 1.83e+00]    
17000     [3.13e+00, 6.32e-02, 4.11e+00]    [0.00e+00, 3.53e+00, 4.11e+00]    [3.10e+00, 3.53e+00, 1.55e+00, 1.76e+00]    
18000     [3.13e+00, 1.55e-01, 4.08e+00]    [0.00e+00, 3.51e+00, 4.08e+00]    [3.09e+00, 3.51e+00, 1.51e+00, 1.70e+00]    
19000     [3.07e+00, 1.47e-01, 4.06e+00]    [0.00e+00, 3.50e+00, 4.06e+00]    [3.11e+00, 3.50e+00, 1.46e+00, 1.63e+00]    
20000     [3.06e+00, 1.42e-01, 4.03e+00]    [0.00e+00, 3.48e+00, 4.03e+00]    [3.10e+00, 3.48e+00, 1.44e+00, 1.60e+00]    
21000     [3.09e+00, 1.87e-01, 4.01e+00]    [0.00e+00, 3.46e+00, 4.01e+00]    [3.11e+00, 3.46e+00, 1.40e+00, 1.54e+00]    
22000     [3.00e+00, 9.76e-02, 3.99e+00]    [0.00e+00, 3.50e+00, 3.99e+00]    [3.18e+00, 3.50e+00, 1.36e+00, 1.50e+00]    
23000     [2.99e+00, 1.06e-01, 3.96e+00]    [0.00e+00, 3.47e+00, 3.96e+00]    [3.17e+00, 3.47e+00, 1.32e+00, 1.45e+00]    
24000     [2.96e+00, 1.91e-01, 3.94e+00]    [0.00e+00, 3.48e+00, 3.94e+00]    [3.20e+00, 3.48e+00, 1.29e+00, 1.41e+00]    
25000     [2.95e+00, 4.36e-02, 3.92e+00]    [0.00e+00, 3.45e+00, 3.92e+00]    [3.19e+00, 3.45e+00, 1.28e+00, 1.37e+00]    
26000     [2.94e+00, 7.16e-02, 3.90e+00]    [0.00e+00, 3.47e+00, 3.90e+00]    [3.23e+00, 3.47e+00, 1.25e+00, 1.33e+00]    
27000     [2.91e+00, 6.11e-02, 3.88e+00]    [0.00e+00, 3.44e+00, 3.88e+00]    [3.23e+00, 3.44e+00, 1.21e+00, 1.28e+00]    
28000     [2.93e+00, 1.25e-01, 3.86e+00]    [0.00e+00, 3.42e+00, 3.86e+00]    [3.22e+00, 3.42e+00, 1.20e+00, 1.26e+00]    
29000     [2.89e+00, 3.55e-02, 3.84e+00]    [0.00e+00, 3.42e+00, 3.84e+00]    [3.23e+00, 3.42e+00, 1.19e+00, 1.25e+00]    
30000     [2.91e+00, 6.17e-02, 3.82e+00]    [0.00e+00, 3.40e+00, 3.82e+00]    [3.23e+00, 3.40e+00, 1.18e+00, 1.23e+00]    

Best model at step 29000:
  train loss: 6.76e+00
  test loss: 7.25e+00
  test metric: [3.23e+00, 3.42e+00, 1.19e+00, 1.25e+00]

'train' took 81.371713 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[3.0883033098381913, 2.781596005850318, 3.7666280036394473, 5.0390906462994725, 3.1612465732470745, 4.204771577946514, 2.7026621303515435, 2.586411940600367, 3.373904160876634, 3.4169524342600317]
E* 7 3.412156678290959 0.7178418504345971
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.627322 s

'compile' took 2.443465 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.02e+02, 2.80e+00]    [0.00e+00, 9.97e+01, 2.80e+00]    [9.97e+01, 9.97e+01, 1.07e+00, 1.07e+00]    
1000      [7.87e+01, 6.00e+01, 2.88e+00]    [0.00e+00, 9.26e+01, 2.88e+00]    [9.27e+01, 9.26e+01, 4.10e+00, 4.12e+00]    
2000      [4.80e+01, 2.49e+01, 3.64e+00]    [0.00e+00, 6.04e+01, 3.64e+00]    [6.14e+01, 6.04e+01, 2.50e+01, 2.56e+01]    
3000      [2.25e+01, 7.57e+00, 4.10e+00]    [0.00e+00, 3.36e+01, 4.10e+00]    [3.41e+01, 3.36e+01, 2.40e+01, 2.31e+01]    
4000      [1.06e+01, 4.18e+00, 4.21e+00]    [0.00e+00, 1.43e+01, 4.21e+00]    [1.48e+01, 1.43e+01, 1.30e+01, 1.20e+01]    
5000      [5.09e+00, 1.20e+00, 4.29e+00]    [0.00e+00, 4.31e+00, 4.29e+00]    [4.73e+00, 4.31e+00, 1.82e+00, 2.27e+00]    
6000      [4.37e+00, 3.99e-01, 4.24e+00]    [0.00e+00, 3.08e+00, 4.24e+00]    [3.11e+00, 3.08e+00, 2.00e+00, 2.49e+00]    
7000      [4.17e+00, 3.75e-01, 4.19e+00]    [0.00e+00, 3.02e+00, 4.19e+00]    [3.00e+00, 3.02e+00, 1.99e+00, 2.41e+00]    
8000      [4.04e+00, 1.47e-01, 4.14e+00]    [0.00e+00, 3.14e+00, 4.14e+00]    [3.02e+00, 3.14e+00, 2.06e+00, 2.24e+00]    
9000      [3.98e+00, 5.19e-02, 4.10e+00]    [0.00e+00, 3.16e+00, 4.10e+00]    [3.10e+00, 3.16e+00, 2.06e+00, 2.20e+00]    
10000     [3.89e+00, 8.95e-02, 4.06e+00]    [0.00e+00, 3.17e+00, 4.06e+00]    [3.16e+00, 3.17e+00, 2.05e+00, 2.22e+00]    
11000     [3.81e+00, 1.04e-01, 4.02e+00]    [0.00e+00, 3.22e+00, 4.02e+00]    [3.22e+00, 3.22e+00, 2.05e+00, 2.16e+00]    
12000     [3.73e+00, 9.20e-02, 3.98e+00]    [0.00e+00, 3.28e+00, 3.98e+00]    [3.27e+00, 3.28e+00, 2.03e+00, 2.09e+00]    
13000     [3.68e+00, 7.15e-02, 3.95e+00]    [0.00e+00, 3.28e+00, 3.95e+00]    [3.27e+00, 3.28e+00, 2.02e+00, 2.05e+00]    
14000     [3.65e+00, 1.79e-01, 3.92e+00]    [0.00e+00, 3.26e+00, 3.92e+00]    [3.26e+00, 3.26e+00, 2.04e+00, 2.05e+00]    
15000     [3.59e+00, 3.39e-02, 3.89e+00]    [0.00e+00, 3.24e+00, 3.89e+00]    [3.24e+00, 3.24e+00, 2.05e+00, 2.04e+00]    
16000     [3.53e+00, 1.86e-02, 3.86e+00]    [0.00e+00, 3.26e+00, 3.86e+00]    [3.26e+00, 3.26e+00, 2.03e+00, 2.00e+00]    
17000     [3.50e+00, 9.63e-02, 3.83e+00]    [0.00e+00, 3.27e+00, 3.83e+00]    [3.27e+00, 3.27e+00, 1.99e+00, 1.94e+00]    
18000     [3.46e+00, 2.37e-02, 3.80e+00]    [0.00e+00, 3.30e+00, 3.80e+00]    [3.30e+00, 3.30e+00, 1.96e+00, 1.88e+00]    
19000     [3.44e+00, 8.71e-02, 3.78e+00]    [0.00e+00, 3.37e+00, 3.78e+00]    [3.37e+00, 3.37e+00, 1.89e+00, 1.79e+00]    
20000     [3.40e+00, 4.10e-02, 3.75e+00]    [0.00e+00, 3.40e+00, 3.75e+00]    [3.41e+00, 3.40e+00, 1.83e+00, 1.72e+00]    
21000     [3.39e+00, 8.28e-02, 3.73e+00]    [0.00e+00, 3.45e+00, 3.73e+00]    [3.46e+00, 3.45e+00, 1.78e+00, 1.65e+00]    
22000     [3.39e+00, 2.24e-01, 3.71e+00]    [0.00e+00, 3.52e+00, 3.71e+00]    [3.53e+00, 3.52e+00, 1.71e+00, 1.57e+00]    
23000     [3.32e+00, 5.61e-02, 3.68e+00]    [0.00e+00, 3.54e+00, 3.68e+00]    [3.55e+00, 3.54e+00, 1.70e+00, 1.54e+00]    
24000     [3.28e+00, 8.76e-02, 3.66e+00]    [0.00e+00, 3.56e+00, 3.66e+00]    [3.57e+00, 3.56e+00, 1.70e+00, 1.51e+00]    
25000     [3.27e+00, 5.21e-02, 3.64e+00]    [0.00e+00, 3.54e+00, 3.64e+00]    [3.55e+00, 3.54e+00, 1.72e+00, 1.51e+00]    
26000     [3.26e+00, 9.21e-02, 3.63e+00]    [0.00e+00, 3.55e+00, 3.63e+00]    [3.57e+00, 3.55e+00, 1.72e+00, 1.49e+00]    
27000     [3.22e+00, 1.05e-01, 3.61e+00]    [0.00e+00, 3.56e+00, 3.61e+00]    [3.57e+00, 3.56e+00, 1.71e+00, 1.46e+00]    
28000     [3.19e+00, 3.51e-02, 3.59e+00]    [0.00e+00, 3.55e+00, 3.59e+00]    [3.57e+00, 3.55e+00, 1.74e+00, 1.46e+00]    
29000     [3.17e+00, 4.36e-02, 3.57e+00]    [0.00e+00, 3.56e+00, 3.57e+00]    [3.58e+00, 3.56e+00, 1.75e+00, 1.45e+00]    
30000     [3.16e+00, 1.57e-01, 3.55e+00]    [0.00e+00, 3.54e+00, 3.55e+00]    [3.57e+00, 3.54e+00, 1.77e+00, 1.45e+00]    

Best model at step 29000:
  train loss: 6.79e+00
  test loss: 7.13e+00
  test metric: [3.58e+00, 3.56e+00, 1.75e+00, 1.45e+00]

'train' took 83.935502 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.470743 s

'compile' took 2.269931 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.00e+02, 9.95e+01, 2.69e+00]    [0.00e+00, 1.04e+02, 2.69e+00]    [1.04e+02, 1.04e+02, 3.84e+00, 3.84e+00]    
1000      [6.19e+01, 5.98e+01, 3.24e+00]    [0.00e+00, 6.47e+01, 3.24e+00]    [6.49e+01, 6.47e+01, 3.55e+01, 3.51e+01]    
2000      [1.89e+01, 1.21e+01, 4.28e+00]    [0.00e+00, 2.94e+01, 4.28e+00]    [3.13e+01, 2.94e+01, 2.21e+01, 2.09e+01]    
3000      [4.95e+00, 2.05e+00, 4.43e+00]    [0.00e+00, 7.49e+00, 4.43e+00]    [8.24e+00, 7.49e+00, 5.82e+00, 5.49e+00]    
4000      [4.09e+00, 1.68e+00, 4.34e+00]    [0.00e+00, 6.10e+00, 4.34e+00]    [6.12e+00, 6.10e+00, 4.17e+00, 4.15e+00]    
5000      [3.72e+00, 1.24e+00, 4.27e+00]    [0.00e+00, 5.88e+00, 4.27e+00]    [5.29e+00, 5.88e+00, 3.03e+00, 3.58e+00]    
6000      [3.43e+00, 1.07e+00, 4.20e+00]    [0.00e+00, 5.28e+00, 4.20e+00]    [4.40e+00, 5.28e+00, 2.38e+00, 3.55e+00]    
7000      [3.27e+00, 8.93e-01, 4.13e+00]    [0.00e+00, 5.24e+00, 4.13e+00]    [4.25e+00, 5.24e+00, 2.11e+00, 3.60e+00]    
8000      [3.23e+00, 7.75e-01, 4.07e+00]    [0.00e+00, 5.48e+00, 4.07e+00]    [4.22e+00, 5.48e+00, 2.33e+00, 3.81e+00]    
9000      [3.17e+00, 6.08e-01, 4.02e+00]    [0.00e+00, 5.69e+00, 4.02e+00]    [4.40e+00, 5.69e+00, 2.35e+00, 3.86e+00]    
10000     [3.11e+00, 4.75e-01, 3.98e+00]    [0.00e+00, 5.85e+00, 3.98e+00]    [4.31e+00, 5.85e+00, 2.69e+00, 4.06e+00]    
11000     [3.10e+00, 3.74e-01, 3.94e+00]    [0.00e+00, 5.97e+00, 3.94e+00]    [4.53e+00, 5.97e+00, 2.65e+00, 4.03e+00]    
12000     [3.07e+00, 2.75e-01, 3.91e+00]    [0.00e+00, 5.99e+00, 3.91e+00]    [4.50e+00, 5.99e+00, 2.80e+00, 4.22e+00]    
13000     [3.09e+00, 2.69e-01, 3.86e+00]    [0.00e+00, 5.98e+00, 3.86e+00]    [4.26e+00, 5.98e+00, 2.96e+00, 4.19e+00]    
14000     [3.03e+00, 2.46e-01, 3.83e+00]    [0.00e+00, 5.67e+00, 3.83e+00]    [4.23e+00, 5.67e+00, 2.79e+00, 4.21e+00]    
15000     [3.01e+00, 2.20e-01, 3.79e+00]    [0.00e+00, 5.61e+00, 3.79e+00]    [4.25e+00, 5.61e+00, 2.76e+00, 4.11e+00]    
16000     [2.99e+00, 2.01e-01, 3.76e+00]    [0.00e+00, 5.41e+00, 3.76e+00]    [4.10e+00, 5.41e+00, 2.68e+00, 4.20e+00]    
17000     [2.94e+00, 1.69e-01, 3.73e+00]    [0.00e+00, 5.32e+00, 3.73e+00]    [4.18e+00, 5.32e+00, 2.50e+00, 4.12e+00]    
18000     [2.93e+00, 1.59e-01, 3.70e+00]    [0.00e+00, 5.13e+00, 3.70e+00]    [4.15e+00, 5.13e+00, 2.24e+00, 4.10e+00]    
19000     [2.88e+00, 1.23e-01, 3.67e+00]    [0.00e+00, 5.04e+00, 3.67e+00]    [4.19e+00, 5.04e+00, 2.07e+00, 4.03e+00]    
20000     [2.93e+00, 1.47e-01, 3.64e+00]    [0.00e+00, 5.00e+00, 3.64e+00]    [4.30e+00, 5.00e+00, 1.93e+00, 3.85e+00]    
21000     [2.85e+00, 8.73e-02, 3.61e+00]    [0.00e+00, 4.89e+00, 3.61e+00]    [4.18e+00, 4.89e+00, 1.95e+00, 3.91e+00]    
22000     [2.83e+00, 6.63e-02, 3.59e+00]    [0.00e+00, 4.82e+00, 3.59e+00]    [4.05e+00, 4.82e+00, 2.08e+00, 3.99e+00]    
23000     [2.79e+00, 3.57e-02, 3.56e+00]    [0.00e+00, 4.72e+00, 3.56e+00]    [4.10e+00, 4.72e+00, 1.96e+00, 3.87e+00]    
24000     [2.79e+00, 3.91e-02, 3.54e+00]    [0.00e+00, 4.65e+00, 3.54e+00]    [4.08e+00, 4.65e+00, 1.93e+00, 3.83e+00]    
25000     [2.76e+00, 2.47e-02, 3.51e+00]    [0.00e+00, 4.63e+00, 3.51e+00]    [4.04e+00, 4.63e+00, 1.98e+00, 3.83e+00]    
26000     [2.78e+00, 3.84e-02, 3.48e+00]    [0.00e+00, 4.52e+00, 3.48e+00]    [4.06e+00, 4.52e+00, 1.81e+00, 3.68e+00]    
27000     [2.75e+00, 4.28e-02, 3.46e+00]    [0.00e+00, 4.50e+00, 3.46e+00]    [3.96e+00, 4.50e+00, 1.93e+00, 3.75e+00]    
28000     [2.72e+00, 4.31e-02, 3.43e+00]    [0.00e+00, 4.39e+00, 3.43e+00]    [3.78e+00, 4.39e+00, 2.03e+00, 3.89e+00]    
29000     [2.76e+00, 4.55e-02, 3.41e+00]    [0.00e+00, 4.32e+00, 3.41e+00]    [3.68e+00, 4.32e+00, 1.98e+00, 3.84e+00]    
30000     [2.66e+00, 2.13e-02, 3.39e+00]    [0.00e+00, 4.27e+00, 3.39e+00]    [3.80e+00, 4.27e+00, 1.90e+00, 3.74e+00]    

Best model at step 30000:
  train loss: 6.07e+00
  test loss: 7.66e+00
  test metric: [3.80e+00, 4.27e+00, 1.90e+00, 3.74e+00]

'train' took 79.889326 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.517613 s

'compile' took 2.259954 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.92e+01, 9.75e+01, 2.78e+00]    [0.00e+00, 1.00e+02, 2.78e+00]    [1.00e+02, 1.00e+02, 5.29e-01, 5.29e-01]    
1000      [7.78e+01, 5.29e+01, 2.96e+00]    [0.00e+00, 9.32e+01, 2.96e+00]    [9.33e+01, 9.32e+01, 1.05e+01, 1.06e+01]    
2000      [4.97e+01, 2.83e+01, 3.68e+00]    [0.00e+00, 6.17e+01, 3.68e+00]    [6.34e+01, 6.17e+01, 2.16e+01, 2.24e+01]    
3000      [1.89e+01, 8.15e-01, 4.26e+00]    [0.00e+00, 2.25e+01, 4.26e+00]    [2.70e+01, 2.25e+01, 1.40e+01, 1.35e+01]    
4000      [6.32e+00, 1.69e+00, 4.35e+00]    [0.00e+00, 5.07e+00, 4.35e+00]    [7.15e+00, 5.07e+00, 3.10e+00, 3.50e+00]    
5000      [4.08e+00, 1.76e+00, 4.35e+00]    [0.00e+00, 4.05e+00, 4.35e+00]    [3.43e+00, 4.05e+00, 3.82e+00, 4.86e+00]    
6000      [3.75e+00, 1.44e+00, 4.30e+00]    [0.00e+00, 4.36e+00, 4.30e+00]    [3.50e+00, 4.36e+00, 3.57e+00, 4.14e+00]    
7000      [3.55e+00, 1.31e+00, 4.26e+00]    [0.00e+00, 4.49e+00, 4.26e+00]    [3.75e+00, 4.49e+00, 3.25e+00, 3.59e+00]    
8000      [3.49e+00, 1.23e+00, 4.22e+00]    [0.00e+00, 4.49e+00, 4.22e+00]    [3.82e+00, 4.49e+00, 3.10e+00, 3.18e+00]    
9000      [3.43e+00, 1.06e+00, 4.17e+00]    [0.00e+00, 4.43e+00, 4.17e+00]    [3.97e+00, 4.43e+00, 2.79e+00, 2.81e+00]    
10000     [3.39e+00, 8.59e-01, 4.13e+00]    [0.00e+00, 4.47e+00, 4.13e+00]    [4.19e+00, 4.47e+00, 2.58e+00, 2.58e+00]    
11000     [3.34e+00, 7.63e-01, 4.09e+00]    [0.00e+00, 4.37e+00, 4.09e+00]    [4.24e+00, 4.37e+00, 2.33e+00, 2.32e+00]    
12000     [3.32e+00, 6.96e-01, 4.06e+00]    [0.00e+00, 4.33e+00, 4.06e+00]    [4.31e+00, 4.33e+00, 2.17e+00, 2.17e+00]    
13000     [3.28e+00, 6.51e-01, 4.02e+00]    [0.00e+00, 4.21e+00, 4.02e+00]    [4.28e+00, 4.21e+00, 1.99e+00, 2.00e+00]    
14000     [3.26e+00, 6.31e-01, 3.98e+00]    [0.00e+00, 4.09e+00, 3.98e+00]    [4.23e+00, 4.09e+00, 1.81e+00, 1.84e+00]    
15000     [3.21e+00, 5.28e-01, 3.95e+00]    [0.00e+00, 4.02e+00, 3.95e+00]    [4.21e+00, 4.02e+00, 1.74e+00, 1.78e+00]    
16000     [3.19e+00, 5.41e-01, 3.92e+00]    [0.00e+00, 3.92e+00, 3.92e+00]    [4.15e+00, 3.92e+00, 1.64e+00, 1.71e+00]    
17000     [3.19e+00, 6.89e-01, 3.89e+00]    [0.00e+00, 3.85e+00, 3.89e+00]    [4.11e+00, 3.85e+00, 1.55e+00, 1.64e+00]    
18000     [3.13e+00, 4.53e-01, 3.86e+00]    [0.00e+00, 3.84e+00, 3.86e+00]    [4.12e+00, 3.84e+00, 1.52e+00, 1.61e+00]    
19000     [3.11e+00, 4.49e-01, 3.83e+00]    [0.00e+00, 3.80e+00, 3.83e+00]    [4.10e+00, 3.80e+00, 1.46e+00, 1.56e+00]    
20000     [3.10e+00, 4.58e-01, 3.80e+00]    [0.00e+00, 3.71e+00, 3.80e+00]    [4.03e+00, 3.71e+00, 1.37e+00, 1.49e+00]    
21000     [3.09e+00, 3.98e-01, 3.77e+00]    [0.00e+00, 3.63e+00, 3.77e+00]    [3.97e+00, 3.63e+00, 1.34e+00, 1.48e+00]    
22000     [3.06e+00, 3.47e-01, 3.75e+00]    [0.00e+00, 3.49e+00, 3.75e+00]    [3.86e+00, 3.49e+00, 1.29e+00, 1.46e+00]    
23000     [3.06e+00, 3.95e-01, 3.72e+00]    [0.00e+00, 3.39e+00, 3.72e+00]    [3.77e+00, 3.39e+00, 1.26e+00, 1.44e+00]    
24000     [3.03e+00, 2.67e-01, 3.70e+00]    [0.00e+00, 3.36e+00, 3.70e+00]    [3.76e+00, 3.36e+00, 1.25e+00, 1.44e+00]    
25000     [3.02e+00, 2.56e-01, 3.68e+00]    [0.00e+00, 3.33e+00, 3.68e+00]    [3.75e+00, 3.33e+00, 1.24e+00, 1.44e+00]    
26000     [3.01e+00, 2.44e-01, 3.66e+00]    [0.00e+00, 3.32e+00, 3.66e+00]    [3.75e+00, 3.32e+00, 1.24e+00, 1.44e+00]    
27000     [3.01e+00, 3.10e-01, 3.63e+00]    [0.00e+00, 3.30e+00, 3.63e+00]    [3.75e+00, 3.30e+00, 1.25e+00, 1.46e+00]    
28000     [2.98e+00, 1.85e-01, 3.61e+00]    [0.00e+00, 3.26e+00, 3.61e+00]    [3.73e+00, 3.26e+00, 1.24e+00, 1.45e+00]    
29000     [2.98e+00, 1.84e-01, 3.59e+00]    [0.00e+00, 3.25e+00, 3.59e+00]    [3.73e+00, 3.25e+00, 1.25e+00, 1.45e+00]    
30000     [2.99e+00, 3.29e-01, 3.57e+00]    [0.00e+00, 3.22e+00, 3.57e+00]    [3.71e+00, 3.22e+00, 1.24e+00, 1.44e+00]    

Best model at step 29000:
  train loss: 6.76e+00
  test loss: 6.84e+00
  test metric: [3.73e+00, 3.25e+00, 1.25e+00, 1.45e+00]

'train' took 81.416239 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.465754 s

'compile' took 2.086421 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.02e+02, 1.02e+02, 2.93e+00]    [0.00e+00, 1.02e+02, 2.93e+00]    [1.02e+02, 1.02e+02, 2.23e+00, 2.23e+00]    
1000      [7.75e+01, 6.92e+01, 3.02e+00]    [0.00e+00, 8.19e+01, 3.02e+00]    [8.19e+01, 8.19e+01, 2.06e+01, 2.07e+01]    
2000      [4.45e+01, 2.58e+01, 3.94e+00]    [0.00e+00, 5.93e+01, 3.94e+00]    [6.03e+01, 5.93e+01, 2.19e+01, 2.25e+01]    
3000      [2.23e+01, 1.24e+01, 4.30e+00]    [0.00e+00, 4.06e+01, 4.30e+00]    [4.17e+01, 4.06e+01, 8.34e+00, 8.50e+00]    
4000      [1.05e+01, 2.93e+00, 4.49e+00]    [0.00e+00, 2.15e+01, 4.49e+00]    [2.24e+01, 2.15e+01, 1.31e+01, 1.32e+01]    
5000      [7.24e+00, 1.23e+00, 4.46e+00]    [0.00e+00, 1.50e+01, 4.46e+00]    [1.53e+01, 1.50e+01, 1.16e+01, 1.16e+01]    
6000      [6.09e+00, 6.23e-01, 4.40e+00]    [0.00e+00, 1.22e+01, 4.40e+00]    [1.22e+01, 1.22e+01, 1.16e+01, 1.16e+01]    
7000      [5.64e+00, 4.11e-01, 4.33e+00]    [0.00e+00, 1.12e+01, 4.33e+00]    [1.10e+01, 1.12e+01, 1.13e+01, 1.13e+01]    
8000      [5.31e+00, 2.87e-01, 4.27e+00]    [0.00e+00, 1.06e+01, 4.27e+00]    [1.02e+01, 1.06e+01, 1.09e+01, 1.09e+01]    
9000      [5.09e+00, 2.04e-01, 4.22e+00]    [0.00e+00, 1.02e+01, 4.22e+00]    [9.55e+00, 1.02e+01, 1.07e+01, 1.08e+01]    
10000     [4.84e+00, 1.00e-01, 4.19e+00]    [0.00e+00, 9.77e+00, 4.19e+00]    [8.82e+00, 9.77e+00, 1.04e+01, 1.05e+01]    
11000     [4.64e+00, 1.36e-02, 4.15e+00]    [0.00e+00, 9.39e+00, 4.15e+00]    [8.24e+00, 9.39e+00, 1.01e+01, 1.03e+01]    
12000     [4.52e+00, 2.47e-02, 4.11e+00]    [0.00e+00, 9.01e+00, 4.11e+00]    [7.78e+00, 9.01e+00, 9.74e+00, 9.96e+00]    
13000     [4.40e+00, 5.44e-02, 4.07e+00]    [0.00e+00, 8.72e+00, 4.07e+00]    [7.44e+00, 8.72e+00, 9.49e+00, 9.73e+00]    
14000     [4.32e+00, 2.74e-02, 4.04e+00]    [0.00e+00, 8.37e+00, 4.04e+00]    [7.06e+00, 8.37e+00, 9.18e+00, 9.43e+00]    
15000     [4.23e+00, 5.22e-02, 4.01e+00]    [0.00e+00, 8.02e+00, 4.01e+00]    [6.69e+00, 8.02e+00, 8.95e+00, 9.19e+00]    
16000     [4.16e+00, 1.09e-01, 3.98e+00]    [0.00e+00, 7.73e+00, 3.98e+00]    [6.41e+00, 7.73e+00, 8.83e+00, 9.09e+00]    
17000     [4.01e+00, 2.35e-02, 3.95e+00]    [0.00e+00, 7.32e+00, 3.95e+00]    [6.37e+00, 7.32e+00, 8.34e+00, 8.89e+00]    
18000     [3.93e+00, 9.45e-02, 3.92e+00]    [0.00e+00, 7.06e+00, 3.92e+00]    [6.38e+00, 7.06e+00, 8.09e+00, 8.82e+00]    
19000     [3.84e+00, 1.27e-02, 3.89e+00]    [0.00e+00, 6.93e+00, 3.89e+00]    [6.33e+00, 6.93e+00, 7.68e+00, 8.46e+00]    
20000     [3.79e+00, 5.97e-02, 3.86e+00]    [0.00e+00, 6.86e+00, 3.86e+00]    [6.25e+00, 6.86e+00, 7.45e+00, 8.24e+00]    
21000     [3.75e+00, 5.85e-02, 3.83e+00]    [0.00e+00, 6.75e+00, 3.83e+00]    [6.13e+00, 6.75e+00, 7.11e+00, 7.92e+00]    
22000     [3.72e+00, 1.06e-01, 3.80e+00]    [0.00e+00, 6.62e+00, 3.80e+00]    [5.99e+00, 6.62e+00, 6.98e+00, 7.82e+00]    
23000     [3.65e+00, 7.59e-02, 3.77e+00]    [0.00e+00, 6.50e+00, 3.77e+00]    [5.87e+00, 6.50e+00, 6.56e+00, 7.41e+00]    
24000     [3.58e+00, 6.61e-02, 3.74e+00]    [0.00e+00, 6.41e+00, 3.74e+00]    [5.78e+00, 6.41e+00, 6.46e+00, 7.33e+00]    
25000     [3.62e+00, 1.22e-01, 3.72e+00]    [0.00e+00, 6.33e+00, 3.72e+00]    [5.70e+00, 6.33e+00, 6.08e+00, 6.95e+00]    
26000     [3.49e+00, 2.20e-02, 3.69e+00]    [0.00e+00, 6.23e+00, 3.69e+00]    [5.59e+00, 6.23e+00, 5.98e+00, 6.87e+00]    
27000     [3.49e+00, 9.44e-02, 3.67e+00]    [0.00e+00, 6.11e+00, 3.67e+00]    [5.47e+00, 6.11e+00, 5.67e+00, 6.58e+00]    
28000     [3.41e+00, 3.23e-02, 3.65e+00]    [0.00e+00, 6.05e+00, 3.65e+00]    [5.41e+00, 6.05e+00, 5.57e+00, 6.50e+00]    
29000     [3.36e+00, 3.83e-02, 3.62e+00]    [0.00e+00, 5.97e+00, 3.62e+00]    [5.33e+00, 5.97e+00, 5.35e+00, 6.29e+00]    
30000     [3.39e+00, 9.77e-02, 3.60e+00]    [0.00e+00, 5.92e+00, 3.60e+00]    [5.27e+00, 5.92e+00, 5.06e+00, 6.01e+00]    

Best model at step 29000:
  train loss: 7.02e+00
  test loss: 9.60e+00
  test metric: [5.33e+00, 5.97e+00, 5.35e+00, 6.29e+00]

'train' took 85.397560 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.564924 s

'compile' took 3.055987 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.96e+01, 9.89e+01, 2.89e+00]    [0.00e+00, 9.93e+01, 2.89e+00]    [9.93e+01, 9.93e+01, 1.76e+00, 1.76e+00]    
1000      [7.82e+01, 7.32e+01, 2.95e+00]    [0.00e+00, 7.76e+01, 2.95e+00]    [7.76e+01, 7.76e+01, 2.13e+01, 2.14e+01]    
2000      [4.58e+01, 3.06e+01, 3.97e+00]    [0.00e+00, 4.72e+01, 3.97e+00]    [4.66e+01, 4.72e+01, 2.39e+01, 2.10e+01]    
3000      [1.79e+01, 8.38e+00, 4.58e+00]    [0.00e+00, 2.56e+01, 4.58e+00]    [2.60e+01, 2.56e+01, 7.67e+00, 6.37e+00]    
4000      [5.91e+00, 1.83e+00, 4.63e+00]    [0.00e+00, 6.79e+00, 4.63e+00]    [6.64e+00, 6.79e+00, 4.12e+00, 4.47e+00]    
5000      [4.89e+00, 8.52e-01, 4.57e+00]    [0.00e+00, 5.55e+00, 4.57e+00]    [5.50e+00, 5.55e+00, 3.76e+00, 4.18e+00]    
6000      [4.53e+00, 5.11e-01, 4.51e+00]    [0.00e+00, 5.54e+00, 4.51e+00]    [5.53e+00, 5.54e+00, 3.10e+00, 3.15e+00]    
7000      [4.31e+00, 3.46e-01, 4.46e+00]    [0.00e+00, 5.33e+00, 4.46e+00]    [5.35e+00, 5.33e+00, 2.77e+00, 2.69e+00]    
8000      [4.12e+00, 1.00e-01, 4.42e+00]    [0.00e+00, 5.01e+00, 4.42e+00]    [5.07e+00, 5.01e+00, 2.78e+00, 2.64e+00]    
9000      [3.98e+00, 1.62e-01, 4.38e+00]    [0.00e+00, 4.82e+00, 4.38e+00]    [4.90e+00, 4.82e+00, 2.85e+00, 2.65e+00]    
10000     [3.83e+00, 3.14e-02, 4.33e+00]    [0.00e+00, 4.67e+00, 4.33e+00]    [4.75e+00, 4.67e+00, 2.71e+00, 2.62e+00]    
11000     [3.75e+00, 4.66e-02, 4.29e+00]    [0.00e+00, 4.60e+00, 4.29e+00]    [4.69e+00, 4.60e+00, 2.61e+00, 2.53e+00]    
12000     [3.69e+00, 3.42e-02, 4.25e+00]    [0.00e+00, 4.50e+00, 4.25e+00]    [4.59e+00, 4.50e+00, 2.56e+00, 2.43e+00]    
13000     [3.66e+00, 1.10e-01, 4.21e+00]    [0.00e+00, 4.42e+00, 4.21e+00]    [4.51e+00, 4.42e+00, 2.52e+00, 2.35e+00]    
14000     [3.57e+00, 4.40e-02, 4.17e+00]    [0.00e+00, 4.41e+00, 4.17e+00]    [4.51e+00, 4.41e+00, 2.42e+00, 2.30e+00]    
15000     [3.55e+00, 7.63e-02, 4.14e+00]    [0.00e+00, 4.36e+00, 4.14e+00]    [4.45e+00, 4.36e+00, 2.38e+00, 2.19e+00]    
16000     [3.54e+00, 1.02e-01, 4.11e+00]    [0.00e+00, 4.36e+00, 4.11e+00]    [4.45e+00, 4.36e+00, 2.25e+00, 2.12e+00]    
17000     [3.54e+00, 2.14e-01, 4.08e+00]    [0.00e+00, 4.23e+00, 4.08e+00]    [4.32e+00, 4.23e+00, 2.34e+00, 1.97e+00]    
18000     [3.57e+00, 2.70e-01, 4.05e+00]    [0.00e+00, 4.29e+00, 4.05e+00]    [4.38e+00, 4.29e+00, 2.11e+00, 2.01e+00]    
19000     [3.45e+00, 1.52e-01, 4.02e+00]    [0.00e+00, 4.23e+00, 4.02e+00]    [4.32e+00, 4.23e+00, 2.13e+00, 1.87e+00]    
20000     [3.37e+00, 1.34e-01, 3.99e+00]    [0.00e+00, 4.17e+00, 3.99e+00]    [4.26e+00, 4.17e+00, 2.18e+00, 1.69e+00]    
21000     [3.35e+00, 1.27e-01, 3.96e+00]    [0.00e+00, 4.18e+00, 3.96e+00]    [4.26e+00, 4.18e+00, 2.11e+00, 1.70e+00]    
22000     [3.34e+00, 1.91e-01, 3.93e+00]    [0.00e+00, 3.99e+00, 3.93e+00]    [4.07e+00, 3.99e+00, 2.26e+00, 1.67e+00]    
23000     [3.30e+00, 1.75e-01, 3.91e+00]    [0.00e+00, 3.94e+00, 3.91e+00]    [4.02e+00, 3.94e+00, 2.24e+00, 1.65e+00]    
24000     [3.34e+00, 2.87e-01, 3.88e+00]    [0.00e+00, 3.84e+00, 3.88e+00]    [3.92e+00, 3.84e+00, 2.27e+00, 1.62e+00]    
25000     [3.20e+00, 3.75e-02, 3.85e+00]    [0.00e+00, 3.77e+00, 3.85e+00]    [3.84e+00, 3.77e+00, 2.17e+00, 1.74e+00]    
26000     [3.18e+00, 7.93e-02, 3.83e+00]    [0.00e+00, 3.71e+00, 3.83e+00]    [3.78e+00, 3.71e+00, 2.15e+00, 1.71e+00]    
27000     [3.19e+00, 8.28e-02, 3.80e+00]    [0.00e+00, 3.70e+00, 3.80e+00]    [3.77e+00, 3.70e+00, 2.11e+00, 1.78e+00]    
28000     [3.12e+00, 4.08e-02, 3.78e+00]    [0.00e+00, 3.57e+00, 3.78e+00]    [3.63e+00, 3.57e+00, 2.17e+00, 1.79e+00]    
29000     [3.11e+00, 6.73e-02, 3.75e+00]    [0.00e+00, 3.52e+00, 3.75e+00]    [3.58e+00, 3.52e+00, 2.15e+00, 1.76e+00]    
30000     [3.09e+00, 6.40e-02, 3.73e+00]    [0.00e+00, 3.46e+00, 3.73e+00]    [3.52e+00, 3.46e+00, 2.14e+00, 1.76e+00]    

Best model at step 30000:
  train loss: 6.88e+00
  test loss: 7.19e+00
  test metric: [3.52e+00, 3.46e+00, 2.14e+00, 1.76e+00]

'train' took 93.406889 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.553519 s

'compile' took 2.547189 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.91e+01, 9.95e+01, 2.56e+00]    [0.00e+00, 9.92e+01, 2.56e+00]    [9.92e+01, 9.92e+01, 2.00e+00, 2.00e+00]    
1000      [5.31e+01, 4.31e+01, 3.39e+00]    [0.00e+00, 7.35e+01, 3.39e+00]    [7.33e+01, 7.35e+01, 3.85e+01, 3.67e+01]    
2000      [2.80e+01, 1.16e+01, 3.79e+00]    [0.00e+00, 5.44e+01, 3.79e+00]    [5.54e+01, 5.44e+01, 3.21e+01, 3.16e+01]    
3000      [1.26e+01, 1.23e+00, 3.95e+00]    [0.00e+00, 3.08e+01, 3.95e+00]    [3.14e+01, 3.08e+01, 2.02e+01, 1.97e+01]    
4000      [8.54e+00, 9.45e-01, 3.86e+00]    [0.00e+00, 1.75e+01, 3.86e+00]    [1.77e+01, 1.75e+01, 1.39e+01, 1.38e+01]    
5000      [5.86e+00, 1.07e+00, 3.82e+00]    [0.00e+00, 9.80e+00, 3.82e+00]    [9.47e+00, 9.80e+00, 9.24e+00, 9.38e+00]    
6000      [4.25e+00, 1.02e+00, 3.81e+00]    [0.00e+00, 6.17e+00, 3.81e+00]    [5.79e+00, 6.17e+00, 4.82e+00, 5.64e+00]    
7000      [3.47e+00, 9.48e-01, 3.78e+00]    [0.00e+00, 4.76e+00, 3.78e+00]    [4.21e+00, 4.76e+00, 1.64e+00, 3.15e+00]    
8000      [3.22e+00, 7.60e-01, 3.73e+00]    [0.00e+00, 4.76e+00, 3.73e+00]    [4.19e+00, 4.76e+00, 9.27e-01, 2.63e+00]    
9000      [3.07e+00, 6.04e-01, 3.69e+00]    [0.00e+00, 4.66e+00, 3.69e+00]    [4.04e+00, 4.66e+00, 6.84e-01, 2.56e+00]    
10000     [2.92e+00, 4.18e-01, 3.66e+00]    [0.00e+00, 4.46e+00, 3.66e+00]    [3.82e+00, 4.46e+00, 1.14e+00, 2.42e+00]    
11000     [2.84e+00, 2.72e-01, 3.63e+00]    [0.00e+00, 4.54e+00, 3.63e+00]    [3.84e+00, 4.54e+00, 1.53e+00, 2.56e+00]    
12000     [2.80e+00, 1.52e-01, 3.59e+00]    [0.00e+00, 4.43e+00, 3.59e+00]    [3.71e+00, 4.43e+00, 1.66e+00, 2.64e+00]    
13000     [2.79e+00, 4.92e-02, 3.56e+00]    [0.00e+00, 4.49e+00, 3.56e+00]    [3.73e+00, 4.49e+00, 1.76e+00, 2.64e+00]    
14000     [2.74e+00, 2.13e-02, 3.52e+00]    [0.00e+00, 4.28e+00, 3.52e+00]    [3.51e+00, 4.28e+00, 1.56e+00, 2.79e+00]    
15000     [2.72e+00, 8.71e-03, 3.47e+00]    [0.00e+00, 4.24e+00, 3.47e+00]    [3.46e+00, 4.24e+00, 1.53e+00, 2.77e+00]    
16000     [2.72e+00, 3.93e-02, 3.43e+00]    [0.00e+00, 4.21e+00, 3.43e+00]    [3.26e+00, 4.21e+00, 1.47e+00, 2.78e+00]    
17000     [2.67e+00, 1.73e-02, 3.39e+00]    [0.00e+00, 4.16e+00, 3.39e+00]    [3.28e+00, 4.16e+00, 1.44e+00, 2.75e+00]    
18000     [2.65e+00, 1.55e-02, 3.36e+00]    [0.00e+00, 4.19e+00, 3.36e+00]    [3.27e+00, 4.19e+00, 1.39e+00, 2.70e+00]    
19000     [2.65e+00, 1.78e-02, 3.32e+00]    [0.00e+00, 4.22e+00, 3.32e+00]    [3.25e+00, 4.22e+00, 1.31e+00, 2.66e+00]    
20000     [2.63e+00, 1.30e-02, 3.29e+00]    [0.00e+00, 4.18e+00, 3.29e+00]    [3.25e+00, 4.18e+00, 1.27e+00, 2.66e+00]    
21000     [2.61e+00, 1.81e-02, 3.26e+00]    [0.00e+00, 4.06e+00, 3.26e+00]    [3.27e+00, 4.06e+00, 1.33e+00, 2.66e+00]    
22000     [2.59e+00, 1.36e-02, 3.22e+00]    [0.00e+00, 4.12e+00, 3.22e+00]    [3.17e+00, 4.12e+00, 1.29e+00, 2.52e+00]    
23000     [2.59e+00, 2.67e-02, 3.20e+00]    [0.00e+00, 4.07e+00, 3.20e+00]    [3.28e+00, 4.07e+00, 1.31e+00, 2.50e+00]    
24000     [2.57e+00, 2.06e-02, 3.17e+00]    [0.00e+00, 4.07e+00, 3.17e+00]    [3.27e+00, 4.07e+00, 1.33e+00, 2.43e+00]    
25000     [2.54e+00, 1.40e-02, 3.14e+00]    [0.00e+00, 3.97e+00, 3.14e+00]    [3.18e+00, 3.97e+00, 1.29e+00, 2.49e+00]    
26000     [2.53e+00, 3.01e-02, 3.11e+00]    [0.00e+00, 4.04e+00, 3.11e+00]    [3.13e+00, 4.04e+00, 1.18e+00, 2.46e+00]    
27000     [2.54e+00, 3.73e-02, 3.09e+00]    [0.00e+00, 3.98e+00, 3.09e+00]    [3.19e+00, 3.98e+00, 1.33e+00, 2.36e+00]    
28000     [2.49e+00, 1.11e-02, 3.06e+00]    [0.00e+00, 3.92e+00, 3.06e+00]    [3.12e+00, 3.92e+00, 1.29e+00, 2.42e+00]    
29000     [2.49e+00, 1.39e-02, 3.04e+00]    [0.00e+00, 3.93e+00, 3.04e+00]    [3.14e+00, 3.93e+00, 1.32e+00, 2.36e+00]    
30000     [2.47e+00, 1.70e-02, 3.02e+00]    [0.00e+00, 3.90e+00, 3.02e+00]    [3.11e+00, 3.90e+00, 1.33e+00, 2.37e+00]    

Best model at step 30000:
  train loss: 5.51e+00
  test loss: 6.92e+00
  test metric: [3.11e+00, 3.90e+00, 1.33e+00, 2.37e+00]

'train' took 86.420183 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.477812 s

'compile' took 2.269793 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.01e+02, 1.03e+02, 2.59e+00]    [0.00e+00, 9.96e+01, 2.59e+00]    [9.96e+01, 9.96e+01, 7.92e-01, 7.92e-01]    
1000      [7.86e+01, 5.29e+01, 2.77e+00]    [0.00e+00, 9.56e+01, 2.77e+00]    [9.56e+01, 9.56e+01, 1.10e+01, 1.11e+01]    
2000      [4.78e+01, 2.49e+01, 3.59e+00]    [0.00e+00, 6.21e+01, 3.59e+00]    [6.42e+01, 6.21e+01, 2.22e+01, 2.35e+01]    
3000      [2.10e+01, 1.97e+00, 4.01e+00]    [0.00e+00, 2.64e+01, 4.01e+00]    [3.17e+01, 2.64e+01, 1.57e+01, 1.69e+01]    
4000      [9.31e+00, 2.06e+00, 4.07e+00]    [0.00e+00, 1.03e+01, 4.07e+00]    [1.28e+01, 1.03e+01, 7.78e+00, 8.01e+00]    
5000      [5.35e+00, 2.45e+00, 4.11e+00]    [0.00e+00, 4.76e+00, 4.11e+00]    [5.77e+00, 4.76e+00, 3.11e+00, 3.68e+00]    
6000      [4.41e+00, 2.16e+00, 4.09e+00]    [0.00e+00, 3.32e+00, 4.09e+00]    [3.69e+00, 3.32e+00, 3.02e+00, 3.44e+00]    
7000      [3.91e+00, 1.72e+00, 4.06e+00]    [0.00e+00, 3.13e+00, 4.06e+00]    [3.10e+00, 3.13e+00, 2.71e+00, 2.88e+00]    
8000      [3.74e+00, 1.66e+00, 4.02e+00]    [0.00e+00, 2.72e+00, 4.02e+00]    [2.74e+00, 2.72e+00, 2.37e+00, 2.52e+00]    
9000      [3.55e+00, 1.31e+00, 4.00e+00]    [0.00e+00, 2.58e+00, 4.00e+00]    [2.58e+00, 2.58e+00, 2.46e+00, 2.52e+00]    
10000     [3.45e+00, 1.29e+00, 3.97e+00]    [0.00e+00, 2.53e+00, 3.97e+00]    [2.53e+00, 2.53e+00, 2.35e+00, 2.35e+00]    
11000     [3.38e+00, 1.08e+00, 3.94e+00]    [0.00e+00, 2.57e+00, 3.94e+00]    [2.58e+00, 2.57e+00, 2.26e+00, 2.19e+00]    
12000     [3.33e+00, 1.09e+00, 3.91e+00]    [0.00e+00, 2.61e+00, 3.91e+00]    [2.62e+00, 2.61e+00, 2.17e+00, 2.01e+00]    
13000     [3.25e+00, 8.97e-01, 3.88e+00]    [0.00e+00, 2.58e+00, 3.88e+00]    [2.69e+00, 2.58e+00, 2.11e+00, 1.99e+00]    
14000     [3.20e+00, 8.27e-01, 3.85e+00]    [0.00e+00, 2.53e+00, 3.85e+00]    [2.71e+00, 2.53e+00, 2.08e+00, 1.95e+00]    
15000     [3.15e+00, 7.89e-01, 3.82e+00]    [0.00e+00, 2.54e+00, 3.82e+00]    [2.86e+00, 2.54e+00, 1.93e+00, 1.88e+00]    
16000     [3.14e+00, 8.39e-01, 3.80e+00]    [0.00e+00, 2.53e+00, 3.80e+00]    [3.01e+00, 2.53e+00, 1.78e+00, 1.87e+00]    
17000     [3.07e+00, 6.90e-01, 3.77e+00]    [0.00e+00, 2.54e+00, 3.77e+00]    [3.13e+00, 2.54e+00, 1.65e+00, 1.82e+00]    
18000     [3.04e+00, 6.18e-01, 3.74e+00]    [0.00e+00, 2.56e+00, 3.74e+00]    [3.24e+00, 2.56e+00, 1.52e+00, 1.71e+00]    
19000     [3.00e+00, 7.07e-01, 3.72e+00]    [0.00e+00, 2.50e+00, 3.72e+00]    [3.25e+00, 2.50e+00, 1.41e+00, 1.65e+00]    
20000     [2.97e+00, 5.58e-01, 3.69e+00]    [0.00e+00, 2.42e+00, 3.69e+00]    [3.23e+00, 2.42e+00, 1.29e+00, 1.57e+00]    
21000     [2.94e+00, 6.14e-01, 3.66e+00]    [0.00e+00, 2.37e+00, 3.66e+00]    [3.22e+00, 2.37e+00, 1.21e+00, 1.52e+00]    
22000     [2.94e+00, 5.66e-01, 3.64e+00]    [0.00e+00, 2.30e+00, 3.64e+00]    [3.17e+00, 2.30e+00, 1.16e+00, 1.50e+00]    
23000     [2.90e+00, 4.81e-01, 3.62e+00]    [0.00e+00, 2.22e+00, 3.62e+00]    [3.13e+00, 2.22e+00, 1.11e+00, 1.50e+00]    
24000     [2.91e+00, 5.79e-01, 3.59e+00]    [0.00e+00, 2.21e+00, 3.59e+00]    [3.15e+00, 2.21e+00, 1.10e+00, 1.50e+00]    
25000     [2.88e+00, 5.14e-01, 3.57e+00]    [0.00e+00, 2.18e+00, 3.57e+00]    [3.16e+00, 2.18e+00, 1.09e+00, 1.53e+00]    
26000     [2.87e+00, 4.18e-01, 3.55e+00]    [0.00e+00, 2.18e+00, 3.55e+00]    [3.19e+00, 2.18e+00, 1.06e+00, 1.53e+00]    
27000     [2.87e+00, 4.60e-01, 3.53e+00]    [0.00e+00, 2.16e+00, 3.53e+00]    [3.20e+00, 2.16e+00, 1.05e+00, 1.54e+00]    
28000     [2.90e+00, 4.37e-01, 3.51e+00]    [0.00e+00, 2.15e+00, 3.51e+00]    [3.21e+00, 2.15e+00, 1.04e+00, 1.55e+00]    
29000     [2.89e+00, 4.93e-01, 3.49e+00]    [0.00e+00, 2.17e+00, 3.49e+00]    [3.25e+00, 2.17e+00, 1.05e+00, 1.58e+00]    
30000     [2.86e+00, 4.45e-01, 3.47e+00]    [0.00e+00, 2.20e+00, 3.47e+00]    [3.30e+00, 2.20e+00, 1.03e+00, 1.57e+00]    

Best model at step 30000:
  train loss: 6.78e+00
  test loss: 5.67e+00
  test metric: [3.30e+00, 2.20e+00, 1.03e+00, 1.57e+00]

'train' took 85.210692 s


Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.465754 s

'compile' took 2.281898 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.86e+01, 9.95e+01, 2.81e+00]    [0.00e+00, 9.90e+01, 2.81e+00]    [9.90e+01, 9.90e+01, 1.54e+00, 1.54e+00]    
1000      [7.65e+01, 5.68e+01, 3.02e+00]    [0.00e+00, 8.47e+01, 3.02e+00]    [8.48e+01, 8.47e+01, 8.94e+00, 9.00e+00]    
2000      [5.10e+01, 3.46e+01, 3.68e+00]    [0.00e+00, 5.49e+01, 3.68e+00]    [5.76e+01, 5.49e+01, 1.54e+01, 1.64e+01]    
3000      [1.47e+01, 4.78e+00, 4.42e+00]    [0.00e+00, 9.24e+00, 4.42e+00]    [1.53e+01, 9.24e+00, 7.97e+00, 2.72e+00]    
4000      [6.49e+00, 1.33e+00, 4.46e+00]    [0.00e+00, 5.56e+00, 4.46e+00]    [8.06e+00, 5.56e+00, 2.70e+00, 3.97e+00]    
5000      [5.03e+00, 1.14e+00, 4.42e+00]    [0.00e+00, 5.30e+00, 4.42e+00]    [5.95e+00, 5.30e+00, 3.23e+00, 3.24e+00]    
6000      [4.63e+00, 1.15e+00, 4.38e+00]    [0.00e+00, 4.96e+00, 4.38e+00]    [5.36e+00, 4.96e+00, 2.67e+00, 2.89e+00]    
7000      [4.36e+00, 1.05e+00, 4.33e+00]    [0.00e+00, 4.43e+00, 4.33e+00]    [4.70e+00, 4.43e+00, 2.35e+00, 2.58e+00]    
8000      [4.14e+00, 9.65e-01, 4.29e+00]    [0.00e+00, 3.97e+00, 4.29e+00]    [4.16e+00, 3.97e+00, 2.14e+00, 2.38e+00]    
9000      [3.98e+00, 9.47e-01, 4.25e+00]    [0.00e+00, 3.60e+00, 4.25e+00]    [3.73e+00, 3.60e+00, 2.04e+00, 2.24e+00]    
10000     [3.79e+00, 8.46e-01, 4.21e+00]    [0.00e+00, 3.15e+00, 4.21e+00]    [3.21e+00, 3.15e+00, 1.98e+00, 2.11e+00]    
11000     [3.63e+00, 8.24e-01, 4.17e+00]    [0.00e+00, 2.87e+00, 4.17e+00]    [2.89e+00, 2.87e+00, 2.02e+00, 2.07e+00]    
12000     [3.51e+00, 7.73e-01, 4.14e+00]    [0.00e+00, 2.57e+00, 4.14e+00]    [2.56e+00, 2.57e+00, 2.08e+00, 2.06e+00]    
13000     [3.46e+00, 7.68e-01, 4.11e+00]    [0.00e+00, 2.41e+00, 4.11e+00]    [2.36e+00, 2.41e+00, 2.13e+00, 2.04e+00]    
14000     [3.34e+00, 6.75e-01, 4.08e+00]    [0.00e+00, 2.23e+00, 4.08e+00]    [2.31e+00, 2.23e+00, 2.11e+00, 2.11e+00]    
15000     [3.26e+00, 5.67e-01, 4.05e+00]    [0.00e+00, 2.26e+00, 4.05e+00]    [2.40e+00, 2.26e+00, 1.93e+00, 1.93e+00]    
16000     [3.21e+00, 5.15e-01, 4.02e+00]    [0.00e+00, 2.26e+00, 4.02e+00]    [2.45e+00, 2.26e+00, 1.78e+00, 1.85e+00]    
17000     [3.18e+00, 4.46e-01, 3.99e+00]    [0.00e+00, 2.24e+00, 3.99e+00]    [2.50e+00, 2.24e+00, 1.66e+00, 1.81e+00]    
18000     [3.17e+00, 5.50e-01, 3.97e+00]    [0.00e+00, 2.40e+00, 3.97e+00]    [2.68e+00, 2.40e+00, 1.55e+00, 1.71e+00]    
19000     [3.10e+00, 3.84e-01, 3.94e+00]    [0.00e+00, 2.52e+00, 3.94e+00]    [2.82e+00, 2.52e+00, 1.49e+00, 1.68e+00]    
20000     [3.06e+00, 3.96e-01, 3.92e+00]    [0.00e+00, 2.63e+00, 3.92e+00]    [2.96e+00, 2.63e+00, 1.40e+00, 1.61e+00]    
21000     [3.04e+00, 3.73e-01, 3.90e+00]    [0.00e+00, 2.71e+00, 3.90e+00]    [3.08e+00, 2.71e+00, 1.30e+00, 1.55e+00]    
22000     [3.02e+00, 3.64e-01, 3.88e+00]    [0.00e+00, 2.79e+00, 3.88e+00]    [3.20e+00, 2.79e+00, 1.25e+00, 1.53e+00]    
23000     [3.00e+00, 2.98e-01, 3.86e+00]    [0.00e+00, 2.82e+00, 3.86e+00]    [3.24e+00, 2.82e+00, 1.20e+00, 1.50e+00]    
24000     [2.97e+00, 2.39e-01, 3.84e+00]    [0.00e+00, 2.85e+00, 3.84e+00]    [3.27e+00, 2.85e+00, 1.20e+00, 1.51e+00]    
25000     [3.04e+00, 4.07e-01, 3.82e+00]    [0.00e+00, 2.85e+00, 3.82e+00]    [3.28e+00, 2.85e+00, 1.21e+00, 1.53e+00]    
26000     [2.94e+00, 1.70e-01, 3.80e+00]    [0.00e+00, 2.88e+00, 3.80e+00]    [3.34e+00, 2.88e+00, 1.16e+00, 1.51e+00]    
27000     [2.95e+00, 1.93e-01, 3.78e+00]    [0.00e+00, 2.95e+00, 3.78e+00]    [3.42e+00, 2.95e+00, 1.18e+00, 1.53e+00]    
28000     [2.93e+00, 1.95e-01, 3.76e+00]    [0.00e+00, 2.92e+00, 3.76e+00]    [3.39e+00, 2.92e+00, 1.20e+00, 1.57e+00]    
29000     [2.99e+00, 3.53e-01, 3.74e+00]    [0.00e+00, 2.90e+00, 3.74e+00]    [3.37e+00, 2.90e+00, 1.24e+00, 1.61e+00]    
30000     [2.97e+00, 3.04e-01, 3.72e+00]    [0.00e+00, 2.97e+00, 3.72e+00]    [3.44e+00, 2.97e+00, 1.18e+00, 1.54e+00]    

Best model at step 28000:
  train loss: 6.88e+00
  test loss: 6.68e+00
  test metric: [3.39e+00, 2.92e+00, 1.20e+00, 1.57e+00]

'train' took 85.485967 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.487195 s

'compile' took 2.175712 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.88e+01, 9.74e+01, 2.81e+00]    [0.00e+00, 9.80e+01, 2.81e+00]    [9.80e+01, 9.80e+01, 3.35e+00, 3.35e+00]    
1000      [7.73e+01, 7.12e+01, 2.88e+00]    [0.00e+00, 8.36e+01, 2.88e+00]    [8.36e+01, 8.36e+01, 1.61e+01, 1.61e+01]    
2000      [4.66e+01, 2.79e+01, 3.82e+00]    [0.00e+00, 5.41e+01, 3.82e+00]    [5.43e+01, 5.41e+01, 1.73e+01, 1.68e+01]    
3000      [2.24e+01, 7.38e+00, 4.40e+00]    [0.00e+00, 3.48e+01, 4.40e+00]    [3.52e+01, 3.48e+01, 1.15e+01, 1.25e+01]    
4000      [9.94e+00, 2.45e+00, 4.57e+00]    [0.00e+00, 1.25e+01, 4.57e+00]    [1.26e+01, 1.25e+01, 8.81e+00, 9.10e+00]    
5000      [6.86e+00, 3.64e-01, 4.55e+00]    [0.00e+00, 7.27e+00, 4.55e+00]    [7.22e+00, 7.27e+00, 5.25e+00, 5.41e+00]    
6000      [6.04e+00, 1.58e-01, 4.49e+00]    [0.00e+00, 5.88e+00, 4.49e+00]    [5.86e+00, 5.88e+00, 2.81e+00, 2.87e+00]    
7000      [5.44e+00, 1.93e-01, 4.43e+00]    [0.00e+00, 4.91e+00, 4.43e+00]    [4.89e+00, 4.91e+00, 2.20e+00, 2.19e+00]    
8000      [5.10e+00, 1.78e-01, 4.37e+00]    [0.00e+00, 4.54e+00, 4.37e+00]    [4.53e+00, 4.54e+00, 2.54e+00, 2.52e+00]    
9000      [4.76e+00, 2.88e-02, 4.32e+00]    [0.00e+00, 4.48e+00, 4.32e+00]    [4.46e+00, 4.48e+00, 2.52e+00, 2.49e+00]    
10000     [4.53e+00, 2.32e-01, 4.27e+00]    [0.00e+00, 4.56e+00, 4.27e+00]    [4.51e+00, 4.56e+00, 2.37e+00, 2.27e+00]    
11000     [4.30e+00, 6.01e-02, 4.23e+00]    [0.00e+00, 4.68e+00, 4.23e+00]    [4.59e+00, 4.68e+00, 1.98e+00, 1.82e+00]    
12000     [4.17e+00, 2.21e-01, 4.20e+00]    [0.00e+00, 4.47e+00, 4.20e+00]    [4.35e+00, 4.47e+00, 2.12e+00, 1.90e+00]    
13000     [4.01e+00, 1.47e-01, 4.17e+00]    [0.00e+00, 4.31e+00, 4.17e+00]    [4.16e+00, 4.31e+00, 2.35e+00, 2.03e+00]    
14000     [3.82e+00, 9.20e-02, 4.14e+00]    [0.00e+00, 4.25e+00, 4.14e+00]    [4.05e+00, 4.25e+00, 2.33e+00, 1.92e+00]    
15000     [3.69e+00, 5.09e-02, 4.11e+00]    [0.00e+00, 4.28e+00, 4.11e+00]    [4.05e+00, 4.28e+00, 2.19e+00, 1.70e+00]    
16000     [3.58e+00, 8.10e-02, 4.08e+00]    [0.00e+00, 4.43e+00, 4.08e+00]    [4.16e+00, 4.43e+00, 2.00e+00, 1.43e+00]    
17000     [3.44e+00, 6.44e-02, 4.05e+00]    [0.00e+00, 4.52e+00, 4.05e+00]    [4.22e+00, 4.52e+00, 1.95e+00, 1.34e+00]    
18000     [3.32e+00, 6.55e-02, 4.03e+00]    [0.00e+00, 4.53e+00, 4.03e+00]    [4.18e+00, 4.53e+00, 2.00e+00, 1.40e+00]    
19000     [3.30e+00, 1.33e-01, 4.00e+00]    [0.00e+00, 4.50e+00, 4.00e+00]    [4.11e+00, 4.50e+00, 2.10e+00, 1.60e+00]    
20000     [3.24e+00, 1.98e-01, 3.97e+00]    [0.00e+00, 4.36e+00, 3.97e+00]    [3.93e+00, 4.36e+00, 2.13e+00, 1.56e+00]    
21000     [3.20e+00, 1.37e-01, 3.95e+00]    [0.00e+00, 4.35e+00, 3.95e+00]    [3.87e+00, 4.35e+00, 2.27e+00, 1.86e+00]    
22000     [3.15e+00, 8.46e-02, 3.92e+00]    [0.00e+00, 4.32e+00, 3.92e+00]    [3.80e+00, 4.32e+00, 2.34e+00, 1.82e+00]    
23000     [3.09e+00, 8.26e-02, 3.90e+00]    [0.00e+00, 4.20e+00, 3.90e+00]    [3.64e+00, 4.20e+00, 2.35e+00, 1.87e+00]    
24000     [3.03e+00, 1.76e-02, 3.88e+00]    [0.00e+00, 4.13e+00, 3.88e+00]    [3.55e+00, 4.13e+00, 2.37e+00, 1.84e+00]    
25000     [3.00e+00, 3.18e-02, 3.85e+00]    [0.00e+00, 4.07e+00, 3.85e+00]    [3.46e+00, 4.07e+00, 2.38e+00, 1.85e+00]    
26000     [3.01e+00, 1.23e-01, 3.83e+00]    [0.00e+00, 3.99e+00, 3.83e+00]    [3.38e+00, 3.99e+00, 2.36e+00, 1.74e+00]    
27000     [3.08e+00, 2.83e-01, 3.81e+00]    [0.00e+00, 3.92e+00, 3.81e+00]    [3.29e+00, 3.92e+00, 2.33e+00, 1.61e+00]    
28000     [2.96e+00, 8.04e-02, 3.79e+00]    [0.00e+00, 3.93e+00, 3.79e+00]    [3.28e+00, 3.93e+00, 2.38e+00, 1.89e+00]    
29000     [2.93e+00, 2.51e-02, 3.76e+00]    [0.00e+00, 3.91e+00, 3.76e+00]    [3.25e+00, 3.91e+00, 2.38e+00, 1.88e+00]    
30000     [2.94e+00, 1.29e-01, 3.74e+00]    [0.00e+00, 3.84e+00, 3.74e+00]    [3.17e+00, 3.84e+00, 2.36e+00, 1.79e+00]    

Best model at step 29000:
  train loss: 6.72e+00
  test loss: 7.67e+00
  test metric: [3.25e+00, 3.91e+00, 2.38e+00, 1.88e+00]

'train' took 81.496867 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.443407 s

'compile' took 2.139546 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.97e+01, 1.00e+02, 2.71e+00]    [0.00e+00, 9.99e+01, 2.71e+00]    [9.99e+01, 9.99e+01, 5.32e-01, 5.32e-01]    
1000      [7.87e+01, 6.26e+01, 2.75e+00]    [0.00e+00, 8.99e+01, 2.75e+00]    [8.99e+01, 8.99e+01, 5.20e+00, 5.20e+00]    
2000      [5.18e+01, 3.54e+01, 3.39e+00]    [0.00e+00, 5.55e+01, 3.39e+00]    [5.59e+01, 5.55e+01, 1.21e+01, 1.22e+01]    
3000      [9.31e+00, 3.92e+00, 4.32e+00]    [0.00e+00, 7.99e+00, 4.32e+00]    [9.71e+00, 7.99e+00, 3.59e+00, 5.59e+00]    
4000      [4.24e+00, 3.79e-01, 4.34e+00]    [0.00e+00, 4.42e+00, 4.34e+00]    [3.81e+00, 4.42e+00, 2.28e+00, 3.10e+00]    
5000      [3.98e+00, 3.10e-02, 4.30e+00]    [0.00e+00, 4.26e+00, 4.30e+00]    [3.64e+00, 4.26e+00, 2.78e+00, 3.18e+00]    
6000      [3.82e+00, 8.45e-02, 4.25e+00]    [0.00e+00, 4.20e+00, 4.25e+00]    [3.66e+00, 4.20e+00, 2.85e+00, 3.11e+00]    
7000      [3.73e+00, 6.14e-02, 4.21e+00]    [0.00e+00, 4.17e+00, 4.21e+00]    [3.69e+00, 4.17e+00, 2.87e+00, 3.05e+00]    
8000      [3.66e+00, 1.12e-01, 4.17e+00]    [0.00e+00, 4.14e+00, 4.17e+00]    [3.72e+00, 4.14e+00, 2.85e+00, 2.98e+00]    
9000      [3.62e+00, 2.37e-01, 4.13e+00]    [0.00e+00, 4.08e+00, 4.13e+00]    [3.72e+00, 4.08e+00, 2.77e+00, 2.91e+00]    
10000     [3.55e+00, 7.39e-02, 4.09e+00]    [0.00e+00, 4.00e+00, 4.09e+00]    [3.74e+00, 4.00e+00, 2.66e+00, 2.85e+00]    
11000     [3.51e+00, 1.71e-01, 4.06e+00]    [0.00e+00, 3.90e+00, 4.06e+00]    [3.72e+00, 3.90e+00, 2.58e+00, 2.81e+00]    
12000     [3.51e+00, 2.68e-01, 4.02e+00]    [0.00e+00, 3.83e+00, 4.02e+00]    [3.70e+00, 3.83e+00, 2.49e+00, 2.71e+00]    
13000     [3.44e+00, 2.03e-01, 3.99e+00]    [0.00e+00, 3.83e+00, 3.99e+00]    [3.71e+00, 3.83e+00, 2.42e+00, 2.61e+00]    
14000     [3.38e+00, 9.76e-02, 3.96e+00]    [0.00e+00, 3.80e+00, 3.96e+00]    [3.70e+00, 3.80e+00, 2.35e+00, 2.52e+00]    
15000     [3.39e+00, 1.94e-01, 3.93e+00]    [0.00e+00, 3.77e+00, 3.93e+00]    [3.69e+00, 3.77e+00, 2.29e+00, 2.42e+00]    
16000     [3.34e+00, 4.76e-02, 3.90e+00]    [0.00e+00, 3.75e+00, 3.90e+00]    [3.69e+00, 3.75e+00, 2.28e+00, 2.38e+00]    
17000     [3.35e+00, 2.49e-01, 3.87e+00]    [0.00e+00, 3.75e+00, 3.87e+00]    [3.70e+00, 3.75e+00, 2.25e+00, 2.33e+00]    
18000     [3.29e+00, 6.40e-02, 3.84e+00]    [0.00e+00, 3.71e+00, 3.84e+00]    [3.67e+00, 3.71e+00, 2.21e+00, 2.27e+00]    
19000     [3.31e+00, 2.52e-01, 3.81e+00]    [0.00e+00, 3.70e+00, 3.81e+00]    [3.67e+00, 3.70e+00, 2.18e+00, 2.22e+00]    
20000     [3.26e+00, 1.08e-01, 3.79e+00]    [0.00e+00, 3.69e+00, 3.79e+00]    [3.67e+00, 3.69e+00, 2.13e+00, 2.15e+00]    
21000     [3.24e+00, 4.15e-02, 3.76e+00]    [0.00e+00, 3.65e+00, 3.76e+00]    [3.65e+00, 3.65e+00, 2.11e+00, 2.12e+00]    
22000     [3.22e+00, 7.06e-02, 3.74e+00]    [0.00e+00, 3.63e+00, 3.74e+00]    [3.64e+00, 3.63e+00, 2.07e+00, 2.06e+00]    
23000     [3.22e+00, 6.50e-02, 3.71e+00]    [0.00e+00, 3.63e+00, 3.71e+00]    [3.65e+00, 3.63e+00, 2.03e+00, 2.01e+00]    
24000     [3.21e+00, 6.05e-02, 3.69e+00]    [0.00e+00, 3.62e+00, 3.69e+00]    [3.64e+00, 3.62e+00, 2.00e+00, 1.97e+00]    
25000     [3.20e+00, 1.44e-01, 3.67e+00]    [0.00e+00, 3.62e+00, 3.67e+00]    [3.65e+00, 3.62e+00, 1.97e+00, 1.92e+00]    
26000     [3.17e+00, 2.88e-02, 3.64e+00]    [0.00e+00, 3.60e+00, 3.64e+00]    [3.64e+00, 3.60e+00, 1.94e+00, 1.88e+00]    
27000     [3.17e+00, 5.77e-02, 3.62e+00]    [0.00e+00, 3.58e+00, 3.62e+00]    [3.63e+00, 3.58e+00, 1.91e+00, 1.85e+00]    
28000     [3.16e+00, 9.69e-02, 3.60e+00]    [0.00e+00, 3.57e+00, 3.60e+00]    [3.63e+00, 3.57e+00, 1.89e+00, 1.82e+00]    
29000     [3.14e+00, 6.18e-02, 3.58e+00]    [0.00e+00, 3.57e+00, 3.58e+00]    [3.63e+00, 3.57e+00, 1.85e+00, 1.77e+00]    
30000     [3.13e+00, 5.39e-02, 3.56e+00]    [0.00e+00, 3.55e+00, 3.56e+00]    [3.63e+00, 3.55e+00, 1.84e+00, 1.75e+00]    

Best model at step 30000:
  train loss: 6.74e+00
  test loss: 7.11e+00
  test metric: [3.63e+00, 3.55e+00, 1.84e+00, 1.75e+00]

'train' took 83.229645 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[3.5600845832226433, 4.269543558462299, 3.2494949139747167, 5.972981153187167, 3.4565341410553785, 3.8969452565420926, 2.198897508208994, 2.919086767857735, 3.910134640072556, 3.5545988706092433]
E* 8 3.698830139319283 0.9348708969320372
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  ...  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804  ...   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628  ...   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000  ...   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000  ...   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000  ...   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000  ...   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000  ...   0.971835  190.913667   0.038209

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.528174 s

'compile' took 2.210269 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.72e+01, 9.59e+01, 2.67e+00]    [0.00e+00, 9.98e+01, 2.67e+00]    [9.98e+01, 9.98e+01, 5.31e-01, 5.31e-01]    
1000      [7.36e+01, 5.01e+01, 3.00e+00]    [0.00e+00, 9.01e+01, 3.00e+00]    [9.02e+01, 9.01e+01, 7.94e+00, 8.04e+00]    
2000      [3.98e+01, 1.70e+01, 3.77e+00]    [0.00e+00, 6.25e+01, 3.77e+00]    [6.35e+01, 6.25e+01, 2.53e+01, 2.60e+01]    
3000      [2.18e+01, 8.00e+00, 3.94e+00]    [0.00e+00, 3.69e+01, 3.94e+00]    [3.80e+01, 3.69e+01, 2.43e+01, 2.47e+01]    
4000      [8.89e+00, 2.29e+00, 4.13e+00]    [0.00e+00, 1.44e+01, 4.13e+00]    [1.45e+01, 1.44e+01, 1.29e+01, 1.21e+01]    
5000      [6.50e+00, 4.30e-01, 4.08e+00]    [0.00e+00, 9.60e+00, 4.08e+00]    [9.64e+00, 9.60e+00, 6.19e+00, 5.95e+00]    
6000      [5.42e+00, 5.53e-02, 4.03e+00]    [0.00e+00, 7.19e+00, 4.03e+00]    [7.18e+00, 7.19e+00, 3.50e+00, 3.51e+00]    
7000      [4.85e+00, 1.30e-01, 3.99e+00]    [0.00e+00, 5.66e+00, 3.99e+00]    [5.62e+00, 5.66e+00, 2.47e+00, 2.43e+00]    
8000      [4.50e+00, 4.01e-02, 3.96e+00]    [0.00e+00, 4.73e+00, 3.96e+00]    [4.67e+00, 4.73e+00, 2.57e+00, 2.36e+00]    
9000      [4.28e+00, 3.99e-02, 3.92e+00]    [0.00e+00, 4.37e+00, 3.92e+00]    [4.30e+00, 4.37e+00, 2.70e+00, 2.44e+00]    
10000     [4.14e+00, 7.75e-02, 3.88e+00]    [0.00e+00, 4.10e+00, 3.88e+00]    [4.02e+00, 4.10e+00, 2.80e+00, 2.48e+00]    
11000     [4.03e+00, 3.47e-02, 3.85e+00]    [0.00e+00, 3.86e+00, 3.85e+00]    [3.77e+00, 3.86e+00, 2.92e+00, 2.55e+00]    
12000     [3.95e+00, 1.18e-01, 3.82e+00]    [0.00e+00, 3.76e+00, 3.82e+00]    [3.67e+00, 3.76e+00, 2.91e+00, 2.53e+00]    
13000     [3.82e+00, 2.16e-01, 3.79e+00]    [0.00e+00, 3.72e+00, 3.79e+00]    [3.63e+00, 3.72e+00, 2.87e+00, 2.47e+00]    
14000     [3.74e+00, 3.10e-02, 3.76e+00]    [0.00e+00, 3.64e+00, 3.76e+00]    [3.54e+00, 3.64e+00, 2.86e+00, 2.45e+00]    
15000     [3.70e+00, 1.62e-01, 3.73e+00]    [0.00e+00, 3.57e+00, 3.73e+00]    [3.47e+00, 3.57e+00, 2.84e+00, 2.40e+00]    
16000     [3.60e+00, 8.15e-02, 3.71e+00]    [0.00e+00, 3.52e+00, 3.71e+00]    [3.42e+00, 3.52e+00, 2.82e+00, 2.35e+00]    
17000     [3.51e+00, 1.91e-01, 3.68e+00]    [0.00e+00, 3.49e+00, 3.68e+00]    [3.39e+00, 3.49e+00, 2.77e+00, 2.29e+00]    
18000     [3.47e+00, 9.33e-02, 3.66e+00]    [0.00e+00, 3.44e+00, 3.66e+00]    [3.32e+00, 3.44e+00, 2.80e+00, 2.25e+00]    
19000     [3.38e+00, 4.20e-02, 3.64e+00]    [0.00e+00, 3.41e+00, 3.64e+00]    [3.28e+00, 3.41e+00, 2.79e+00, 2.19e+00]    
20000     [3.35e+00, 8.37e-02, 3.61e+00]    [0.00e+00, 3.36e+00, 3.61e+00]    [3.23e+00, 3.36e+00, 2.78e+00, 2.14e+00]    
21000     [3.31e+00, 3.18e-02, 3.59e+00]    [0.00e+00, 3.32e+00, 3.59e+00]    [3.18e+00, 3.32e+00, 2.76e+00, 2.10e+00]    
22000     [3.27e+00, 6.65e-02, 3.57e+00]    [0.00e+00, 3.32e+00, 3.57e+00]    [3.18e+00, 3.32e+00, 2.69e+00, 2.01e+00]    
23000     [3.24e+00, 1.40e-02, 3.55e+00]    [0.00e+00, 3.27e+00, 3.55e+00]    [3.12e+00, 3.27e+00, 2.69e+00, 1.98e+00]    
24000     [3.22e+00, 2.78e-02, 3.53e+00]    [0.00e+00, 3.24e+00, 3.53e+00]    [3.09e+00, 3.24e+00, 2.67e+00, 1.93e+00]    
25000     [3.20e+00, 5.48e-02, 3.51e+00]    [0.00e+00, 3.20e+00, 3.51e+00]    [3.04e+00, 3.20e+00, 2.64e+00, 1.89e+00]    
26000     [3.18e+00, 7.42e-02, 3.49e+00]    [0.00e+00, 3.18e+00, 3.49e+00]    [3.02e+00, 3.18e+00, 2.61e+00, 1.84e+00]    
27000     [3.19e+00, 2.04e-01, 3.47e+00]    [0.00e+00, 3.20e+00, 3.47e+00]    [3.04e+00, 3.20e+00, 2.54e+00, 1.75e+00]    
28000     [3.14e+00, 4.80e-02, 3.45e+00]    [0.00e+00, 3.20e+00, 3.45e+00]    [3.03e+00, 3.20e+00, 2.51e+00, 1.70e+00]    
29000     [3.14e+00, 8.34e-02, 3.44e+00]    [0.00e+00, 3.21e+00, 3.44e+00]    [3.04e+00, 3.21e+00, 2.48e+00, 1.65e+00]    
30000     [3.11e+00, 4.31e-02, 3.42e+00]    [0.00e+00, 3.20e+00, 3.42e+00]    [3.02e+00, 3.20e+00, 2.47e+00, 1.61e+00]    

Best model at step 30000:
  train loss: 6.57e+00
  test loss: 6.62e+00
  test metric: [3.02e+00, 3.20e+00, 2.47e+00, 1.61e+00]

'train' took 82.210988 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.478881 s

'compile' took 2.301737 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.68e+01, 9.74e+01, 2.80e+00]    [0.00e+00, 9.81e+01, 2.80e+00]    [9.81e+01, 9.81e+01, 2.17e+00, 2.17e+00]    
1000      [7.49e+01, 7.25e+01, 2.87e+00]    [0.00e+00, 8.49e+01, 2.87e+00]    [8.50e+01, 8.49e+01, 2.11e+01, 2.12e+01]    
2000      [4.26e+01, 3.19e+01, 3.64e+00]    [0.00e+00, 6.83e+01, 3.64e+00]    [6.88e+01, 6.83e+01, 3.38e+01, 3.43e+01]    
3000      [2.28e+01, 1.03e+01, 3.89e+00]    [0.00e+00, 4.45e+01, 3.89e+00]    [4.48e+01, 4.45e+01, 3.12e+01, 3.09e+01]    
4000      [1.14e+01, 2.45e+00, 4.02e+00]    [0.00e+00, 2.03e+01, 4.02e+00]    [2.04e+01, 2.03e+01, 2.14e+01, 2.08e+01]    
5000      [8.36e+00, 1.55e+00, 3.97e+00]    [0.00e+00, 1.40e+01, 3.97e+00]    [1.41e+01, 1.40e+01, 1.40e+01, 1.36e+01]    
6000      [6.39e+00, 1.37e+00, 3.91e+00]    [0.00e+00, 9.80e+00, 3.91e+00]    [9.81e+00, 9.80e+00, 9.14e+00, 9.04e+00]    
7000      [4.84e+00, 1.18e+00, 3.89e+00]    [0.00e+00, 7.29e+00, 3.89e+00]    [6.91e+00, 7.29e+00, 5.35e+00, 5.65e+00]    
8000      [4.03e+00, 1.10e+00, 3.87e+00]    [0.00e+00, 5.42e+00, 3.87e+00]    [4.50e+00, 5.42e+00, 3.14e+00, 3.85e+00]    
9000      [3.60e+00, 9.49e-01, 3.83e+00]    [0.00e+00, 4.79e+00, 3.83e+00]    [3.48e+00, 4.79e+00, 2.15e+00, 3.09e+00]    
10000     [3.39e+00, 1.01e+00, 3.79e+00]    [0.00e+00, 4.60e+00, 3.79e+00]    [3.15e+00, 4.60e+00, 1.78e+00, 2.82e+00]    
11000     [3.26e+00, 9.62e-01, 3.74e+00]    [0.00e+00, 4.67e+00, 3.74e+00]    [3.12e+00, 4.67e+00, 1.50e+00, 2.66e+00]    
12000     [3.16e+00, 8.41e-01, 3.70e+00]    [0.00e+00, 4.62e+00, 3.70e+00]    [2.98e+00, 4.62e+00, 1.32e+00, 2.51e+00]    
13000     [3.11e+00, 8.73e-01, 3.66e+00]    [0.00e+00, 4.38e+00, 3.66e+00]    [2.67e+00, 4.38e+00, 1.34e+00, 2.40e+00]    
14000     [3.04e+00, 8.02e-01, 3.63e+00]    [0.00e+00, 4.39e+00, 3.63e+00]    [2.61e+00, 4.39e+00, 1.31e+00, 2.35e+00]    
15000     [2.97e+00, 7.31e-01, 3.59e+00]    [0.00e+00, 4.48e+00, 3.59e+00]    [2.67e+00, 4.48e+00, 1.23e+00, 2.34e+00]    
16000     [2.93e+00, 7.18e-01, 3.57e+00]    [0.00e+00, 4.39e+00, 3.57e+00]    [2.54e+00, 4.39e+00, 1.30e+00, 2.32e+00]    
17000     [2.92e+00, 7.25e-01, 3.54e+00]    [0.00e+00, 4.45e+00, 3.54e+00]    [2.57e+00, 4.45e+00, 1.31e+00, 2.32e+00]    
18000     [2.85e+00, 6.74e-01, 3.51e+00]    [0.00e+00, 4.38e+00, 3.51e+00]    [2.51e+00, 4.38e+00, 1.41e+00, 2.37e+00]    
19000     [2.84e+00, 7.19e-01, 3.49e+00]    [0.00e+00, 4.51e+00, 3.49e+00]    [2.61e+00, 4.51e+00, 1.35e+00, 2.36e+00]    
20000     [2.78e+00, 6.40e-01, 3.46e+00]    [0.00e+00, 4.17e+00, 3.46e+00]    [2.27e+00, 4.17e+00, 1.52e+00, 2.34e+00]    
21000     [2.73e+00, 6.35e-01, 3.44e+00]    [0.00e+00, 4.16e+00, 3.44e+00]    [2.27e+00, 4.16e+00, 1.48e+00, 2.32e+00]    
22000     [2.72e+00, 6.17e-01, 3.42e+00]    [0.00e+00, 4.04e+00, 3.42e+00]    [2.15e+00, 4.04e+00, 1.54e+00, 2.31e+00]    
23000     [2.74e+00, 6.09e-01, 3.40e+00]    [0.00e+00, 3.76e+00, 3.40e+00]    [2.03e+00, 3.76e+00, 1.46e+00, 2.28e+00]    
24000     [2.73e+00, 5.93e-01, 3.38e+00]    [0.00e+00, 3.71e+00, 3.38e+00]    [2.06e+00, 3.71e+00, 1.44e+00, 2.31e+00]    
25000     [2.62e+00, 4.91e-01, 3.36e+00]    [0.00e+00, 3.59e+00, 3.36e+00]    [2.01e+00, 3.59e+00, 1.41e+00, 2.29e+00]    
26000     [2.61e+00, 4.99e-01, 3.34e+00]    [0.00e+00, 3.38e+00, 3.34e+00]    [2.01e+00, 3.38e+00, 1.24e+00, 2.25e+00]    
27000     [2.63e+00, 5.45e-01, 3.33e+00]    [0.00e+00, 3.21e+00, 3.33e+00]    [2.04e+00, 3.21e+00, 1.18e+00, 2.28e+00]    
28000     [2.60e+00, 5.41e-01, 3.31e+00]    [0.00e+00, 3.47e+00, 3.31e+00]    [2.08e+00, 3.47e+00, 1.27e+00, 2.30e+00]    
29000     [2.56e+00, 4.23e-01, 3.29e+00]    [0.00e+00, 3.27e+00, 3.29e+00]    [2.14e+00, 3.27e+00, 1.19e+00, 2.36e+00]    
30000     [2.56e+00, 4.72e-01, 3.28e+00]    [0.00e+00, 3.13e+00, 3.28e+00]    [2.13e+00, 3.13e+00, 1.19e+00, 2.37e+00]    

Best model at step 29000:
  train loss: 6.28e+00
  test loss: 6.57e+00
  test metric: [2.14e+00, 3.27e+00, 1.19e+00, 2.36e+00]

'train' took 83.629539 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.472131 s

'compile' took 2.241910 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+02, 1.02e+02, 2.67e+00]    [0.00e+00, 1.01e+02, 2.67e+00]    [1.01e+02, 1.01e+02, 1.64e+00, 1.64e+00]    
1000      [7.68e+01, 5.87e+01, 2.81e+00]    [0.00e+00, 8.80e+01, 2.81e+00]    [8.81e+01, 8.80e+01, 7.73e+00, 7.81e+00]    
2000      [4.79e+01, 3.42e+01, 3.47e+00]    [0.00e+00, 5.03e+01, 3.47e+00]    [5.29e+01, 5.03e+01, 1.15e+01, 1.21e+01]    
3000      [1.16e+01, 4.04e+00, 4.20e+00]    [0.00e+00, 1.10e+01, 4.20e+00]    [1.35e+01, 1.10e+01, 4.71e+00, 7.11e+00]    
4000      [5.85e+00, 1.56e+00, 4.22e+00]    [0.00e+00, 7.58e+00, 4.22e+00]    [8.01e+00, 7.58e+00, 4.00e+00, 5.06e+00]    
5000      [4.58e+00, 1.47e+00, 4.20e+00]    [0.00e+00, 6.55e+00, 4.20e+00]    [6.71e+00, 6.55e+00, 2.90e+00, 3.53e+00]    
6000      [4.12e+00, 1.13e+00, 4.17e+00]    [0.00e+00, 5.89e+00, 4.17e+00]    [5.86e+00, 5.89e+00, 2.92e+00, 2.84e+00]    
7000      [3.89e+00, 1.06e+00, 4.13e+00]    [0.00e+00, 5.37e+00, 4.13e+00]    [5.21e+00, 5.37e+00, 3.07e+00, 2.55e+00]    
8000      [3.81e+00, 9.19e-01, 4.09e+00]    [0.00e+00, 5.10e+00, 4.09e+00]    [4.87e+00, 5.10e+00, 3.11e+00, 2.40e+00]    
9000      [3.73e+00, 9.46e-01, 4.05e+00]    [0.00e+00, 4.90e+00, 4.05e+00]    [4.63e+00, 4.90e+00, 3.18e+00, 2.37e+00]    
10000     [3.68e+00, 8.64e-01, 4.02e+00]    [0.00e+00, 4.76e+00, 4.02e+00]    [4.46e+00, 4.76e+00, 3.23e+00, 2.40e+00]    
11000     [3.61e+00, 7.44e-01, 3.98e+00]    [0.00e+00, 4.61e+00, 3.98e+00]    [4.31e+00, 4.61e+00, 3.27e+00, 2.47e+00]    
12000     [3.60e+00, 7.42e-01, 3.95e+00]    [0.00e+00, 4.43e+00, 3.95e+00]    [4.11e+00, 4.43e+00, 3.27e+00, 2.47e+00]    
13000     [3.54e+00, 6.64e-01, 3.92e+00]    [0.00e+00, 4.26e+00, 3.92e+00]    [4.02e+00, 4.26e+00, 3.16e+00, 2.49e+00]    
14000     [3.50e+00, 6.43e-01, 3.90e+00]    [0.00e+00, 4.08e+00, 3.90e+00]    [3.93e+00, 4.08e+00, 3.03e+00, 2.49e+00]    
15000     [3.45e+00, 5.99e-01, 3.87e+00]    [0.00e+00, 3.91e+00, 3.87e+00]    [3.83e+00, 3.91e+00, 2.91e+00, 2.49e+00]    
16000     [3.41e+00, 5.20e-01, 3.85e+00]    [0.00e+00, 3.69e+00, 3.85e+00]    [3.75e+00, 3.69e+00, 2.70e+00, 2.52e+00]    
17000     [3.40e+00, 6.04e-01, 3.83e+00]    [0.00e+00, 3.57e+00, 3.83e+00]    [3.76e+00, 3.57e+00, 2.56e+00, 2.55e+00]    
18000     [3.35e+00, 5.15e-01, 3.81e+00]    [0.00e+00, 3.39e+00, 3.81e+00]    [3.69e+00, 3.39e+00, 2.38e+00, 2.57e+00]    
19000     [3.36e+00, 6.76e-01, 3.78e+00]    [0.00e+00, 3.27e+00, 3.78e+00]    [3.59e+00, 3.27e+00, 2.25e+00, 2.53e+00]    
20000     [3.29e+00, 4.90e-01, 3.76e+00]    [0.00e+00, 3.13e+00, 3.76e+00]    [3.52e+00, 3.13e+00, 2.12e+00, 2.55e+00]    
21000     [3.27e+00, 3.90e-01, 3.74e+00]    [0.00e+00, 3.03e+00, 3.74e+00]    [3.45e+00, 3.03e+00, 2.04e+00, 2.59e+00]    
22000     [3.25e+00, 4.46e-01, 3.72e+00]    [0.00e+00, 2.92e+00, 3.72e+00]    [3.35e+00, 2.92e+00, 1.97e+00, 2.62e+00]    
23000     [3.24e+00, 4.69e-01, 3.70e+00]    [0.00e+00, 2.87e+00, 3.70e+00]    [3.35e+00, 2.87e+00, 1.91e+00, 2.62e+00]    
24000     [3.21e+00, 3.54e-01, 3.68e+00]    [0.00e+00, 2.82e+00, 3.68e+00]    [3.34e+00, 2.82e+00, 1.87e+00, 2.63e+00]    
25000     [3.21e+00, 3.72e-01, 3.66e+00]    [0.00e+00, 2.81e+00, 3.66e+00]    [3.35e+00, 2.81e+00, 1.86e+00, 2.63e+00]    
26000     [3.20e+00, 3.44e-01, 3.64e+00]    [0.00e+00, 2.79e+00, 3.64e+00]    [3.35e+00, 2.79e+00, 1.82e+00, 2.61e+00]    
27000     [3.17e+00, 3.26e-01, 3.62e+00]    [0.00e+00, 2.72e+00, 3.62e+00]    [3.32e+00, 2.72e+00, 1.76e+00, 2.60e+00]    
28000     [3.19e+00, 3.32e-01, 3.61e+00]    [0.00e+00, 2.65e+00, 3.61e+00]    [3.28e+00, 2.65e+00, 1.71e+00, 2.58e+00]    
29000     [3.19e+00, 3.94e-01, 3.59e+00]    [0.00e+00, 2.64e+00, 3.59e+00]    [3.28e+00, 2.64e+00, 1.67e+00, 2.51e+00]    
30000     [3.12e+00, 2.36e-01, 3.57e+00]    [0.00e+00, 2.57e+00, 3.57e+00]    [3.25e+00, 2.57e+00, 1.59e+00, 2.47e+00]    

Best model at step 30000:
  train loss: 6.93e+00
  test loss: 6.15e+00
  test metric: [3.25e+00, 2.57e+00, 1.59e+00, 2.47e+00]

'train' took 83.276281 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.498579 s

'compile' took 2.122351 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.93e+01, 9.90e+01, 2.81e+00]    [0.00e+00, 9.92e+01, 2.81e+00]    [9.92e+01, 9.92e+01, 1.26e+00, 1.26e+00]    
1000      [7.60e+01, 6.75e+01, 2.92e+00]    [0.00e+00, 8.55e+01, 2.92e+00]    [8.56e+01, 8.55e+01, 2.03e+01, 2.04e+01]    
2000      [4.32e+01, 2.71e+01, 3.75e+00]    [0.00e+00, 5.71e+01, 3.75e+00]    [5.94e+01, 5.71e+01, 2.98e+01, 3.14e+01]    
3000      [2.18e+01, 8.94e+00, 4.09e+00]    [0.00e+00, 3.54e+01, 4.09e+00]    [3.82e+01, 3.54e+01, 2.42e+01, 2.52e+01]    
4000      [1.08e+01, 1.21e+00, 4.25e+00]    [0.00e+00, 1.91e+01, 4.25e+00]    [2.07e+01, 1.91e+01, 1.26e+01, 1.26e+01]    
5000      [7.91e+00, 6.69e-01, 4.23e+00]    [0.00e+00, 1.52e+01, 4.23e+00]    [1.54e+01, 1.52e+01, 9.39e+00, 9.04e+00]    
6000      [6.12e+00, 4.83e-01, 4.20e+00]    [0.00e+00, 1.16e+01, 4.20e+00]    [1.16e+01, 1.16e+01, 8.03e+00, 8.07e+00]    
7000      [5.16e+00, 6.06e-01, 4.17e+00]    [0.00e+00, 9.38e+00, 4.17e+00]    [9.10e+00, 9.38e+00, 8.33e+00, 8.46e+00]    
8000      [4.73e+00, 4.98e-01, 4.13e+00]    [0.00e+00, 8.56e+00, 4.13e+00]    [8.09e+00, 8.56e+00, 8.76e+00, 8.92e+00]    
9000      [4.48e+00, 4.94e-01, 4.09e+00]    [0.00e+00, 8.45e+00, 4.09e+00]    [7.81e+00, 8.45e+00, 8.32e+00, 8.55e+00]    
10000     [4.27e+00, 3.78e-01, 4.05e+00]    [0.00e+00, 8.46e+00, 4.05e+00]    [7.64e+00, 8.46e+00, 7.71e+00, 8.06e+00]    
11000     [4.09e+00, 3.48e-01, 4.02e+00]    [0.00e+00, 8.30e+00, 4.02e+00]    [7.26e+00, 8.30e+00, 7.08e+00, 7.56e+00]    
12000     [3.91e+00, 2.36e-01, 3.99e+00]    [0.00e+00, 7.96e+00, 3.99e+00]    [6.74e+00, 7.96e+00, 6.45e+00, 7.04e+00]    
13000     [3.80e+00, 1.64e-01, 3.97e+00]    [0.00e+00, 7.50e+00, 3.97e+00]    [6.10e+00, 7.50e+00, 5.95e+00, 6.65e+00]    
14000     [3.68e+00, 7.82e-02, 3.95e+00]    [0.00e+00, 7.32e+00, 3.95e+00]    [5.69e+00, 7.32e+00, 5.55e+00, 6.23e+00]    
15000     [3.62e+00, 8.73e-02, 3.93e+00]    [0.00e+00, 7.23e+00, 3.93e+00]    [5.41e+00, 7.23e+00, 5.30e+00, 5.87e+00]    
16000     [3.52e+00, 5.57e-02, 3.90e+00]    [0.00e+00, 7.05e+00, 3.90e+00]    [5.17e+00, 7.05e+00, 4.93e+00, 5.49e+00]    
17000     [3.46e+00, 3.11e-02, 3.87e+00]    [0.00e+00, 6.97e+00, 3.87e+00]    [5.07e+00, 6.97e+00, 4.75e+00, 5.32e+00]    
18000     [3.42e+00, 5.48e-02, 3.84e+00]    [0.00e+00, 6.81e+00, 3.84e+00]    [4.89e+00, 6.81e+00, 4.46e+00, 5.06e+00]    
19000     [3.39e+00, 5.74e-02, 3.82e+00]    [0.00e+00, 6.68e+00, 3.82e+00]    [4.78e+00, 6.68e+00, 4.25e+00, 4.88e+00]    
20000     [3.37e+00, 8.97e-02, 3.79e+00]    [0.00e+00, 6.58e+00, 3.79e+00]    [4.70e+00, 6.58e+00, 4.13e+00, 4.77e+00]    
21000     [3.33e+00, 1.94e-02, 3.77e+00]    [0.00e+00, 6.44e+00, 3.77e+00]    [4.55e+00, 6.44e+00, 3.89e+00, 4.55e+00]    
22000     [3.30e+00, 2.78e-02, 3.74e+00]    [0.00e+00, 6.32e+00, 3.74e+00]    [4.46e+00, 6.32e+00, 3.71e+00, 4.39e+00]    
23000     [3.28e+00, 6.28e-02, 3.72e+00]    [0.00e+00, 6.22e+00, 3.72e+00]    [4.38e+00, 6.22e+00, 3.53e+00, 4.25e+00]    
24000     [3.25e+00, 3.82e-02, 3.70e+00]    [0.00e+00, 6.18e+00, 3.70e+00]    [4.33e+00, 6.18e+00, 3.45e+00, 4.17e+00]    
25000     [3.27e+00, 5.22e-02, 3.67e+00]    [0.00e+00, 6.09e+00, 3.67e+00]    [4.25e+00, 6.09e+00, 3.29e+00, 4.04e+00]    
26000     [3.22e+00, 4.95e-02, 3.65e+00]    [0.00e+00, 5.98e+00, 3.65e+00]    [4.12e+00, 5.98e+00, 3.13e+00, 3.88e+00]    
27000     [3.22e+00, 1.38e-01, 3.63e+00]    [0.00e+00, 5.83e+00, 3.63e+00]    [3.97e+00, 5.83e+00, 2.86e+00, 3.65e+00]    
28000     [3.18e+00, 6.00e-02, 3.61e+00]    [0.00e+00, 5.86e+00, 3.61e+00]    [3.99e+00, 5.86e+00, 2.86e+00, 3.65e+00]    
29000     [3.18e+00, 1.15e-01, 3.59e+00]    [0.00e+00, 5.68e+00, 3.59e+00]    [3.83e+00, 5.68e+00, 2.59e+00, 3.43e+00]    
30000     [3.15e+00, 6.98e-02, 3.57e+00]    [0.00e+00, 5.63e+00, 3.57e+00]    [3.81e+00, 5.63e+00, 2.52e+00, 3.39e+00]    

Best model at step 30000:
  train loss: 6.79e+00
  test loss: 9.20e+00
  test metric: [3.81e+00, 5.63e+00, 2.52e+00, 3.39e+00]

'train' took 83.071256 s


Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.467750 s

'compile' took 2.178934 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.92e+01, 9.90e+01, 2.86e+00]    [0.00e+00, 9.93e+01, 2.86e+00]    [9.93e+01, 9.93e+01, 1.10e+00, 1.10e+00]    
1000      [7.73e+01, 7.27e+01, 2.94e+00]    [0.00e+00, 7.72e+01, 2.94e+00]    [7.73e+01, 7.72e+01, 2.26e+01, 2.26e+01]    
2000      [4.57e+01, 3.25e+01, 3.96e+00]    [0.00e+00, 4.36e+01, 3.96e+00]    [4.45e+01, 4.36e+01, 1.67e+01, 1.61e+01]    
3000      [1.42e+01, 5.58e+00, 4.64e+00]    [0.00e+00, 1.85e+01, 4.64e+00]    [1.85e+01, 1.85e+01, 3.84e+00, 5.86e+00]    
4000      [5.40e+00, 1.90e+00, 4.71e+00]    [0.00e+00, 7.18e+00, 4.71e+00]    [6.38e+00, 7.18e+00, 3.73e+00, 3.55e+00]    
5000      [4.82e+00, 6.09e-01, 4.63e+00]    [0.00e+00, 5.98e+00, 4.63e+00]    [6.16e+00, 5.98e+00, 3.02e+00, 3.88e+00]    
6000      [4.37e+00, 4.60e-01, 4.56e+00]    [0.00e+00, 6.05e+00, 4.56e+00]    [6.14e+00, 6.05e+00, 2.49e+00, 2.91e+00]    
7000      [4.08e+00, 1.34e-01, 4.49e+00]    [0.00e+00, 5.83e+00, 4.49e+00]    [5.86e+00, 5.83e+00, 2.32e+00, 2.44e+00]    
8000      [3.85e+00, 1.76e-02, 4.42e+00]    [0.00e+00, 5.30e+00, 4.42e+00]    [5.30e+00, 5.30e+00, 2.61e+00, 2.58e+00]    
9000      [3.71e+00, 4.78e-02, 4.36e+00]    [0.00e+00, 5.04e+00, 4.36e+00]    [5.02e+00, 5.04e+00, 2.59e+00, 2.47e+00]    
10000     [3.61e+00, 5.77e-02, 4.30e+00]    [0.00e+00, 4.84e+00, 4.30e+00]    [4.81e+00, 4.84e+00, 2.53e+00, 2.33e+00]    
11000     [3.52e+00, 6.95e-02, 4.25e+00]    [0.00e+00, 4.67e+00, 4.25e+00]    [4.62e+00, 4.67e+00, 2.50e+00, 2.24e+00]    
12000     [3.42e+00, 2.41e-02, 4.20e+00]    [0.00e+00, 4.44e+00, 4.20e+00]    [4.38e+00, 4.44e+00, 2.57e+00, 2.26e+00]    
13000     [3.37e+00, 4.60e-02, 4.15e+00]    [0.00e+00, 4.32e+00, 4.15e+00]    [4.25e+00, 4.32e+00, 2.61e+00, 2.26e+00]    
14000     [3.34e+00, 1.18e-01, 4.10e+00]    [0.00e+00, 4.25e+00, 4.10e+00]    [4.17e+00, 4.25e+00, 2.62e+00, 2.24e+00]    
15000     [3.29e+00, 1.85e-02, 4.06e+00]    [0.00e+00, 4.26e+00, 4.06e+00]    [4.17e+00, 4.26e+00, 2.53e+00, 2.14e+00]    
16000     [3.31e+00, 2.07e-01, 4.02e+00]    [0.00e+00, 4.02e+00, 4.02e+00]    [3.93e+00, 4.02e+00, 2.71e+00, 2.28e+00]    
17000     [3.23e+00, 4.57e-02, 3.98e+00]    [0.00e+00, 4.01e+00, 3.98e+00]    [3.91e+00, 4.01e+00, 2.67e+00, 2.22e+00]    
18000     [3.21e+00, 5.53e-02, 3.94e+00]    [0.00e+00, 4.04e+00, 3.94e+00]    [3.94e+00, 4.04e+00, 2.59e+00, 2.12e+00]    
19000     [3.26e+00, 1.83e-01, 3.91e+00]    [0.00e+00, 4.05e+00, 3.91e+00]    [3.95e+00, 4.05e+00, 2.53e+00, 2.06e+00]    
20000     [3.19e+00, 9.42e-02, 3.88e+00]    [0.00e+00, 3.95e+00, 3.88e+00]    [3.84e+00, 3.95e+00, 2.56e+00, 2.09e+00]    
21000     [3.18e+00, 1.14e-01, 3.84e+00]    [0.00e+00, 3.89e+00, 3.84e+00]    [3.78e+00, 3.89e+00, 2.56e+00, 2.09e+00]    
22000     [3.12e+00, 4.08e-02, 3.81e+00]    [0.00e+00, 3.76e+00, 3.81e+00]    [3.65e+00, 3.76e+00, 2.64e+00, 2.15e+00]    
23000     [3.11e+00, 4.83e-02, 3.78e+00]    [0.00e+00, 3.78e+00, 3.78e+00]    [3.67e+00, 3.78e+00, 2.59e+00, 2.09e+00]    
24000     [3.07e+00, 3.67e-02, 3.76e+00]    [0.00e+00, 3.72e+00, 3.76e+00]    [3.61e+00, 3.72e+00, 2.62e+00, 2.10e+00]    
25000     [3.08e+00, 1.14e-01, 3.73e+00]    [0.00e+00, 3.61e+00, 3.73e+00]    [3.50e+00, 3.61e+00, 2.65e+00, 2.13e+00]    
26000     [3.07e+00, 6.61e-02, 3.70e+00]    [0.00e+00, 3.67e+00, 3.70e+00]    [3.55e+00, 3.67e+00, 2.60e+00, 2.06e+00]    
27000     [3.02e+00, 1.47e-02, 3.68e+00]    [0.00e+00, 3.59e+00, 3.68e+00]    [3.47e+00, 3.59e+00, 2.61e+00, 2.06e+00]    
28000     [3.03e+00, 3.47e-02, 3.65e+00]    [0.00e+00, 3.52e+00, 3.65e+00]    [3.40e+00, 3.52e+00, 2.61e+00, 2.05e+00]    
29000     [3.05e+00, 1.22e-01, 3.63e+00]    [0.00e+00, 3.56e+00, 3.63e+00]    [3.43e+00, 3.56e+00, 2.58e+00, 2.01e+00]    
30000     [3.09e+00, 1.96e-01, 3.60e+00]    [0.00e+00, 3.60e+00, 3.60e+00]    [3.47e+00, 3.60e+00, 2.52e+00, 1.96e+00]    

Best model at step 28000:
  train loss: 6.71e+00
  test loss: 7.18e+00
  test metric: [3.40e+00, 3.52e+00, 2.61e+00, 2.05e+00]

'train' took 80.806062 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.497080 s

'compile' took 2.243638 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.04e+02, 1.02e+02, 2.69e+00]    [0.00e+00, 1.09e+02, 2.69e+00]    [1.09e+02, 1.09e+02, 1.16e+01, 1.16e+01]    
1000      [6.59e+01, 4.87e+01, 3.28e+00]    [0.00e+00, 7.62e+01, 3.28e+00]    [7.58e+01, 7.62e+01, 4.01e+01, 4.09e+01]    
2000      [2.77e+01, 1.08e+01, 3.88e+00]    [0.00e+00, 4.41e+01, 3.88e+00]    [4.76e+01, 4.41e+01, 3.50e+01, 3.75e+01]    
3000      [1.25e+01, 2.36e+00, 4.01e+00]    [0.00e+00, 2.47e+01, 4.01e+00]    [2.67e+01, 2.47e+01, 2.02e+01, 2.08e+01]    
4000      [8.37e+00, 2.09e+00, 3.99e+00]    [0.00e+00, 1.73e+01, 3.99e+00]    [1.78e+01, 1.73e+01, 1.36e+01, 1.31e+01]    
5000      [6.31e+00, 1.27e+00, 3.96e+00]    [0.00e+00, 1.29e+01, 3.96e+00]    [1.30e+01, 1.29e+01, 8.40e+00, 8.17e+00]    
6000      [5.15e+00, 9.85e-01, 3.93e+00]    [0.00e+00, 1.03e+01, 3.93e+00]    [1.03e+01, 1.03e+01, 5.69e+00, 5.68e+00]    
7000      [4.18e+00, 9.61e-01, 3.92e+00]    [0.00e+00, 8.12e+00, 3.92e+00]    [7.92e+00, 8.12e+00, 3.50e+00, 3.71e+00]    
8000      [3.66e+00, 7.80e-01, 3.89e+00]    [0.00e+00, 6.73e+00, 3.89e+00]    [6.28e+00, 6.73e+00, 2.47e+00, 2.83e+00]    
9000      [3.49e+00, 7.10e-01, 3.87e+00]    [0.00e+00, 5.98e+00, 3.87e+00]    [5.36e+00, 5.98e+00, 2.22e+00, 2.70e+00]    
10000     [3.31e+00, 5.62e-01, 3.85e+00]    [0.00e+00, 5.29e+00, 3.85e+00]    [4.48e+00, 5.29e+00, 2.53e+00, 3.23e+00]    
11000     [3.23e+00, 4.91e-01, 3.82e+00]    [0.00e+00, 5.38e+00, 3.82e+00]    [4.40e+00, 5.38e+00, 2.17e+00, 3.09e+00]    
12000     [3.16e+00, 3.86e-01, 3.79e+00]    [0.00e+00, 5.54e+00, 3.79e+00]    [4.42e+00, 5.54e+00, 2.05e+00, 3.21e+00]    
13000     [3.17e+00, 3.49e-01, 3.77e+00]    [0.00e+00, 5.65e+00, 3.77e+00]    [4.22e+00, 5.65e+00, 2.21e+00, 3.19e+00]    
14000     [3.14e+00, 2.73e-01, 3.74e+00]    [0.00e+00, 5.38e+00, 3.74e+00]    [4.09e+00, 5.38e+00, 2.06e+00, 3.28e+00]    
15000     [3.12e+00, 2.23e-01, 3.72e+00]    [0.00e+00, 5.21e+00, 3.72e+00]    [3.75e+00, 5.21e+00, 2.14e+00, 3.16e+00]    
16000     [3.08e+00, 1.59e-01, 3.70e+00]    [0.00e+00, 4.86e+00, 3.70e+00]    [3.53e+00, 4.86e+00, 1.97e+00, 3.12e+00]    
17000     [3.09e+00, 9.55e-02, 3.68e+00]    [0.00e+00, 4.96e+00, 3.68e+00]    [3.48e+00, 4.96e+00, 1.93e+00, 3.03e+00]    
18000     [3.07e+00, 4.66e-02, 3.66e+00]    [0.00e+00, 4.74e+00, 3.66e+00]    [3.51e+00, 4.74e+00, 1.64e+00, 3.09e+00]    
19000     [3.01e+00, 5.30e-02, 3.63e+00]    [0.00e+00, 4.70e+00, 3.63e+00]    [3.54e+00, 4.70e+00, 1.65e+00, 3.12e+00]    
20000     [3.04e+00, 4.60e-02, 3.60e+00]    [0.00e+00, 4.61e+00, 3.60e+00]    [3.57e+00, 4.61e+00, 1.43e+00, 2.82e+00]    
21000     [2.96e+00, 2.88e-02, 3.58e+00]    [0.00e+00, 4.45e+00, 3.58e+00]    [3.44e+00, 4.45e+00, 1.37e+00, 2.82e+00]    
22000     [2.95e+00, 1.55e-02, 3.55e+00]    [0.00e+00, 4.33e+00, 3.55e+00]    [3.35e+00, 4.33e+00, 1.32e+00, 2.80e+00]    
23000     [2.92e+00, 4.34e-02, 3.52e+00]    [0.00e+00, 4.31e+00, 3.52e+00]    [3.32e+00, 4.31e+00, 1.35e+00, 2.84e+00]    
24000     [2.93e+00, 5.93e-02, 3.50e+00]    [0.00e+00, 4.20e+00, 3.50e+00]    [3.21e+00, 4.20e+00, 1.32e+00, 2.82e+00]    
25000     [2.91e+00, 4.47e-02, 3.47e+00]    [0.00e+00, 4.14e+00, 3.47e+00]    [3.16e+00, 4.14e+00, 1.28e+00, 2.74e+00]    
26000     [2.94e+00, 6.94e-02, 3.45e+00]    [0.00e+00, 4.21e+00, 3.45e+00]    [3.21e+00, 4.21e+00, 1.30e+00, 2.36e+00]    
27000     [2.87e+00, 2.11e-02, 3.42e+00]    [0.00e+00, 4.10e+00, 3.42e+00]    [3.09e+00, 4.10e+00, 1.27e+00, 2.48e+00]    
28000     [2.89e+00, 4.25e-02, 3.40e+00]    [0.00e+00, 3.99e+00, 3.40e+00]    [2.98e+00, 3.99e+00, 1.28e+00, 2.60e+00]    
29000     [2.84e+00, 1.38e-02, 3.38e+00]    [0.00e+00, 3.99e+00, 3.38e+00]    [2.97e+00, 3.99e+00, 1.29e+00, 2.45e+00]    
30000     [2.84e+00, 3.76e-02, 3.36e+00]    [0.00e+00, 4.00e+00, 3.36e+00]    [2.96e+00, 4.00e+00, 1.36e+00, 2.32e+00]    

Best model at step 29000:
  train loss: 6.23e+00
  test loss: 7.37e+00
  test metric: [2.97e+00, 3.99e+00, 1.29e+00, 2.45e+00]

'train' took 82.393305 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.507640 s

'compile' took 2.084424 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.88e+01, 9.86e+01, 2.90e+00]    [0.00e+00, 9.96e+01, 2.90e+00]    [9.96e+01, 9.96e+01, 5.77e-01, 5.77e-01]    
1000      [7.74e+01, 5.76e+01, 3.01e+00]    [0.00e+00, 9.18e+01, 3.01e+00]    [9.20e+01, 9.18e+01, 1.09e+01, 1.12e+01]    
2000      [4.96e+01, 2.99e+01, 3.69e+00]    [0.00e+00, 5.78e+01, 3.69e+00]    [6.23e+01, 5.78e+01, 2.57e+01, 2.88e+01]    
3000      [2.05e+01, 3.48e+00, 4.22e+00]    [0.00e+00, 2.34e+01, 4.22e+00]    [2.88e+01, 2.34e+01, 1.88e+01, 1.54e+01]    
4000      [9.47e+00, 1.77e+00, 4.31e+00]    [0.00e+00, 1.15e+01, 4.31e+00]    [1.47e+01, 1.15e+01, 6.95e+00, 7.54e+00]    
5000      [5.29e+00, 1.73e+00, 4.34e+00]    [0.00e+00, 6.00e+00, 4.34e+00]    [7.21e+00, 6.00e+00, 4.90e+00, 6.79e+00]    
6000      [4.22e+00, 1.58e+00, 4.33e+00]    [0.00e+00, 4.47e+00, 4.33e+00]    [3.73e+00, 4.47e+00, 4.71e+00, 5.22e+00]    
7000      [3.78e+00, 1.48e+00, 4.31e+00]    [0.00e+00, 3.65e+00, 4.31e+00]    [3.19e+00, 3.65e+00, 3.13e+00, 3.78e+00]    
8000      [3.63e+00, 1.35e+00, 4.27e+00]    [0.00e+00, 3.49e+00, 4.27e+00]    [3.12e+00, 3.49e+00, 2.93e+00, 3.43e+00]    
9000      [3.52e+00, 1.21e+00, 4.24e+00]    [0.00e+00, 3.66e+00, 4.24e+00]    [3.29e+00, 3.66e+00, 2.93e+00, 3.17e+00]    
10000     [3.49e+00, 1.23e+00, 4.21e+00]    [0.00e+00, 3.93e+00, 4.21e+00]    [3.67e+00, 3.93e+00, 2.70e+00, 2.90e+00]    
11000     [3.42e+00, 1.11e+00, 4.18e+00]    [0.00e+00, 4.18e+00, 4.18e+00]    [3.95e+00, 4.18e+00, 2.57e+00, 2.76e+00]    
12000     [3.33e+00, 1.02e+00, 4.15e+00]    [0.00e+00, 4.33e+00, 4.15e+00]    [4.08e+00, 4.33e+00, 2.45e+00, 2.68e+00]    
13000     [3.28e+00, 9.92e-01, 4.12e+00]    [0.00e+00, 4.32e+00, 4.12e+00]    [4.05e+00, 4.32e+00, 2.23e+00, 2.50e+00]    
14000     [3.27e+00, 9.20e-01, 4.09e+00]    [0.00e+00, 4.26e+00, 4.09e+00]    [3.99e+00, 4.26e+00, 2.13e+00, 2.40e+00]    
15000     [3.22e+00, 9.07e-01, 4.06e+00]    [0.00e+00, 4.23e+00, 4.06e+00]    [3.96e+00, 4.23e+00, 2.13e+00, 2.39e+00]    
16000     [3.24e+00, 9.36e-01, 4.03e+00]    [0.00e+00, 4.20e+00, 4.03e+00]    [3.94e+00, 4.20e+00, 2.10e+00, 2.37e+00]    
17000     [3.17e+00, 8.51e-01, 4.01e+00]    [0.00e+00, 4.13e+00, 4.01e+00]    [3.88e+00, 4.13e+00, 2.15e+00, 2.42e+00]    
18000     [3.16e+00, 8.26e-01, 3.98e+00]    [0.00e+00, 4.12e+00, 3.98e+00]    [3.82e+00, 4.12e+00, 2.16e+00, 2.37e+00]    
19000     [3.16e+00, 8.41e-01, 3.96e+00]    [0.00e+00, 4.08e+00, 3.96e+00]    [3.73e+00, 4.08e+00, 2.15e+00, 2.26e+00]    
20000     [3.10e+00, 7.92e-01, 3.94e+00]    [0.00e+00, 4.12e+00, 3.94e+00]    [3.70e+00, 4.12e+00, 2.18e+00, 2.19e+00]    
21000     [3.13e+00, 7.97e-01, 3.92e+00]    [0.00e+00, 4.07e+00, 3.92e+00]    [3.67e+00, 4.07e+00, 2.10e+00, 2.11e+00]    
22000     [3.07e+00, 7.60e-01, 3.90e+00]    [0.00e+00, 4.01e+00, 3.90e+00]    [3.69e+00, 4.01e+00, 2.03e+00, 2.04e+00]    
23000     [3.07e+00, 8.27e-01, 3.88e+00]    [0.00e+00, 3.89e+00, 3.88e+00]    [3.64e+00, 3.89e+00, 1.94e+00, 1.95e+00]    
24000     [3.03e+00, 6.43e-01, 3.86e+00]    [0.00e+00, 3.83e+00, 3.86e+00]    [3.63e+00, 3.83e+00, 1.88e+00, 1.90e+00]    
25000     [3.01e+00, 5.96e-01, 3.84e+00]    [0.00e+00, 3.72e+00, 3.84e+00]    [3.59e+00, 3.72e+00, 1.83e+00, 1.85e+00]    
26000     [3.00e+00, 6.46e-01, 3.82e+00]    [0.00e+00, 3.60e+00, 3.82e+00]    [3.54e+00, 3.60e+00, 1.79e+00, 1.80e+00]    
27000     [3.00e+00, 6.69e-01, 3.80e+00]    [0.00e+00, 3.54e+00, 3.80e+00]    [3.53e+00, 3.54e+00, 1.74e+00, 1.76e+00]    
28000     [3.01e+00, 6.61e-01, 3.79e+00]    [0.00e+00, 3.59e+00, 3.79e+00]    [3.58e+00, 3.59e+00, 1.76e+00, 1.78e+00]    
29000     [2.94e+00, 5.55e-01, 3.77e+00]    [0.00e+00, 3.61e+00, 3.77e+00]    [3.60e+00, 3.61e+00, 1.84e+00, 1.86e+00]    
30000     [2.97e+00, 5.37e-01, 3.75e+00]    [0.00e+00, 3.55e+00, 3.75e+00]    [3.54e+00, 3.55e+00, 1.89e+00, 1.91e+00]    

Best model at step 30000:
  train loss: 7.26e+00
  test loss: 7.30e+00
  test metric: [3.54e+00, 3.55e+00, 1.89e+00, 1.91e+00]

'train' took 81.050719 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.482710 s

'compile' took 2.391602 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.82e+01, 9.73e+01, 2.62e+00]    [0.00e+00, 9.93e+01, 2.62e+00]    [9.93e+01, 9.93e+01, 1.68e+00, 1.68e+00]    
1000      [7.64e+01, 5.80e+01, 2.84e+00]    [0.00e+00, 8.48e+01, 2.84e+00]    [8.48e+01, 8.48e+01, 7.80e+00, 7.83e+00]    
2000      [4.02e+01, 2.96e+01, 3.70e+00]    [0.00e+00, 4.31e+01, 3.70e+00]    [4.46e+01, 4.31e+01, 1.62e+01, 1.67e+01]    
3000      [8.55e+00, 5.03e+00, 4.35e+00]    [0.00e+00, 8.21e+00, 4.35e+00]    [9.65e+00, 8.21e+00, 4.38e+00, 4.52e+00]    
4000      [5.53e+00, 1.16e+00, 4.32e+00]    [0.00e+00, 3.32e+00, 4.32e+00]    [3.52e+00, 3.32e+00, 2.40e+00, 2.35e+00]    
5000      [5.13e+00, 5.66e-01, 4.26e+00]    [0.00e+00, 2.92e+00, 4.26e+00]    [3.05e+00, 2.92e+00, 2.53e+00, 2.51e+00]    
6000      [4.76e+00, 5.25e-01, 4.20e+00]    [0.00e+00, 2.91e+00, 4.20e+00]    [2.80e+00, 2.91e+00, 2.61e+00, 2.42e+00]    
7000      [4.55e+00, 5.62e-01, 4.15e+00]    [0.00e+00, 2.88e+00, 4.15e+00]    [2.81e+00, 2.88e+00, 2.47e+00, 2.37e+00]    
8000      [4.36e+00, 4.87e-01, 4.11e+00]    [0.00e+00, 2.82e+00, 4.11e+00]    [2.78e+00, 2.82e+00, 2.38e+00, 2.34e+00]    
9000      [4.22e+00, 4.51e-01, 4.08e+00]    [0.00e+00, 2.75e+00, 4.08e+00]    [2.73e+00, 2.75e+00, 2.33e+00, 2.30e+00]    
10000     [4.07e+00, 4.46e-01, 4.05e+00]    [0.00e+00, 2.85e+00, 4.05e+00]    [2.87e+00, 2.85e+00, 2.13e+00, 2.15e+00]    
11000     [3.90e+00, 3.84e-01, 4.02e+00]    [0.00e+00, 2.92e+00, 4.02e+00]    [2.99e+00, 2.92e+00, 2.02e+00, 2.10e+00]    
12000     [3.74e+00, 3.06e-01, 3.99e+00]    [0.00e+00, 3.05e+00, 3.99e+00]    [3.20e+00, 3.05e+00, 1.85e+00, 2.02e+00]    
13000     [3.61e+00, 3.07e-01, 3.97e+00]    [0.00e+00, 3.07e+00, 3.97e+00]    [3.33e+00, 3.07e+00, 1.67e+00, 1.97e+00]    
14000     [3.49e+00, 3.16e-01, 3.95e+00]    [0.00e+00, 3.06e+00, 3.95e+00]    [3.44e+00, 3.06e+00, 1.51e+00, 1.95e+00]    
15000     [3.39e+00, 3.46e-01, 3.93e+00]    [0.00e+00, 2.98e+00, 3.93e+00]    [3.45e+00, 2.98e+00, 1.40e+00, 1.96e+00]    
16000     [3.30e+00, 2.24e-01, 3.91e+00]    [0.00e+00, 2.90e+00, 3.91e+00]    [3.48e+00, 2.90e+00, 1.25e+00, 1.94e+00]    
17000     [3.18e+00, 1.77e-01, 3.89e+00]    [0.00e+00, 2.86e+00, 3.89e+00]    [3.54e+00, 2.86e+00, 1.13e+00, 1.95e+00]    
18000     [3.11e+00, 1.60e-01, 3.87e+00]    [0.00e+00, 2.83e+00, 3.87e+00]    [3.59e+00, 2.83e+00, 1.05e+00, 1.96e+00]    
19000     [3.08e+00, 1.27e-01, 3.86e+00]    [0.00e+00, 2.84e+00, 3.86e+00]    [3.65e+00, 2.84e+00, 9.82e-01, 1.95e+00]    
20000     [3.10e+00, 1.61e-01, 3.84e+00]    [0.00e+00, 2.80e+00, 3.84e+00]    [3.66e+00, 2.80e+00, 9.65e-01, 1.99e+00]    
21000     [3.02e+00, 3.50e-02, 3.82e+00]    [0.00e+00, 2.86e+00, 3.82e+00]    [3.75e+00, 2.86e+00, 9.38e-01, 1.98e+00]    
22000     [3.00e+00, 3.37e-02, 3.79e+00]    [0.00e+00, 2.87e+00, 3.79e+00]    [3.74e+00, 2.87e+00, 9.79e-01, 1.98e+00]    
23000     [3.02e+00, 9.26e-02, 3.77e+00]    [0.00e+00, 2.86e+00, 3.77e+00]    [3.71e+00, 2.86e+00, 1.04e+00, 1.99e+00]    
24000     [2.97e+00, 1.72e-02, 3.74e+00]    [0.00e+00, 2.80e+00, 3.74e+00]    [3.63e+00, 2.80e+00, 1.14e+00, 2.06e+00]    
25000     [2.95e+00, 3.88e-02, 3.72e+00]    [0.00e+00, 2.77e+00, 3.72e+00]    [3.58e+00, 2.77e+00, 1.21e+00, 2.08e+00]    
26000     [2.94e+00, 4.01e-02, 3.70e+00]    [0.00e+00, 2.74e+00, 3.70e+00]    [3.53e+00, 2.74e+00, 1.29e+00, 2.13e+00]    
27000     [2.92e+00, 8.06e-02, 3.67e+00]    [0.00e+00, 2.74e+00, 3.67e+00]    [3.52e+00, 2.74e+00, 1.37e+00, 2.16e+00]    
28000     [2.93e+00, 1.16e-01, 3.65e+00]    [0.00e+00, 2.71e+00, 3.65e+00]    [3.49e+00, 2.71e+00, 1.40e+00, 2.20e+00]    
29000     [2.93e+00, 1.02e-01, 3.63e+00]    [0.00e+00, 2.65e+00, 3.63e+00]    [3.43e+00, 2.65e+00, 1.42e+00, 2.22e+00]    
30000     [2.89e+00, 6.15e-02, 3.61e+00]    [0.00e+00, 2.68e+00, 3.61e+00]    [3.46e+00, 2.68e+00, 1.44e+00, 2.21e+00]    

Best model at step 30000:
  train loss: 6.56e+00
  test loss: 6.29e+00
  test metric: [3.46e+00, 2.68e+00, 1.44e+00, 2.21e+00]

'train' took 81.626002 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.485702 s

'compile' took 2.206102 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.77e+01, 9.67e+01, 2.73e+00]    [0.00e+00, 9.93e+01, 2.73e+00]    [9.93e+01, 9.93e+01, 1.70e+00, 1.70e+00]    
1000      [7.54e+01, 5.31e+01, 3.04e+00]    [0.00e+00, 8.60e+01, 3.04e+00]    [8.61e+01, 8.60e+01, 7.23e+00, 7.29e+00]    
2000      [4.03e+01, 2.33e+01, 3.96e+00]    [0.00e+00, 5.37e+01, 3.96e+00]    [5.48e+01, 5.37e+01, 9.12e+00, 9.34e+00]    
3000      [8.39e+00, 3.23e+00, 4.70e+00]    [0.00e+00, 6.62e+00, 4.70e+00]    [8.07e+00, 6.62e+00, 3.74e+00, 3.80e+00]    
4000      [5.43e+00, 1.32e+00, 4.65e+00]    [0.00e+00, 3.25e+00, 4.65e+00]    [3.49e+00, 3.25e+00, 2.25e+00, 2.49e+00]    
5000      [4.79e+00, 8.41e-01, 4.58e+00]    [0.00e+00, 3.16e+00, 4.58e+00]    [3.14e+00, 3.16e+00, 2.82e+00, 2.77e+00]    
6000      [4.58e+00, 3.99e-01, 4.51e+00]    [0.00e+00, 3.59e+00, 4.51e+00]    [3.50e+00, 3.59e+00, 2.43e+00, 2.21e+00]    
7000      [4.39e+00, 1.28e-01, 4.45e+00]    [0.00e+00, 3.80e+00, 4.45e+00]    [3.66e+00, 3.80e+00, 2.41e+00, 2.07e+00]    
8000      [4.21e+00, 4.33e-02, 4.39e+00]    [0.00e+00, 3.86e+00, 4.39e+00]    [3.68e+00, 3.86e+00, 2.60e+00, 2.03e+00]    
9000      [4.05e+00, 6.89e-02, 4.34e+00]    [0.00e+00, 3.81e+00, 4.34e+00]    [3.59e+00, 3.81e+00, 2.74e+00, 2.03e+00]    
10000     [3.94e+00, 2.67e-02, 4.29e+00]    [0.00e+00, 3.73e+00, 4.29e+00]    [3.49e+00, 3.73e+00, 2.78e+00, 2.02e+00]    
11000     [3.81e+00, 3.94e-02, 4.24e+00]    [0.00e+00, 3.68e+00, 4.24e+00]    [3.41e+00, 3.68e+00, 2.80e+00, 1.97e+00]    
12000     [3.74e+00, 6.63e-02, 4.20e+00]    [0.00e+00, 3.64e+00, 4.20e+00]    [3.35e+00, 3.64e+00, 2.84e+00, 1.91e+00]    
13000     [3.60e+00, 6.24e-02, 4.16e+00]    [0.00e+00, 3.55e+00, 4.16e+00]    [3.24e+00, 3.55e+00, 2.91e+00, 1.94e+00]    
14000     [3.57e+00, 1.10e-01, 4.12e+00]    [0.00e+00, 3.47e+00, 4.12e+00]    [3.14e+00, 3.47e+00, 2.98e+00, 1.97e+00]    
15000     [3.49e+00, 1.14e-01, 4.08e+00]    [0.00e+00, 3.46e+00, 4.08e+00]    [3.11e+00, 3.46e+00, 2.96e+00, 1.91e+00]    
16000     [3.43e+00, 2.95e-02, 4.04e+00]    [0.00e+00, 3.43e+00, 4.04e+00]    [3.09e+00, 3.43e+00, 2.92e+00, 1.86e+00]    
17000     [3.40e+00, 7.48e-02, 4.00e+00]    [0.00e+00, 3.42e+00, 4.00e+00]    [3.07e+00, 3.42e+00, 2.87e+00, 1.81e+00]    
18000     [3.39e+00, 4.25e-02, 3.97e+00]    [0.00e+00, 3.38e+00, 3.97e+00]    [3.03e+00, 3.38e+00, 2.83e+00, 1.81e+00]    
19000     [3.34e+00, 9.24e-02, 3.93e+00]    [0.00e+00, 3.37e+00, 3.93e+00]    [3.02e+00, 3.37e+00, 2.80e+00, 1.74e+00]    
20000     [3.30e+00, 6.08e-02, 3.90e+00]    [0.00e+00, 3.33e+00, 3.90e+00]    [2.98e+00, 3.33e+00, 2.79e+00, 1.73e+00]    
21000     [3.29e+00, 5.81e-02, 3.86e+00]    [0.00e+00, 3.31e+00, 3.86e+00]    [2.96e+00, 3.31e+00, 2.79e+00, 1.70e+00]    
22000     [3.24e+00, 2.59e-02, 3.83e+00]    [0.00e+00, 3.26e+00, 3.83e+00]    [2.91e+00, 3.26e+00, 2.78e+00, 1.71e+00]    
23000     [3.22e+00, 2.66e-02, 3.80e+00]    [0.00e+00, 3.23e+00, 3.80e+00]    [2.88e+00, 3.23e+00, 2.76e+00, 1.69e+00]    
24000     [3.21e+00, 3.62e-02, 3.77e+00]    [0.00e+00, 3.19e+00, 3.77e+00]    [2.85e+00, 3.19e+00, 2.75e+00, 1.69e+00]    
25000     [3.19e+00, 3.25e-02, 3.74e+00]    [0.00e+00, 3.17e+00, 3.74e+00]    [2.83e+00, 3.17e+00, 2.71e+00, 1.68e+00]    
26000     [3.18e+00, 3.95e-02, 3.71e+00]    [0.00e+00, 3.15e+00, 3.71e+00]    [2.80e+00, 3.15e+00, 2.70e+00, 1.66e+00]    
27000     [3.15e+00, 4.52e-02, 3.69e+00]    [0.00e+00, 3.13e+00, 3.69e+00]    [2.79e+00, 3.13e+00, 2.68e+00, 1.63e+00]    
28000     [3.15e+00, 1.06e-01, 3.66e+00]    [0.00e+00, 3.11e+00, 3.66e+00]    [2.76e+00, 3.11e+00, 2.65e+00, 1.61e+00]    
29000     [3.13e+00, 5.87e-02, 3.63e+00]    [0.00e+00, 3.09e+00, 3.63e+00]    [2.74e+00, 3.09e+00, 2.61e+00, 1.58e+00]    
30000     [3.14e+00, 1.27e-01, 3.61e+00]    [0.00e+00, 3.09e+00, 3.61e+00]    [2.74e+00, 3.09e+00, 2.58e+00, 1.51e+00]    

Best model at step 29000:
  train loss: 6.83e+00
  test loss: 6.72e+00
  test metric: [2.74e+00, 3.09e+00, 2.61e+00, 1.58e+00]

'train' took 84.594797 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.487580 s

'compile' took 2.315358 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [9.96e+01, 9.94e+01, 2.75e+00]    [0.00e+00, 9.99e+01, 2.75e+00]    [9.99e+01, 9.99e+01, 1.15e-01, 1.15e-01]    
1000      [7.65e+01, 5.52e+01, 2.97e+00]    [0.00e+00, 8.84e+01, 2.97e+00]    [8.85e+01, 8.84e+01, 2.88e+00, 2.90e+00]    
2000      [4.25e+01, 2.47e+01, 3.85e+00]    [0.00e+00, 5.32e+01, 3.85e+00]    [5.51e+01, 5.32e+01, 7.67e+00, 7.98e+00]    
3000      [1.00e+01, 5.14e+00, 4.63e+00]    [0.00e+00, 5.87e+00, 4.63e+00]    [8.22e+00, 5.87e+00, 5.84e+00, 4.71e+00]    
4000      [5.36e+00, 3.82e-01, 4.62e+00]    [0.00e+00, 4.34e+00, 4.62e+00]    [3.46e+00, 4.34e+00, 2.47e+00, 2.38e+00]    
5000      [4.83e+00, 2.90e-01, 4.54e+00]    [0.00e+00, 4.18e+00, 4.54e+00]    [3.36e+00, 4.18e+00, 2.12e+00, 2.17e+00]    
6000      [4.58e+00, 2.21e-01, 4.48e+00]    [0.00e+00, 4.02e+00, 4.48e+00]    [3.35e+00, 4.02e+00, 2.11e+00, 2.23e+00]    
7000      [4.31e+00, 3.94e-02, 4.42e+00]    [0.00e+00, 3.95e+00, 4.42e+00]    [3.38e+00, 3.95e+00, 2.16e+00, 2.41e+00]    
8000      [4.16e+00, 1.42e-01, 4.37e+00]    [0.00e+00, 3.85e+00, 4.37e+00]    [3.36e+00, 3.85e+00, 2.24e+00, 2.55e+00]    
9000      [3.98e+00, 1.46e-01, 4.32e+00]    [0.00e+00, 3.76e+00, 4.32e+00]    [3.36e+00, 3.76e+00, 2.29e+00, 2.67e+00]    
10000     [3.79e+00, 4.01e-02, 4.28e+00]    [0.00e+00, 3.66e+00, 4.28e+00]    [3.34e+00, 3.66e+00, 2.37e+00, 2.78e+00]    
11000     [3.70e+00, 7.46e-02, 4.24e+00]    [0.00e+00, 3.59e+00, 4.24e+00]    [3.31e+00, 3.59e+00, 2.49e+00, 2.87e+00]    
12000     [3.56e+00, 5.62e-02, 4.20e+00]    [0.00e+00, 3.53e+00, 4.20e+00]    [3.31e+00, 3.53e+00, 2.57e+00, 2.95e+00]    
13000     [3.49e+00, 2.45e-02, 4.16e+00]    [0.00e+00, 3.47e+00, 4.16e+00]    [3.30e+00, 3.47e+00, 2.63e+00, 2.99e+00]    
14000     [3.44e+00, 9.57e-02, 4.13e+00]    [0.00e+00, 3.41e+00, 4.13e+00]    [3.31e+00, 3.41e+00, 2.61e+00, 2.99e+00]    
15000     [3.36e+00, 3.63e-02, 4.09e+00]    [0.00e+00, 3.36e+00, 4.09e+00]    [3.27e+00, 3.36e+00, 2.49e+00, 2.83e+00]    
16000     [3.37e+00, 1.13e-01, 4.05e+00]    [0.00e+00, 3.32e+00, 4.05e+00]    [3.25e+00, 3.32e+00, 2.36e+00, 2.65e+00]    
17000     [3.31e+00, 1.45e-01, 4.02e+00]    [0.00e+00, 3.30e+00, 4.02e+00]    [3.24e+00, 3.30e+00, 2.25e+00, 2.49e+00]    
18000     [3.27e+00, 1.49e-01, 3.99e+00]    [0.00e+00, 3.27e+00, 3.99e+00]    [3.22e+00, 3.27e+00, 2.16e+00, 2.35e+00]    
19000     [3.22e+00, 5.23e-02, 3.96e+00]    [0.00e+00, 3.26e+00, 3.96e+00]    [3.22e+00, 3.26e+00, 2.07e+00, 2.22e+00]    
20000     [3.21e+00, 5.76e-02, 3.92e+00]    [0.00e+00, 3.23e+00, 3.92e+00]    [3.20e+00, 3.23e+00, 2.00e+00, 2.11e+00]    
21000     [3.18e+00, 9.57e-02, 3.90e+00]    [0.00e+00, 3.22e+00, 3.90e+00]    [3.20e+00, 3.22e+00, 1.97e+00, 2.04e+00]    
22000     [3.15e+00, 9.96e-02, 3.87e+00]    [0.00e+00, 3.23e+00, 3.87e+00]    [3.22e+00, 3.23e+00, 1.95e+00, 1.98e+00]    
23000     [3.16e+00, 1.04e-01, 3.84e+00]    [0.00e+00, 3.23e+00, 3.84e+00]    [3.23e+00, 3.23e+00, 1.91e+00, 1.91e+00]    
24000     [3.13e+00, 8.28e-02, 3.81e+00]    [0.00e+00, 3.23e+00, 3.81e+00]    [3.23e+00, 3.23e+00, 1.87e+00, 1.84e+00]    
25000     [3.10e+00, 9.15e-02, 3.79e+00]    [0.00e+00, 3.22e+00, 3.79e+00]    [3.23e+00, 3.22e+00, 1.86e+00, 1.80e+00]    
26000     [3.08e+00, 5.84e-02, 3.76e+00]    [0.00e+00, 3.21e+00, 3.76e+00]    [3.23e+00, 3.21e+00, 1.81e+00, 1.73e+00]    
27000     [3.07e+00, 1.04e-01, 3.73e+00]    [0.00e+00, 3.22e+00, 3.73e+00]    [3.25e+00, 3.22e+00, 1.76e+00, 1.65e+00]    
28000     [3.06e+00, 1.08e-01, 3.71e+00]    [0.00e+00, 3.22e+00, 3.71e+00]    [3.26e+00, 3.22e+00, 1.75e+00, 1.62e+00]    
29000     [3.02e+00, 6.13e-02, 3.69e+00]    [0.00e+00, 3.22e+00, 3.69e+00]    [3.27e+00, 3.22e+00, 1.71e+00, 1.56e+00]    
30000     [3.01e+00, 4.50e-02, 3.66e+00]    [0.00e+00, 3.21e+00, 3.66e+00]    [3.27e+00, 3.21e+00, 1.69e+00, 1.51e+00]    

Best model at step 30000:
  train loss: 6.72e+00
  test loss: 6.88e+00
  test metric: [3.27e+00, 3.21e+00, 1.69e+00, 1.51e+00]

'train' took 83.001502 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[3.2011497039838552, 3.271926233878941, 2.5731319503053807, 5.630427519908543, 3.5249082673821652, 3.9884133989262454, 3.54935226645255, 2.6805589903336027, 3.086383462536787, 3.2148249937406144]
E* 9 3.4721076787448686 0.8183425831716087
=======================================================
=======================================================
