Using TensorFlow 2 backend.

              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.300201 s

'compile' took 1.123000 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.43e+02, 9.91e+01, 2.76e+00]    [0.00e+00, 1.92e+02, 2.76e+00]    [1.92e+02, 1.92e+02, 2.90e+02, 2.90e+02]    
1000      [3.31e+01, 3.86e-04, 2.74e+00]    [0.00e+00, 2.91e+01, 2.74e+00]    [2.92e+01, 2.91e+01, 1.99e+01, 1.88e+01]    
2000      [2.96e+01, 2.53e-01, 2.72e+00]    [0.00e+00, 2.36e+01, 2.72e+00]    [2.31e+01, 2.36e+01, 1.44e+01, 1.37e+01]    
3000      [2.81e+01, 3.32e-01, 2.70e+00]    [0.00e+00, 2.70e+01, 2.70e+00]    [2.26e+01, 2.70e+01, 1.54e+01, 1.68e+01]    
4000      [2.91e+01, 3.36e-01, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.02e+01, 2.01e+01, 1.48e+01, 1.41e+01]    
5000      [2.57e+01, 1.45e-01, 2.68e+00]    [0.00e+00, 2.63e+01, 2.68e+00]    [2.06e+01, 2.63e+01, 1.43e+01, 1.66e+01]    
6000      [2.61e+01, 2.32e-01, 2.67e+00]    [0.00e+00, 2.48e+01, 2.67e+00]    [1.97e+01, 2.48e+01, 1.36e+01, 1.51e+01]    
7000      [2.54e+01, 1.60e-01, 2.66e+00]    [0.00e+00, 2.85e+01, 2.66e+00]    [2.03e+01, 2.85e+01, 1.42e+01, 1.92e+01]    
8000      [2.53e+01, 1.27e-01, 2.66e+00]    [0.00e+00, 3.02e+01, 2.66e+00]    [2.08e+01, 3.02e+01, 1.44e+01, 2.13e+01]    
9000      [2.81e+01, 7.74e-02, 2.66e+00]    [0.00e+00, 3.56e+01, 2.66e+00]    [2.53e+01, 3.56e+01, 1.55e+01, 3.09e+01]    
10000     [2.49e+01, 2.20e-01, 2.65e+00]    [0.00e+00, 3.24e+01, 2.65e+00]    [2.12e+01, 3.24e+01, 1.45e+01, 2.51e+01]    
11000     [2.44e+01, 7.96e-02, 2.65e+00]    [0.00e+00, 3.31e+01, 2.65e+00]    [2.09e+01, 3.31e+01, 1.45e+01, 2.71e+01]    
12000     [2.54e+01, 4.07e-01, 2.65e+00]    [0.00e+00, 2.89e+01, 2.65e+00]    [2.01e+01, 2.89e+01, 1.51e+01, 1.97e+01]    
13000     [2.34e+01, 4.95e-02, 2.65e+00]    [0.00e+00, 3.31e+01, 2.65e+00]    [2.05e+01, 3.31e+01, 1.39e+01, 2.66e+01]    
14000     [2.72e+01, 5.00e-01, 2.65e+00]    [0.00e+00, 4.10e+01, 2.65e+00]    [2.52e+01, 4.10e+01, 1.53e+01, 4.16e+01]    
15000     [2.38e+01, 6.02e-03, 2.64e+00]    [0.00e+00, 3.77e+01, 2.64e+00]    [2.04e+01, 3.77e+01, 1.47e+01, 3.56e+01]    
16000     [2.51e+01, 3.23e-01, 2.64e+00]    [0.00e+00, 3.24e+01, 2.64e+00]    [2.01e+01, 3.24e+01, 1.57e+01, 2.67e+01]    
17000     [2.26e+01, 6.58e-02, 2.64e+00]    [0.00e+00, 3.84e+01, 2.64e+00]    [1.90e+01, 3.84e+01, 1.44e+01, 3.81e+01]    
18000     [2.57e+01, 3.59e-01, 2.64e+00]    [0.00e+00, 3.36e+01, 2.64e+00]    [2.02e+01, 3.36e+01, 1.73e+01, 2.85e+01]    
19000     [2.34e+01, 6.44e-02, 2.64e+00]    [0.00e+00, 3.83e+01, 2.64e+00]    [1.98e+01, 3.83e+01, 1.42e+01, 3.65e+01]    
20000     [2.54e+01, 4.19e-01, 2.64e+00]    [0.00e+00, 4.70e+01, 2.64e+00]    [2.28e+01, 4.70e+01, 1.44e+01, 5.40e+01]    
21000     [2.33e+01, 1.96e-01, 2.64e+00]    [0.00e+00, 4.59e+01, 2.64e+00]    [2.01e+01, 4.59e+01, 1.43e+01, 5.20e+01]    
22000     [2.32e+01, 1.23e-01, 2.64e+00]    [0.00e+00, 4.75e+01, 2.64e+00]    [2.01e+01, 4.75e+01, 1.41e+01, 5.48e+01]    
23000     [2.74e+01, 5.50e-01, 2.64e+00]    [0.00e+00, 5.47e+01, 2.64e+00]    [2.61e+01, 5.47e+01, 1.78e+01, 7.09e+01]    
24000     [2.27e+01, 1.77e-01, 2.64e+00]    [0.00e+00, 4.48e+01, 2.64e+00]    [1.91e+01, 4.48e+01, 1.39e+01, 5.04e+01]    
25000     [2.37e+01, 6.06e-02, 2.64e+00]    [0.00e+00, 4.45e+01, 2.64e+00]    [1.93e+01, 4.45e+01, 1.50e+01, 5.03e+01]    
26000     [2.28e+01, 1.53e-01, 2.64e+00]    [0.00e+00, 4.79e+01, 2.64e+00]    [1.86e+01, 4.79e+01, 1.39e+01, 5.71e+01]    
27000     [2.18e+01, 1.57e-01, 2.64e+00]    [0.00e+00, 5.06e+01, 2.64e+00]    [1.88e+01, 5.06e+01, 1.33e+01, 6.29e+01]    
28000     [2.33e+01, 1.93e-01, 2.64e+00]    [0.00e+00, 5.80e+01, 2.64e+00]    [2.14e+01, 5.80e+01, 1.36e+01, 7.94e+01]    
29000     [2.40e+01, 1.13e-02, 2.64e+00]    [0.00e+00, 6.10e+01, 2.64e+00]    [2.23e+01, 6.10e+01, 1.42e+01, 8.28e+01]    
30000     [2.09e+01, 1.35e-01, 2.64e+00]    [0.00e+00, 5.72e+01, 2.64e+00]    [1.87e+01, 5.72e+01, 1.34e+01, 7.50e+01]    

Best model at step 30000:
  train loss: 2.37e+01
  test loss: 5.98e+01
  test metric: [1.87e+01, 5.72e+01, 1.34e+01, 7.50e+01]

'train' took 51.196978 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.191458 s

'compile' took 0.852630 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.13e+02, 1.35e+02, 2.69e+00]    [0.00e+00, 6.69e+02, 2.69e+00]    [6.69e+02, 6.69e+02, 1.05e+03, 1.05e+03]    
1000      [3.13e+01, 4.18e-03, 2.70e+00]    [0.00e+00, 2.89e+01, 2.70e+00]    [2.86e+01, 2.89e+01, 1.91e+01, 2.05e+01]    
2000      [2.86e+01, 4.97e-01, 2.69e+00]    [0.00e+00, 2.09e+01, 2.69e+00]    [2.04e+01, 2.09e+01, 1.48e+01, 1.59e+01]    
3000      [2.76e+01, 1.17e-01, 2.68e+00]    [0.00e+00, 2.15e+01, 2.68e+00]    [2.24e+01, 2.15e+01, 1.53e+01, 1.58e+01]    
4000      [2.78e+01, 4.39e-01, 2.67e+00]    [0.00e+00, 2.10e+01, 2.67e+00]    [2.24e+01, 2.10e+01, 1.52e+01, 1.44e+01]    
5000      [2.89e+01, 6.02e-02, 2.66e+00]    [0.00e+00, 2.38e+01, 2.66e+00]    [2.53e+01, 2.38e+01, 1.84e+01, 1.62e+01]    
6000      [2.85e+01, 1.55e-01, 2.65e+00]    [0.00e+00, 2.31e+01, 2.65e+00]    [2.47e+01, 2.31e+01, 1.78e+01, 1.57e+01]    
7000      [2.81e+01, 5.96e-02, 2.65e+00]    [0.00e+00, 2.51e+01, 2.65e+00]    [1.93e+01, 2.51e+01, 1.62e+01, 1.98e+01]    
8000      [2.60e+01, 2.10e-01, 2.65e+00]    [0.00e+00, 2.28e+01, 2.65e+00]    [2.05e+01, 2.28e+01, 1.45e+01, 1.27e+01]    
9000      [2.73e+01, 3.22e-01, 2.64e+00]    [0.00e+00, 2.25e+01, 2.64e+00]    [2.32e+01, 2.25e+01, 1.55e+01, 1.39e+01]    
10000     [3.03e+01, 6.71e-02, 2.64e+00]    [0.00e+00, 2.85e+01, 2.64e+00]    [2.05e+01, 2.85e+01, 1.98e+01, 2.77e+01]    
11000     [2.61e+01, 1.41e-01, 2.64e+00]    [0.00e+00, 2.57e+01, 2.64e+00]    [1.78e+01, 2.57e+01, 1.36e+01, 1.87e+01]    
12000     [2.63e+01, 1.15e-01, 2.63e+00]    [0.00e+00, 2.61e+01, 2.63e+00]    [1.76e+01, 2.61e+01, 1.39e+01, 2.03e+01]    
13000     [2.42e+01, 3.66e-01, 2.63e+00]    [0.00e+00, 2.31e+01, 2.63e+00]    [1.88e+01, 2.31e+01, 1.23e+01, 1.13e+01]    
14000     [2.50e+01, 2.69e-01, 2.63e+00]    [0.00e+00, 2.66e+01, 2.63e+00]    [1.71e+01, 2.66e+01, 1.23e+01, 1.84e+01]    
15000     [2.99e+01, 2.73e-01, 2.63e+00]    [0.00e+00, 3.10e+01, 2.63e+00]    [2.04e+01, 3.10e+01, 1.89e+01, 3.27e+01]    
16000     [2.34e+01, 1.48e-01, 2.62e+00]    [0.00e+00, 2.44e+01, 2.62e+00]    [1.79e+01, 2.44e+01, 1.22e+01, 1.41e+01]    
17000     [2.40e+01, 3.68e-01, 2.62e+00]    [0.00e+00, 2.44e+01, 2.62e+00]    [1.89e+01, 2.44e+01, 1.22e+01, 1.35e+01]    
18000     [2.72e+01, 1.58e-02, 2.62e+00]    [0.00e+00, 2.41e+01, 2.62e+00]    [2.40e+01, 2.41e+01, 1.69e+01, 1.03e+01]    
19000     [2.73e+01, 3.40e-01, 2.62e+00]    [0.00e+00, 3.20e+01, 2.62e+00]    [1.82e+01, 3.20e+01, 1.66e+01, 3.22e+01]    
20000     [2.55e+01, 1.64e-01, 2.62e+00]    [0.00e+00, 3.24e+01, 2.62e+00]    [1.79e+01, 3.24e+01, 1.40e+01, 2.93e+01]    
21000     [2.40e+01, 2.16e-01, 2.62e+00]    [0.00e+00, 3.13e+01, 2.62e+00]    [1.70e+01, 3.13e+01, 1.22e+01, 2.56e+01]    
22000     [2.35e+01, 5.18e-01, 2.61e+00]    [0.00e+00, 2.86e+01, 2.61e+00]    [1.57e+01, 2.86e+01, 1.21e+01, 2.23e+01]    
23000     [2.33e+01, 3.27e-01, 2.61e+00]    [0.00e+00, 2.94e+01, 2.61e+00]    [1.82e+01, 2.94e+01, 1.17e+01, 1.92e+01]    
24000     [2.23e+01, 1.02e-01, 2.61e+00]    [0.00e+00, 2.94e+01, 2.61e+00]    [1.63e+01, 2.94e+01, 1.23e+01, 2.12e+01]    
25000     [2.29e+01, 1.33e-02, 2.61e+00]    [0.00e+00, 2.99e+01, 2.61e+00]    [1.79e+01, 2.99e+01, 1.20e+01, 2.04e+01]    
26000     [2.41e+01, 5.10e-02, 2.61e+00]    [0.00e+00, 2.86e+01, 2.61e+00]    [1.96e+01, 2.86e+01, 1.28e+01, 1.81e+01]    
27000     [2.52e+01, 3.29e-02, 2.61e+00]    [0.00e+00, 3.71e+01, 2.61e+00]    [1.79e+01, 3.71e+01, 1.48e+01, 3.95e+01]    
28000     [2.72e+01, 1.41e-02, 2.61e+00]    [0.00e+00, 4.02e+01, 2.61e+00]    [2.04e+01, 4.02e+01, 1.79e+01, 4.66e+01]    
29000     [2.45e+01, 2.19e-01, 2.61e+00]    [0.00e+00, 3.59e+01, 2.61e+00]    [1.57e+01, 3.59e+01, 1.35e+01, 3.76e+01]    
30000     [2.25e+01, 7.51e-02, 2.61e+00]    [0.00e+00, 3.61e+01, 2.61e+00]    [1.59e+01, 3.61e+01, 1.23e+01, 3.53e+01]    

Best model at step 24000:
  train loss: 2.50e+01
  test loss: 3.20e+01
  test metric: [1.63e+01, 2.94e+01, 1.23e+01, 2.12e+01]

'train' took 46.661227 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.184641 s

'compile' took 0.832931 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.88e+02, 1.26e+01, 2.84e+00]    [0.00e+00, 4.95e+02, 2.84e+00]    [4.95e+02, 4.95e+02, 4.66e+02, 4.66e+02]    
1000      [3.31e+01, 2.12e-01, 2.83e+00]    [0.00e+00, 2.64e+01, 2.83e+00]    [2.61e+01, 2.64e+01, 1.55e+01, 1.47e+01]    
2000      [3.13e+01, 2.84e-01, 2.82e+00]    [0.00e+00, 2.38e+01, 2.82e+00]    [2.43e+01, 2.38e+01, 1.53e+01, 1.61e+01]    
3000      [2.74e+01, 3.85e-01, 2.81e+00]    [0.00e+00, 1.69e+01, 2.81e+00]    [1.78e+01, 1.69e+01, 1.35e+01, 1.36e+01]    
4000      [2.73e+01, 7.30e-02, 2.80e+00]    [0.00e+00, 1.91e+01, 2.80e+00]    [1.95e+01, 1.91e+01, 1.33e+01, 1.27e+01]    
5000      [2.57e+01, 2.89e-02, 2.79e+00]    [0.00e+00, 1.70e+01, 2.79e+00]    [1.58e+01, 1.70e+01, 1.39e+01, 1.19e+01]    
6000      [2.57e+01, 6.73e-01, 2.79e+00]    [0.00e+00, 1.98e+01, 2.79e+00]    [1.78e+01, 1.98e+01, 1.27e+01, 1.04e+01]    
7000      [2.83e+01, 2.47e-01, 2.78e+00]    [0.00e+00, 2.31e+01, 2.78e+00]    [2.35e+01, 2.31e+01, 1.71e+01, 1.42e+01]    
8000      [2.74e+01, 4.40e-02, 2.78e+00]    [0.00e+00, 2.07e+01, 2.78e+00]    [1.80e+01, 2.07e+01, 1.54e+01, 1.51e+01]    
9000      [2.59e+01, 1.50e-01, 2.78e+00]    [0.00e+00, 1.98e+01, 2.78e+00]    [1.70e+01, 1.98e+01, 1.41e+01, 1.34e+01]    
10000     [2.40e+01, 3.09e-02, 2.78e+00]    [0.00e+00, 1.89e+01, 2.78e+00]    [1.61e+01, 1.89e+01, 1.36e+01, 1.02e+01]    
11000     [2.89e+01, 3.68e-01, 2.77e+00]    [0.00e+00, 2.61e+01, 2.77e+00]    [2.17e+01, 2.61e+01, 1.80e+01, 2.04e+01]    
12000     [2.59e+01, 1.16e-01, 2.77e+00]    [0.00e+00, 2.04e+01, 2.77e+00]    [2.05e+01, 2.04e+01, 1.32e+01, 9.28e+00]    
13000     [2.43e+01, 2.18e-01, 2.77e+00]    [0.00e+00, 2.08e+01, 2.77e+00]    [1.66e+01, 2.08e+01, 1.31e+01, 1.18e+01]    
14000     [2.39e+01, 4.05e-01, 2.77e+00]    [0.00e+00, 1.78e+01, 2.77e+00]    [1.72e+01, 1.78e+01, 1.23e+01, 9.95e+00]    
15000     [2.97e+01, 1.67e-01, 2.77e+00]    [0.00e+00, 2.61e+01, 2.77e+00]    [2.59e+01, 2.61e+01, 1.93e+01, 1.10e+01]    
16000     [2.75e+01, 4.19e-02, 2.77e+00]    [0.00e+00, 2.69e+01, 2.77e+00]    [2.00e+01, 2.69e+01, 1.76e+01, 2.27e+01]    
17000     [2.40e+01, 1.31e-02, 2.77e+00]    [0.00e+00, 2.06e+01, 2.77e+00]    [1.81e+01, 2.06e+01, 1.27e+01, 8.95e+00]    
18000     [2.41e+01, 2.32e-01, 2.77e+00]    [0.00e+00, 1.98e+01, 2.77e+00]    [1.86e+01, 1.98e+01, 1.30e+01, 9.38e+00]    
19000     [2.67e+01, 6.39e-01, 2.77e+00]    [0.00e+00, 2.60e+01, 2.77e+00]    [1.76e+01, 2.60e+01, 1.47e+01, 2.02e+01]    
20000     [2.55e+01, 1.77e-01, 2.77e+00]    [0.00e+00, 2.05e+01, 2.77e+00]    [2.04e+01, 2.05e+01, 1.43e+01, 9.51e+00]    
21000     [2.46e+01, 2.88e-01, 2.77e+00]    [0.00e+00, 2.23e+01, 2.77e+00]    [1.99e+01, 2.23e+01, 1.25e+01, 1.03e+01]    
22000     [2.23e+01, 1.91e-01, 2.77e+00]    [0.00e+00, 2.25e+01, 2.77e+00]    [1.59e+01, 2.25e+01, 1.29e+01, 1.40e+01]    
23000     [2.32e+01, 6.90e-02, 2.77e+00]    [0.00e+00, 2.32e+01, 2.77e+00]    [1.75e+01, 2.32e+01, 1.24e+01, 1.32e+01]    
24000     [2.63e+01, 3.15e-02, 2.77e+00]    [0.00e+00, 2.14e+01, 2.77e+00]    [2.21e+01, 2.14e+01, 1.71e+01, 1.06e+01]    
25000     [2.30e+01, 2.16e-01, 2.77e+00]    [0.00e+00, 2.57e+01, 2.77e+00]    [1.45e+01, 2.57e+01, 1.35e+01, 1.95e+01]    
26000     [2.18e+01, 3.19e-01, 2.77e+00]    [0.00e+00, 2.65e+01, 2.77e+00]    [1.58e+01, 2.65e+01, 1.27e+01, 1.90e+01]    
27000     [2.18e+01, 1.77e-01, 2.77e+00]    [0.00e+00, 2.69e+01, 2.77e+00]    [1.61e+01, 2.69e+01, 1.25e+01, 1.94e+01]    
28000     [2.96e+01, 4.46e-01, 2.77e+00]    [0.00e+00, 3.88e+01, 2.77e+00]    [2.32e+01, 3.88e+01, 2.16e+01, 4.24e+01]    
29000     [2.16e+01, 1.68e-01, 2.77e+00]    [0.00e+00, 2.88e+01, 2.77e+00]    [1.56e+01, 2.88e+01, 1.28e+01, 2.29e+01]    
30000     [2.25e+01, 2.70e-01, 2.77e+00]    [0.00e+00, 2.76e+01, 2.77e+00]    [1.75e+01, 2.76e+01, 1.20e+01, 2.03e+01]    

Best model at step 29000:
  train loss: 2.45e+01
  test loss: 3.15e+01
  test metric: [1.56e+01, 2.88e+01, 1.28e+01, 2.29e+01]

'train' took 45.265277 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.194508 s

'compile' took 0.859217 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [8.64e+02, 6.35e+03, 2.68e+00]    [0.00e+00, 5.95e+02, 2.68e+00]    [5.95e+02, 5.95e+02, 1.00e+03, 1.00e+03]    
1000      [6.88e+01, 1.09e+01, 2.67e+00]    [0.00e+00, 6.76e+01, 2.67e+00]    [6.76e+01, 6.76e+01, 2.95e+01, 2.95e+01]    
2000      [5.73e+01, 2.58e+00, 2.66e+00]    [0.00e+00, 5.99e+01, 2.66e+00]    [5.99e+01, 5.99e+01, 2.88e+01, 2.88e+01]    
3000      [5.28e+01, 8.25e+01, 2.66e+00]    [0.00e+00, 4.88e+01, 2.66e+00]    [4.87e+01, 4.88e+01, 2.76e+01, 2.75e+01]    
4000      [4.30e+01, 3.09e+01, 2.67e+00]    [0.00e+00, 5.00e+01, 2.67e+00]    [4.98e+01, 5.00e+01, 1.98e+01, 1.98e+01]    
5000      [3.75e+01, 3.29e+01, 2.68e+00]    [0.00e+00, 4.08e+01, 2.68e+00]    [4.04e+01, 4.08e+01, 1.62e+01, 1.62e+01]    
6000      [3.54e+01, 6.18e+01, 2.68e+00]    [0.00e+00, 3.42e+01, 2.68e+00]    [3.34e+01, 3.42e+01, 1.52e+01, 1.50e+01]    
7000      [3.44e+01, 6.61e+01, 2.69e+00]    [0.00e+00, 3.83e+01, 2.69e+00]    [3.70e+01, 3.83e+01, 1.95e+01, 2.03e+01]    
8000      [3.21e+01, 6.73e+01, 2.70e+00]    [0.00e+00, 2.98e+01, 2.70e+00]    [2.75e+01, 2.98e+01, 1.52e+01, 1.46e+01]    
9000      [2.98e+01, 4.95e+01, 2.71e+00]    [0.00e+00, 3.08e+01, 2.71e+00]    [2.70e+01, 3.08e+01, 1.43e+01, 1.44e+01]    
10000     [3.15e+01, 5.21e+01, 2.72e+00]    [0.00e+00, 3.72e+01, 2.72e+00]    [3.11e+01, 3.72e+01, 1.76e+01, 2.13e+01]    
11000     [2.82e+01, 4.01e+01, 2.72e+00]    [0.00e+00, 3.40e+01, 2.72e+00]    [2.45e+01, 3.40e+01, 1.47e+01, 1.77e+01]    
12000     [2.99e+01, 3.88e+01, 2.73e+00]    [0.00e+00, 4.38e+01, 2.73e+00]    [2.92e+01, 4.38e+01, 1.73e+01, 2.66e+01]    
13000     [2.74e+01, 1.19e+01, 2.74e+00]    [0.00e+00, 4.84e+01, 2.74e+00]    [2.70e+01, 4.84e+01, 1.56e+01, 2.84e+01]    
14000     [2.66e+01, 1.25e+01, 2.75e+00]    [0.00e+00, 5.13e+01, 2.75e+00]    [2.48e+01, 5.13e+01, 1.42e+01, 3.00e+01]    
15000     [2.70e+01, 7.12e+00, 2.76e+00]    [0.00e+00, 5.97e+01, 2.76e+00]    [2.65e+01, 5.97e+01, 1.58e+01, 3.79e+01]    
16000     [2.58e+01, 1.64e+00, 2.76e+00]    [0.00e+00, 6.42e+01, 2.76e+00]    [2.44e+01, 6.42e+01, 1.48e+01, 4.10e+01]    
17000     [2.54e+01, 9.90e-02, 2.77e+00]    [0.00e+00, 7.01e+01, 2.77e+00]    [2.52e+01, 7.01e+01, 1.53e+01, 4.67e+01]    
18000     [2.54e+01, 1.41e-01, 2.77e+00]    [0.00e+00, 7.50e+01, 2.77e+00]    [2.56e+01, 7.50e+01, 1.55e+01, 5.11e+01]    
19000     [2.48e+01, 5.86e-01, 2.77e+00]    [0.00e+00, 7.81e+01, 2.77e+00]    [2.47e+01, 7.81e+01, 1.51e+01, 5.30e+01]    
20000     [2.44e+01, 2.43e-01, 2.77e+00]    [0.00e+00, 8.01e+01, 2.77e+00]    [2.51e+01, 8.01e+01, 1.57e+01, 5.49e+01]    
21000     [2.90e+01, 3.83e+00, 2.77e+00]    [0.00e+00, 7.76e+01, 2.77e+00]    [2.08e+01, 7.76e+01, 1.55e+01, 5.08e+01]    
22000     [2.45e+01, 2.72e-01, 2.77e+00]    [0.00e+00, 7.97e+01, 2.77e+00]    [2.36e+01, 7.97e+01, 1.54e+01, 5.43e+01]    
23000     [2.54e+01, 6.22e-01, 2.77e+00]    [0.00e+00, 8.22e+01, 2.77e+00]    [2.64e+01, 8.22e+01, 1.81e+01, 5.85e+01]    
24000     [2.38e+01, 2.38e-01, 2.77e+00]    [0.00e+00, 8.20e+01, 2.77e+00]    [2.54e+01, 8.20e+01, 1.68e+01, 5.78e+01]    
25000     [2.35e+01, 2.80e-01, 2.77e+00]    [0.00e+00, 8.29e+01, 2.77e+00]    [2.54e+01, 8.29e+01, 1.64e+01, 5.85e+01]    
26000     [2.35e+01, 3.83e-01, 2.76e+00]    [0.00e+00, 8.15e+01, 2.76e+00]    [2.42e+01, 8.15e+01, 1.49e+01, 5.71e+01]    
27000     [2.51e+01, 5.80e-01, 2.76e+00]    [0.00e+00, 8.28e+01, 2.76e+00]    [2.71e+01, 8.28e+01, 1.71e+01, 5.95e+01]    
28000     [2.26e+01, 6.09e-01, 2.76e+00]    [0.00e+00, 8.21e+01, 2.76e+00]    [2.37e+01, 8.21e+01, 1.52e+01, 5.70e+01]    
29000     [2.28e+01, 2.06e-01, 2.76e+00]    [0.00e+00, 8.28e+01, 2.76e+00]    [2.56e+01, 8.28e+01, 1.60e+01, 5.88e+01]    
30000     [2.26e+01, 1.80e-01, 2.76e+00]    [0.00e+00, 8.17e+01, 2.76e+00]    [2.34e+01, 8.17e+01, 1.48e+01, 5.65e+01]    

Best model at step 30000:
  train loss: 2.55e+01
  test loss: 8.44e+01
  test metric: [2.34e+01, 8.17e+01, 1.48e+01, 5.65e+01]

'train' took 46.173400 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.196504 s

'compile' took 0.850206 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.39e+02, 7.41e+01, 2.74e+00]    [0.00e+00, 7.00e+02, 2.74e+00]    [7.00e+02, 7.00e+02, 1.19e+03, 1.19e+03]    
1000      [4.26e+01, 2.13e+00, 2.70e+00]    [0.00e+00, 4.28e+01, 2.70e+00]    [4.28e+01, 4.28e+01, 2.36e+01, 2.36e+01]    
2000      [3.11e+01, 5.43e+00, 2.68e+00]    [0.00e+00, 2.72e+01, 2.68e+00]    [2.72e+01, 2.72e+01, 1.59e+01, 1.59e+01]    
3000      [2.88e+01, 5.88e+00, 2.66e+00]    [0.00e+00, 2.28e+01, 2.66e+00]    [2.28e+01, 2.28e+01, 1.41e+01, 1.41e+01]    
4000      [2.75e+01, 5.84e-01, 2.65e+00]    [0.00e+00, 1.94e+01, 2.65e+00]    [1.94e+01, 1.94e+01, 1.46e+01, 1.46e+01]    
5000      [2.71e+01, 1.87e+00, 2.65e+00]    [0.00e+00, 1.88e+01, 2.65e+00]    [1.88e+01, 1.88e+01, 1.40e+01, 1.40e+01]    
6000      [2.79e+01, 8.74e-01, 2.64e+00]    [0.00e+00, 2.37e+01, 2.64e+00]    [2.37e+01, 2.37e+01, 1.33e+01, 1.33e+01]    
7000      [2.69e+01, 3.98e+00, 2.63e+00]    [0.00e+00, 1.73e+01, 2.63e+00]    [1.73e+01, 1.73e+01, 1.44e+01, 1.44e+01]    
8000      [2.54e+01, 6.45e+00, 2.63e+00]    [0.00e+00, 1.92e+01, 2.63e+00]    [1.92e+01, 1.92e+01, 1.30e+01, 1.30e+01]    
9000      [2.54e+01, 2.91e+00, 2.62e+00]    [0.00e+00, 1.69e+01, 2.62e+00]    [1.69e+01, 1.69e+01, 1.42e+01, 1.42e+01]    
10000     [2.51e+01, 4.13e+00, 2.62e+00]    [0.00e+00, 2.02e+01, 2.62e+00]    [2.02e+01, 2.02e+01, 1.33e+01, 1.33e+01]    
11000     [2.51e+01, 6.54e+00, 2.61e+00]    [0.00e+00, 1.86e+01, 2.61e+00]    [1.86e+01, 1.86e+01, 1.32e+01, 1.32e+01]    
12000     [2.66e+01, 2.55e+00, 2.61e+00]    [0.00e+00, 2.24e+01, 2.61e+00]    [2.24e+01, 2.24e+01, 1.44e+01, 1.44e+01]    
13000     [2.71e+01, 3.61e+00, 2.61e+00]    [0.00e+00, 1.80e+01, 2.61e+00]    [1.80e+01, 1.80e+01, 1.44e+01, 1.44e+01]    
14000     [2.52e+01, 9.47e+00, 2.60e+00]    [0.00e+00, 1.67e+01, 2.60e+00]    [1.67e+01, 1.67e+01, 1.43e+01, 1.43e+01]    
15000     [2.74e+01, 8.32e+00, 2.60e+00]    [0.00e+00, 2.37e+01, 2.60e+00]    [2.37e+01, 2.37e+01, 1.51e+01, 1.51e+01]    
16000     [2.56e+01, 1.02e+01, 2.59e+00]    [0.00e+00, 2.10e+01, 2.59e+00]    [2.09e+01, 2.10e+01, 1.44e+01, 1.45e+01]    
17000     [2.45e+01, 8.61e+00, 2.59e+00]    [0.00e+00, 1.73e+01, 2.59e+00]    [1.73e+01, 1.73e+01, 1.42e+01, 1.42e+01]    
18000     [2.51e+01, 1.07e+01, 2.59e+00]    [0.00e+00, 1.84e+01, 2.59e+00]    [1.83e+01, 1.84e+01, 1.41e+01, 1.41e+01]    
19000     [2.78e+01, 7.86e+00, 2.58e+00]    [0.00e+00, 2.53e+01, 2.58e+00]    [2.52e+01, 2.53e+01, 1.72e+01, 1.73e+01]    
20000     [2.44e+01, 6.98e+00, 2.58e+00]    [0.00e+00, 1.71e+01, 2.58e+00]    [1.70e+01, 1.71e+01, 1.47e+01, 1.47e+01]    
21000     [2.43e+01, 8.03e+00, 2.58e+00]    [0.00e+00, 1.90e+01, 2.58e+00]    [1.90e+01, 1.90e+01, 1.39e+01, 1.39e+01]    
22000     [2.33e+01, 4.55e+00, 2.57e+00]    [0.00e+00, 1.96e+01, 2.57e+00]    [1.95e+01, 1.96e+01, 1.31e+01, 1.31e+01]    
23000     [2.49e+01, 4.54e+00, 2.57e+00]    [0.00e+00, 1.69e+01, 2.57e+00]    [1.70e+01, 1.69e+01, 1.50e+01, 1.51e+01]    
24000     [2.44e+01, 2.26e+00, 2.57e+00]    [0.00e+00, 1.69e+01, 2.57e+00]    [1.70e+01, 1.69e+01, 1.52e+01, 1.52e+01]    
25000     [2.65e+01, 3.67e+00, 2.57e+00]    [0.00e+00, 2.10e+01, 2.57e+00]    [2.11e+01, 2.10e+01, 1.52e+01, 1.51e+01]    
26000     [2.70e+01, 3.62e+00, 2.56e+00]    [0.00e+00, 2.00e+01, 2.56e+00]    [2.01e+01, 2.00e+01, 1.47e+01, 1.46e+01]    
27000     [2.30e+01, 1.52e+00, 2.56e+00]    [0.00e+00, 1.81e+01, 2.56e+00]    [1.83e+01, 1.81e+01, 1.44e+01, 1.45e+01]    
28000     [2.40e+01, 3.82e+00, 2.56e+00]    [0.00e+00, 2.11e+01, 2.56e+00]    [2.10e+01, 2.11e+01, 1.29e+01, 1.29e+01]    
29000     [2.34e+01, 6.64e+00, 2.56e+00]    [0.00e+00, 1.89e+01, 2.56e+00]    [1.88e+01, 1.89e+01, 1.45e+01, 1.45e+01]    
30000     [2.38e+01, 6.59e+00, 2.56e+00]    [0.00e+00, 1.72e+01, 2.56e+00]    [1.70e+01, 1.72e+01, 1.55e+01, 1.54e+01]    

Best model at step 27000:
  train loss: 2.71e+01
  test loss: 2.07e+01
  test metric: [1.83e+01, 1.81e+01, 1.44e+01, 1.45e+01]

'train' took 45.648098 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.247423 s

'compile' took 0.881727 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.49e+02, 9.87e+01, 2.71e+00]    [0.00e+00, 8.62e+02, 2.71e+00]    [8.62e+02, 8.62e+02, 1.44e+03, 1.44e+03]    
1000      [2.99e+01, 1.49e-01, 2.71e+00]    [0.00e+00, 2.93e+01, 2.71e+00]    [2.91e+01, 2.93e+01, 1.62e+01, 1.79e+01]    
2000      [2.68e+01, 2.80e-01, 2.70e+00]    [0.00e+00, 2.72e+01, 2.70e+00]    [2.67e+01, 2.72e+01, 1.56e+01, 2.24e+01]    
3000      [2.64e+01, 3.15e-01, 2.69e+00]    [0.00e+00, 2.82e+01, 2.69e+00]    [2.75e+01, 2.82e+01, 2.02e+01, 1.89e+01]    
4000      [2.64e+01, 8.86e-02, 2.68e+00]    [0.00e+00, 2.62e+01, 2.68e+00]    [2.74e+01, 2.62e+01, 2.02e+01, 1.76e+01]    
5000      [2.54e+01, 2.38e-01, 2.67e+00]    [0.00e+00, 3.05e+01, 2.67e+00]    [2.74e+01, 3.05e+01, 1.65e+01, 2.71e+01]    
6000      [2.46e+01, 1.04e-01, 2.67e+00]    [0.00e+00, 2.69e+01, 2.67e+00]    [2.53e+01, 2.69e+01, 1.94e+01, 2.05e+01]    
7000      [2.40e+01, 1.70e-01, 2.66e+00]    [0.00e+00, 2.75e+01, 2.66e+00]    [2.54e+01, 2.75e+01, 1.84e+01, 2.30e+01]    
8000      [2.38e+01, 2.34e-01, 2.65e+00]    [0.00e+00, 2.86e+01, 2.65e+00]    [2.47e+01, 2.86e+01, 1.77e+01, 2.60e+01]    
9000      [2.57e+01, 4.42e-01, 2.65e+00]    [0.00e+00, 2.49e+01, 2.65e+00]    [2.60e+01, 2.49e+01, 2.00e+01, 2.04e+01]    
10000     [2.37e+01, 1.96e-01, 2.64e+00]    [0.00e+00, 2.91e+01, 2.64e+00]    [2.48e+01, 2.91e+01, 1.72e+01, 3.06e+01]    
11000     [2.36e+01, 1.77e-01, 2.64e+00]    [0.00e+00, 2.62e+01, 2.64e+00]    [2.35e+01, 2.62e+01, 1.98e+01, 2.31e+01]    
12000     [2.27e+01, 1.01e-01, 2.64e+00]    [0.00e+00, 2.82e+01, 2.64e+00]    [2.38e+01, 2.82e+01, 1.76e+01, 2.98e+01]    
13000     [2.56e+01, 3.72e-01, 2.63e+00]    [0.00e+00, 3.29e+01, 2.63e+00]    [2.42e+01, 3.29e+01, 1.99e+01, 3.82e+01]    
14000     [2.24e+01, 1.30e-01, 2.63e+00]    [0.00e+00, 2.92e+01, 2.63e+00]    [2.37e+01, 2.92e+01, 1.72e+01, 3.14e+01]    
15000     [2.22e+01, 5.29e-02, 2.63e+00]    [0.00e+00, 2.96e+01, 2.63e+00]    [2.40e+01, 2.96e+01, 1.69e+01, 3.27e+01]    
16000     [2.30e+01, 2.08e-01, 2.62e+00]    [0.00e+00, 2.77e+01, 2.62e+00]    [2.33e+01, 2.77e+01, 1.89e+01, 2.86e+01]    
17000     [2.18e+01, 1.19e-01, 2.62e+00]    [0.00e+00, 2.95e+01, 2.62e+00]    [2.28e+01, 2.95e+01, 1.74e+01, 3.22e+01]    
18000     [2.60e+01, 5.14e-01, 2.62e+00]    [0.00e+00, 3.94e+01, 2.62e+00]    [2.52e+01, 3.94e+01, 2.30e+01, 4.85e+01]    
19000     [2.20e+01, 6.33e-02, 2.62e+00]    [0.00e+00, 3.29e+01, 2.62e+00]    [2.37e+01, 3.29e+01, 1.65e+01, 3.75e+01]    
20000     [2.18e+01, 1.80e-01, 2.61e+00]    [0.00e+00, 3.02e+01, 2.61e+00]    [2.33e+01, 3.02e+01, 1.72e+01, 3.45e+01]    
21000     [2.33e+01, 1.64e-01, 2.61e+00]    [0.00e+00, 3.65e+01, 2.61e+00]    [2.39e+01, 3.65e+01, 1.75e+01, 4.28e+01]    
22000     [2.16e+01, 2.29e-01, 2.61e+00]    [0.00e+00, 3.42e+01, 2.61e+00]    [2.33e+01, 3.42e+01, 1.60e+01, 3.80e+01]    
23000     [2.38e+01, 2.07e-01, 2.61e+00]    [0.00e+00, 2.75e+01, 2.61e+00]    [2.51e+01, 2.75e+01, 1.81e+01, 2.85e+01]    
24000     [2.21e+01, 1.39e-01, 2.61e+00]    [0.00e+00, 3.08e+01, 2.61e+00]    [2.24e+01, 3.08e+01, 1.82e+01, 3.40e+01]    
25000     [2.34e+01, 2.21e-01, 2.60e+00]    [0.00e+00, 3.93e+01, 2.60e+00]    [2.36e+01, 3.93e+01, 1.84e+01, 4.78e+01]    
26000     [2.22e+01, 3.28e-01, 2.60e+00]    [0.00e+00, 3.14e+01, 2.60e+00]    [2.27e+01, 3.14e+01, 1.79e+01, 3.48e+01]    
27000     [2.17e+01, 1.83e-01, 2.60e+00]    [0.00e+00, 3.29e+01, 2.60e+00]    [2.21e+01, 3.29e+01, 1.79e+01, 3.59e+01]    
28000     [2.47e+01, 5.94e-01, 2.60e+00]    [0.00e+00, 2.85e+01, 2.60e+00]    [2.68e+01, 2.85e+01, 1.86e+01, 2.87e+01]    
29000     [2.07e+01, 1.60e-02, 2.59e+00]    [0.00e+00, 3.70e+01, 2.59e+00]    [2.23e+01, 3.70e+01, 1.62e+01, 4.27e+01]    
30000     [2.41e+01, 3.44e-01, 2.59e+00]    [0.00e+00, 3.00e+01, 2.59e+00]    [2.58e+01, 3.00e+01, 1.80e+01, 3.19e+01]    

Best model at step 29000:
  train loss: 2.33e+01
  test loss: 3.96e+01
  test metric: [2.23e+01, 3.70e+01, 1.62e+01, 4.27e+01]

'train' took 46.362643 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.189287 s

'compile' took 0.830985 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.43e+02, 9.50e+02, 2.74e+00]    [0.00e+00, 2.49e+02, 2.74e+00]    [2.49e+02, 2.49e+02, 2.69e+02, 2.69e+02]    
1000      [6.69e+01, 1.15e+01, 2.71e+00]    [0.00e+00, 6.05e+01, 2.71e+00]    [6.09e+01, 6.05e+01, 3.45e+01, 3.49e+01]    
2000      [4.86e+01, 7.29e+00, 2.71e+00]    [0.00e+00, 5.06e+01, 2.71e+00]    [4.99e+01, 5.06e+01, 2.39e+01, 2.38e+01]    
3000      [3.81e+01, 1.18e+01, 2.71e+00]    [0.00e+00, 4.11e+01, 2.71e+00]    [4.01e+01, 4.11e+01, 2.00e+01, 2.07e+01]    
4000      [3.28e+01, 1.00e+01, 2.71e+00]    [0.00e+00, 2.93e+01, 2.71e+00]    [3.08e+01, 2.93e+01, 1.67e+01, 1.82e+01]    
5000      [3.21e+01, 9.80e+00, 2.71e+00]    [0.00e+00, 3.06e+01, 2.71e+00]    [2.89e+01, 3.06e+01, 1.82e+01, 1.98e+01]    
6000      [2.90e+01, 4.58e+00, 2.71e+00]    [0.00e+00, 2.43e+01, 2.71e+00]    [2.39e+01, 2.43e+01, 1.64e+01, 1.57e+01]    
7000      [3.05e+01, 1.60e+01, 2.71e+00]    [0.00e+00, 2.68e+01, 2.71e+00]    [2.44e+01, 2.68e+01, 1.84e+01, 2.08e+01]    
8000      [3.11e+01, 1.69e+01, 2.70e+00]    [0.00e+00, 2.89e+01, 2.70e+00]    [2.61e+01, 2.89e+01, 2.13e+01, 2.57e+01]    
9000      [2.77e+01, 6.61e+00, 2.70e+00]    [0.00e+00, 2.43e+01, 2.70e+00]    [2.31e+01, 2.43e+01, 1.59e+01, 1.97e+01]    
10000     [2.66e+01, 7.30e+00, 2.70e+00]    [0.00e+00, 2.31e+01, 2.70e+00]    [2.18e+01, 2.31e+01, 1.66e+01, 1.55e+01]    
11000     [3.04e+01, 3.26e+01, 2.70e+00]    [0.00e+00, 2.48e+01, 2.70e+00]    [2.86e+01, 2.48e+01, 2.13e+01, 1.70e+01]    
12000     [2.60e+01, 6.44e+00, 2.70e+00]    [0.00e+00, 2.35e+01, 2.70e+00]    [2.19e+01, 2.35e+01, 1.67e+01, 1.61e+01]    
13000     [2.72e+01, 1.63e+01, 2.69e+00]    [0.00e+00, 2.21e+01, 2.69e+00]    [2.48e+01, 2.21e+01, 1.76e+01, 1.64e+01]    
14000     [2.66e+01, 1.30e+01, 2.69e+00]    [0.00e+00, 2.24e+01, 2.69e+00]    [2.40e+01, 2.24e+01, 1.73e+01, 1.58e+01]    
15000     [3.01e+01, 3.25e+01, 2.69e+00]    [0.00e+00, 2.40e+01, 2.69e+00]    [2.86e+01, 2.40e+01, 2.24e+01, 1.76e+01]    
16000     [2.61e+01, 2.20e+00, 2.69e+00]    [0.00e+00, 2.60e+01, 2.69e+00]    [2.40e+01, 2.60e+01, 1.66e+01, 2.42e+01]    
17000     [2.65e+01, 6.65e+00, 2.69e+00]    [0.00e+00, 2.64e+01, 2.69e+00]    [2.42e+01, 2.64e+01, 1.76e+01, 2.68e+01]    
18000     [2.74e+01, 1.22e+01, 2.69e+00]    [0.00e+00, 2.78e+01, 2.69e+00]    [2.45e+01, 2.78e+01, 1.95e+01, 2.95e+01]    
19000     [2.53e+01, 2.91e+00, 2.69e+00]    [0.00e+00, 2.51e+01, 2.69e+00]    [2.28e+01, 2.51e+01, 1.56e+01, 2.19e+01]    
20000     [2.50e+01, 1.28e+01, 2.69e+00]    [0.00e+00, 2.38e+01, 2.69e+00]    [2.21e+01, 2.38e+01, 1.76e+01, 1.69e+01]    
21000     [2.51e+01, 1.36e+01, 2.68e+00]    [0.00e+00, 2.36e+01, 2.68e+00]    [2.26e+01, 2.36e+01, 1.75e+01, 1.67e+01]    
22000     [2.57e+01, 1.83e+01, 2.68e+00]    [0.00e+00, 2.34e+01, 2.68e+00]    [2.37e+01, 2.34e+01, 1.76e+01, 1.61e+01]    
23000     [2.86e+01, 1.72e+01, 2.68e+00]    [0.00e+00, 3.06e+01, 2.68e+00]    [2.54e+01, 3.06e+01, 2.40e+01, 3.55e+01]    
24000     [2.81e+01, 1.46e+01, 2.68e+00]    [0.00e+00, 3.01e+01, 2.68e+00]    [2.51e+01, 3.01e+01, 2.26e+01, 3.41e+01]    
25000     [2.43e+01, 1.27e+01, 2.68e+00]    [0.00e+00, 2.42e+01, 2.68e+00]    [2.19e+01, 2.42e+01, 1.75e+01, 1.77e+01]    
26000     [2.61e+01, 1.13e+01, 2.68e+00]    [0.00e+00, 2.74e+01, 2.68e+00]    [2.40e+01, 2.74e+01, 1.73e+01, 2.86e+01]    
27000     [2.75e+01, 2.57e+01, 2.68e+00]    [0.00e+00, 2.24e+01, 2.68e+00]    [2.66e+01, 2.24e+01, 1.93e+01, 1.62e+01]    
28000     [2.71e+01, 2.50e+01, 2.68e+00]    [0.00e+00, 2.26e+01, 2.68e+00]    [2.63e+01, 2.26e+01, 1.89e+01, 1.60e+01]    
29000     [2.69e+01, 2.53e+01, 2.68e+00]    [0.00e+00, 2.30e+01, 2.68e+00]    [2.59e+01, 2.30e+01, 1.84e+01, 1.58e+01]    
30000     [2.69e+01, 2.45e+01, 2.67e+00]    [0.00e+00, 2.27e+01, 2.67e+00]    [2.63e+01, 2.27e+01, 1.88e+01, 1.59e+01]    

Best model at step 19000:
  train loss: 3.09e+01
  test loss: 2.78e+01
  test metric: [2.28e+01, 2.51e+01, 1.56e+01, 2.19e+01]

'train' took 47.652051 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.560502 s

'compile' took 2.436573 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.32e+02, 9.82e+01, 2.69e+00]    [0.00e+00, 2.51e+02, 2.69e+00]    [2.51e+02, 2.51e+02, 1.98e+02, 1.98e+02]    
1000      [3.09e+01, 2.03e-01, 2.68e+00]    [0.00e+00, 2.39e+01, 2.68e+00]    [2.44e+01, 2.39e+01, 1.42e+01, 1.51e+01]    
2000      [2.70e+01, 9.09e-02, 2.66e+00]    [0.00e+00, 1.88e+01, 2.66e+00]    [1.71e+01, 1.88e+01, 1.38e+01, 1.32e+01]    
3000      [3.04e+01, 1.98e-01, 2.65e+00]    [0.00e+00, 2.02e+01, 2.65e+00]    [1.84e+01, 2.02e+01, 1.72e+01, 1.87e+01]    
4000      [2.62e+01, 1.08e-01, 2.64e+00]    [0.00e+00, 2.03e+01, 2.64e+00]    [1.73e+01, 2.03e+01, 1.33e+01, 1.26e+01]    
5000      [2.97e+01, 6.25e-02, 2.63e+00]    [0.00e+00, 2.25e+01, 2.63e+00]    [1.96e+01, 2.25e+01, 2.10e+01, 2.40e+01]    
6000      [2.66e+01, 4.19e-02, 2.63e+00]    [0.00e+00, 2.03e+01, 2.63e+00]    [1.90e+01, 2.03e+01, 1.43e+01, 1.36e+01]    
7000      [2.77e+01, 3.29e-01, 2.62e+00]    [0.00e+00, 2.17e+01, 2.62e+00]    [1.77e+01, 2.17e+01, 1.69e+01, 2.02e+01]    
8000      [2.62e+01, 2.06e-01, 2.62e+00]    [0.00e+00, 2.32e+01, 2.62e+00]    [1.82e+01, 2.32e+01, 1.41e+01, 1.74e+01]    
9000      [2.68e+01, 3.91e-02, 2.61e+00]    [0.00e+00, 2.29e+01, 2.61e+00]    [1.74e+01, 2.29e+01, 1.57e+01, 2.01e+01]    
10000     [2.52e+01, 9.24e-02, 2.61e+00]    [0.00e+00, 2.16e+01, 2.61e+00]    [1.78e+01, 2.16e+01, 1.39e+01, 1.18e+01]    
11000     [2.45e+01, 3.30e-02, 2.61e+00]    [0.00e+00, 2.25e+01, 2.61e+00]    [1.57e+01, 2.25e+01, 1.36e+01, 1.51e+01]    
12000     [2.38e+01, 5.79e-02, 2.61e+00]    [0.00e+00, 2.35e+01, 2.61e+00]    [1.60e+01, 2.35e+01, 1.38e+01, 1.38e+01]    
13000     [2.39e+01, 1.69e-01, 2.61e+00]    [0.00e+00, 2.46e+01, 2.61e+00]    [1.65e+01, 2.46e+01, 1.35e+01, 1.46e+01]    
14000     [2.69e+01, 5.37e-01, 2.60e+00]    [0.00e+00, 2.44e+01, 2.60e+00]    [1.61e+01, 2.44e+01, 1.48e+01, 2.32e+01]    
15000     [2.61e+01, 1.15e-01, 2.60e+00]    [0.00e+00, 2.69e+01, 2.60e+00]    [1.69e+01, 2.69e+01, 1.49e+01, 2.50e+01]    
16000     [2.46e+01, 1.59e-01, 2.60e+00]    [0.00e+00, 2.80e+01, 2.60e+00]    [1.76e+01, 2.80e+01, 1.33e+01, 2.43e+01]    
17000     [2.87e+01, 2.10e-01, 2.60e+00]    [0.00e+00, 2.52e+01, 2.60e+00]    [2.33e+01, 2.52e+01, 1.85e+01, 1.26e+01]    
18000     [2.55e+01, 1.03e-02, 2.60e+00]    [0.00e+00, 2.97e+01, 2.60e+00]    [1.69e+01, 2.97e+01, 1.49e+01, 2.87e+01]    
19000     [2.54e+01, 1.97e-01, 2.60e+00]    [0.00e+00, 3.16e+01, 2.60e+00]    [1.73e+01, 3.16e+01, 1.54e+01, 3.11e+01]    
20000     [2.40e+01, 6.43e-02, 2.60e+00]    [0.00e+00, 3.03e+01, 2.60e+00]    [1.62e+01, 3.03e+01, 1.35e+01, 2.85e+01]    
21000     [2.59e+01, 1.28e-01, 2.59e+00]    [0.00e+00, 3.30e+01, 2.59e+00]    [1.70e+01, 3.30e+01, 1.58e+01, 3.49e+01]    
22000     [2.51e+01, 1.70e-01, 2.59e+00]    [0.00e+00, 3.36e+01, 2.59e+00]    [1.65e+01, 3.36e+01, 1.49e+01, 3.52e+01]    
23000     [2.59e+01, 2.24e-02, 2.59e+00]    [0.00e+00, 3.57e+01, 2.59e+00]    [1.74e+01, 3.57e+01, 1.57e+01, 3.93e+01]    
24000     [2.70e+01, 3.53e-01, 2.59e+00]    [0.00e+00, 3.69e+01, 2.59e+00]    [1.81e+01, 3.69e+01, 1.71e+01, 4.37e+01]    
25000     [2.44e+01, 6.25e-02, 2.59e+00]    [0.00e+00, 3.66e+01, 2.59e+00]    [1.64e+01, 3.66e+01, 1.42e+01, 4.00e+01]    
26000     [2.96e+01, 1.81e-01, 2.59e+00]    [0.00e+00, 3.09e+01, 2.59e+00]    [2.62e+01, 3.09e+01, 2.18e+01, 1.82e+01]    
27000     [2.44e+01, 1.59e-01, 2.59e+00]    [0.00e+00, 4.00e+01, 2.59e+00]    [1.70e+01, 4.00e+01, 1.53e+01, 4.63e+01]    
28000     [2.63e+01, 6.14e-01, 2.58e+00]    [0.00e+00, 4.09e+01, 2.58e+00]    [1.74e+01, 4.09e+01, 1.70e+01, 5.17e+01]    
29000     [2.18e+01, 9.28e-02, 2.58e+00]    [0.00e+00, 3.89e+01, 2.58e+00]    [1.58e+01, 3.89e+01, 1.25e+01, 4.13e+01]    
30000     [2.19e+01, 1.73e-01, 2.58e+00]    [0.00e+00, 3.94e+01, 2.58e+00]    [1.55e+01, 3.94e+01, 1.29e+01, 4.22e+01]    

Best model at step 29000:
  train loss: 2.44e+01
  test loss: 4.14e+01
  test metric: [1.58e+01, 3.89e+01, 1.25e+01, 4.13e+01]

'train' took 154.567058 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.411425 s

'compile' took 2.567935 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.71e+02, 1.49e+03, 2.83e+00]    [0.00e+00, 5.08e+02, 2.83e+00]    [5.08e+02, 5.08e+02, 6.93e+02, 6.93e+02]    
1000      [5.95e+01, 6.27e+00, 2.80e+00]    [0.00e+00, 6.70e+01, 2.80e+00]    [6.70e+01, 6.70e+01, 3.20e+01, 3.19e+01]    
2000      [4.56e+01, 7.49e+00, 2.80e+00]    [0.00e+00, 5.44e+01, 2.80e+00]    [5.44e+01, 5.44e+01, 2.75e+01, 2.75e+01]    
3000      [3.53e+01, 1.72e+00, 2.80e+00]    [0.00e+00, 4.24e+01, 2.80e+00]    [4.23e+01, 4.24e+01, 2.22e+01, 2.23e+01]    
4000      [3.44e+01, 1.91e+01, 2.80e+00]    [0.00e+00, 4.08e+01, 2.80e+00]    [4.07e+01, 4.08e+01, 2.90e+01, 2.93e+01]    
5000      [2.97e+01, 2.88e+00, 2.80e+00]    [0.00e+00, 3.39e+01, 2.80e+00]    [3.37e+01, 3.39e+01, 1.96e+01, 1.97e+01]    
6000      [2.87e+01, 6.40e+00, 2.80e+00]    [0.00e+00, 2.93e+01, 2.80e+00]    [2.91e+01, 2.93e+01, 1.69e+01, 1.67e+01]    
7000      [2.95e+01, 1.45e+01, 2.80e+00]    [0.00e+00, 3.07e+01, 2.80e+00]    [3.04e+01, 3.07e+01, 1.92e+01, 1.95e+01]    
8000      [2.94e+01, 2.02e+01, 2.80e+00]    [0.00e+00, 3.00e+01, 2.80e+00]    [2.96e+01, 3.00e+01, 2.12e+01, 2.17e+01]    
9000      [3.16e+01, 3.23e+01, 2.80e+00]    [0.00e+00, 3.22e+01, 2.80e+00]    [3.18e+01, 3.22e+01, 2.80e+01, 2.87e+01]    
10000     [3.15e+01, 3.11e+01, 2.80e+00]    [0.00e+00, 3.07e+01, 2.80e+00]    [3.03e+01, 3.07e+01, 2.63e+01, 2.70e+01]    
11000     [3.15e+01, 3.05e+01, 2.79e+00]    [0.00e+00, 3.06e+01, 2.79e+00]    [3.01e+01, 3.06e+01, 2.64e+01, 2.71e+01]    
12000     [2.63e+01, 5.60e+00, 2.79e+00]    [0.00e+00, 2.56e+01, 2.79e+00]    [2.50e+01, 2.56e+01, 1.39e+01, 1.42e+01]    
13000     [2.78e+01, 1.70e+01, 2.79e+00]    [0.00e+00, 2.65e+01, 2.79e+00]    [2.58e+01, 2.65e+01, 1.82e+01, 1.91e+01]    
14000     [2.52e+01, 6.86e+00, 2.79e+00]    [0.00e+00, 2.26e+01, 2.79e+00]    [2.18e+01, 2.26e+01, 1.51e+01, 1.42e+01]    
15000     [2.66e+01, 1.97e+01, 2.79e+00]    [0.00e+00, 2.42e+01, 2.79e+00]    [2.46e+01, 2.42e+01, 1.38e+01, 1.34e+01]    
16000     [2.60e+01, 9.35e+00, 2.79e+00]    [0.00e+00, 2.42e+01, 2.79e+00]    [2.31e+01, 2.42e+01, 1.47e+01, 1.55e+01]    
17000     [2.41e+01, 2.82e+00, 2.79e+00]    [0.00e+00, 2.27e+01, 2.79e+00]    [2.15e+01, 2.27e+01, 1.45e+01, 1.37e+01]    
18000     [2.78e+01, 2.74e+01, 2.79e+00]    [0.00e+00, 2.50e+01, 2.79e+00]    [2.58e+01, 2.50e+01, 1.60e+01, 1.43e+01]    
19000     [2.38e+01, 6.32e+00, 2.79e+00]    [0.00e+00, 2.22e+01, 2.79e+00]    [2.07e+01, 2.22e+01, 1.49e+01, 1.38e+01]    
20000     [2.42e+01, 5.29e+00, 2.79e+00]    [0.00e+00, 2.33e+01, 2.79e+00]    [2.17e+01, 2.33e+01, 1.40e+01, 1.45e+01]    
21000     [2.54e+01, 1.71e+01, 2.79e+00]    [0.00e+00, 2.17e+01, 2.79e+00]    [2.28e+01, 2.17e+01, 1.40e+01, 1.43e+01]    
22000     [2.38e+01, 2.29e+00, 2.79e+00]    [0.00e+00, 2.38e+01, 2.79e+00]    [2.20e+01, 2.38e+01, 1.41e+01, 1.55e+01]    
23000     [2.38e+01, 5.25e+00, 2.78e+00]    [0.00e+00, 2.36e+01, 2.78e+00]    [2.17e+01, 2.36e+01, 1.39e+01, 1.53e+01]    
24000     [2.80e+01, 3.03e+01, 2.78e+00]    [0.00e+00, 2.41e+01, 2.78e+00]    [2.55e+01, 2.41e+01, 1.66e+01, 1.39e+01]    
25000     [2.81e+01, 3.28e+01, 2.78e+00]    [0.00e+00, 2.44e+01, 2.78e+00]    [2.57e+01, 2.44e+01, 1.68e+01, 1.38e+01]    
26000     [2.72e+01, 2.89e+01, 2.78e+00]    [0.00e+00, 2.35e+01, 2.78e+00]    [2.49e+01, 2.35e+01, 1.46e+01, 1.29e+01]    
27000     [2.31e+01, 2.06e+00, 2.78e+00]    [0.00e+00, 2.33e+01, 2.78e+00]    [2.10e+01, 2.33e+01, 1.32e+01, 1.46e+01]    
28000     [2.70e+01, 2.06e+01, 2.78e+00]    [0.00e+00, 2.67e+01, 2.78e+00]    [2.38e+01, 2.67e+01, 1.91e+01, 2.34e+01]    
29000     [2.41e+01, 1.24e+01, 2.78e+00]    [0.00e+00, 2.22e+01, 2.78e+00]    [2.10e+01, 2.22e+01, 1.34e+01, 1.25e+01]    
30000     [2.45e+01, 1.69e+01, 2.78e+00]    [0.00e+00, 2.14e+01, 2.78e+00]    [2.14e+01, 2.14e+01, 1.33e+01, 1.32e+01]    

Best model at step 27000:
  train loss: 2.79e+01
  test loss: 2.61e+01
  test metric: [2.10e+01, 2.33e+01, 1.32e+01, 1.46e+01]

'train' took 89.286492 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.287202 s

'compile' took 1.093049 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.49e+02, 1.54e+02, 2.78e+00]    [0.00e+00, 1.65e+02, 2.78e+00]    [1.65e+02, 1.65e+02, 1.97e+02, 1.97e+02]    
1000      [2.98e+01, 7.41e-01, 2.76e+00]    [0.00e+00, 2.62e+01, 2.76e+00]    [2.64e+01, 2.62e+01, 1.83e+01, 1.63e+01]    
2000      [2.76e+01, 6.99e-04, 2.74e+00]    [0.00e+00, 2.62e+01, 2.74e+00]    [2.50e+01, 2.62e+01, 1.39e+01, 1.50e+01]    
3000      [2.50e+01, 2.53e-01, 2.72e+00]    [0.00e+00, 2.15e+01, 2.72e+00]    [2.16e+01, 2.15e+01, 1.27e+01, 1.32e+01]    
4000      [2.51e+01, 3.32e-01, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [2.21e+01, 2.30e+01, 1.30e+01, 1.28e+01]    
5000      [2.46e+01, 1.53e-01, 2.69e+00]    [0.00e+00, 1.91e+01, 2.69e+00]    [1.93e+01, 1.91e+01, 1.43e+01, 1.44e+01]    
6000      [2.34e+01, 8.30e-02, 2.69e+00]    [0.00e+00, 2.03e+01, 2.69e+00]    [2.02e+01, 2.03e+01, 1.41e+01, 1.44e+01]    
7000      [2.56e+01, 4.57e-01, 2.68e+00]    [0.00e+00, 2.43e+01, 2.68e+00]    [2.36e+01, 2.43e+01, 1.37e+01, 1.39e+01]    
8000      [2.44e+01, 6.74e-01, 2.67e+00]    [0.00e+00, 2.24e+01, 2.67e+00]    [2.18e+01, 2.24e+01, 1.43e+01, 1.39e+01]    
9000      [2.78e+01, 5.20e-01, 2.66e+00]    [0.00e+00, 2.20e+01, 2.66e+00]    [2.27e+01, 2.20e+01, 1.91e+01, 1.87e+01]    
10000     [2.46e+01, 4.75e-01, 2.66e+00]    [0.00e+00, 2.03e+01, 2.66e+00]    [2.04e+01, 2.03e+01, 1.60e+01, 1.55e+01]    
11000     [2.37e+01, 8.03e-02, 2.65e+00]    [0.00e+00, 2.24e+01, 2.65e+00]    [2.20e+01, 2.24e+01, 1.47e+01, 1.47e+01]    
12000     [2.26e+01, 2.56e-01, 2.65e+00]    [0.00e+00, 2.12e+01, 2.65e+00]    [2.13e+01, 2.12e+01, 1.42e+01, 1.45e+01]    
13000     [2.33e+01, 3.91e-01, 2.64e+00]    [0.00e+00, 2.16e+01, 2.64e+00]    [2.12e+01, 2.16e+01, 1.54e+01, 1.53e+01]    
14000     [2.39e+01, 5.65e-01, 2.64e+00]    [0.00e+00, 2.23e+01, 2.64e+00]    [2.20e+01, 2.23e+01, 1.56e+01, 1.54e+01]    
15000     [2.36e+01, 5.94e-02, 2.63e+00]    [0.00e+00, 2.32e+01, 2.63e+00]    [2.30e+01, 2.32e+01, 1.50e+01, 1.51e+01]    
16000     [2.19e+01, 3.06e-01, 2.63e+00]    [0.00e+00, 2.02e+01, 2.63e+00]    [2.00e+01, 2.02e+01, 1.64e+01, 1.63e+01]    
17000     [2.17e+01, 2.73e-01, 2.62e+00]    [0.00e+00, 2.05e+01, 2.62e+00]    [2.04e+01, 2.05e+01, 1.64e+01, 1.62e+01]    
18000     [2.33e+01, 4.80e-01, 2.62e+00]    [0.00e+00, 2.31e+01, 2.62e+00]    [2.30e+01, 2.31e+01, 1.51e+01, 1.51e+01]    
19000     [2.15e+01, 2.60e-04, 2.62e+00]    [0.00e+00, 2.10e+01, 2.62e+00]    [2.10e+01, 2.10e+01, 1.60e+01, 1.60e+01]    
20000     [2.25e+01, 4.92e-01, 2.62e+00]    [0.00e+00, 2.19e+01, 2.62e+00]    [2.19e+01, 2.19e+01, 1.62e+01, 1.62e+01]    
21000     [2.16e+01, 3.95e-02, 2.62e+00]    [0.00e+00, 2.06e+01, 2.62e+00]    [2.06e+01, 2.06e+01, 1.70e+01, 1.70e+01]    
22000     [2.12e+01, 1.23e-01, 2.61e+00]    [0.00e+00, 2.11e+01, 2.61e+00]    [2.11e+01, 2.11e+01, 1.67e+01, 1.66e+01]    
23000     [2.12e+01, 2.40e-01, 2.61e+00]    [0.00e+00, 2.13e+01, 2.61e+00]    [2.12e+01, 2.13e+01, 1.64e+01, 1.64e+01]    
24000     [2.22e+01, 3.72e-01, 2.61e+00]    [0.00e+00, 2.18e+01, 2.61e+00]    [2.21e+01, 2.18e+01, 1.66e+01, 1.66e+01]    
25000     [2.21e+01, 6.47e-01, 2.61e+00]    [0.00e+00, 2.19e+01, 2.61e+00]    [2.17e+01, 2.19e+01, 1.68e+01, 1.65e+01]    
26000     [2.25e+01, 2.98e-01, 2.61e+00]    [0.00e+00, 2.19e+01, 2.61e+00]    [2.17e+01, 2.19e+01, 1.78e+01, 1.83e+01]    
27000     [2.10e+01, 4.39e-01, 2.61e+00]    [0.00e+00, 2.12e+01, 2.61e+00]    [2.11e+01, 2.12e+01, 1.68e+01, 1.67e+01]    
28000     [2.09e+01, 3.73e-02, 2.61e+00]    [0.00e+00, 2.12e+01, 2.61e+00]    [2.09e+01, 2.12e+01, 1.70e+01, 1.66e+01]    
29000     [2.13e+01, 1.95e-01, 2.61e+00]    [0.00e+00, 2.07e+01, 2.61e+00]    [2.05e+01, 2.07e+01, 1.71e+01, 1.74e+01]    
30000     [2.45e+01, 8.06e-01, 2.60e+00]    [0.00e+00, 2.19e+01, 2.60e+00]    [2.13e+01, 2.19e+01, 1.91e+01, 2.01e+01]    

Best model at step 28000:
  train loss: 2.35e+01
  test loss: 2.38e+01
  test metric: [2.09e+01, 2.12e+01, 1.70e+01, 1.66e+01]

'train' took 55.061492 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[57.16336533521541, 29.394157191507695, 28.75257231395552, 81.6618922252557, 18.132093965615134, 36.9684515506629, 25.08700561110343, 38.854278406042745, 23.291536625114894, 21.156837530092563]
sigma_y 1 36.0462190754566 18.59651114811055
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.266289 s

'compile' took 1.320469 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.23e+02, 4.22e+02, 2.75e+00]    [0.00e+00, 2.89e+02, 2.75e+00]    [2.89e+02, 2.89e+02, 3.35e+02, 3.35e+02]    
1000      [5.58e+01, 4.18e+01, 2.75e+00]    [0.00e+00, 6.62e+01, 2.75e+00]    [6.80e+01, 6.62e+01, 5.50e+01, 5.20e+01]    
2000      [3.98e+01, 7.76e+00, 2.75e+00]    [0.00e+00, 3.77e+01, 2.75e+00]    [3.92e+01, 3.77e+01, 3.02e+01, 3.06e+01]    
3000      [3.38e+01, 8.37e+00, 2.74e+00]    [0.00e+00, 3.19e+01, 2.74e+00]    [3.14e+01, 3.19e+01, 2.08e+01, 2.09e+01]    
4000      [3.20e+01, 3.85e+00, 2.74e+00]    [0.00e+00, 2.73e+01, 2.74e+00]    [2.82e+01, 2.73e+01, 1.98e+01, 1.99e+01]    
5000      [3.04e+01, 1.55e+00, 2.73e+00]    [0.00e+00, 2.44e+01, 2.73e+00]    [2.50e+01, 2.44e+01, 1.77e+01, 1.77e+01]    
6000      [3.16e+01, 9.56e+00, 2.73e+00]    [0.00e+00, 2.67e+01, 2.73e+00]    [2.72e+01, 2.67e+01, 1.92e+01, 1.86e+01]    
7000      [3.11e+01, 1.15e+01, 2.72e+00]    [0.00e+00, 2.37e+01, 2.72e+00]    [2.36e+01, 2.37e+01, 1.57e+01, 1.62e+01]    
8000      [2.88e+01, 6.58e+00, 2.72e+00]    [0.00e+00, 2.28e+01, 2.72e+00]    [2.32e+01, 2.28e+01, 1.57e+01, 1.55e+01]    
9000      [2.88e+01, 7.70e+00, 2.71e+00]    [0.00e+00, 2.27e+01, 2.71e+00]    [2.31e+01, 2.27e+01, 1.58e+01, 1.55e+01]    
10000     [2.96e+01, 8.97e+00, 2.71e+00]    [0.00e+00, 2.19e+01, 2.71e+00]    [2.18e+01, 2.19e+01, 1.48e+01, 1.52e+01]    
11000     [2.80e+01, 6.51e+00, 2.71e+00]    [0.00e+00, 2.17e+01, 2.71e+00]    [2.20e+01, 2.17e+01, 1.46e+01, 1.43e+01]    
12000     [2.84e+01, 8.18e+00, 2.71e+00]    [0.00e+00, 2.21e+01, 2.71e+00]    [2.24e+01, 2.21e+01, 1.56e+01, 1.54e+01]    
13000     [2.88e+01, 8.64e+00, 2.70e+00]    [0.00e+00, 2.06e+01, 2.70e+00]    [2.05e+01, 2.06e+01, 1.46e+01, 1.48e+01]    
14000     [2.72e+01, 6.12e+00, 2.70e+00]    [0.00e+00, 2.06e+01, 2.70e+00]    [2.08e+01, 2.06e+01, 1.43e+01, 1.42e+01]    
15000     [2.84e+01, 1.01e+01, 2.70e+00]    [0.00e+00, 2.20e+01, 2.70e+00]    [2.22e+01, 2.20e+01, 1.61e+01, 1.59e+01]    
16000     [2.90e+01, 9.80e+00, 2.70e+00]    [0.00e+00, 2.05e+01, 2.70e+00]    [2.05e+01, 2.05e+01, 1.72e+01, 1.74e+01]    
17000     [2.61e+01, 6.56e+00, 2.70e+00]    [0.00e+00, 1.89e+01, 2.70e+00]    [1.91e+01, 1.89e+01, 1.44e+01, 1.43e+01]    
18000     [2.67e+01, 9.00e+00, 2.70e+00]    [0.00e+00, 1.98e+01, 2.70e+00]    [1.99e+01, 1.98e+01, 1.45e+01, 1.44e+01]    
19000     [2.82e+01, 8.01e+00, 2.70e+00]    [0.00e+00, 2.03e+01, 2.70e+00]    [2.03e+01, 2.03e+01, 1.66e+01, 1.66e+01]    
20000     [2.51e+01, 6.38e+00, 2.69e+00]    [0.00e+00, 1.78e+01, 2.69e+00]    [1.79e+01, 1.78e+01, 1.34e+01, 1.34e+01]    
21000     [2.58e+01, 9.19e+00, 2.69e+00]    [0.00e+00, 1.93e+01, 2.69e+00]    [1.94e+01, 1.93e+01, 1.33e+01, 1.32e+01]    
22000     [2.58e+01, 8.60e+00, 2.69e+00]    [0.00e+00, 1.97e+01, 2.69e+00]    [1.97e+01, 1.97e+01, 1.31e+01, 1.31e+01]    
23000     [2.82e+01, 7.80e+00, 2.69e+00]    [0.00e+00, 2.08e+01, 2.69e+00]    [2.08e+01, 2.08e+01, 1.75e+01, 1.74e+01]    
24000     [2.81e+01, 8.82e+00, 2.69e+00]    [0.00e+00, 2.05e+01, 2.69e+00]    [2.06e+01, 2.05e+01, 1.66e+01, 1.66e+01]    
25000     [2.71e+01, 6.17e+00, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.02e+01, 2.01e+01, 1.48e+01, 1.47e+01]    
26000     [2.68e+01, 4.73e+00, 2.68e+00]    [0.00e+00, 1.95e+01, 2.68e+00]    [1.95e+01, 1.95e+01, 1.38e+01, 1.37e+01]    
27000     [2.70e+01, 5.79e+00, 2.68e+00]    [0.00e+00, 1.98e+01, 2.68e+00]    [1.98e+01, 1.98e+01, 1.50e+01, 1.50e+01]    
28000     [2.60e+01, 4.44e+00, 2.68e+00]    [0.00e+00, 1.87e+01, 2.68e+00]    [1.87e+01, 1.87e+01, 1.26e+01, 1.27e+01]    
29000     [2.64e+01, 1.13e+01, 2.68e+00]    [0.00e+00, 2.02e+01, 2.68e+00]    [2.04e+01, 2.02e+01, 1.28e+01, 1.27e+01]    
30000     [2.39e+01, 5.12e+00, 2.68e+00]    [0.00e+00, 1.68e+01, 2.68e+00]    [1.70e+01, 1.68e+01, 1.23e+01, 1.23e+01]    

Best model at step 30000:
  train loss: 3.17e+01
  test loss: 1.95e+01
  test metric: [1.70e+01, 1.68e+01, 1.23e+01, 1.23e+01]

'train' took 49.592013 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.184786 s

'compile' took 0.799268 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.80e+02, 9.92e+01, 2.76e+00]    [0.00e+00, 2.89e+02, 2.76e+00]    [2.89e+02, 2.89e+02, 2.94e+02, 2.94e+02]    
1000      [3.76e+01, 2.06e-01, 2.76e+00]    [0.00e+00, 3.73e+01, 2.76e+00]    [3.56e+01, 3.73e+01, 1.50e+01, 2.01e+01]    
2000      [3.11e+01, 2.40e-01, 2.76e+00]    [0.00e+00, 2.45e+01, 2.76e+00]    [1.77e+01, 2.45e+01, 1.10e+01, 1.14e+01]    
3000      [2.81e+01, 3.00e-02, 2.76e+00]    [0.00e+00, 2.79e+01, 2.76e+00]    [1.49e+01, 2.79e+01, 1.09e+01, 1.82e+01]    
4000      [3.00e+01, 1.94e-01, 2.76e+00]    [0.00e+00, 3.72e+01, 2.76e+00]    [1.89e+01, 3.72e+01, 1.25e+01, 2.43e+01]    
5000      [3.04e+01, 3.06e-01, 2.76e+00]    [0.00e+00, 2.96e+01, 2.76e+00]    [1.55e+01, 2.96e+01, 1.53e+01, 1.96e+01]    
6000      [2.85e+01, 2.78e-01, 2.75e+00]    [0.00e+00, 4.28e+01, 2.75e+00]    [1.80e+01, 4.28e+01, 1.13e+01, 2.93e+01]    
7000      [2.84e+01, 1.49e-01, 2.75e+00]    [0.00e+00, 3.66e+01, 2.75e+00]    [1.46e+01, 3.66e+01, 1.28e+01, 2.43e+01]    
8000      [2.79e+01, 1.30e-01, 2.75e+00]    [0.00e+00, 4.01e+01, 2.75e+00]    [1.41e+01, 4.01e+01, 1.24e+01, 2.64e+01]    
9000      [2.62e+01, 3.34e-02, 2.75e+00]    [0.00e+00, 5.03e+01, 2.75e+00]    [1.49e+01, 5.03e+01, 1.00e+01, 3.65e+01]    
10000     [2.91e+01, 2.88e-01, 2.75e+00]    [0.00e+00, 4.39e+01, 2.75e+00]    [1.56e+01, 4.39e+01, 1.29e+01, 3.01e+01]    
11000     [2.54e+01, 1.20e-01, 2.75e+00]    [0.00e+00, 5.52e+01, 2.75e+00]    [1.42e+01, 5.52e+01, 9.39e+00, 4.27e+01]    
12000     [2.59e+01, 6.29e-02, 2.75e+00]    [0.00e+00, 6.10e+01, 2.75e+00]    [1.56e+01, 6.10e+01, 9.76e+00, 4.99e+01]    
13000     [2.52e+01, 1.69e-01, 2.74e+00]    [0.00e+00, 6.36e+01, 2.74e+00]    [1.45e+01, 6.36e+01, 9.53e+00, 5.38e+01]    
14000     [2.56e+01, 2.15e-01, 2.74e+00]    [0.00e+00, 6.22e+01, 2.74e+00]    [1.40e+01, 6.22e+01, 1.13e+01, 5.00e+01]    
15000     [2.45e+01, 1.67e-01, 2.74e+00]    [0.00e+00, 6.88e+01, 2.74e+00]    [1.43e+01, 6.88e+01, 9.60e+00, 5.83e+01]    
16000     [2.49e+01, 9.28e-02, 2.74e+00]    [0.00e+00, 7.57e+01, 2.74e+00]    [1.43e+01, 7.57e+01, 1.02e+01, 6.90e+01]    
17000     [2.37e+01, 8.93e-02, 2.74e+00]    [0.00e+00, 7.65e+01, 2.74e+00]    [1.25e+01, 7.65e+01, 1.03e+01, 7.00e+01]    
18000     [2.65e+01, 2.34e-01, 2.74e+00]    [0.00e+00, 8.82e+01, 2.74e+00]    [1.81e+01, 8.82e+01, 1.21e+01, 8.80e+01]    
19000     [2.62e+01, 2.66e-01, 2.74e+00]    [0.00e+00, 7.94e+01, 2.74e+00]    [1.48e+01, 7.94e+01, 1.27e+01, 7.30e+01]    
20000     [2.77e+01, 3.49e-01, 2.74e+00]    [0.00e+00, 8.04e+01, 2.74e+00]    [1.79e+01, 8.04e+01, 1.54e+01, 7.36e+01]    
21000     [2.49e+01, 1.08e-01, 2.74e+00]    [0.00e+00, 8.74e+01, 2.74e+00]    [1.33e+01, 8.74e+01, 1.18e+01, 8.47e+01]    
22000     [2.39e+01, 5.08e-02, 2.74e+00]    [0.00e+00, 9.81e+01, 2.74e+00]    [1.46e+01, 9.81e+01, 9.63e+00, 1.01e+02]    
23000     [2.33e+01, 7.36e-02, 2.74e+00]    [0.00e+00, 1.01e+02, 2.74e+00]    [1.39e+01, 1.01e+02, 9.91e+00, 1.06e+02]    
24000     [2.48e+01, 1.03e-01, 2.74e+00]    [0.00e+00, 1.06e+02, 2.74e+00]    [1.62e+01, 1.06e+02, 1.06e+01, 1.14e+02]    
25000     [2.23e+01, 9.78e-02, 2.74e+00]    [0.00e+00, 1.04e+02, 2.74e+00]    [1.24e+01, 1.04e+02, 1.02e+01, 1.10e+02]    
26000     [2.55e+01, 2.41e-01, 2.74e+00]    [0.00e+00, 1.15e+02, 2.74e+00]    [1.79e+01, 1.15e+02, 1.22e+01, 1.28e+02]    
27000     [2.52e+01, 4.08e-01, 2.74e+00]    [0.00e+00, 1.05e+02, 2.74e+00]    [1.40e+01, 1.05e+02, 1.14e+01, 1.11e+02]    
28000     [2.47e+01, 2.68e-01, 2.74e+00]    [0.00e+00, 1.08e+02, 2.74e+00]    [1.48e+01, 1.08e+02, 1.24e+01, 1.14e+02]    
29000     [2.17e+01, 7.02e-02, 2.74e+00]    [0.00e+00, 1.16e+02, 2.74e+00]    [1.24e+01, 1.16e+02, 1.02e+01, 1.29e+02]    
30000     [2.52e+01, 2.74e-01, 2.74e+00]    [0.00e+00, 1.26e+02, 2.74e+00]    [1.87e+01, 1.26e+02, 1.31e+01, 1.44e+02]    

Best model at step 29000:
  train loss: 2.45e+01
  test loss: 1.19e+02
  test metric: [1.24e+01, 1.16e+02, 1.02e+01, 1.29e+02]

'train' took 48.147833 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.282247 s

'compile' took 1.260635 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.23e+02, 1.11e+02, 2.86e+00]    [0.00e+00, 1.18e+02, 2.86e+00]    [1.18e+02, 1.18e+02, 5.13e+01, 5.13e+01]    
1000      [3.43e+01, 1.35e+00, 2.85e+00]    [0.00e+00, 2.81e+01, 2.85e+00]    [2.85e+01, 2.81e+01, 1.80e+01, 1.79e+01]    
2000      [2.92e+01, 2.02e-01, 2.84e+00]    [0.00e+00, 2.00e+01, 2.84e+00]    [2.07e+01, 2.00e+01, 1.66e+01, 1.60e+01]    
3000      [3.06e+01, 1.31e+00, 2.83e+00]    [0.00e+00, 1.99e+01, 2.83e+00]    [1.98e+01, 1.99e+01, 1.93e+01, 1.97e+01]    
4000      [2.99e+01, 9.77e-01, 2.83e+00]    [0.00e+00, 2.03e+01, 2.83e+00]    [1.98e+01, 2.03e+01, 1.97e+01, 2.13e+01]    
5000      [2.62e+01, 2.19e-01, 2.83e+00]    [0.00e+00, 1.79e+01, 2.83e+00]    [1.80e+01, 1.79e+01, 1.54e+01, 1.36e+01]    
6000      [2.86e+01, 6.52e-01, 2.82e+00]    [0.00e+00, 2.38e+01, 2.82e+00]    [2.17e+01, 2.38e+01, 1.83e+01, 2.22e+01]    
7000      [2.59e+01, 8.16e-01, 2.82e+00]    [0.00e+00, 1.72e+01, 2.82e+00]    [1.93e+01, 1.72e+01, 1.46e+01, 1.25e+01]    
8000      [2.51e+01, 6.37e-01, 2.82e+00]    [0.00e+00, 1.71e+01, 2.82e+00]    [1.62e+01, 1.71e+01, 1.47e+01, 1.49e+01]    
9000      [2.51e+01, 5.96e-01, 2.82e+00]    [0.00e+00, 1.76e+01, 2.82e+00]    [1.78e+01, 1.76e+01, 1.45e+01, 1.26e+01]    
10000     [2.44e+01, 2.57e-01, 2.82e+00]    [0.00e+00, 1.78e+01, 2.82e+00]    [1.65e+01, 1.78e+01, 1.41e+01, 1.59e+01]    
11000     [2.52e+01, 2.65e-01, 2.81e+00]    [0.00e+00, 2.13e+01, 2.81e+00]    [1.63e+01, 2.13e+01, 1.44e+01, 1.81e+01]    
12000     [2.66e+01, 8.19e-01, 2.81e+00]    [0.00e+00, 1.62e+01, 2.81e+00]    [2.10e+01, 1.62e+01, 1.44e+01, 1.17e+01]    
13000     [2.40e+01, 6.32e-01, 2.81e+00]    [0.00e+00, 1.94e+01, 2.81e+00]    [1.66e+01, 1.94e+01, 1.45e+01, 1.59e+01]    
14000     [2.50e+01, 6.46e-01, 2.81e+00]    [0.00e+00, 1.87e+01, 2.81e+00]    [1.83e+01, 1.87e+01, 1.40e+01, 1.40e+01]    
15000     [2.66e+01, 7.93e-01, 2.81e+00]    [0.00e+00, 1.65e+01, 2.81e+00]    [2.15e+01, 1.65e+01, 1.46e+01, 1.22e+01]    
16000     [2.59e+01, 8.09e-01, 2.81e+00]    [0.00e+00, 2.73e+01, 2.81e+00]    [1.63e+01, 2.73e+01, 1.55e+01, 2.55e+01]    
17000     [2.45e+01, 1.34e+00, 2.80e+00]    [0.00e+00, 2.34e+01, 2.80e+00]    [1.81e+01, 2.34e+01, 1.35e+01, 1.85e+01]    
18000     [2.45e+01, 6.34e-01, 2.80e+00]    [0.00e+00, 2.22e+01, 2.80e+00]    [1.87e+01, 2.22e+01, 1.36e+01, 1.72e+01]    
19000     [2.31e+01, 4.22e-01, 2.80e+00]    [0.00e+00, 2.43e+01, 2.80e+00]    [1.61e+01, 2.43e+01, 1.42e+01, 2.12e+01]    
20000     [2.54e+01, 6.39e-01, 2.80e+00]    [0.00e+00, 3.15e+01, 2.80e+00]    [1.76e+01, 3.15e+01, 1.56e+01, 3.37e+01]    
21000     [2.68e+01, 1.50e+00, 2.80e+00]    [0.00e+00, 2.10e+01, 2.80e+00]    [2.24e+01, 2.10e+01, 1.56e+01, 1.40e+01]    
22000     [2.42e+01, 8.72e-01, 2.80e+00]    [0.00e+00, 2.58e+01, 2.80e+00]    [1.85e+01, 2.58e+01, 1.35e+01, 2.32e+01]    
23000     [2.31e+01, 6.68e-01, 2.80e+00]    [0.00e+00, 3.07e+01, 2.80e+00]    [1.77e+01, 3.07e+01, 1.31e+01, 2.96e+01]    
24000     [2.32e+01, 9.34e-01, 2.80e+00]    [0.00e+00, 2.87e+01, 2.80e+00]    [1.73e+01, 2.87e+01, 1.43e+01, 2.97e+01]    
25000     [2.35e+01, 1.47e+00, 2.80e+00]    [0.00e+00, 3.32e+01, 2.80e+00]    [1.83e+01, 3.32e+01, 1.29e+01, 3.53e+01]    
26000     [2.62e+01, 5.54e-01, 2.80e+00]    [0.00e+00, 4.01e+01, 2.80e+00]    [1.95e+01, 4.01e+01, 1.63e+01, 4.59e+01]    
27000     [2.21e+01, 3.77e-01, 2.80e+00]    [0.00e+00, 3.21e+01, 2.80e+00]    [1.63e+01, 3.21e+01, 1.39e+01, 3.29e+01]    
28000     [2.55e+01, 8.14e-01, 2.80e+00]    [0.00e+00, 4.10e+01, 2.80e+00]    [1.93e+01, 4.10e+01, 1.69e+01, 5.09e+01]    
29000     [2.94e+01, 2.73e+00, 2.79e+00]    [0.00e+00, 2.53e+01, 2.79e+00]    [2.69e+01, 2.53e+01, 1.98e+01, 1.72e+01]    
30000     [2.21e+01, 3.78e-01, 2.79e+00]    [0.00e+00, 3.43e+01, 2.79e+00]    [1.69e+01, 3.43e+01, 1.33e+01, 3.66e+01]    

Best model at step 30000:
  train loss: 2.52e+01
  test loss: 3.71e+01
  test metric: [1.69e+01, 3.43e+01, 1.33e+01, 3.66e+01]

'train' took 50.379555 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.270273 s

'compile' took 0.897669 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.51e+02, 1.28e+03, 2.74e+00]    [0.00e+00, 1.76e+02, 2.74e+00]    [1.76e+02, 1.76e+02, 2.22e+02, 2.22e+02]    
1000      [6.43e+01, 5.78e+01, 2.73e+00]    [0.00e+00, 6.61e+01, 2.73e+00]    [6.60e+01, 6.61e+01, 3.03e+01, 3.03e+01]    
2000      [5.08e+01, 6.99e+01, 2.73e+00]    [0.00e+00, 5.55e+01, 2.73e+00]    [5.55e+01, 5.55e+01, 2.77e+01, 2.76e+01]    
3000      [4.01e+01, 2.41e+01, 2.75e+00]    [0.00e+00, 3.33e+01, 2.75e+00]    [3.36e+01, 3.33e+01, 2.70e+01, 2.70e+01]    
4000      [3.87e+01, 2.37e+01, 2.76e+00]    [0.00e+00, 2.79e+01, 2.76e+00]    [2.83e+01, 2.79e+01, 2.56e+01, 2.55e+01]    
5000      [3.73e+01, 1.09e+00, 2.76e+00]    [0.00e+00, 2.79e+01, 2.76e+00]    [2.83e+01, 2.79e+01, 2.28e+01, 2.25e+01]    
6000      [3.73e+01, 1.32e+01, 2.76e+00]    [0.00e+00, 2.66e+01, 2.76e+00]    [2.70e+01, 2.66e+01, 2.07e+01, 2.03e+01]    
7000      [3.50e+01, 2.58e+00, 2.76e+00]    [0.00e+00, 2.30e+01, 2.76e+00]    [2.36e+01, 2.30e+01, 1.85e+01, 1.83e+01]    
8000      [3.34e+01, 1.93e+01, 2.76e+00]    [0.00e+00, 2.16e+01, 2.76e+00]    [2.24e+01, 2.16e+01, 1.58e+01, 1.58e+01]    
9000      [3.42e+01, 2.55e+01, 2.76e+00]    [0.00e+00, 2.12e+01, 2.76e+00]    [2.22e+01, 2.12e+01, 1.51e+01, 1.54e+01]    
10000     [3.36e+01, 2.86e+01, 2.76e+00]    [0.00e+00, 2.04e+01, 2.76e+00]    [2.14e+01, 2.04e+01, 1.50e+01, 1.53e+01]    
11000     [3.26e+01, 3.66e+00, 2.76e+00]    [0.00e+00, 1.99e+01, 2.76e+00]    [2.05e+01, 1.99e+01, 1.64e+01, 1.62e+01]    
12000     [3.29e+01, 1.14e+01, 2.76e+00]    [0.00e+00, 2.06e+01, 2.76e+00]    [2.13e+01, 2.06e+01, 1.58e+01, 1.56e+01]    
13000     [3.24e+01, 2.62e+01, 2.76e+00]    [0.00e+00, 1.84e+01, 2.76e+00]    [1.96e+01, 1.84e+01, 1.41e+01, 1.46e+01]    
14000     [3.15e+01, 5.67e+00, 2.76e+00]    [0.00e+00, 1.86e+01, 2.76e+00]    [1.94e+01, 1.86e+01, 1.52e+01, 1.50e+01]    
15000     [3.20e+01, 1.29e+01, 2.76e+00]    [0.00e+00, 1.93e+01, 2.76e+00]    [2.01e+01, 1.93e+01, 1.51e+01, 1.48e+01]    
16000     [3.22e+01, 2.78e+01, 2.75e+00]    [0.00e+00, 1.65e+01, 2.75e+00]    [1.75e+01, 1.65e+01, 1.40e+01, 1.42e+01]    
17000     [3.08e+01, 9.66e+00, 2.75e+00]    [0.00e+00, 1.84e+01, 2.75e+00]    [1.95e+01, 1.84e+01, 1.42e+01, 1.41e+01]    
18000     [3.09e+01, 1.38e+01, 2.75e+00]    [0.00e+00, 1.85e+01, 2.75e+00]    [1.96e+01, 1.85e+01, 1.43e+01, 1.39e+01]    
19000     [3.06e+01, 2.12e+01, 2.75e+00]    [0.00e+00, 1.52e+01, 2.75e+00]    [1.66e+01, 1.52e+01, 1.33e+01, 1.37e+01]    
20000     [2.98e+01, 9.27e+00, 2.75e+00]    [0.00e+00, 1.73e+01, 2.75e+00]    [1.85e+01, 1.73e+01, 1.34e+01, 1.31e+01]    
21000     [3.02e+01, 1.54e+01, 2.75e+00]    [0.00e+00, 1.80e+01, 2.75e+00]    [1.93e+01, 1.80e+01, 1.37e+01, 1.32e+01]    
22000     [3.01e+01, 2.04e+01, 2.75e+00]    [0.00e+00, 1.44e+01, 2.75e+00]    [1.57e+01, 1.44e+01, 1.32e+01, 1.35e+01]    
23000     [2.91e+01, 9.07e+00, 2.75e+00]    [0.00e+00, 1.66e+01, 2.75e+00]    [1.82e+01, 1.66e+01, 1.29e+01, 1.27e+01]    
24000     [2.97e+01, 1.65e+01, 2.75e+00]    [0.00e+00, 1.76e+01, 2.75e+00]    [1.93e+01, 1.76e+01, 1.35e+01, 1.30e+01]    
25000     [2.92e+01, 1.76e+01, 2.75e+00]    [0.00e+00, 1.41e+01, 2.75e+00]    [1.50e+01, 1.41e+01, 1.31e+01, 1.30e+01]    
26000     [2.84e+01, 1.06e+01, 2.75e+00]    [0.00e+00, 1.59e+01, 2.75e+00]    [1.80e+01, 1.59e+01, 1.27e+01, 1.26e+01]    
27000     [2.88e+01, 1.74e+01, 2.75e+00]    [0.00e+00, 1.66e+01, 2.75e+00]    [1.88e+01, 1.66e+01, 1.32e+01, 1.28e+01]    
28000     [2.86e+01, 1.75e+01, 2.76e+00]    [0.00e+00, 1.41e+01, 2.76e+00]    [1.45e+01, 1.41e+01, 1.33e+01, 1.28e+01]    
29000     [2.80e+01, 1.43e+01, 2.76e+00]    [0.00e+00, 1.58e+01, 2.76e+00]    [1.85e+01, 1.58e+01, 1.27e+01, 1.30e+01]    
30000     [2.82e+01, 1.74e+01, 2.76e+00]    [0.00e+00, 1.62e+01, 2.76e+00]    [1.91e+01, 1.62e+01, 1.32e+01, 1.32e+01]    

Best model at step 11000:
  train loss: 3.90e+01
  test loss: 2.26e+01
  test metric: [2.05e+01, 1.99e+01, 1.64e+01, 1.62e+01]

'train' took 51.028064 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.166648 s

'compile' took 0.778337 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.95e+02, 1.83e+02, 2.72e+00]    [0.00e+00, 4.22e+02, 2.72e+00]    [4.22e+02, 4.22e+02, 5.59e+02, 5.59e+02]    
1000      [3.51e+01, 1.38e+00, 2.67e+00]    [0.00e+00, 4.04e+01, 2.67e+00]    [4.04e+01, 4.04e+01, 2.04e+01, 2.04e+01]    
2000      [2.87e+01, 3.65e-01, 2.64e+00]    [0.00e+00, 3.35e+01, 2.64e+00]    [3.35e+01, 3.35e+01, 1.29e+01, 1.29e+01]    
3000      [2.62e+01, 1.96e+00, 2.62e+00]    [0.00e+00, 2.94e+01, 2.62e+00]    [2.93e+01, 2.94e+01, 1.50e+01, 1.50e+01]    
4000      [2.57e+01, 1.80e+00, 2.60e+00]    [0.00e+00, 2.93e+01, 2.60e+00]    [2.92e+01, 2.93e+01, 1.45e+01, 1.45e+01]    
5000      [2.55e+01, 5.12e+00, 2.59e+00]    [0.00e+00, 2.95e+01, 2.59e+00]    [2.94e+01, 2.95e+01, 1.32e+01, 1.32e+01]    
6000      [2.63e+01, 5.15e+00, 2.58e+00]    [0.00e+00, 3.05e+01, 2.58e+00]    [3.04e+01, 3.05e+01, 1.42e+01, 1.42e+01]    
7000      [2.50e+01, 2.73e+00, 2.57e+00]    [0.00e+00, 2.92e+01, 2.57e+00]    [2.91e+01, 2.92e+01, 1.45e+01, 1.45e+01]    
8000      [2.54e+01, 1.44e+00, 2.56e+00]    [0.00e+00, 2.77e+01, 2.56e+00]    [2.77e+01, 2.77e+01, 1.52e+01, 1.52e+01]    
9000      [2.42e+01, 9.33e-01, 2.55e+00]    [0.00e+00, 2.78e+01, 2.55e+00]    [2.78e+01, 2.78e+01, 1.42e+01, 1.42e+01]    
10000     [2.54e+01, 5.88e+00, 2.55e+00]    [0.00e+00, 2.94e+01, 2.55e+00]    [2.94e+01, 2.94e+01, 1.37e+01, 1.38e+01]    
11000     [2.86e+01, 2.51e+00, 2.54e+00]    [0.00e+00, 2.89e+01, 2.54e+00]    [2.90e+01, 2.89e+01, 1.94e+01, 1.93e+01]    
12000     [2.40e+01, 4.94e+00, 2.53e+00]    [0.00e+00, 2.56e+01, 2.53e+00]    [2.54e+01, 2.56e+01, 1.64e+01, 1.63e+01]    
13000     [2.43e+01, 4.08e+00, 2.53e+00]    [0.00e+00, 2.79e+01, 2.53e+00]    [2.80e+01, 2.79e+01, 1.45e+01, 1.47e+01]    
14000     [2.55e+01, 3.78e-01, 2.53e+00]    [0.00e+00, 2.58e+01, 2.53e+00]    [2.58e+01, 2.58e+01, 1.63e+01, 1.63e+01]    
15000     [2.68e+01, 1.32e+00, 2.52e+00]    [0.00e+00, 2.77e+01, 2.52e+00]    [2.77e+01, 2.77e+01, 1.92e+01, 1.91e+01]    
16000     [2.33e+01, 3.52e-01, 2.52e+00]    [0.00e+00, 2.70e+01, 2.52e+00]    [2.69e+01, 2.70e+01, 1.46e+01, 1.46e+01]    
17000     [2.57e+01, 3.03e+00, 2.52e+00]    [0.00e+00, 2.57e+01, 2.52e+00]    [2.57e+01, 2.57e+01, 1.73e+01, 1.73e+01]    
18000     [2.46e+01, 4.42e+00, 2.52e+00]    [0.00e+00, 2.81e+01, 2.52e+00]    [2.80e+01, 2.81e+01, 1.40e+01, 1.40e+01]    
19000     [2.55e+01, 4.01e+00, 2.51e+00]    [0.00e+00, 2.53e+01, 2.51e+00]    [2.53e+01, 2.53e+01, 1.70e+01, 1.69e+01]    
20000     [2.56e+01, 4.83e-01, 2.51e+00]    [0.00e+00, 2.59e+01, 2.51e+00]    [2.59e+01, 2.59e+01, 1.75e+01, 1.74e+01]    
21000     [2.25e+01, 2.76e+00, 2.51e+00]    [0.00e+00, 2.49e+01, 2.51e+00]    [2.48e+01, 2.49e+01, 1.62e+01, 1.62e+01]    
22000     [2.28e+01, 4.05e-01, 2.51e+00]    [0.00e+00, 2.52e+01, 2.51e+00]    [2.53e+01, 2.52e+01, 1.49e+01, 1.51e+01]    
23000     [2.31e+01, 3.19e+00, 2.50e+00]    [0.00e+00, 2.66e+01, 2.50e+00]    [2.65e+01, 2.66e+01, 1.50e+01, 1.50e+01]    
24000     [2.31e+01, 2.20e+00, 2.50e+00]    [0.00e+00, 2.66e+01, 2.50e+00]    [2.65e+01, 2.66e+01, 1.48e+01, 1.48e+01]    
25000     [2.43e+01, 3.49e+00, 2.50e+00]    [0.00e+00, 2.86e+01, 2.50e+00]    [2.86e+01, 2.86e+01, 1.38e+01, 1.39e+01]    
26000     [2.43e+01, 3.68e+00, 2.50e+00]    [0.00e+00, 2.39e+01, 2.50e+00]    [2.40e+01, 2.39e+01, 1.67e+01, 1.69e+01]    
27000     [2.23e+01, 3.59e+00, 2.50e+00]    [0.00e+00, 2.48e+01, 2.50e+00]    [2.47e+01, 2.48e+01, 1.61e+01, 1.62e+01]    
28000     [2.25e+01, 1.74e+00, 2.49e+00]    [0.00e+00, 2.40e+01, 2.49e+00]    [2.41e+01, 2.40e+01, 1.57e+01, 1.60e+01]    
29000     [2.21e+01, 2.45e+00, 2.49e+00]    [0.00e+00, 2.55e+01, 2.49e+00]    [2.56e+01, 2.55e+01, 1.52e+01, 1.55e+01]    
30000     [2.17e+01, 1.90e-01, 2.49e+00]    [0.00e+00, 2.50e+01, 2.49e+00]    [2.51e+01, 2.50e+01, 1.48e+01, 1.51e+01]    

Best model at step 30000:
  train loss: 2.44e+01
  test loss: 2.74e+01
  test metric: [2.51e+01, 2.50e+01, 1.48e+01, 1.51e+01]

'train' took 51.166940 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.246586 s

'compile' took 1.193143 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.78e+02, 9.64e+01, 2.70e+00]    [0.00e+00, 2.48e+02, 2.70e+00]    [2.48e+02, 2.48e+02, 3.44e+02, 3.44e+02]    
1000      [3.18e+01, 1.29e-01, 2.71e+00]    [0.00e+00, 2.00e+01, 2.71e+00]    [1.95e+01, 2.00e+01, 1.58e+01, 2.03e+01]    
2000      [2.91e+01, 2.08e-01, 2.71e+00]    [0.00e+00, 1.93e+01, 2.71e+00]    [1.92e+01, 1.93e+01, 1.32e+01, 2.05e+01]    
3000      [2.79e+01, 7.35e-02, 2.70e+00]    [0.00e+00, 1.86e+01, 2.70e+00]    [1.93e+01, 1.86e+01, 1.25e+01, 1.57e+01]    
4000      [2.70e+01, 1.82e-01, 2.70e+00]    [0.00e+00, 2.19e+01, 2.70e+00]    [1.90e+01, 2.19e+01, 1.06e+01, 2.27e+01]    
5000      [2.59e+01, 4.51e-02, 2.69e+00]    [0.00e+00, 2.00e+01, 2.69e+00]    [1.85e+01, 2.00e+01, 1.03e+01, 1.90e+01]    
6000      [3.00e+01, 4.18e-01, 2.69e+00]    [0.00e+00, 2.80e+01, 2.69e+00]    [2.05e+01, 2.80e+01, 1.90e+01, 3.11e+01]    
7000      [2.68e+01, 2.17e-01, 2.69e+00]    [0.00e+00, 1.63e+01, 2.69e+00]    [2.07e+01, 1.63e+01, 1.16e+01, 1.14e+01]    
8000      [2.46e+01, 1.01e-01, 2.69e+00]    [0.00e+00, 1.77e+01, 2.69e+00]    [1.89e+01, 1.77e+01, 1.15e+01, 1.49e+01]    
9000      [2.75e+01, 4.18e-01, 2.69e+00]    [0.00e+00, 2.43e+01, 2.69e+00]    [2.03e+01, 2.43e+01, 1.69e+01, 2.27e+01]    
10000     [2.81e+01, 4.99e-01, 2.69e+00]    [0.00e+00, 2.15e+01, 2.69e+00]    [1.98e+01, 2.15e+01, 1.57e+01, 1.68e+01]    
11000     [2.88e+01, 4.98e-01, 2.69e+00]    [0.00e+00, 2.45e+01, 2.69e+00]    [2.62e+01, 2.45e+01, 1.68e+01, 1.96e+01]    
12000     [2.53e+01, 1.55e-01, 2.69e+00]    [0.00e+00, 1.44e+01, 2.69e+00]    [1.95e+01, 1.44e+01, 1.39e+01, 1.21e+01]    
13000     [2.49e+01, 2.52e-01, 2.69e+00]    [0.00e+00, 2.32e+01, 2.69e+00]    [2.05e+01, 2.32e+01, 1.26e+01, 1.73e+01]    
14000     [2.34e+01, 8.52e-02, 2.69e+00]    [0.00e+00, 2.41e+01, 2.69e+00]    [1.85e+01, 2.41e+01, 1.30e+01, 1.83e+01]    
15000     [2.63e+01, 4.08e-01, 2.69e+00]    [0.00e+00, 1.73e+01, 2.69e+00]    [1.92e+01, 1.73e+01, 1.74e+01, 1.07e+01]    
16000     [2.31e+01, 1.22e-01, 2.68e+00]    [0.00e+00, 2.53e+01, 2.68e+00]    [1.77e+01, 2.53e+01, 1.32e+01, 1.84e+01]    
17000     [2.50e+01, 3.51e-01, 2.68e+00]    [0.00e+00, 2.51e+01, 2.68e+00]    [1.71e+01, 2.51e+01, 1.53e+01, 1.88e+01]    
18000     [2.24e+01, 1.17e-01, 2.68e+00]    [0.00e+00, 3.38e+01, 2.68e+00]    [1.67e+01, 3.38e+01, 1.38e+01, 3.35e+01]    
19000     [2.44e+01, 2.41e-01, 2.68e+00]    [0.00e+00, 3.17e+01, 2.68e+00]    [1.70e+01, 3.17e+01, 1.50e+01, 2.99e+01]    
20000     [2.46e+01, 2.44e-01, 2.68e+00]    [0.00e+00, 4.55e+01, 2.68e+00]    [2.19e+01, 4.55e+01, 1.38e+01, 5.57e+01]    
21000     [2.52e+01, 3.21e-01, 2.67e+00]    [0.00e+00, 4.96e+01, 2.67e+00]    [2.33e+01, 4.96e+01, 1.51e+01, 6.33e+01]    
22000     [2.16e+01, 1.42e-02, 2.67e+00]    [0.00e+00, 4.38e+01, 2.67e+00]    [1.78e+01, 4.38e+01, 1.34e+01, 5.09e+01]    
23000     [2.24e+01, 1.89e-01, 2.67e+00]    [0.00e+00, 4.91e+01, 2.67e+00]    [1.89e+01, 4.91e+01, 1.39e+01, 6.00e+01]    
24000     [2.12e+01, 8.66e-02, 2.67e+00]    [0.00e+00, 4.95e+01, 2.67e+00]    [1.75e+01, 4.95e+01, 1.43e+01, 6.20e+01]    
25000     [2.16e+01, 7.17e-02, 2.67e+00]    [0.00e+00, 4.96e+01, 2.67e+00]    [1.85e+01, 4.96e+01, 1.38e+01, 6.08e+01]    
26000     [2.43e+01, 2.50e-01, 2.66e+00]    [0.00e+00, 6.03e+01, 2.66e+00]    [2.31e+01, 6.03e+01, 1.49e+01, 8.28e+01]    
27000     [2.29e+01, 1.93e-01, 2.66e+00]    [0.00e+00, 5.11e+01, 2.66e+00]    [2.03e+01, 5.11e+01, 1.58e+01, 6.13e+01]    
28000     [2.11e+01, 1.12e-01, 2.66e+00]    [0.00e+00, 5.87e+01, 2.66e+00]    [1.88e+01, 5.87e+01, 1.42e+01, 7.74e+01]    
29000     [2.39e+01, 2.58e-01, 2.66e+00]    [0.00e+00, 6.68e+01, 2.66e+00]    [2.31e+01, 6.68e+01, 1.47e+01, 9.44e+01]    
30000     [2.29e+01, 1.43e-01, 2.66e+00]    [0.00e+00, 5.80e+01, 2.66e+00]    [1.96e+01, 5.80e+01, 1.58e+01, 7.56e+01]    

Best model at step 28000:
  train loss: 2.39e+01
  test loss: 6.14e+01
  test metric: [1.88e+01, 5.87e+01, 1.42e+01, 7.74e+01]

'train' took 55.576996 s


Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.212127 s

'compile' took 0.998867 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [8.52e+02, 5.38e+03, 2.77e+00]    [0.00e+00, 4.40e+02, 2.77e+00]    [4.40e+02, 4.40e+02, 8.99e+02, 8.99e+02]    
1000      [6.64e+01, 1.48e+01, 2.74e+00]    [0.00e+00, 6.85e+01, 2.74e+00]    [6.86e+01, 6.85e+01, 2.89e+01, 2.90e+01]    
2000      [5.76e+01, 4.15e+01, 2.73e+00]    [0.00e+00, 5.57e+01, 2.73e+00]    [5.58e+01, 5.57e+01, 2.97e+01, 2.98e+01]    
3000      [4.74e+01, 2.56e+01, 2.72e+00]    [0.00e+00, 4.63e+01, 2.72e+00]    [4.64e+01, 4.63e+01, 2.33e+01, 2.34e+01]    
4000      [4.04e+01, 1.67e+01, 2.72e+00]    [0.00e+00, 4.05e+01, 2.72e+00]    [4.06e+01, 4.05e+01, 1.71e+01, 1.72e+01]    
5000      [3.68e+01, 3.06e+01, 2.71e+00]    [0.00e+00, 3.70e+01, 2.71e+00]    [3.72e+01, 3.70e+01, 1.83e+01, 1.82e+01]    
6000      [3.31e+01, 5.93e+00, 2.71e+00]    [0.00e+00, 3.09e+01, 2.71e+00]    [3.10e+01, 3.09e+01, 1.69e+01, 1.70e+01]    
7000      [3.17e+01, 9.01e+00, 2.71e+00]    [0.00e+00, 2.69e+01, 2.71e+00]    [2.70e+01, 2.69e+01, 1.59e+01, 1.60e+01]    
8000      [3.30e+01, 3.73e+01, 2.71e+00]    [0.00e+00, 2.44e+01, 2.71e+00]    [2.43e+01, 2.44e+01, 1.50e+01, 1.47e+01]    
9000      [2.98e+01, 1.44e+01, 2.70e+00]    [0.00e+00, 2.23e+01, 2.70e+00]    [2.26e+01, 2.23e+01, 1.42e+01, 1.44e+01]    
10000     [3.18e+01, 3.58e+01, 2.70e+00]    [0.00e+00, 2.03e+01, 2.70e+00]    [2.00e+01, 2.03e+01, 1.54e+01, 1.50e+01]    
11000     [2.83e+01, 9.58e+00, 2.70e+00]    [0.00e+00, 2.09e+01, 2.70e+00]    [2.13e+01, 2.09e+01, 1.34e+01, 1.37e+01]    
12000     [2.79e+01, 8.30e+00, 2.70e+00]    [0.00e+00, 2.19e+01, 2.70e+00]    [2.24e+01, 2.19e+01, 1.30e+01, 1.29e+01]    
13000     [2.83e+01, 1.49e+01, 2.70e+00]    [0.00e+00, 1.98e+01, 2.70e+00]    [2.04e+01, 1.98e+01, 1.31e+01, 1.35e+01]    
14000     [2.89e+01, 1.56e+01, 2.70e+00]    [0.00e+00, 1.83e+01, 2.70e+00]    [1.87e+01, 1.83e+01, 1.42e+01, 1.44e+01]    
15000     [2.77e+01, 1.14e+01, 2.70e+00]    [0.00e+00, 1.84e+01, 2.70e+00]    [1.91e+01, 1.84e+01, 1.38e+01, 1.41e+01]    
16000     [2.72e+01, 9.89e+00, 2.70e+00]    [0.00e+00, 2.03e+01, 2.70e+00]    [2.10e+01, 2.03e+01, 1.28e+01, 1.28e+01]    
17000     [2.67e+01, 2.61e+00, 2.70e+00]    [0.00e+00, 1.87e+01, 2.70e+00]    [1.95e+01, 1.87e+01, 1.39e+01, 1.40e+01]    
18000     [2.92e+01, 2.74e+01, 2.69e+00]    [0.00e+00, 1.66e+01, 2.69e+00]    [1.74e+01, 1.66e+01, 1.47e+01, 1.54e+01]    
19000     [2.82e+01, 2.42e+01, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.09e+01, 2.01e+01, 1.58e+01, 1.51e+01]    
20000     [2.68e+01, 1.28e+01, 2.69e+00]    [0.00e+00, 1.80e+01, 2.69e+00]    [1.88e+01, 1.80e+01, 1.41e+01, 1.44e+01]    
21000     [2.63e+01, 4.36e+00, 2.69e+00]    [0.00e+00, 1.89e+01, 2.69e+00]    [1.97e+01, 1.89e+01, 1.37e+01, 1.39e+01]    
22000     [2.77e+01, 1.45e+01, 2.69e+00]    [0.00e+00, 1.86e+01, 2.69e+00]    [1.91e+01, 1.86e+01, 1.61e+01, 1.54e+01]    
23000     [2.64e+01, 4.58e+00, 2.69e+00]    [0.00e+00, 1.85e+01, 2.69e+00]    [1.94e+01, 1.85e+01, 1.50e+01, 1.49e+01]    
24000     [2.70e+01, 6.03e+00, 2.68e+00]    [0.00e+00, 1.78e+01, 2.68e+00]    [1.84e+01, 1.78e+01, 1.50e+01, 1.49e+01]    
25000     [2.65e+01, 8.76e+00, 2.68e+00]    [0.00e+00, 1.88e+01, 2.68e+00]    [1.97e+01, 1.88e+01, 1.51e+01, 1.49e+01]    
26000     [2.76e+01, 1.19e+01, 2.68e+00]    [0.00e+00, 1.74e+01, 2.68e+00]    [1.80e+01, 1.74e+01, 1.50e+01, 1.52e+01]    
27000     [2.57e+01, 4.59e+00, 2.68e+00]    [0.00e+00, 1.85e+01, 2.68e+00]    [1.95e+01, 1.85e+01, 1.45e+01, 1.46e+01]    
28000     [2.61e+01, 1.14e+01, 2.68e+00]    [0.00e+00, 1.82e+01, 2.68e+00]    [1.91e+01, 1.82e+01, 1.43e+01, 1.47e+01]    
29000     [2.62e+01, 6.54e+00, 2.68e+00]    [0.00e+00, 1.76e+01, 2.68e+00]    [1.83e+01, 1.76e+01, 1.54e+01, 1.54e+01]    
30000     [2.66e+01, 1.70e+01, 2.68e+00]    [0.00e+00, 1.71e+01, 2.68e+00]    [1.77e+01, 1.71e+01, 1.53e+01, 1.57e+01]    

Best model at step 17000:
  train loss: 3.20e+01
  test loss: 2.14e+01
  test metric: [1.95e+01, 1.87e+01, 1.39e+01, 1.40e+01]

'train' took 51.154755 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.157183 s

'compile' took 0.784686 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.71e+02, 1.53e+02, 2.79e+00]    [0.00e+00, 4.02e+02, 2.79e+00]    [4.02e+02, 4.02e+02, 5.66e+02, 5.66e+02]    
1000      [3.61e+01, 4.85e-01, 2.76e+00]    [0.00e+00, 3.75e+01, 2.76e+00]    [3.64e+01, 3.75e+01, 1.49e+01, 1.51e+01]    
2000      [3.50e+01, 1.92e+00, 2.73e+00]    [0.00e+00, 3.62e+01, 2.73e+00]    [3.50e+01, 3.62e+01, 2.00e+01, 2.21e+01]    
3000      [2.78e+01, 1.24e+00, 2.72e+00]    [0.00e+00, 2.05e+01, 2.72e+00]    [2.05e+01, 2.05e+01, 1.21e+01, 1.27e+01]    
4000      [2.76e+01, 6.68e-01, 2.70e+00]    [0.00e+00, 2.28e+01, 2.70e+00]    [2.21e+01, 2.28e+01, 1.26e+01, 1.26e+01]    
5000      [2.77e+01, 8.69e-01, 2.70e+00]    [0.00e+00, 1.95e+01, 2.70e+00]    [1.97e+01, 1.95e+01, 1.18e+01, 1.15e+01]    
6000      [3.00e+01, 5.91e-01, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.02e+01, 2.01e+01, 1.45e+01, 1.45e+01]    
7000      [2.76e+01, 4.77e-01, 2.69e+00]    [0.00e+00, 1.87e+01, 2.69e+00]    [1.88e+01, 1.87e+01, 1.34e+01, 1.38e+01]    
8000      [2.58e+01, 3.96e+00, 2.69e+00]    [0.00e+00, 1.81e+01, 2.69e+00]    [1.90e+01, 1.81e+01, 1.19e+01, 1.21e+01]    
9000      [2.60e+01, 8.83e-01, 2.68e+00]    [0.00e+00, 1.99e+01, 2.68e+00]    [2.09e+01, 1.99e+01, 1.28e+01, 1.28e+01]    
10000     [2.44e+01, 5.41e-01, 2.68e+00]    [0.00e+00, 1.77e+01, 2.68e+00]    [1.82e+01, 1.77e+01, 1.28e+01, 1.25e+01]    
11000     [2.71e+01, 1.91e+00, 2.68e+00]    [0.00e+00, 2.21e+01, 2.68e+00]    [2.32e+01, 2.21e+01, 1.29e+01, 1.22e+01]    
12000     [2.48e+01, 7.54e-01, 2.68e+00]    [0.00e+00, 1.80e+01, 2.68e+00]    [1.79e+01, 1.80e+01, 1.23e+01, 1.24e+01]    
13000     [2.54e+01, 2.15e+00, 2.67e+00]    [0.00e+00, 1.93e+01, 2.67e+00]    [1.90e+01, 1.93e+01, 1.22e+01, 1.27e+01]    
14000     [2.46e+01, 4.41e-01, 2.67e+00]    [0.00e+00, 1.85e+01, 2.67e+00]    [1.96e+01, 1.85e+01, 1.24e+01, 1.21e+01]    
15000     [2.41e+01, 1.84e+00, 2.67e+00]    [0.00e+00, 1.86e+01, 2.67e+00]    [1.84e+01, 1.86e+01, 1.18e+01, 1.10e+01]    
16000     [2.49e+01, 1.85e+00, 2.67e+00]    [0.00e+00, 1.87e+01, 2.67e+00]    [1.85e+01, 1.87e+01, 1.20e+01, 1.24e+01]    
17000     [2.73e+01, 1.09e-01, 2.66e+00]    [0.00e+00, 2.00e+01, 2.66e+00]    [1.91e+01, 2.00e+01, 1.72e+01, 1.79e+01]    
18000     [2.70e+01, 2.62e+00, 2.66e+00]    [0.00e+00, 2.12e+01, 2.66e+00]    [2.04e+01, 2.12e+01, 1.80e+01, 1.90e+01]    
19000     [2.43e+01, 7.68e-01, 2.66e+00]    [0.00e+00, 1.75e+01, 2.66e+00]    [1.73e+01, 1.75e+01, 1.21e+01, 1.19e+01]    
20000     [2.83e+01, 2.40e+00, 2.66e+00]    [0.00e+00, 2.25e+01, 2.66e+00]    [2.18e+01, 2.25e+01, 1.90e+01, 1.98e+01]    
21000     [2.68e+01, 1.70e+00, 2.65e+00]    [0.00e+00, 1.74e+01, 2.65e+00]    [1.71e+01, 1.74e+01, 1.40e+01, 1.41e+01]    
22000     [2.48e+01, 1.37e+00, 2.65e+00]    [0.00e+00, 2.05e+01, 2.65e+00]    [2.10e+01, 2.05e+01, 1.17e+01, 1.10e+01]    
23000     [2.35e+01, 1.70e+00, 2.65e+00]    [0.00e+00, 1.54e+01, 2.65e+00]    [1.56e+01, 1.54e+01, 1.32e+01, 1.28e+01]    
24000     [2.41e+01, 7.39e-01, 2.64e+00]    [0.00e+00, 1.98e+01, 2.64e+00]    [2.00e+01, 1.98e+01, 1.25e+01, 1.19e+01]    
25000     [2.61e+01, 9.93e-01, 2.64e+00]    [0.00e+00, 1.72e+01, 2.64e+00]    [1.73e+01, 1.72e+01, 1.46e+01, 1.41e+01]    
26000     [2.28e+01, 5.45e-01, 2.64e+00]    [0.00e+00, 1.59e+01, 2.64e+00]    [1.58e+01, 1.59e+01, 1.32e+01, 1.25e+01]    
27000     [2.41e+01, 2.17e+00, 2.63e+00]    [0.00e+00, 2.05e+01, 2.63e+00]    [2.03e+01, 2.05e+01, 1.34e+01, 1.34e+01]    
28000     [2.40e+01, 5.56e-02, 2.63e+00]    [0.00e+00, 1.55e+01, 2.63e+00]    [1.58e+01, 1.55e+01, 1.29e+01, 1.24e+01]    
29000     [2.51e+01, 7.54e-01, 2.63e+00]    [0.00e+00, 2.26e+01, 2.63e+00]    [2.20e+01, 2.26e+01, 1.42e+01, 1.47e+01]    
30000     [2.69e+01, 2.03e+00, 2.62e+00]    [0.00e+00, 1.82e+01, 2.62e+00]    [1.93e+01, 1.82e+01, 1.60e+01, 1.49e+01]    

Best model at step 26000:
  train loss: 2.60e+01
  test loss: 1.85e+01
  test metric: [1.58e+01, 1.59e+01, 1.32e+01, 1.25e+01]

'train' took 49.977221 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.228386 s

'compile' took 1.207767 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.91e+02, 2.41e+03, 2.70e+00]    [0.00e+00, 6.24e+02, 2.70e+00]    [6.24e+02, 6.24e+02, 1.59e+03, 1.59e+03]    
1000      [6.66e+01, 1.48e+01, 2.69e+00]    [0.00e+00, 7.58e+01, 2.69e+00]    [7.45e+01, 7.58e+01, 2.31e+01, 2.36e+01]    
2000      [4.85e+01, 9.49e+00, 2.69e+00]    [0.00e+00, 6.46e+01, 2.69e+00]    [6.29e+01, 6.46e+01, 3.77e+01, 4.19e+01]    
3000      [3.64e+01, 6.24e+00, 2.69e+00]    [0.00e+00, 4.80e+01, 2.69e+00]    [4.58e+01, 4.80e+01, 2.84e+01, 3.16e+01]    
4000      [3.41e+01, 2.00e+01, 2.69e+00]    [0.00e+00, 3.45e+01, 2.69e+00]    [3.43e+01, 3.45e+01, 2.24e+01, 2.21e+01]    
5000      [3.38e+01, 2.97e+01, 2.69e+00]    [0.00e+00, 3.21e+01, 2.69e+00]    [3.40e+01, 3.21e+01, 2.04e+01, 1.92e+01]    
6000      [3.04e+01, 2.17e+01, 2.69e+00]    [0.00e+00, 2.73e+01, 2.69e+00]    [2.89e+01, 2.73e+01, 1.81e+01, 1.93e+01]    
7000      [2.74e+01, 2.60e+00, 2.69e+00]    [0.00e+00, 2.81e+01, 2.69e+00]    [2.58e+01, 2.81e+01, 1.78e+01, 1.68e+01]    
8000      [2.84e+01, 1.67e+01, 2.68e+00]    [0.00e+00, 2.48e+01, 2.68e+00]    [2.71e+01, 2.48e+01, 1.68e+01, 1.77e+01]    
9000      [3.18e+01, 2.81e+01, 2.68e+00]    [0.00e+00, 3.42e+01, 2.68e+00]    [3.12e+01, 3.42e+01, 2.36e+01, 3.01e+01]    
10000     [2.93e+01, 2.72e+01, 2.68e+00]    [0.00e+00, 2.51e+01, 2.68e+00]    [2.75e+01, 2.51e+01, 1.94e+01, 1.67e+01]    
11000     [2.97e+01, 2.19e+01, 2.68e+00]    [0.00e+00, 3.15e+01, 2.68e+00]    [2.81e+01, 3.15e+01, 2.18e+01, 2.85e+01]    
12000     [2.98e+01, 2.43e+01, 2.68e+00]    [0.00e+00, 3.07e+01, 2.68e+00]    [2.73e+01, 3.07e+01, 2.16e+01, 2.84e+01]    
13000     [2.94e+01, 2.29e+01, 2.69e+00]    [0.00e+00, 3.02e+01, 2.69e+00]    [2.68e+01, 3.02e+01, 2.13e+01, 2.80e+01]    
14000     [2.63e+01, 1.25e+01, 2.69e+00]    [0.00e+00, 2.74e+01, 2.69e+00]    [2.37e+01, 2.74e+01, 1.80e+01, 2.31e+01]    
15000     [2.47e+01, 1.32e+01, 2.69e+00]    [0.00e+00, 2.34e+01, 2.69e+00]    [1.97e+01, 2.34e+01, 1.85e+01, 1.77e+01]    
16000     [2.51e+01, 1.27e+01, 2.69e+00]    [0.00e+00, 2.18e+01, 2.69e+00]    [2.04e+01, 2.18e+01, 1.77e+01, 1.72e+01]    
17000     [2.47e+01, 1.69e+01, 2.70e+00]    [0.00e+00, 2.26e+01, 2.70e+00]    [1.89e+01, 2.26e+01, 1.85e+01, 1.82e+01]    
18000     [2.51e+01, 7.59e+00, 2.70e+00]    [0.00e+00, 2.73e+01, 2.70e+00]    [2.33e+01, 2.73e+01, 1.98e+01, 2.73e+01]    
19000     [2.56e+01, 2.19e+01, 2.70e+00]    [0.00e+00, 2.17e+01, 2.70e+00]    [2.02e+01, 2.17e+01, 1.72e+01, 1.70e+01]    
20000     [2.38e+01, 1.35e+01, 2.71e+00]    [0.00e+00, 2.38e+01, 2.71e+00]    [1.95e+01, 2.38e+01, 1.74e+01, 1.90e+01]    
21000     [2.37e+01, 1.61e+00, 2.71e+00]    [0.00e+00, 2.65e+01, 2.71e+00]    [2.26e+01, 2.65e+01, 1.82e+01, 2.60e+01]    
22000     [2.58e+01, 2.32e+01, 2.71e+00]    [0.00e+00, 2.18e+01, 2.71e+00]    [2.02e+01, 2.18e+01, 1.68e+01, 1.69e+01]    
23000     [2.38e+01, 2.64e+00, 2.71e+00]    [0.00e+00, 2.72e+01, 2.71e+00]    [2.24e+01, 2.72e+01, 1.87e+01, 2.74e+01]    
24000     [2.45e+01, 6.20e+00, 2.71e+00]    [0.00e+00, 2.81e+01, 2.71e+00]    [2.33e+01, 2.81e+01, 2.03e+01, 3.01e+01]    
25000     [2.53e+01, 2.07e+01, 2.70e+00]    [0.00e+00, 2.14e+01, 2.70e+00]    [2.01e+01, 2.14e+01, 1.66e+01, 1.67e+01]    
26000     [2.39e+01, 4.40e+00, 2.70e+00]    [0.00e+00, 2.77e+01, 2.70e+00]    [2.25e+01, 2.77e+01, 1.92e+01, 2.93e+01]    
27000     [2.39e+01, 4.77e+00, 2.70e+00]    [0.00e+00, 2.78e+01, 2.70e+00]    [2.26e+01, 2.78e+01, 1.94e+01, 2.98e+01]    
28000     [2.46e+01, 2.10e+01, 2.70e+00]    [0.00e+00, 2.25e+01, 2.70e+00]    [1.90e+01, 2.25e+01, 1.67e+01, 1.77e+01]    
29000     [2.45e+01, 8.02e+00, 2.70e+00]    [0.00e+00, 2.88e+01, 2.70e+00]    [2.34e+01, 2.88e+01, 2.07e+01, 3.22e+01]    
30000     [2.31e+01, 1.58e+01, 2.70e+00]    [0.00e+00, 2.39e+01, 2.70e+00]    [1.86e+01, 2.39e+01, 1.70e+01, 1.99e+01]    

Best model at step 21000:
  train loss: 2.80e+01
  test loss: 2.92e+01
  test metric: [2.26e+01, 2.65e+01, 1.82e+01, 2.60e+01]

'train' took 49.244574 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.202585 s

'compile' took 1.002771 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.66e+02, 1.23e+03, 2.74e+00]    [0.00e+00, 8.58e+02, 2.74e+00]    [8.58e+02, 8.58e+02, 1.75e+03, 1.75e+03]    
1000      [4.66e+01, 4.76e+00, 2.74e+00]    [0.00e+00, 5.55e+01, 2.74e+00]    [5.50e+01, 5.55e+01, 3.02e+01, 2.92e+01]    
2000      [3.33e+01, 5.33e+00, 2.74e+00]    [0.00e+00, 4.22e+01, 2.74e+00]    [4.17e+01, 4.22e+01, 2.03e+01, 2.12e+01]    
3000      [2.87e+01, 3.09e+00, 2.74e+00]    [0.00e+00, 3.41e+01, 2.74e+00]    [3.36e+01, 3.41e+01, 1.58e+01, 1.57e+01]    
4000      [2.65e+01, 8.23e-01, 2.73e+00]    [0.00e+00, 2.73e+01, 2.73e+00]    [2.67e+01, 2.73e+01, 1.64e+01, 1.51e+01]    
5000      [2.60e+01, 3.68e-01, 2.73e+00]    [0.00e+00, 2.50e+01, 2.73e+00]    [2.47e+01, 2.50e+01, 1.48e+01, 1.39e+01]    
6000      [2.61e+01, 5.36e+00, 2.73e+00]    [0.00e+00, 2.48e+01, 2.73e+00]    [2.62e+01, 2.48e+01, 1.38e+01, 1.40e+01]    
7000      [2.61e+01, 7.11e+00, 2.73e+00]    [0.00e+00, 2.48e+01, 2.73e+00]    [2.62e+01, 2.48e+01, 1.44e+01, 1.38e+01]    
8000      [2.83e+01, 1.43e+01, 2.72e+00]    [0.00e+00, 2.80e+01, 2.72e+00]    [2.97e+01, 2.80e+01, 1.90e+01, 1.62e+01]    
9000      [2.78e+01, 1.40e+01, 2.72e+00]    [0.00e+00, 2.71e+01, 2.72e+00]    [2.88e+01, 2.71e+01, 1.79e+01, 1.53e+01]    
10000     [2.72e+01, 1.33e+01, 2.72e+00]    [0.00e+00, 2.62e+01, 2.72e+00]    [2.80e+01, 2.62e+01, 1.69e+01, 1.46e+01]    
11000     [2.72e+01, 1.42e+01, 2.72e+00]    [0.00e+00, 2.60e+01, 2.72e+00]    [2.77e+01, 2.60e+01, 1.66e+01, 1.45e+01]    
12000     [2.51e+01, 6.50e+00, 2.72e+00]    [0.00e+00, 2.33e+01, 2.72e+00]    [2.52e+01, 2.33e+01, 1.43e+01, 1.38e+01]    
13000     [2.47e+01, 6.65e+00, 2.71e+00]    [0.00e+00, 2.28e+01, 2.71e+00]    [2.48e+01, 2.28e+01, 1.45e+01, 1.42e+01]    
14000     [2.53e+01, 1.02e+01, 2.71e+00]    [0.00e+00, 2.29e+01, 2.71e+00]    [2.51e+01, 2.29e+01, 1.47e+01, 1.45e+01]    
15000     [2.54e+01, 1.13e+01, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [2.53e+01, 2.30e+01, 1.52e+01, 1.44e+01]    
16000     [2.43e+01, 2.98e+00, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [2.14e+01, 2.30e+01, 1.45e+01, 1.50e+01]    
17000     [2.58e+01, 1.17e+01, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [2.54e+01, 2.30e+01, 1.58e+01, 1.46e+01]    
18000     [2.57e+01, 1.16e+01, 2.71e+00]    [0.00e+00, 2.26e+01, 2.71e+00]    [2.51e+01, 2.26e+01, 1.60e+01, 1.52e+01]    
19000     [2.42e+01, 1.68e+00, 2.71e+00]    [0.00e+00, 2.47e+01, 2.71e+00]    [2.28e+01, 2.47e+01, 1.48e+01, 1.77e+01]    
20000     [2.44e+01, 3.79e+00, 2.71e+00]    [0.00e+00, 2.44e+01, 2.71e+00]    [2.24e+01, 2.44e+01, 1.55e+01, 1.91e+01]    
21000     [2.48e+01, 1.03e+01, 2.71e+00]    [0.00e+00, 2.12e+01, 2.71e+00]    [2.38e+01, 2.12e+01, 1.60e+01, 1.60e+01]    
22000     [2.58e+01, 7.89e+00, 2.71e+00]    [0.00e+00, 2.56e+01, 2.71e+00]    [2.34e+01, 2.56e+01, 1.82e+01, 2.40e+01]    
23000     [2.50e+01, 1.21e+01, 2.71e+00]    [0.00e+00, 2.12e+01, 2.71e+00]    [2.41e+01, 2.12e+01, 1.66e+01, 1.60e+01]    
24000     [2.40e+01, 3.45e+00, 2.71e+00]    [0.00e+00, 2.44e+01, 2.71e+00]    [2.21e+01, 2.44e+01, 1.63e+01, 2.10e+01]    
25000     [2.23e+01, 4.08e+00, 2.71e+00]    [0.00e+00, 2.22e+01, 2.71e+00]    [2.06e+01, 2.22e+01, 1.64e+01, 1.60e+01]    
26000     [2.46e+01, 5.49e+00, 2.71e+00]    [0.00e+00, 2.47e+01, 2.71e+00]    [2.21e+01, 2.47e+01, 1.67e+01, 2.23e+01]    
27000     [2.42e+01, 5.16e+00, 2.71e+00]    [0.00e+00, 2.56e+01, 2.71e+00]    [2.29e+01, 2.56e+01, 1.72e+01, 2.32e+01]    
28000     [2.43e+01, 9.46e+00, 2.71e+00]    [0.00e+00, 2.26e+01, 2.71e+00]    [2.30e+01, 2.26e+01, 1.63e+01, 1.59e+01]    
29000     [2.38e+01, 4.04e+00, 2.71e+00]    [0.00e+00, 2.45e+01, 2.71e+00]    [2.17e+01, 2.45e+01, 1.63e+01, 2.21e+01]    
30000     [2.66e+01, 1.52e+01, 2.71e+00]    [0.00e+00, 2.26e+01, 2.71e+00]    [2.60e+01, 2.26e+01, 1.89e+01, 1.65e+01]    

Best model at step 19000:
  train loss: 2.86e+01
  test loss: 2.74e+01
  test metric: [2.28e+01, 2.47e+01, 1.48e+01, 1.77e+01]

'train' took 53.319661 s

[16.849555837455704, 116.47817312647364, 34.30129060979892, 19.88050271085875, 24.951070112898577, 58.705970077492566, 18.7478277961816, 15.857780635408323, 26.521462484408104, 24.70070900101798]
sigma_y 2 35.69943423919942 29.434332498095635
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.210361 s

'compile' took 0.910093 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.39e+02, 6.70e+02, 2.86e+00]    [0.00e+00, 3.84e+02, 2.86e+00]    [3.84e+02, 3.84e+02, 8.51e+02, 8.51e+02]    
1000      [5.03e+01, 2.28e+01, 2.85e+00]    [0.00e+00, 6.22e+01, 2.85e+00]    [6.10e+01, 6.22e+01, 2.64e+01, 2.76e+01]    
2000      [3.52e+01, 1.32e+01, 2.86e+00]    [0.00e+00, 3.98e+01, 2.86e+00]    [3.82e+01, 3.98e+01, 2.73e+01, 2.57e+01]    
3000      [3.46e+01, 4.32e+00, 2.87e+00]    [0.00e+00, 3.75e+01, 2.87e+00]    [3.55e+01, 3.75e+01, 2.43e+01, 2.54e+01]    
4000      [3.30e+01, 4.26e+00, 2.87e+00]    [0.00e+00, 3.59e+01, 2.87e+00]    [3.39e+01, 3.59e+01, 2.08e+01, 2.22e+01]    
5000      [3.21e+01, 1.98e+00, 2.87e+00]    [0.00e+00, 2.73e+01, 2.87e+00]    [2.69e+01, 2.73e+01, 2.06e+01, 2.00e+01]    
6000      [3.31e+01, 1.30e+01, 2.87e+00]    [0.00e+00, 2.79e+01, 2.87e+00]    [2.97e+01, 2.79e+01, 1.81e+01, 1.70e+01]    
7000      [3.04e+01, 7.85e+00, 2.87e+00]    [0.00e+00, 2.43e+01, 2.87e+00]    [2.61e+01, 2.43e+01, 1.55e+01, 1.65e+01]    
8000      [3.04e+01, 9.13e+00, 2.86e+00]    [0.00e+00, 2.47e+01, 2.86e+00]    [2.64e+01, 2.47e+01, 1.55e+01, 1.52e+01]    
9000      [2.91e+01, 6.21e+00, 2.86e+00]    [0.00e+00, 2.28e+01, 2.86e+00]    [2.28e+01, 2.28e+01, 1.66e+01, 1.64e+01]    
10000     [2.93e+01, 5.25e+00, 2.86e+00]    [0.00e+00, 2.72e+01, 2.86e+00]    [2.55e+01, 2.72e+01, 1.57e+01, 1.78e+01]    
11000     [3.24e+01, 1.80e+01, 2.86e+00]    [0.00e+00, 2.51e+01, 2.86e+00]    [2.71e+01, 2.51e+01, 1.90e+01, 1.61e+01]    
12000     [3.06e+01, 1.42e+01, 2.86e+00]    [0.00e+00, 2.31e+01, 2.86e+00]    [2.53e+01, 2.31e+01, 1.57e+01, 1.48e+01]    
13000     [2.80e+01, 3.77e+00, 2.86e+00]    [0.00e+00, 2.63e+01, 2.86e+00]    [2.43e+01, 2.63e+01, 1.54e+01, 1.81e+01]    
14000     [2.75e+01, 3.75e+00, 2.86e+00]    [0.00e+00, 2.55e+01, 2.86e+00]    [2.33e+01, 2.55e+01, 1.47e+01, 1.69e+01]    
15000     [2.85e+01, 6.51e+00, 2.85e+00]    [0.00e+00, 2.71e+01, 2.85e+00]    [2.49e+01, 2.71e+01, 1.60e+01, 2.01e+01]    
16000     [2.76e+01, 3.85e+00, 2.85e+00]    [0.00e+00, 2.61e+01, 2.85e+00]    [2.39e+01, 2.61e+01, 1.51e+01, 1.86e+01]    
17000     [2.75e+01, 3.71e+00, 2.85e+00]    [0.00e+00, 2.73e+01, 2.85e+00]    [2.50e+01, 2.73e+01, 1.30e+01, 1.65e+01]    
18000     [2.70e+01, 3.46e+00, 2.85e+00]    [0.00e+00, 2.54e+01, 2.85e+00]    [2.30e+01, 2.54e+01, 1.48e+01, 1.83e+01]    
19000     [2.79e+01, 1.13e+01, 2.85e+00]    [0.00e+00, 2.06e+01, 2.85e+00]    [2.14e+01, 2.06e+01, 1.44e+01, 1.45e+01]    
20000     [2.87e+01, 9.05e+00, 2.84e+00]    [0.00e+00, 2.65e+01, 2.84e+00]    [2.42e+01, 2.65e+01, 1.71e+01, 2.21e+01]    
21000     [2.66e+01, 3.63e+00, 2.84e+00]    [0.00e+00, 2.48e+01, 2.84e+00]    [2.24e+01, 2.48e+01, 1.47e+01, 1.81e+01]    
22000     [2.62e+01, 6.95e+00, 2.84e+00]    [0.00e+00, 2.07e+01, 2.84e+00]    [1.99e+01, 2.07e+01, 1.51e+01, 1.45e+01]    
23000     [2.72e+01, 8.02e+00, 2.84e+00]    [0.00e+00, 1.94e+01, 2.84e+00]    [2.21e+01, 1.94e+01, 1.55e+01, 1.50e+01]    
24000     [2.71e+01, 5.70e+00, 2.84e+00]    [0.00e+00, 2.55e+01, 2.84e+00]    [2.29e+01, 2.55e+01, 1.56e+01, 2.02e+01]    
25000     [2.67e+01, 8.79e+00, 2.84e+00]    [0.00e+00, 1.95e+01, 2.84e+00]    [2.12e+01, 1.95e+01, 1.47e+01, 1.50e+01]    
26000     [2.70e+01, 6.05e+00, 2.83e+00]    [0.00e+00, 2.44e+01, 2.83e+00]    [2.18e+01, 2.44e+01, 1.58e+01, 2.00e+01]    
27000     [2.60e+01, 8.67e+00, 2.83e+00]    [0.00e+00, 2.07e+01, 2.83e+00]    [1.97e+01, 2.07e+01, 1.52e+01, 1.46e+01]    
28000     [2.59e+01, 3.20e+00, 2.83e+00]    [0.00e+00, 2.47e+01, 2.83e+00]    [2.20e+01, 2.47e+01, 1.58e+01, 2.04e+01]    
29000     [2.59e+01, 4.53e+00, 2.83e+00]    [0.00e+00, 2.49e+01, 2.83e+00]    [2.20e+01, 2.49e+01, 1.60e+01, 2.10e+01]    
30000     [2.71e+01, 6.86e+00, 2.83e+00]    [0.00e+00, 2.65e+01, 2.83e+00]    [2.36e+01, 2.65e+01, 1.82e+01, 2.48e+01]    

Best model at step 28000:
  train loss: 3.19e+01
  test loss: 2.76e+01
  test metric: [2.20e+01, 2.47e+01, 1.58e+01, 2.04e+01]

'train' took 54.115301 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.270657 s

'compile' took 1.166082 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.17e+02, 9.96e+01, 2.70e+00]    [0.00e+00, 3.09e+02, 2.70e+00]    [3.09e+02, 3.09e+02, 3.67e+02, 3.67e+02]    
1000      [3.52e+01, 2.90e+00, 2.71e+00]    [0.00e+00, 1.94e+01, 2.71e+00]    [1.54e+01, 1.94e+01, 1.22e+01, 1.53e+01]    
2000      [3.09e+01, 8.48e-02, 2.72e+00]    [0.00e+00, 4.43e+01, 2.72e+00]    [1.73e+01, 4.43e+01, 1.38e+01, 5.99e+01]    
3000      [2.91e+01, 7.99e-02, 2.73e+00]    [0.00e+00, 6.11e+01, 2.73e+00]    [1.99e+01, 6.11e+01, 1.42e+01, 8.58e+01]    
4000      [2.92e+01, 3.09e-01, 2.73e+00]    [0.00e+00, 7.52e+01, 2.73e+00]    [2.07e+01, 7.52e+01, 1.39e+01, 1.06e+02]    
5000      [2.67e+01, 1.16e-01, 2.74e+00]    [0.00e+00, 7.49e+01, 2.74e+00]    [1.87e+01, 7.49e+01, 1.46e+01, 1.04e+02]    
6000      [2.68e+01, 1.95e-01, 2.74e+00]    [0.00e+00, 7.68e+01, 2.74e+00]    [1.98e+01, 7.68e+01, 1.59e+01, 1.07e+02]    
7000      [2.94e+01, 5.08e-01, 2.74e+00]    [0.00e+00, 9.55e+01, 2.74e+00]    [1.88e+01, 9.55e+01, 1.67e+01, 1.35e+02]    
8000      [2.70e+01, 2.28e-01, 2.74e+00]    [0.00e+00, 8.06e+01, 2.74e+00]    [2.15e+01, 8.06e+01, 1.55e+01, 1.11e+02]    
9000      [3.18e+01, 6.38e-01, 2.74e+00]    [0.00e+00, 7.25e+01, 2.74e+00]    [2.97e+01, 7.25e+01, 2.57e+01, 9.78e+01]    
10000     [2.42e+01, 1.31e-01, 2.74e+00]    [0.00e+00, 8.88e+01, 2.74e+00]    [1.85e+01, 8.88e+01, 1.23e+01, 1.25e+02]    
11000     [2.41e+01, 6.85e-02, 2.73e+00]    [0.00e+00, 8.77e+01, 2.73e+00]    [1.78e+01, 8.77e+01, 1.06e+01, 1.21e+02]    
12000     [2.73e+01, 4.99e-01, 2.73e+00]    [0.00e+00, 9.24e+01, 2.73e+00]    [1.69e+01, 9.24e+01, 1.58e+01, 1.28e+02]    
13000     [2.38e+01, 8.56e-02, 2.73e+00]    [0.00e+00, 8.06e+01, 2.73e+00]    [1.72e+01, 8.06e+01, 9.75e+00, 1.09e+02]    
14000     [2.57e+01, 2.95e-01, 2.73e+00]    [0.00e+00, 8.18e+01, 2.73e+00]    [1.69e+01, 8.18e+01, 1.39e+01, 1.11e+02]    
15000     [2.41e+01, 2.58e-01, 2.73e+00]    [0.00e+00, 6.41e+01, 2.73e+00]    [1.78e+01, 6.41e+01, 1.28e+01, 8.11e+01]    
16000     [2.45e+01, 1.97e-01, 2.73e+00]    [0.00e+00, 5.97e+01, 2.73e+00]    [1.89e+01, 5.97e+01, 1.34e+01, 7.22e+01]    
17000     [2.25e+01, 1.10e-01, 2.73e+00]    [0.00e+00, 6.16e+01, 2.73e+00]    [1.53e+01, 6.16e+01, 1.03e+01, 7.23e+01]    
18000     [2.20e+01, 2.03e-01, 2.73e+00]    [0.00e+00, 5.87e+01, 2.73e+00]    [1.44e+01, 5.87e+01, 1.07e+01, 6.88e+01]    
19000     [2.37e+01, 1.92e-01, 2.73e+00]    [0.00e+00, 6.11e+01, 2.73e+00]    [1.50e+01, 6.11e+01, 1.02e+01, 7.19e+01]    
20000     [2.63e+01, 5.30e-01, 2.73e+00]    [0.00e+00, 4.49e+01, 2.73e+00]    [2.21e+01, 4.49e+01, 1.80e+01, 4.14e+01]    
21000     [2.35e+01, 2.78e-01, 2.73e+00]    [0.00e+00, 4.76e+01, 2.73e+00]    [1.73e+01, 4.76e+01, 1.23e+01, 4.63e+01]    
22000     [2.22e+01, 2.09e-01, 2.73e+00]    [0.00e+00, 4.78e+01, 2.73e+00]    [1.26e+01, 4.78e+01, 1.00e+01, 4.93e+01]    
23000     [2.48e+01, 3.46e-01, 2.72e+00]    [0.00e+00, 5.49e+01, 2.72e+00]    [1.41e+01, 5.49e+01, 1.42e+01, 5.94e+01]    
24000     [2.17e+01, 1.47e-01, 2.72e+00]    [0.00e+00, 4.80e+01, 2.72e+00]    [1.32e+01, 4.80e+01, 9.02e+00, 4.38e+01]    
25000     [2.35e+01, 2.68e-01, 2.72e+00]    [0.00e+00, 5.09e+01, 2.72e+00]    [1.42e+01, 5.09e+01, 1.19e+01, 4.84e+01]    
26000     [2.28e+01, 3.19e-01, 2.72e+00]    [0.00e+00, 4.10e+01, 2.72e+00]    [1.66e+01, 4.10e+01, 1.24e+01, 3.19e+01]    
27000     [2.37e+01, 3.05e-01, 2.72e+00]    [0.00e+00, 5.00e+01, 2.72e+00]    [1.46e+01, 5.00e+01, 1.40e+01, 4.33e+01]    
28000     [2.32e+01, 3.99e-01, 2.72e+00]    [0.00e+00, 3.99e+01, 2.72e+00]    [1.72e+01, 3.99e+01, 1.35e+01, 2.94e+01]    
29000     [2.31e+01, 4.80e-01, 2.72e+00]    [0.00e+00, 4.38e+01, 2.72e+00]    [1.23e+01, 4.38e+01, 1.02e+01, 3.33e+01]    
30000     [2.25e+01, 3.11e-01, 2.72e+00]    [0.00e+00, 4.15e+01, 2.72e+00]    [1.66e+01, 4.15e+01, 1.22e+01, 3.13e+01]    

Best model at step 24000:
  train loss: 2.45e+01
  test loss: 5.07e+01
  test metric: [1.32e+01, 4.80e+01, 9.02e+00, 4.38e+01]

'train' took 54.190135 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.170560 s

'compile' took 0.883194 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.88e+02, 4.57e+02, 2.57e+00]    [0.00e+00, 4.49e+02, 2.57e+00]    [4.49e+02, 4.49e+02, 5.18e+02, 5.18e+02]    
1000      [3.64e+01, 3.64e+00, 2.55e+00]    [0.00e+00, 3.18e+01, 2.55e+00]    [3.23e+01, 3.18e+01, 2.40e+01, 2.37e+01]    
2000      [3.28e+01, 4.38e-01, 2.55e+00]    [0.00e+00, 2.51e+01, 2.55e+00]    [2.80e+01, 2.51e+01, 1.51e+01, 1.41e+01]    
3000      [3.01e+01, 2.48e+00, 2.54e+00]    [0.00e+00, 2.00e+01, 2.54e+00]    [2.18e+01, 2.00e+01, 1.59e+01, 1.49e+01]    
4000      [2.77e+01, 5.92e-01, 2.54e+00]    [0.00e+00, 1.79e+01, 2.54e+00]    [1.86e+01, 1.79e+01, 1.52e+01, 1.59e+01]    
5000      [2.75e+01, 2.27e-01, 2.54e+00]    [0.00e+00, 1.54e+01, 2.54e+00]    [1.99e+01, 1.54e+01, 1.43e+01, 1.52e+01]    
6000      [2.68e+01, 9.69e-01, 2.54e+00]    [0.00e+00, 1.46e+01, 2.54e+00]    [1.98e+01, 1.46e+01, 1.42e+01, 1.50e+01]    
7000      [2.53e+01, 9.60e-01, 2.53e+00]    [0.00e+00, 1.73e+01, 2.53e+00]    [1.71e+01, 1.73e+01, 1.45e+01, 1.36e+01]    
8000      [2.55e+01, 2.12e+00, 2.53e+00]    [0.00e+00, 1.75e+01, 2.53e+00]    [1.72e+01, 1.75e+01, 1.43e+01, 1.27e+01]    
9000      [2.53e+01, 1.81e+00, 2.53e+00]    [0.00e+00, 1.85e+01, 2.53e+00]    [1.68e+01, 1.85e+01, 1.42e+01, 1.21e+01]    
10000     [2.46e+01, 2.80e-01, 2.53e+00]    [0.00e+00, 2.01e+01, 2.53e+00]    [1.65e+01, 2.01e+01, 1.40e+01, 1.16e+01]    
11000     [2.54e+01, 9.11e-01, 2.53e+00]    [0.00e+00, 2.40e+01, 2.53e+00]    [1.66e+01, 2.40e+01, 1.42e+01, 1.52e+01]    
12000     [2.50e+01, 1.95e+00, 2.53e+00]    [0.00e+00, 1.92e+01, 2.53e+00]    [1.66e+01, 1.92e+01, 1.39e+01, 1.17e+01]    
13000     [2.49e+01, 8.92e-01, 2.53e+00]    [0.00e+00, 1.79e+01, 2.53e+00]    [1.82e+01, 1.79e+01, 1.33e+01, 1.10e+01]    
14000     [2.73e+01, 7.39e-01, 2.52e+00]    [0.00e+00, 1.53e+01, 2.52e+00]    [2.24e+01, 1.53e+01, 1.48e+01, 1.17e+01]    
15000     [2.43e+01, 1.05e+00, 2.52e+00]    [0.00e+00, 1.87e+01, 2.52e+00]    [1.70e+01, 1.87e+01, 1.34e+01, 1.10e+01]    
16000     [2.94e+01, 2.46e+00, 2.52e+00]    [0.00e+00, 3.30e+01, 2.52e+00]    [2.35e+01, 3.30e+01, 1.91e+01, 2.86e+01]    
17000     [2.47e+01, 7.51e-01, 2.52e+00]    [0.00e+00, 1.76e+01, 2.52e+00]    [1.83e+01, 1.76e+01, 1.29e+01, 1.05e+01]    
18000     [2.70e+01, 1.12e+00, 2.52e+00]    [0.00e+00, 1.49e+01, 2.52e+00]    [2.22e+01, 1.49e+01, 1.33e+01, 1.13e+01]    
19000     [2.50e+01, 1.30e+00, 2.52e+00]    [0.00e+00, 2.50e+01, 2.52e+00]    [1.73e+01, 2.50e+01, 1.33e+01, 1.50e+01]    
20000     [2.34e+01, 1.24e+00, 2.51e+00]    [0.00e+00, 2.05e+01, 2.51e+00]    [1.69e+01, 2.05e+01, 1.28e+01, 9.96e+00]    
21000     [2.39e+01, 1.79e+00, 2.51e+00]    [0.00e+00, 2.17e+01, 2.51e+00]    [1.71e+01, 2.17e+01, 1.29e+01, 1.14e+01]    
22000     [2.34e+01, 1.65e+00, 2.51e+00]    [0.00e+00, 2.10e+01, 2.51e+00]    [1.64e+01, 2.10e+01, 1.33e+01, 1.02e+01]    
23000     [2.58e+01, 1.25e+00, 2.51e+00]    [0.00e+00, 2.45e+01, 2.51e+00]    [1.64e+01, 2.45e+01, 1.35e+01, 1.58e+01]    
24000     [2.33e+01, 1.03e+00, 2.50e+00]    [0.00e+00, 1.95e+01, 2.50e+00]    [1.52e+01, 1.95e+01, 1.35e+01, 1.13e+01]    
25000     [2.46e+01, 1.05e+00, 2.50e+00]    [0.00e+00, 1.62e+01, 2.50e+00]    [1.89e+01, 1.62e+01, 1.20e+01, 1.03e+01]    
26000     [2.47e+01, 1.04e+00, 2.50e+00]    [0.00e+00, 1.55e+01, 2.50e+00]    [1.95e+01, 1.55e+01, 1.19e+01, 1.08e+01]    
27000     [2.56e+01, 1.03e+00, 2.50e+00]    [0.00e+00, 1.65e+01, 2.50e+00]    [2.14e+01, 1.65e+01, 1.38e+01, 1.09e+01]    
28000     [2.40e+01, 6.95e-01, 2.49e+00]    [0.00e+00, 1.61e+01, 2.49e+00]    [1.87e+01, 1.61e+01, 1.21e+01, 1.06e+01]    
29000     [2.29e+01, 3.41e-01, 2.49e+00]    [0.00e+00, 1.86e+01, 2.49e+00]    [1.49e+01, 1.86e+01, 1.36e+01, 1.18e+01]    
30000     [2.24e+01, 4.39e-02, 2.49e+00]    [0.00e+00, 1.70e+01, 2.49e+00]    [1.62e+01, 1.70e+01, 1.28e+01, 1.20e+01]    

Best model at step 30000:
  train loss: 2.50e+01
  test loss: 1.95e+01
  test metric: [1.62e+01, 1.70e+01, 1.28e+01, 1.20e+01]

'train' took 51.660822 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.170888 s

'compile' took 0.806439 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.67e+02, 3.49e+02, 2.78e+00]    [0.00e+00, 1.36e+02, 2.78e+00]    [1.36e+02, 1.36e+02, 5.69e+01, 5.69e+01]    
1000      [5.43e+01, 3.55e+01, 2.76e+00]    [0.00e+00, 6.69e+01, 2.76e+00]    [6.67e+01, 6.69e+01, 3.50e+01, 3.48e+01]    
2000      [4.25e+01, 2.07e+01, 2.76e+00]    [0.00e+00, 4.77e+01, 2.76e+00]    [4.72e+01, 4.77e+01, 2.79e+01, 2.77e+01]    
3000      [3.69e+01, 3.01e+00, 2.76e+00]    [0.00e+00, 3.96e+01, 2.76e+00]    [3.91e+01, 3.96e+01, 2.29e+01, 2.32e+01]    
4000      [3.37e+01, 1.89e+00, 2.76e+00]    [0.00e+00, 3.19e+01, 2.76e+00]    [3.12e+01, 3.19e+01, 1.89e+01, 1.92e+01]    
5000      [3.17e+01, 1.29e+01, 2.76e+00]    [0.00e+00, 2.54e+01, 2.76e+00]    [2.47e+01, 2.54e+01, 1.65e+01, 1.63e+01]    
6000      [3.08e+01, 7.98e+00, 2.75e+00]    [0.00e+00, 2.41e+01, 2.75e+00]    [2.33e+01, 2.41e+01, 1.53e+01, 1.54e+01]    
7000      [3.25e+01, 1.21e+01, 2.75e+00]    [0.00e+00, 2.61e+01, 2.75e+00]    [2.52e+01, 2.61e+01, 1.70e+01, 1.79e+01]    
8000      [2.98e+01, 1.17e+01, 2.75e+00]    [0.00e+00, 1.88e+01, 2.75e+00]    [1.79e+01, 1.88e+01, 1.30e+01, 1.27e+01]    
9000      [2.98e+01, 1.34e+01, 2.74e+00]    [0.00e+00, 1.76e+01, 2.74e+00]    [1.68e+01, 1.76e+01, 1.27e+01, 1.23e+01]    
10000     [3.10e+01, 1.37e+01, 2.74e+00]    [0.00e+00, 2.29e+01, 2.74e+00]    [2.20e+01, 2.29e+01, 1.56e+01, 1.69e+01]    
11000     [2.83e+01, 7.65e+00, 2.74e+00]    [0.00e+00, 1.81e+01, 2.74e+00]    [1.71e+01, 1.81e+01, 1.24e+01, 1.23e+01]    
12000     [2.88e+01, 1.11e+01, 2.73e+00]    [0.00e+00, 1.70e+01, 2.73e+00]    [1.60e+01, 1.70e+01, 1.25e+01, 1.21e+01]    
13000     [2.96e+01, 1.21e+01, 2.73e+00]    [0.00e+00, 2.18e+01, 2.73e+00]    [2.07e+01, 2.18e+01, 1.49e+01, 1.61e+01]    
14000     [2.79e+01, 7.94e+00, 2.72e+00]    [0.00e+00, 1.74e+01, 2.72e+00]    [1.63e+01, 1.74e+01, 1.25e+01, 1.23e+01]    
15000     [2.84e+01, 1.08e+01, 2.72e+00]    [0.00e+00, 1.66e+01, 2.72e+00]    [1.55e+01, 1.66e+01, 1.26e+01, 1.21e+01]    
16000     [2.97e+01, 1.51e+01, 2.72e+00]    [0.00e+00, 2.22e+01, 2.72e+00]    [2.10e+01, 2.22e+01, 1.53e+01, 1.67e+01]    
17000     [2.81e+01, 1.05e+01, 2.71e+00]    [0.00e+00, 1.68e+01, 2.71e+00]    [1.56e+01, 1.68e+01, 1.26e+01, 1.21e+01]    
18000     [2.75e+01, 8.61e+00, 2.71e+00]    [0.00e+00, 1.72e+01, 2.71e+00]    [1.59e+01, 1.72e+01, 1.25e+01, 1.21e+01]    
19000     [2.88e+01, 1.41e+01, 2.70e+00]    [0.00e+00, 2.17e+01, 2.70e+00]    [2.04e+01, 2.17e+01, 1.48e+01, 1.64e+01]    
20000     [2.67e+01, 4.38e+00, 2.70e+00]    [0.00e+00, 1.79e+01, 2.70e+00]    [1.65e+01, 1.79e+01, 1.24e+01, 1.25e+01]    
21000     [2.73e+01, 8.03e+00, 2.70e+00]    [0.00e+00, 1.68e+01, 2.70e+00]    [1.55e+01, 1.68e+01, 1.25e+01, 1.21e+01]    
22000     [2.83e+01, 1.41e+01, 2.69e+00]    [0.00e+00, 2.15e+01, 2.69e+00]    [2.01e+01, 2.15e+01, 1.46e+01, 1.63e+01]    
23000     [2.67e+01, 4.75e+00, 2.69e+00]    [0.00e+00, 1.72e+01, 2.69e+00]    [1.58e+01, 1.72e+01, 1.25e+01, 1.23e+01]    
24000     [2.68e+01, 5.73e+00, 2.68e+00]    [0.00e+00, 1.72e+01, 2.68e+00]    [1.57e+01, 1.72e+01, 1.26e+01, 1.22e+01]    
25000     [2.83e+01, 1.61e+01, 2.68e+00]    [0.00e+00, 2.20e+01, 2.68e+00]    [2.04e+01, 2.20e+01, 1.47e+01, 1.67e+01]    
26000     [2.59e+01, 1.73e+00, 2.67e+00]    [0.00e+00, 1.81e+01, 2.67e+00]    [1.65e+01, 1.81e+01, 1.25e+01, 1.26e+01]    
27000     [2.71e+01, 7.46e+00, 2.67e+00]    [0.00e+00, 1.68e+01, 2.67e+00]    [1.52e+01, 1.68e+01, 1.28e+01, 1.21e+01]    
28000     [2.73e+01, 1.57e+01, 2.66e+00]    [0.00e+00, 2.16e+01, 2.66e+00]    [1.99e+01, 2.16e+01, 1.42e+01, 1.61e+01]    
29000     [2.61e+01, 2.95e+00, 2.66e+00]    [0.00e+00, 1.74e+01, 2.66e+00]    [1.57e+01, 1.74e+01, 1.28e+01, 1.25e+01]    
30000     [2.65e+01, 5.10e+00, 2.66e+00]    [0.00e+00, 1.71e+01, 2.66e+00]    [1.54e+01, 1.71e+01, 1.30e+01, 1.23e+01]    

Best model at step 26000:
  train loss: 3.03e+01
  test loss: 2.08e+01
  test metric: [1.65e+01, 1.81e+01, 1.25e+01, 1.26e+01]

'train' took 50.975121 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.173078 s

'compile' took 0.769876 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.40e+02, 5.22e+01, 2.77e+00]    [0.00e+00, 9.33e+02, 2.77e+00]    [9.33e+02, 9.33e+02, 1.33e+03, 1.33e+03]    
1000      [3.60e+01, 4.02e+00, 2.75e+00]    [0.00e+00, 3.56e+01, 2.75e+00]    [3.45e+01, 3.56e+01, 2.36e+01, 2.44e+01]    
2000      [3.24e+01, 2.63e+00, 2.76e+00]    [0.00e+00, 2.57e+01, 2.76e+00]    [2.46e+01, 2.57e+01, 1.74e+01, 1.76e+01]    
3000      [3.03e+01, 2.03e-01, 2.76e+00]    [0.00e+00, 2.85e+01, 2.76e+00]    [2.03e+01, 2.85e+01, 1.58e+01, 2.12e+01]    
4000      [3.22e+01, 3.35e+00, 2.76e+00]    [0.00e+00, 4.05e+01, 2.76e+00]    [2.63e+01, 4.05e+01, 1.79e+01, 4.10e+01]    
5000      [2.85e+01, 1.88e-01, 2.76e+00]    [0.00e+00, 3.62e+01, 2.76e+00]    [2.01e+01, 3.62e+01, 1.57e+01, 3.68e+01]    
6000      [2.81e+01, 2.90e+00, 2.76e+00]    [0.00e+00, 3.86e+01, 2.76e+00]    [1.89e+01, 3.86e+01, 1.60e+01, 4.46e+01]    
7000      [2.74e+01, 2.26e+00, 2.76e+00]    [0.00e+00, 4.23e+01, 2.76e+00]    [2.24e+01, 4.23e+01, 1.50e+01, 4.75e+01]    
8000      [2.79e+01, 4.41e-01, 2.76e+00]    [0.00e+00, 4.92e+01, 2.76e+00]    [2.32e+01, 4.92e+01, 1.67e+01, 6.42e+01]    
9000      [2.73e+01, 5.62e-01, 2.76e+00]    [0.00e+00, 5.24e+01, 2.76e+00]    [2.33e+01, 5.24e+01, 1.68e+01, 7.04e+01]    
10000     [2.56e+01, 1.16e+00, 2.76e+00]    [0.00e+00, 5.21e+01, 2.76e+00]    [2.14e+01, 5.21e+01, 1.62e+01, 6.92e+01]    
11000     [2.90e+01, 1.86e+00, 2.76e+00]    [0.00e+00, 4.08e+01, 2.76e+00]    [2.17e+01, 4.08e+01, 2.33e+01, 5.11e+01]    
12000     [2.71e+01, 3.56e+00, 2.76e+00]    [0.00e+00, 4.76e+01, 2.76e+00]    [2.02e+01, 4.76e+01, 1.72e+01, 6.38e+01]    
13000     [2.58e+01, 3.71e+00, 2.76e+00]    [0.00e+00, 5.77e+01, 2.76e+00]    [2.37e+01, 5.77e+01, 1.46e+01, 7.83e+01]    
14000     [2.45e+01, 3.21e+00, 2.76e+00]    [0.00e+00, 5.96e+01, 2.76e+00]    [2.01e+01, 5.96e+01, 1.63e+01, 8.53e+01]    
15000     [2.38e+01, 9.68e-01, 2.76e+00]    [0.00e+00, 6.04e+01, 2.76e+00]    [2.17e+01, 6.04e+01, 1.51e+01, 8.74e+01]    
16000     [2.42e+01, 3.24e+00, 2.76e+00]    [0.00e+00, 6.25e+01, 2.76e+00]    [1.95e+01, 6.25e+01, 1.66e+01, 9.24e+01]    
17000     [2.37e+01, 1.82e+00, 2.77e+00]    [0.00e+00, 6.30e+01, 2.77e+00]    [2.12e+01, 6.30e+01, 1.55e+01, 9.38e+01]    
18000     [2.29e+01, 7.03e-02, 2.77e+00]    [0.00e+00, 6.20e+01, 2.77e+00]    [2.07e+01, 6.20e+01, 1.43e+01, 9.25e+01]    
19000     [2.38e+01, 1.58e+00, 2.77e+00]    [0.00e+00, 6.07e+01, 2.77e+00]    [2.04e+01, 6.07e+01, 1.47e+01, 9.05e+01]    
20000     [2.29e+01, 4.93e-01, 2.77e+00]    [0.00e+00, 6.56e+01, 2.77e+00]    [2.09e+01, 6.56e+01, 1.50e+01, 9.95e+01]    
21000     [2.31e+01, 9.57e-01, 2.77e+00]    [0.00e+00, 6.70e+01, 2.77e+00]    [2.04e+01, 6.70e+01, 1.58e+01, 1.03e+02]    
22000     [2.34e+01, 2.51e+00, 2.77e+00]    [0.00e+00, 6.35e+01, 2.77e+00]    [1.93e+01, 6.35e+01, 1.45e+01, 9.68e+01]    
23000     [2.69e+01, 1.61e+00, 2.77e+00]    [0.00e+00, 7.56e+01, 2.77e+00]    [2.67e+01, 7.56e+01, 1.75e+01, 1.17e+02]    
24000     [2.25e+01, 1.89e+00, 2.76e+00]    [0.00e+00, 6.66e+01, 2.76e+00]    [2.05e+01, 6.66e+01, 1.44e+01, 1.03e+02]    
25000     [2.56e+01, 1.23e+00, 2.76e+00]    [0.00e+00, 7.32e+01, 2.76e+00]    [2.45e+01, 7.32e+01, 1.55e+01, 1.14e+02]    
26000     [2.43e+01, 1.55e+00, 2.76e+00]    [0.00e+00, 7.08e+01, 2.76e+00]    [2.22e+01, 7.08e+01, 1.52e+01, 1.10e+02]    
27000     [2.25e+01, 7.58e-01, 2.76e+00]    [0.00e+00, 6.35e+01, 2.76e+00]    [2.01e+01, 6.35e+01, 1.39e+01, 9.83e+01]    
28000     [2.32e+01, 3.88e-01, 2.76e+00]    [0.00e+00, 6.17e+01, 2.76e+00]    [2.00e+01, 6.17e+01, 1.49e+01, 9.52e+01]    
29000     [2.58e+01, 1.02e+00, 2.76e+00]    [0.00e+00, 5.58e+01, 2.76e+00]    [2.15e+01, 5.58e+01, 2.17e+01, 8.44e+01]    
30000     [2.19e+01, 1.71e+00, 2.76e+00]    [0.00e+00, 6.55e+01, 2.76e+00]    [1.99e+01, 6.55e+01, 1.35e+01, 1.03e+02]    

Best model at step 18000:
  train loss: 2.57e+01
  test loss: 6.47e+01
  test metric: [2.07e+01, 6.20e+01, 1.43e+01, 9.25e+01]

'train' took 54.369382 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.285881 s

'compile' took 0.964122 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.62e+02, 9.68e+01, 2.78e+00]    [0.00e+00, 2.61e+02, 2.78e+00]    [2.61e+02, 2.61e+02, 2.51e+02, 2.51e+02]    
1000      [3.52e+01, 2.20e-01, 2.81e+00]    [0.00e+00, 2.44e+01, 2.81e+00]    [1.40e+01, 2.44e+01, 8.80e+00, 2.91e+01]    
2000      [3.10e+01, 9.41e-02, 2.82e+00]    [0.00e+00, 2.99e+01, 2.82e+00]    [9.36e+00, 2.99e+01, 1.06e+01, 3.61e+01]    
3000      [2.81e+01, 1.86e-01, 2.83e+00]    [0.00e+00, 4.08e+01, 2.83e+00]    [1.11e+01, 4.08e+01, 6.85e+00, 4.74e+01]    
4000      [3.06e+01, 3.14e-01, 2.83e+00]    [0.00e+00, 3.13e+01, 2.83e+00]    [1.82e+01, 3.13e+01, 1.58e+01, 2.92e+01]    
5000      [2.64e+01, 1.37e-01, 2.84e+00]    [0.00e+00, 3.87e+01, 2.84e+00]    [1.38e+01, 3.87e+01, 8.51e+00, 3.80e+01]    
6000      [2.59e+01, 1.97e-01, 2.84e+00]    [0.00e+00, 4.18e+01, 2.84e+00]    [1.40e+01, 4.18e+01, 8.40e+00, 4.01e+01]    
7000      [2.65e+01, 2.00e-01, 2.84e+00]    [0.00e+00, 4.08e+01, 2.84e+00]    [1.45e+01, 4.08e+01, 1.20e+01, 3.68e+01]    
8000      [2.55e+01, 2.75e-01, 2.83e+00]    [0.00e+00, 2.84e+01, 2.83e+00]    [1.57e+01, 2.84e+01, 8.92e+00, 2.44e+01]    
9000      [2.59e+01, 1.17e-01, 2.83e+00]    [0.00e+00, 3.64e+01, 2.83e+00]    [1.50e+01, 3.64e+01, 1.20e+01, 2.96e+01]    
10000     [2.35e+01, 6.74e-02, 2.83e+00]    [0.00e+00, 2.67e+01, 2.83e+00]    [1.33e+01, 2.67e+01, 9.22e+00, 2.35e+01]    
11000     [2.64e+01, 2.84e-01, 2.83e+00]    [0.00e+00, 2.99e+01, 2.83e+00]    [1.81e+01, 2.99e+01, 1.23e+01, 2.11e+01]    
12000     [2.43e+01, 1.03e-01, 2.83e+00]    [0.00e+00, 3.19e+01, 2.83e+00]    [1.54e+01, 3.19e+01, 1.00e+01, 2.22e+01]    
13000     [2.30e+01, 2.04e-02, 2.83e+00]    [0.00e+00, 3.35e+01, 2.83e+00]    [1.37e+01, 3.35e+01, 9.48e+00, 2.28e+01]    
14000     [2.30e+01, 8.78e-02, 2.82e+00]    [0.00e+00, 4.24e+01, 2.82e+00]    [1.43e+01, 4.24e+01, 9.92e+00, 2.88e+01]    
15000     [2.38e+01, 2.02e-01, 2.82e+00]    [0.00e+00, 4.35e+01, 2.82e+00]    [1.40e+01, 4.35e+01, 9.74e+00, 2.99e+01]    
16000     [2.54e+01, 2.21e-01, 2.82e+00]    [0.00e+00, 6.00e+01, 2.82e+00]    [1.89e+01, 6.00e+01, 1.13e+01, 5.37e+01]    
17000     [2.49e+01, 2.90e-01, 2.82e+00]    [0.00e+00, 5.11e+01, 2.82e+00]    [1.47e+01, 5.11e+01, 1.32e+01, 3.79e+01]    
18000     [2.40e+01, 2.86e-01, 2.82e+00]    [0.00e+00, 6.80e+01, 2.82e+00]    [1.69e+01, 6.80e+01, 9.93e+00, 6.75e+01]    
19000     [2.29e+01, 1.52e-01, 2.81e+00]    [0.00e+00, 7.28e+01, 2.81e+00]    [1.58e+01, 7.28e+01, 9.50e+00, 7.55e+01]    
20000     [2.51e+01, 3.60e-01, 2.81e+00]    [0.00e+00, 6.66e+01, 2.81e+00]    [1.43e+01, 6.66e+01, 1.38e+01, 6.26e+01]    
21000     [2.30e+01, 2.12e-01, 2.81e+00]    [0.00e+00, 7.74e+01, 2.81e+00]    [1.34e+01, 7.74e+01, 9.75e+00, 8.18e+01]    
22000     [2.58e+01, 4.35e-01, 2.81e+00]    [0.00e+00, 7.51e+01, 2.81e+00]    [1.60e+01, 7.51e+01, 1.52e+01, 7.76e+01]    
23000     [2.23e+01, 3.46e-02, 2.81e+00]    [0.00e+00, 9.48e+01, 2.81e+00]    [1.53e+01, 9.48e+01, 9.57e+00, 1.12e+02]    
24000     [2.54e+01, 3.74e-01, 2.80e+00]    [0.00e+00, 8.61e+01, 2.80e+00]    [1.58e+01, 8.61e+01, 1.55e+01, 9.50e+01]    
25000     [2.14e+01, 1.06e-01, 2.80e+00]    [0.00e+00, 9.85e+01, 2.80e+00]    [1.32e+01, 9.85e+01, 9.77e+00, 1.19e+02]    
26000     [2.79e+01, 6.04e-01, 2.80e+00]    [0.00e+00, 8.65e+01, 2.80e+00]    [2.17e+01, 8.65e+01, 2.37e+01, 9.14e+01]    
27000     [2.54e+01, 3.84e-01, 2.80e+00]    [0.00e+00, 9.46e+01, 2.80e+00]    [1.71e+01, 9.46e+01, 1.73e+01, 1.08e+02]    
28000     [2.26e+01, 1.22e-01, 2.79e+00]    [0.00e+00, 1.13e+02, 2.79e+00]    [1.64e+01, 1.13e+02, 9.93e+00, 1.44e+02]    
29000     [2.34e+01, 3.49e-01, 2.79e+00]    [0.00e+00, 1.04e+02, 2.79e+00]    [1.51e+01, 1.04e+02, 1.37e+01, 1.23e+02]    
30000     [2.22e+01, 1.78e-01, 2.79e+00]    [0.00e+00, 1.09e+02, 2.79e+00]    [1.42e+01, 1.09e+02, 1.05e+01, 1.34e+02]    

Best model at step 25000:
  train loss: 2.43e+01
  test loss: 1.01e+02
  test metric: [1.32e+01, 9.85e+01, 9.77e+00, 1.19e+02]

'train' took 48.752257 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.179901 s

'compile' took 0.790485 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.60e+02, 4.30e+02, 2.56e+00]    [0.00e+00, 2.34e+02, 2.56e+00]    [2.34e+02, 2.34e+02, 3.36e+02, 3.36e+02]    
1000      [7.11e+01, 3.01e+01, 2.54e+00]    [0.00e+00, 6.66e+01, 2.54e+00]    [6.66e+01, 6.66e+01, 3.47e+01, 3.47e+01]    
2000      [6.08e+01, 2.88e+01, 2.54e+00]    [0.00e+00, 6.03e+01, 2.54e+00]    [6.02e+01, 6.03e+01, 3.15e+01, 3.13e+01]    
3000      [5.57e+01, 4.91e+01, 2.54e+00]    [0.00e+00, 5.44e+01, 2.54e+00]    [5.42e+01, 5.44e+01, 2.84e+01, 2.81e+01]    
4000      [4.86e+01, 3.16e+01, 2.54e+00]    [0.00e+00, 4.90e+01, 2.54e+00]    [4.86e+01, 4.90e+01, 2.53e+01, 2.49e+01]    
5000      [4.23e+01, 2.46e+01, 2.54e+00]    [0.00e+00, 4.38e+01, 2.54e+00]    [4.31e+01, 4.38e+01, 2.08e+01, 2.02e+01]    
6000      [3.93e+01, 3.70e+01, 2.55e+00]    [0.00e+00, 3.72e+01, 2.55e+00]    [3.67e+01, 3.72e+01, 1.92e+01, 1.93e+01]    
7000      [3.24e+01, 7.80e+00, 2.55e+00]    [0.00e+00, 3.33e+01, 2.55e+00]    [3.26e+01, 3.33e+01, 1.79e+01, 1.79e+01]    
8000      [3.26e+01, 2.69e+01, 2.56e+00]    [0.00e+00, 2.98e+01, 2.56e+00]    [2.88e+01, 2.98e+01, 1.64e+01, 1.66e+01]    
9000      [3.04e+01, 2.68e+01, 2.57e+00]    [0.00e+00, 2.74e+01, 2.57e+00]    [2.61e+01, 2.74e+01, 1.62e+01, 1.63e+01]    
10000     [2.75e+01, 1.29e+00, 2.57e+00]    [0.00e+00, 2.70e+01, 2.57e+00]    [2.46e+01, 2.70e+01, 1.66e+01, 1.55e+01]    
11000     [3.03e+01, 3.43e+01, 2.57e+00]    [0.00e+00, 2.60e+01, 2.57e+00]    [2.36e+01, 2.60e+01, 1.63e+01, 1.56e+01]    
12000     [2.88e+01, 2.57e+01, 2.58e+00]    [0.00e+00, 2.70e+01, 2.58e+00]    [2.33e+01, 2.70e+01, 1.65e+01, 1.48e+01]    
13000     [2.71e+01, 1.66e+01, 2.58e+00]    [0.00e+00, 2.60e+01, 2.58e+00]    [2.29e+01, 2.60e+01, 1.63e+01, 1.57e+01]    
14000     [2.71e+01, 1.47e+01, 2.59e+00]    [0.00e+00, 2.75e+01, 2.59e+00]    [2.26e+01, 2.75e+01, 1.67e+01, 1.47e+01]    
15000     [2.65e+01, 8.92e+00, 2.60e+00]    [0.00e+00, 2.86e+01, 2.60e+00]    [2.23e+01, 2.86e+01, 1.68e+01, 1.43e+01]    
16000     [2.62e+01, 5.94e+00, 2.61e+00]    [0.00e+00, 2.98e+01, 2.61e+00]    [2.21e+01, 2.98e+01, 1.69e+01, 1.41e+01]    
17000     [2.68e+01, 9.78e+00, 2.62e+00]    [0.00e+00, 3.05e+01, 2.62e+00]    [2.21e+01, 3.05e+01, 1.71e+01, 1.42e+01]    
18000     [2.52e+01, 1.15e+01, 2.62e+00]    [0.00e+00, 3.12e+01, 2.62e+00]    [2.17e+01, 3.12e+01, 1.71e+01, 1.42e+01]    
19000     [2.58e+01, 9.18e+00, 2.63e+00]    [0.00e+00, 3.23e+01, 2.63e+00]    [2.15e+01, 3.23e+01, 1.71e+01, 1.43e+01]    
20000     [2.53e+01, 7.64e+00, 2.64e+00]    [0.00e+00, 3.28e+01, 2.64e+00]    [2.17e+01, 3.28e+01, 1.70e+01, 1.45e+01]    
21000     [2.62e+01, 1.48e+01, 2.64e+00]    [0.00e+00, 3.41e+01, 2.64e+00]    [2.16e+01, 3.41e+01, 1.70e+01, 1.49e+01]    
22000     [2.67e+01, 1.64e+01, 2.65e+00]    [0.00e+00, 3.59e+01, 2.65e+00]    [2.14e+01, 3.59e+01, 1.74e+01, 1.56e+01]    
23000     [2.71e+01, 1.70e+01, 2.66e+00]    [0.00e+00, 3.67e+01, 2.66e+00]    [2.10e+01, 3.67e+01, 1.78e+01, 1.60e+01]    
24000     [2.53e+01, 1.01e+01, 2.67e+00]    [0.00e+00, 3.80e+01, 2.67e+00]    [2.12e+01, 3.80e+01, 1.79e+01, 1.60e+01]    
25000     [2.55e+01, 7.96e+00, 2.68e+00]    [0.00e+00, 3.92e+01, 2.68e+00]    [2.15e+01, 3.92e+01, 1.80e+01, 1.62e+01]    
26000     [2.70e+01, 1.42e+01, 2.69e+00]    [0.00e+00, 4.03e+01, 2.69e+00]    [2.13e+01, 4.03e+01, 1.82e+01, 1.65e+01]    
27000     [2.50e+01, 5.98e+00, 2.69e+00]    [0.00e+00, 4.27e+01, 2.69e+00]    [2.13e+01, 4.27e+01, 1.84e+01, 1.75e+01]    
28000     [2.46e+01, 3.11e+00, 2.70e+00]    [0.00e+00, 4.34e+01, 2.70e+00]    [2.17e+01, 4.34e+01, 1.84e+01, 1.73e+01]    
29000     [2.41e+01, 1.20e+00, 2.70e+00]    [0.00e+00, 4.42e+01, 2.70e+00]    [2.18e+01, 4.42e+01, 1.84e+01, 1.75e+01]    
30000     [2.41e+01, 2.34e+00, 2.70e+00]    [0.00e+00, 4.53e+01, 2.70e+00]    [2.15e+01, 4.53e+01, 1.87e+01, 1.82e+01]    

Best model at step 29000:
  train loss: 2.80e+01
  test loss: 4.69e+01
  test metric: [2.18e+01, 4.42e+01, 1.84e+01, 1.75e+01]

'train' took 50.862158 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.242352 s

'compile' took 1.066151 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.06e+02, 6.02e+02, 2.61e+00]    [0.00e+00, 2.63e+02, 2.61e+00]    [2.63e+02, 2.63e+02, 3.28e+02, 3.28e+02]    
1000      [5.11e+01, 3.29e+01, 2.60e+00]    [0.00e+00, 5.48e+01, 2.60e+00]    [5.45e+01, 5.48e+01, 2.23e+01, 2.22e+01]    
2000      [3.77e+01, 2.00e+01, 2.61e+00]    [0.00e+00, 4.51e+01, 2.61e+00]    [4.45e+01, 4.51e+01, 1.75e+01, 1.80e+01]    
3000      [3.66e+01, 3.40e+01, 2.61e+00]    [0.00e+00, 2.97e+01, 2.61e+00]    [2.95e+01, 2.97e+01, 1.42e+01, 1.45e+01]    
4000      [3.23e+01, 4.95e+00, 2.61e+00]    [0.00e+00, 3.07e+01, 2.61e+00]    [2.97e+01, 3.07e+01, 1.01e+01, 1.08e+01]    
5000      [3.60e+01, 3.49e+01, 2.61e+00]    [0.00e+00, 2.11e+01, 2.61e+00]    [2.07e+01, 2.11e+01, 1.37e+01, 1.33e+01]    
6000      [3.06e+01, 2.70e+00, 2.61e+00]    [0.00e+00, 2.50e+01, 2.61e+00]    [2.37e+01, 2.50e+01, 1.32e+01, 1.41e+01]    
7000      [3.04e+01, 3.66e+00, 2.61e+00]    [0.00e+00, 2.61e+01, 2.61e+00]    [2.45e+01, 2.61e+01, 1.53e+01, 1.70e+01]    
8000      [3.03e+01, 9.74e+00, 2.61e+00]    [0.00e+00, 2.65e+01, 2.61e+00]    [2.46e+01, 2.65e+01, 1.63e+01, 1.84e+01]    
9000      [2.90e+01, 1.10e+01, 2.60e+00]    [0.00e+00, 2.28e+01, 2.60e+00]    [2.05e+01, 2.28e+01, 1.33e+01, 1.40e+01]    
10000     [3.14e+01, 2.44e+01, 2.60e+00]    [0.00e+00, 1.95e+01, 2.60e+00]    [1.69e+01, 1.95e+01, 1.42e+01, 1.32e+01]    
11000     [2.80e+01, 3.59e+00, 2.60e+00]    [0.00e+00, 2.39e+01, 2.60e+00]    [2.11e+01, 2.39e+01, 1.48e+01, 1.67e+01]    
12000     [2.79e+01, 2.36e+00, 2.60e+00]    [0.00e+00, 2.45e+01, 2.60e+00]    [2.13e+01, 2.45e+01, 1.46e+01, 1.68e+01]    
13000     [2.99e+01, 1.44e+01, 2.59e+00]    [0.00e+00, 2.68e+01, 2.59e+00]    [2.35e+01, 2.68e+01, 1.75e+01, 2.10e+01]    
14000     [2.79e+01, 4.60e+00, 2.59e+00]    [0.00e+00, 2.54e+01, 2.59e+00]    [2.17e+01, 2.54e+01, 1.55e+01, 1.83e+01]    
15000     [2.80e+01, 8.62e+00, 2.59e+00]    [0.00e+00, 2.23e+01, 2.59e+00]    [1.85e+01, 2.23e+01, 1.40e+01, 1.50e+01]    
16000     [2.73e+01, 4.33e+00, 2.58e+00]    [0.00e+00, 2.60e+01, 2.58e+00]    [2.20e+01, 2.60e+01, 1.48e+01, 1.79e+01]    
17000     [2.70e+01, 1.44e+00, 2.58e+00]    [0.00e+00, 2.50e+01, 2.58e+00]    [2.06e+01, 2.50e+01, 1.42e+01, 1.69e+01]    
18000     [2.88e+01, 1.68e+01, 2.58e+00]    [0.00e+00, 2.32e+01, 2.58e+00]    [1.85e+01, 2.32e+01, 1.34e+01, 1.41e+01]    
19000     [3.09e+01, 2.60e+01, 2.57e+00]    [0.00e+00, 2.08e+01, 2.57e+00]    [1.62e+01, 2.08e+01, 1.47e+01, 1.33e+01]    
20000     [3.07e+01, 2.29e+01, 2.57e+00]    [0.00e+00, 2.06e+01, 2.57e+00]    [1.55e+01, 2.06e+01, 1.51e+01, 1.34e+01]    
21000     [2.72e+01, 6.40e+00, 2.56e+00]    [0.00e+00, 2.68e+01, 2.56e+00]    [2.12e+01, 2.68e+01, 1.47e+01, 1.91e+01]    
22000     [2.90e+01, 1.54e+01, 2.56e+00]    [0.00e+00, 2.17e+01, 2.56e+00]    [1.61e+01, 2.17e+01, 1.47e+01, 1.45e+01]    
23000     [2.67e+01, 4.35e+00, 2.55e+00]    [0.00e+00, 2.72e+01, 2.55e+00]    [2.12e+01, 2.72e+01, 1.42e+01, 1.90e+01]    
24000     [2.70e+01, 8.89e+00, 2.55e+00]    [0.00e+00, 2.72e+01, 2.55e+00]    [2.09e+01, 2.72e+01, 1.45e+01, 1.95e+01]    
25000     [2.76e+01, 1.17e+01, 2.55e+00]    [0.00e+00, 2.42e+01, 2.55e+00]    [1.77e+01, 2.42e+01, 1.36e+01, 1.47e+01]    
26000     [2.62e+01, 3.53e+00, 2.54e+00]    [0.00e+00, 2.70e+01, 2.54e+00]    [2.03e+01, 2.70e+01, 1.37e+01, 1.85e+01]    
27000     [2.72e+01, 8.87e+00, 2.54e+00]    [0.00e+00, 2.85e+01, 2.54e+00]    [2.15e+01, 2.85e+01, 1.42e+01, 2.05e+01]    
28000     [2.81e+01, 1.16e+01, 2.54e+00]    [0.00e+00, 2.35e+01, 2.54e+00]    [1.62e+01, 2.35e+01, 1.44e+01, 1.47e+01]    
29000     [2.64e+01, 6.80e+00, 2.54e+00]    [0.00e+00, 2.87e+01, 2.54e+00]    [2.11e+01, 2.87e+01, 1.33e+01, 1.99e+01]    
30000     [2.64e+01, 7.89e+00, 2.54e+00]    [0.00e+00, 2.85e+01, 2.54e+00]    [2.07e+01, 2.85e+01, 1.37e+01, 2.05e+01]    

Best model at step 17000:
  train loss: 3.10e+01
  test loss: 2.76e+01
  test metric: [2.06e+01, 2.50e+01, 1.42e+01, 1.69e+01]

'train' took 55.484835 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.177462 s

'compile' took 1.097002 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.07e+02, 1.92e+02, 2.89e+00]    [0.00e+00, 2.67e+02, 2.89e+00]    [2.67e+02, 2.67e+02, 3.37e+02, 3.37e+02]    
1000      [5.06e+01, 6.69e+00, 2.89e+00]    [0.00e+00, 6.44e+01, 2.89e+00]    [6.29e+01, 6.44e+01, 2.98e+01, 3.19e+01]    
2000      [3.72e+01, 6.32e+00, 2.89e+00]    [0.00e+00, 4.21e+01, 2.89e+00]    [3.99e+01, 4.21e+01, 2.17e+01, 1.97e+01]    
3000      [3.18e+01, 6.13e-01, 2.89e+00]    [0.00e+00, 3.44e+01, 2.89e+00]    [3.19e+01, 3.44e+01, 1.68e+01, 1.63e+01]    
4000      [3.51e+01, 1.95e+01, 2.88e+00]    [0.00e+00, 3.86e+01, 2.88e+00]    [3.58e+01, 3.86e+01, 2.49e+01, 3.23e+01]    
5000      [2.89e+01, 4.40e+00, 2.88e+00]    [0.00e+00, 2.81e+01, 2.88e+00]    [2.52e+01, 2.81e+01, 1.42e+01, 1.55e+01]    
6000      [2.78e+01, 4.60e+00, 2.88e+00]    [0.00e+00, 2.42e+01, 2.88e+00]    [2.24e+01, 2.42e+01, 1.44e+01, 1.36e+01]    
7000      [2.93e+01, 9.22e+00, 2.88e+00]    [0.00e+00, 3.00e+01, 2.88e+00]    [2.61e+01, 3.00e+01, 1.70e+01, 2.60e+01]    
8000      [2.87e+01, 1.22e+01, 2.87e+00]    [0.00e+00, 2.06e+01, 2.87e+00]    [2.41e+01, 2.06e+01, 1.54e+01, 1.51e+01]    
9000      [3.09e+01, 1.71e+01, 2.87e+00]    [0.00e+00, 3.12e+01, 2.87e+00]    [2.67e+01, 3.12e+01, 1.98e+01, 3.12e+01]    
10000     [2.62e+01, 6.45e+00, 2.87e+00]    [0.00e+00, 2.41e+01, 2.87e+00]    [2.08e+01, 2.41e+01, 1.48e+01, 1.49e+01]    
11000     [2.56e+01, 5.85e+00, 2.87e+00]    [0.00e+00, 2.57e+01, 2.87e+00]    [2.04e+01, 2.57e+01, 1.51e+01, 1.70e+01]    
12000     [2.56e+01, 6.77e+00, 2.87e+00]    [0.00e+00, 2.59e+01, 2.87e+00]    [2.03e+01, 2.59e+01, 1.51e+01, 1.71e+01]    
13000     [2.69e+01, 6.16e+00, 2.86e+00]    [0.00e+00, 3.07e+01, 2.86e+00]    [2.50e+01, 3.07e+01, 1.61e+01, 2.88e+01]    
14000     [2.59e+01, 5.69e+00, 2.86e+00]    [0.00e+00, 2.79e+01, 2.86e+00]    [2.21e+01, 2.79e+01, 1.38e+01, 2.18e+01]    
15000     [2.47e+01, 4.20e+00, 2.86e+00]    [0.00e+00, 2.71e+01, 2.86e+00]    [2.11e+01, 2.71e+01, 1.41e+01, 1.97e+01]    
16000     [2.67e+01, 7.52e+00, 2.86e+00]    [0.00e+00, 3.06e+01, 2.86e+00]    [2.45e+01, 3.06e+01, 1.52e+01, 2.85e+01]    
17000     [2.73e+01, 1.00e+01, 2.86e+00]    [0.00e+00, 3.21e+01, 2.86e+00]    [2.59e+01, 3.21e+01, 1.78e+01, 3.27e+01]    
18000     [2.89e+01, 1.82e+01, 2.86e+00]    [0.00e+00, 2.18e+01, 2.86e+00]    [2.45e+01, 2.18e+01, 1.61e+01, 1.35e+01]    
19000     [2.85e+01, 1.71e+01, 2.85e+00]    [0.00e+00, 2.22e+01, 2.85e+00]    [2.43e+01, 2.22e+01, 1.55e+01, 1.33e+01]    
20000     [2.49e+01, 3.08e+00, 2.85e+00]    [0.00e+00, 2.49e+01, 2.85e+00]    [2.16e+01, 2.49e+01, 1.34e+01, 1.53e+01]    
21000     [2.56e+01, 7.19e+00, 2.85e+00]    [0.00e+00, 3.08e+01, 2.85e+00]    [2.41e+01, 3.08e+01, 1.47e+01, 2.91e+01]    
22000     [2.65e+01, 1.15e+01, 2.85e+00]    [0.00e+00, 3.15e+01, 2.85e+00]    [2.48e+01, 3.15e+01, 1.57e+01, 3.14e+01]    
23000     [2.59e+01, 9.57e+00, 2.85e+00]    [0.00e+00, 3.06e+01, 2.85e+00]    [2.38e+01, 3.06e+01, 1.39e+01, 2.85e+01]    
24000     [2.35e+01, 4.38e+00, 2.85e+00]    [0.00e+00, 2.71e+01, 2.85e+00]    [2.03e+01, 2.71e+01, 1.42e+01, 1.98e+01]    
25000     [2.33e+01, 1.89e+00, 2.85e+00]    [0.00e+00, 2.79e+01, 2.85e+00]    [2.11e+01, 2.79e+01, 1.37e+01, 2.17e+01]    
26000     [2.47e+01, 5.28e+00, 2.85e+00]    [0.00e+00, 2.42e+01, 2.85e+00]    [2.25e+01, 2.42e+01, 1.35e+01, 1.41e+01]    
27000     [2.39e+01, 2.07e+00, 2.84e+00]    [0.00e+00, 2.58e+01, 2.84e+00]    [2.09e+01, 2.58e+01, 1.35e+01, 1.70e+01]    
28000     [2.37e+01, 3.24e+00, 2.84e+00]    [0.00e+00, 2.87e+01, 2.84e+00]    [2.32e+01, 2.87e+01, 1.29e+01, 2.38e+01]    
29000     [2.43e+01, 6.59e+00, 2.84e+00]    [0.00e+00, 3.04e+01, 2.84e+00]    [2.38e+01, 3.04e+01, 1.40e+01, 2.77e+01]    
30000     [2.59e+01, 1.08e+01, 2.84e+00]    [0.00e+00, 2.32e+01, 2.84e+00]    [2.36e+01, 2.32e+01, 1.44e+01, 1.35e+01]    

Best model at step 25000:
  train loss: 2.81e+01
  test loss: 3.08e+01
  test metric: [2.11e+01, 2.79e+01, 1.37e+01, 2.17e+01]

'train' took 51.014895 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.203296 s

'compile' took 0.943866 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.70e+02, 5.17e+02, 2.70e+00]    [0.00e+00, 4.67e+02, 2.70e+00]    [4.67e+02, 4.67e+02, 8.18e+02, 8.18e+02]    
1000      [3.67e+01, 2.42e+00, 2.68e+00]    [0.00e+00, 4.20e+01, 2.68e+00]    [4.17e+01, 4.20e+01, 1.91e+01, 1.88e+01]    
2000      [2.91e+01, 4.53e+00, 2.67e+00]    [0.00e+00, 3.14e+01, 2.67e+00]    [3.10e+01, 3.14e+01, 1.73e+01, 1.78e+01]    
3000      [2.65e+01, 3.70e+00, 2.65e+00]    [0.00e+00, 2.62e+01, 2.65e+00]    [2.72e+01, 2.62e+01, 1.63e+01, 1.71e+01]    
4000      [2.82e+01, 5.98e+00, 2.64e+00]    [0.00e+00, 2.76e+01, 2.64e+00]    [2.62e+01, 2.76e+01, 1.84e+01, 1.94e+01]    
5000      [2.62e+01, 4.98e+00, 2.64e+00]    [0.00e+00, 2.48e+01, 2.64e+00]    [2.64e+01, 2.48e+01, 1.67e+01, 1.66e+01]    
6000      [2.51e+01, 1.96e+00, 2.63e+00]    [0.00e+00, 2.42e+01, 2.63e+00]    [2.30e+01, 2.42e+01, 1.75e+01, 1.79e+01]    
7000      [2.67e+01, 7.03e+00, 2.63e+00]    [0.00e+00, 2.51e+01, 2.63e+00]    [2.72e+01, 2.51e+01, 1.75e+01, 1.67e+01]    
8000      [2.70e+01, 5.72e+00, 2.62e+00]    [0.00e+00, 2.66e+01, 2.62e+00]    [2.42e+01, 2.66e+01, 1.78e+01, 1.92e+01]    
9000      [2.59e+01, 5.89e+00, 2.62e+00]    [0.00e+00, 2.35e+01, 2.62e+00]    [2.60e+01, 2.35e+01, 1.70e+01, 1.71e+01]    
10000     [2.67e+01, 7.28e+00, 2.62e+00]    [0.00e+00, 2.40e+01, 2.62e+00]    [2.68e+01, 2.40e+01, 1.78e+01, 1.71e+01]    
11000     [2.56e+01, 4.00e+00, 2.61e+00]    [0.00e+00, 2.60e+01, 2.61e+00]    [2.36e+01, 2.60e+01, 1.79e+01, 2.05e+01]    
12000     [2.50e+01, 3.36e+00, 2.61e+00]    [0.00e+00, 2.54e+01, 2.61e+00]    [2.30e+01, 2.54e+01, 1.79e+01, 2.00e+01]    
13000     [2.63e+01, 7.43e+00, 2.61e+00]    [0.00e+00, 2.39e+01, 2.61e+00]    [2.72e+01, 2.39e+01, 1.85e+01, 1.75e+01]    
14000     [2.43e+01, 2.57e+00, 2.60e+00]    [0.00e+00, 2.49e+01, 2.60e+00]    [2.23e+01, 2.49e+01, 1.82e+01, 1.96e+01]    
15000     [2.55e+01, 6.50e+00, 2.60e+00]    [0.00e+00, 2.25e+01, 2.60e+00]    [2.61e+01, 2.25e+01, 1.79e+01, 1.85e+01]    
16000     [2.36e+01, 1.21e+00, 2.60e+00]    [0.00e+00, 2.40e+01, 2.60e+00]    [2.12e+01, 2.40e+01, 1.90e+01, 1.92e+01]    
17000     [2.52e+01, 6.71e+00, 2.60e+00]    [0.00e+00, 2.20e+01, 2.60e+00]    [2.60e+01, 2.20e+01, 1.83e+01, 1.89e+01]    
18000     [2.69e+01, 7.43e+00, 2.59e+00]    [0.00e+00, 2.96e+01, 2.59e+00]    [2.52e+01, 2.96e+01, 2.16e+01, 2.76e+01]    
19000     [2.32e+01, 1.21e+00, 2.59e+00]    [0.00e+00, 2.49e+01, 2.59e+00]    [2.21e+01, 2.49e+01, 1.84e+01, 2.14e+01]    
20000     [2.45e+01, 3.99e+00, 2.59e+00]    [0.00e+00, 2.68e+01, 2.59e+00]    [2.33e+01, 2.68e+01, 1.89e+01, 2.42e+01]    
21000     [2.35e+01, 3.95e+00, 2.59e+00]    [0.00e+00, 2.25e+01, 2.59e+00]    [2.30e+01, 2.25e+01, 1.80e+01, 1.87e+01]    
22000     [2.56e+01, 7.96e+00, 2.59e+00]    [0.00e+00, 2.14e+01, 2.59e+00]    [2.59e+01, 2.14e+01, 1.88e+01, 1.94e+01]    
23000     [2.47e+01, 4.43e+00, 2.58e+00]    [0.00e+00, 2.74e+01, 2.58e+00]    [2.23e+01, 2.74e+01, 1.93e+01, 2.38e+01]    
24000     [2.40e+01, 3.32e+00, 2.58e+00]    [0.00e+00, 2.68e+01, 2.58e+00]    [2.20e+01, 2.68e+01, 1.90e+01, 2.34e+01]    
25000     [2.57e+01, 8.77e+00, 2.58e+00]    [0.00e+00, 2.11e+01, 2.58e+00]    [2.59e+01, 2.11e+01, 1.91e+01, 1.95e+01]    
26000     [2.36e+01, 4.96e+00, 2.58e+00]    [0.00e+00, 2.21e+01, 2.58e+00]    [2.35e+01, 2.21e+01, 1.81e+01, 1.89e+01]    
27000     [2.27e+01, 1.35e+00, 2.58e+00]    [0.00e+00, 2.56e+01, 2.58e+00]    [2.15e+01, 2.56e+01, 1.89e+01, 2.29e+01]    
28000     [2.37e+01, 5.27e+00, 2.58e+00]    [0.00e+00, 2.29e+01, 2.58e+00]    [2.35e+01, 2.29e+01, 1.81e+01, 1.88e+01]    
29000     [2.44e+01, 5.04e+00, 2.57e+00]    [0.00e+00, 2.88e+01, 2.57e+00]    [2.29e+01, 2.88e+01, 2.00e+01, 2.64e+01]    
30000     [2.36e+01, 4.72e+00, 2.57e+00]    [0.00e+00, 2.22e+01, 2.57e+00]    [2.37e+01, 2.22e+01, 1.83e+01, 1.91e+01]    

Best model at step 27000:
  train loss: 2.66e+01
  test loss: 2.82e+01
  test metric: [2.15e+01, 2.56e+01, 1.89e+01, 2.29e+01]

'train' took 51.711421 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[24.74018213748258, 47.960614535879586, 16.992825070185184, 18.091018648082397, 61.964575498563704, 98.54914480641095, 44.15682124517968, 24.970572748876403, 27.923986978227187, 25.643059593659828]
sigma_y 3 39.09928012625475 24.089445060637754
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.434837 s

'compile' took 1.265113 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.33e+02, 7.06e+02, 2.66e+00]    [0.00e+00, 1.03e+02, 2.66e+00]    [1.03e+02, 1.03e+02, 5.73e+01, 5.73e+01]    
1000      [5.53e+01, 2.85e+01, 2.63e+00]    [0.00e+00, 5.70e+01, 2.63e+00]    [5.71e+01, 5.70e+01, 2.92e+01, 2.93e+01]    
2000      [4.06e+01, 1.48e+01, 2.63e+00]    [0.00e+00, 4.26e+01, 2.63e+00]    [4.26e+01, 4.26e+01, 2.17e+01, 2.15e+01]    
3000      [3.33e+01, 2.11e+01, 2.63e+00]    [0.00e+00, 3.20e+01, 2.63e+00]    [3.21e+01, 3.20e+01, 1.66e+01, 1.65e+01]    
4000      [3.13e+01, 1.64e+01, 2.63e+00]    [0.00e+00, 2.62e+01, 2.63e+00]    [2.64e+01, 2.62e+01, 1.50e+01, 1.50e+01]    
5000      [3.18e+01, 1.07e+01, 2.64e+00]    [0.00e+00, 2.30e+01, 2.64e+00]    [2.31e+01, 2.30e+01, 1.37e+01, 1.35e+01]    
6000      [3.15e+01, 2.05e+01, 2.64e+00]    [0.00e+00, 2.07e+01, 2.64e+00]    [2.09e+01, 2.07e+01, 1.34e+01, 1.33e+01]    
7000      [3.00e+01, 2.19e+00, 2.65e+00]    [0.00e+00, 1.97e+01, 2.65e+00]    [1.99e+01, 1.97e+01, 1.26e+01, 1.25e+01]    
8000      [3.00e+01, 4.07e+00, 2.65e+00]    [0.00e+00, 1.93e+01, 2.65e+00]    [1.96e+01, 1.93e+01, 1.29e+01, 1.28e+01]    
9000      [3.11e+01, 1.81e+01, 2.65e+00]    [0.00e+00, 2.02e+01, 2.65e+00]    [2.07e+01, 2.02e+01, 1.28e+01, 1.29e+01]    
10000     [2.88e+01, 5.21e+00, 2.65e+00]    [0.00e+00, 2.03e+01, 2.65e+00]    [2.09e+01, 2.03e+01, 1.29e+01, 1.31e+01]    
11000     [3.04e+01, 1.85e+01, 2.65e+00]    [0.00e+00, 2.03e+01, 2.65e+00]    [2.09e+01, 2.03e+01, 1.35e+01, 1.36e+01]    
12000     [3.50e+01, 3.95e+01, 2.65e+00]    [0.00e+00, 1.95e+01, 2.65e+00]    [1.99e+01, 1.95e+01, 1.38e+01, 1.37e+01]    
13000     [3.02e+01, 1.99e+01, 2.65e+00]    [0.00e+00, 2.09e+01, 2.65e+00]    [2.16e+01, 2.09e+01, 1.34e+01, 1.35e+01]    
14000     [2.84e+01, 4.42e+00, 2.65e+00]    [0.00e+00, 2.03e+01, 2.65e+00]    [2.07e+01, 2.03e+01, 1.41e+01, 1.40e+01]    
15000     [2.84e+01, 6.59e+00, 2.65e+00]    [0.00e+00, 2.01e+01, 2.65e+00]    [2.06e+01, 2.01e+01, 1.39e+01, 1.38e+01]    
16000     [2.79e+01, 1.67e+00, 2.65e+00]    [0.00e+00, 1.99e+01, 2.65e+00]    [2.04e+01, 1.99e+01, 1.40e+01, 1.39e+01]    
17000     [2.75e+01, 3.37e+00, 2.65e+00]    [0.00e+00, 2.06e+01, 2.65e+00]    [2.13e+01, 2.06e+01, 1.35e+01, 1.37e+01]    
18000     [3.09e+01, 2.69e+01, 2.64e+00]    [0.00e+00, 1.98e+01, 2.64e+00]    [2.06e+01, 1.98e+01, 1.37e+01, 1.39e+01]    
19000     [2.87e+01, 1.40e+01, 2.64e+00]    [0.00e+00, 2.04e+01, 2.64e+00]    [2.11e+01, 2.04e+01, 1.36e+01, 1.38e+01]    
20000     [2.81e+01, 1.02e+01, 2.64e+00]    [0.00e+00, 1.95e+01, 2.64e+00]    [2.00e+01, 1.95e+01, 1.36e+01, 1.36e+01]    
21000     [2.88e+01, 1.83e+01, 2.64e+00]    [0.00e+00, 2.00e+01, 2.64e+00]    [2.07e+01, 2.00e+01, 1.33e+01, 1.36e+01]    
22000     [2.81e+01, 1.35e+01, 2.64e+00]    [0.00e+00, 1.96e+01, 2.64e+00]    [2.02e+01, 1.96e+01, 1.37e+01, 1.36e+01]    
23000     [2.77e+01, 8.67e+00, 2.64e+00]    [0.00e+00, 1.99e+01, 2.64e+00]    [2.07e+01, 1.99e+01, 1.34e+01, 1.36e+01]    
24000     [2.77e+01, 8.30e+00, 2.64e+00]    [0.00e+00, 2.00e+01, 2.64e+00]    [2.04e+01, 2.00e+01, 1.35e+01, 1.33e+01]    
25000     [2.73e+01, 4.91e+00, 2.64e+00]    [0.00e+00, 1.99e+01, 2.64e+00]    [2.03e+01, 1.99e+01, 1.35e+01, 1.33e+01]    
26000     [2.76e+01, 1.49e+01, 2.63e+00]    [0.00e+00, 1.94e+01, 2.63e+00]    [2.00e+01, 1.94e+01, 1.34e+01, 1.35e+01]    
27000     [3.10e+01, 2.64e+01, 2.63e+00]    [0.00e+00, 1.99e+01, 2.63e+00]    [2.03e+01, 1.99e+01, 1.35e+01, 1.33e+01]    
28000     [3.06e+01, 2.44e+01, 2.63e+00]    [0.00e+00, 1.98e+01, 2.63e+00]    [2.03e+01, 1.98e+01, 1.34e+01, 1.32e+01]    
29000     [2.80e+01, 1.29e+01, 2.63e+00]    [0.00e+00, 1.95e+01, 2.63e+00]    [2.00e+01, 1.95e+01, 1.34e+01, 1.32e+01]    
30000     [2.96e+01, 2.18e+01, 2.63e+00]    [0.00e+00, 1.94e+01, 2.63e+00]    [1.98e+01, 1.94e+01, 1.35e+01, 1.33e+01]    

Best model at step 16000:
  train loss: 3.22e+01
  test loss: 2.26e+01
  test metric: [2.04e+01, 1.99e+01, 1.40e+01, 1.39e+01]

'train' took 48.792542 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.183891 s

'compile' took 0.845618 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.49e+02, 1.02e+02, 2.63e+00]    [0.00e+00, 5.64e+02, 2.63e+00]    [5.64e+02, 5.64e+02, 6.91e+02, 6.91e+02]    
1000      [3.38e+01, 1.18e+00, 2.63e+00]    [0.00e+00, 1.46e+01, 2.63e+00]    [1.64e+01, 1.46e+01, 9.56e+00, 9.25e+00]    
2000      [2.97e+01, 1.52e-01, 2.64e+00]    [0.00e+00, 2.33e+01, 2.64e+00]    [1.27e+01, 2.33e+01, 9.96e+00, 2.92e+01]    
3000      [2.79e+01, 2.05e-01, 2.64e+00]    [0.00e+00, 2.80e+01, 2.64e+00]    [1.53e+01, 2.80e+01, 8.49e+00, 3.98e+01]    
4000      [2.81e+01, 1.49e-01, 2.64e+00]    [0.00e+00, 4.34e+01, 2.64e+00]    [1.63e+01, 4.34e+01, 1.16e+01, 6.68e+01]    
5000      [2.72e+01, 2.25e-01, 2.64e+00]    [0.00e+00, 4.14e+01, 2.64e+00]    [1.36e+01, 4.14e+01, 9.96e+00, 6.69e+01]    
6000      [2.84e+01, 2.25e-01, 2.64e+00]    [0.00e+00, 6.16e+01, 2.64e+00]    [2.04e+01, 6.16e+01, 1.57e+01, 9.95e+01]    
7000      [2.47e+01, 1.22e-01, 2.64e+00]    [0.00e+00, 5.70e+01, 2.64e+00]    [1.28e+01, 5.70e+01, 1.09e+01, 9.06e+01]    
8000      [2.95e+01, 4.80e-01, 2.64e+00]    [0.00e+00, 4.92e+01, 2.64e+00]    [1.61e+01, 4.92e+01, 1.66e+01, 7.60e+01]    
9000      [2.50e+01, 1.63e-01, 2.63e+00]    [0.00e+00, 6.16e+01, 2.63e+00]    [1.25e+01, 6.16e+01, 1.01e+01, 9.47e+01]    
10000     [2.89e+01, 5.95e-01, 2.63e+00]    [0.00e+00, 5.55e+01, 2.63e+00]    [1.65e+01, 5.55e+01, 1.73e+01, 8.31e+01]    
11000     [2.42e+01, 1.47e-01, 2.63e+00]    [0.00e+00, 7.25e+01, 2.63e+00]    [1.38e+01, 7.25e+01, 1.06e+01, 1.11e+02]    
12000     [2.36e+01, 1.06e-01, 2.63e+00]    [0.00e+00, 7.50e+01, 2.63e+00]    [1.28e+01, 7.50e+01, 1.11e+01, 1.11e+02]    
13000     [2.53e+01, 1.81e-01, 2.63e+00]    [0.00e+00, 8.19e+01, 2.63e+00]    [1.67e+01, 8.19e+01, 1.19e+01, 1.20e+02]    
14000     [2.57e+01, 2.70e-01, 2.62e+00]    [0.00e+00, 8.46e+01, 2.62e+00]    [1.79e+01, 8.46e+01, 1.26e+01, 1.25e+02]    
15000     [2.52e+01, 1.96e-01, 2.62e+00]    [0.00e+00, 8.71e+01, 2.62e+00]    [1.72e+01, 8.71e+01, 1.23e+01, 1.26e+02]    
16000     [2.39e+01, 2.12e-01, 2.62e+00]    [0.00e+00, 8.64e+01, 2.62e+00]    [1.50e+01, 8.64e+01, 1.07e+01, 1.25e+02]    
17000     [2.31e+01, 9.82e-02, 2.62e+00]    [0.00e+00, 8.45e+01, 2.62e+00]    [1.28e+01, 8.45e+01, 1.02e+01, 1.17e+02]    
18000     [2.80e+01, 3.81e-01, 2.62e+00]    [0.00e+00, 1.00e+02, 2.62e+00]    [2.31e+01, 1.00e+02, 1.85e+01, 1.42e+02]    
19000     [2.45e+01, 2.23e-01, 2.61e+00]    [0.00e+00, 8.43e+01, 2.61e+00]    [1.33e+01, 8.43e+01, 1.22e+01, 1.13e+02]    
20000     [2.29e+01, 6.89e-02, 2.61e+00]    [0.00e+00, 9.39e+01, 2.61e+00]    [1.44e+01, 9.39e+01, 1.03e+01, 1.29e+02]    
21000     [2.24e+01, 4.70e-02, 2.61e+00]    [0.00e+00, 9.19e+01, 2.61e+00]    [1.31e+01, 9.19e+01, 1.00e+01, 1.23e+02]    
22000     [2.26e+01, 1.75e-01, 2.61e+00]    [0.00e+00, 9.59e+01, 2.61e+00]    [1.37e+01, 9.59e+01, 1.10e+01, 1.25e+02]    
23000     [2.68e+01, 3.29e-01, 2.61e+00]    [0.00e+00, 1.06e+02, 2.61e+00]    [2.19e+01, 1.06e+02, 1.65e+01, 1.45e+02]    
24000     [2.32e+01, 2.08e-01, 2.61e+00]    [0.00e+00, 1.01e+02, 2.61e+00]    [1.50e+01, 1.01e+02, 1.10e+01, 1.31e+02]    
25000     [2.27e+01, 6.66e-02, 2.61e+00]    [0.00e+00, 9.70e+01, 2.61e+00]    [1.39e+01, 9.70e+01, 1.04e+01, 1.22e+02]    
26000     [2.35e+01, 2.91e-01, 2.61e+00]    [0.00e+00, 9.57e+01, 2.61e+00]    [1.34e+01, 9.57e+01, 1.14e+01, 1.20e+02]    
27000     [2.25e+01, 1.06e-01, 2.60e+00]    [0.00e+00, 9.97e+01, 2.60e+00]    [1.31e+01, 9.97e+01, 9.95e+00, 1.25e+02]    
28000     [2.25e+01, 1.76e-01, 2.60e+00]    [0.00e+00, 1.07e+02, 2.60e+00]    [1.52e+01, 1.07e+02, 1.01e+01, 1.36e+02]    
29000     [2.30e+01, 1.52e-01, 2.60e+00]    [0.00e+00, 1.10e+02, 2.60e+00]    [1.61e+01, 1.10e+02, 1.06e+01, 1.38e+02]    
30000     [2.38e+01, 2.49e-01, 2.60e+00]    [0.00e+00, 1.00e+02, 2.60e+00]    [1.51e+01, 1.00e+02, 1.31e+01, 1.20e+02]    

Best model at step 21000:
  train loss: 2.51e+01
  test loss: 9.45e+01
  test metric: [1.31e+01, 9.19e+01, 1.00e+01, 1.23e+02]

'train' took 60.302584 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.349147 s

'compile' took 1.266692 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.46e+02, 4.20e+02, 2.78e+00]    [0.00e+00, 3.31e+02, 2.78e+00]    [3.31e+02, 3.31e+02, 4.51e+02, 4.51e+02]    
1000      [5.52e+01, 3.29e+01, 2.74e+00]    [0.00e+00, 5.87e+01, 2.74e+00]    [5.89e+01, 5.87e+01, 1.82e+01, 1.81e+01]    
2000      [3.65e+01, 1.87e+01, 2.73e+00]    [0.00e+00, 3.78e+01, 2.73e+00]    [3.78e+01, 3.78e+01, 1.45e+01, 1.39e+01]    
3000      [3.19e+01, 1.97e+01, 2.73e+00]    [0.00e+00, 2.51e+01, 2.73e+00]    [2.52e+01, 2.51e+01, 1.32e+01, 1.30e+01]    
4000      [3.06e+01, 1.48e+01, 2.72e+00]    [0.00e+00, 2.06e+01, 2.72e+00]    [2.07e+01, 2.06e+01, 1.32e+01, 1.28e+01]    
5000      [3.21e+01, 1.67e+01, 2.72e+00]    [0.00e+00, 2.63e+01, 2.72e+00]    [2.64e+01, 2.63e+01, 1.34e+01, 1.29e+01]    
6000      [2.90e+01, 7.49e+00, 2.72e+00]    [0.00e+00, 1.95e+01, 2.72e+00]    [1.96e+01, 1.95e+01, 1.30e+01, 1.25e+01]    
7000      [2.76e+01, 1.53e+00, 2.71e+00]    [0.00e+00, 2.06e+01, 2.71e+00]    [2.07e+01, 2.06e+01, 1.23e+01, 1.17e+01]    
8000      [2.79e+01, 6.63e+00, 2.71e+00]    [0.00e+00, 1.84e+01, 2.71e+00]    [1.84e+01, 1.84e+01, 1.28e+01, 1.24e+01]    
9000      [2.76e+01, 4.70e+00, 2.71e+00]    [0.00e+00, 1.81e+01, 2.71e+00]    [1.80e+01, 1.81e+01, 1.23e+01, 1.20e+01]    
10000     [2.97e+01, 1.53e+01, 2.71e+00]    [0.00e+00, 1.69e+01, 2.71e+00]    [1.74e+01, 1.69e+01, 1.27e+01, 1.30e+01]    
11000     [2.64e+01, 2.11e+00, 2.70e+00]    [0.00e+00, 1.93e+01, 2.70e+00]    [1.92e+01, 1.93e+01, 1.20e+01, 1.16e+01]    
12000     [2.80e+01, 1.13e+01, 2.70e+00]    [0.00e+00, 1.60e+01, 2.70e+00]    [1.64e+01, 1.60e+01, 1.32e+01, 1.35e+01]    
13000     [2.86e+01, 1.04e+01, 2.70e+00]    [0.00e+00, 2.17e+01, 2.70e+00]    [2.15e+01, 2.17e+01, 1.32e+01, 1.30e+01]    
14000     [2.72e+01, 9.64e+00, 2.70e+00]    [0.00e+00, 1.60e+01, 2.70e+00]    [1.64e+01, 1.60e+01, 1.34e+01, 1.36e+01]    
15000     [2.79e+01, 9.40e+00, 2.70e+00]    [0.00e+00, 2.11e+01, 2.70e+00]    [2.09e+01, 2.11e+01, 1.27e+01, 1.25e+01]    
16000     [2.85e+01, 1.06e+01, 2.69e+00]    [0.00e+00, 2.16e+01, 2.69e+00]    [2.14e+01, 2.16e+01, 1.33e+01, 1.31e+01]    
17000     [2.59e+01, 4.12e+00, 2.69e+00]    [0.00e+00, 1.72e+01, 2.69e+00]    [1.73e+01, 1.72e+01, 1.28e+01, 1.28e+01]    
18000     [2.58e+01, 5.22e+00, 2.69e+00]    [0.00e+00, 1.70e+01, 2.69e+00]    [1.71e+01, 1.70e+01, 1.29e+01, 1.28e+01]    
19000     [2.57e+01, 4.43e+00, 2.69e+00]    [0.00e+00, 1.72e+01, 2.69e+00]    [1.72e+01, 1.72e+01, 1.28e+01, 1.28e+01]    
20000     [2.65e+01, 6.69e+00, 2.68e+00]    [0.00e+00, 1.56e+01, 2.68e+00]    [1.58e+01, 1.56e+01, 1.39e+01, 1.39e+01]    
21000     [2.54e+01, 3.67e+00, 2.68e+00]    [0.00e+00, 1.90e+01, 2.68e+00]    [1.89e+01, 1.90e+01, 1.24e+01, 1.24e+01]    
22000     [2.58e+01, 5.28e+00, 2.68e+00]    [0.00e+00, 1.92e+01, 2.68e+00]    [1.88e+01, 1.92e+01, 1.25e+01, 1.22e+01]    
23000     [2.53e+01, 4.00e+00, 2.68e+00]    [0.00e+00, 1.85e+01, 2.68e+00]    [1.82e+01, 1.85e+01, 1.25e+01, 1.22e+01]    
24000     [2.62e+01, 5.42e+00, 2.67e+00]    [0.00e+00, 1.95e+01, 2.67e+00]    [1.92e+01, 1.95e+01, 1.24e+01, 1.24e+01]    
25000     [2.56e+01, 4.85e+00, 2.67e+00]    [0.00e+00, 1.91e+01, 2.67e+00]    [1.88e+01, 1.91e+01, 1.24e+01, 1.23e+01]    
26000     [2.84e+01, 1.31e+01, 2.67e+00]    [0.00e+00, 1.62e+01, 2.67e+00]    [1.67e+01, 1.62e+01, 1.34e+01, 1.36e+01]    
27000     [2.61e+01, 7.35e+00, 2.67e+00]    [0.00e+00, 1.55e+01, 2.67e+00]    [1.55e+01, 1.55e+01, 1.40e+01, 1.40e+01]    
28000     [2.64e+01, 8.33e+00, 2.66e+00]    [0.00e+00, 1.55e+01, 2.66e+00]    [1.59e+01, 1.55e+01, 1.38e+01, 1.41e+01]    
29000     [2.49e+01, 2.72e+00, 2.66e+00]    [0.00e+00, 1.85e+01, 2.66e+00]    [1.84e+01, 1.85e+01, 1.23e+01, 1.24e+01]    
30000     [2.58e+01, 5.66e+00, 2.66e+00]    [0.00e+00, 1.93e+01, 2.66e+00]    [1.90e+01, 1.93e+01, 1.24e+01, 1.22e+01]    

Best model at step 29000:
  train loss: 3.02e+01
  test loss: 2.12e+01
  test metric: [1.84e+01, 1.85e+01, 1.23e+01, 1.24e+01]

'train' took 59.120589 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.245347 s

'compile' took 1.016333 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.87e+02, 7.48e+02, 2.71e+00]    [0.00e+00, 1.75e+02, 2.71e+00]    [1.75e+02, 1.75e+02, 2.23e+02, 2.23e+02]    
1000      [4.97e+01, 2.70e+01, 2.69e+00]    [0.00e+00, 5.64e+01, 2.69e+00]    [5.65e+01, 5.64e+01, 2.30e+01, 2.31e+01]    
2000      [3.69e+01, 9.48e+00, 2.70e+00]    [0.00e+00, 3.84e+01, 2.70e+00]    [3.85e+01, 3.84e+01, 1.93e+01, 1.94e+01]    
3000      [3.48e+01, 1.13e+01, 2.71e+00]    [0.00e+00, 2.54e+01, 2.71e+00]    [2.53e+01, 2.54e+01, 1.67e+01, 1.67e+01]    
4000      [3.45e+01, 1.09e+01, 2.71e+00]    [0.00e+00, 2.58e+01, 2.71e+00]    [2.56e+01, 2.58e+01, 1.68e+01, 1.70e+01]    
5000      [3.20e+01, 1.32e+01, 2.70e+00]    [0.00e+00, 1.81e+01, 2.70e+00]    [1.79e+01, 1.81e+01, 1.46e+01, 1.45e+01]    
6000      [3.13e+01, 1.36e+00, 2.70e+00]    [0.00e+00, 1.96e+01, 2.70e+00]    [1.94e+01, 1.96e+01, 1.45e+01, 1.46e+01]    
7000      [3.11e+01, 1.21e+01, 2.70e+00]    [0.00e+00, 1.49e+01, 2.70e+00]    [1.46e+01, 1.49e+01, 1.31e+01, 1.30e+01]    
8000      [3.26e+01, 9.93e+00, 2.70e+00]    [0.00e+00, 2.07e+01, 2.70e+00]    [2.05e+01, 2.07e+01, 1.68e+01, 1.71e+01]    
9000      [2.95e+01, 7.85e+00, 2.70e+00]    [0.00e+00, 1.63e+01, 2.70e+00]    [1.60e+01, 1.63e+01, 1.30e+01, 1.30e+01]    
10000     [3.01e+01, 1.16e+01, 2.70e+00]    [0.00e+00, 1.50e+01, 2.70e+00]    [1.47e+01, 1.50e+01, 1.30e+01, 1.28e+01]    
11000     [3.13e+01, 9.10e+00, 2.70e+00]    [0.00e+00, 2.03e+01, 2.70e+00]    [2.00e+01, 2.03e+01, 1.59e+01, 1.62e+01]    
12000     [2.84e+01, 6.12e+00, 2.70e+00]    [0.00e+00, 1.67e+01, 2.70e+00]    [1.64e+01, 1.67e+01, 1.30e+01, 1.29e+01]    
13000     [2.94e+01, 4.89e+00, 2.70e+00]    [0.00e+00, 1.95e+01, 2.70e+00]    [1.92e+01, 1.95e+01, 1.42e+01, 1.45e+01]    
14000     [2.96e+01, 1.33e+01, 2.70e+00]    [0.00e+00, 1.44e+01, 2.70e+00]    [1.41e+01, 1.44e+01, 1.37e+01, 1.34e+01]    
15000     [2.83e+01, 2.18e+00, 2.70e+00]    [0.00e+00, 1.85e+01, 2.70e+00]    [1.81e+01, 1.85e+01, 1.40e+01, 1.41e+01]    
16000     [2.94e+01, 7.11e+00, 2.70e+00]    [0.00e+00, 1.95e+01, 2.70e+00]    [1.91e+01, 1.95e+01, 1.52e+01, 1.55e+01]    
17000     [2.91e+01, 1.25e+01, 2.69e+00]    [0.00e+00, 1.46e+01, 2.69e+00]    [1.42e+01, 1.46e+01, 1.43e+01, 1.40e+01]    
18000     [2.80e+01, 3.87e+00, 2.69e+00]    [0.00e+00, 1.88e+01, 2.69e+00]    [1.83e+01, 1.88e+01, 1.45e+01, 1.47e+01]    
19000     [2.86e+01, 6.10e+00, 2.69e+00]    [0.00e+00, 1.96e+01, 2.69e+00]    [1.92e+01, 1.96e+01, 1.51e+01, 1.53e+01]    
20000     [2.87e+01, 1.17e+01, 2.69e+00]    [0.00e+00, 1.48e+01, 2.69e+00]    [1.44e+01, 1.48e+01, 1.50e+01, 1.46e+01]    
21000     [2.74e+01, 3.46e+00, 2.69e+00]    [0.00e+00, 1.88e+01, 2.69e+00]    [1.83e+01, 1.88e+01, 1.47e+01, 1.48e+01]    
22000     [2.81e+01, 6.60e+00, 2.68e+00]    [0.00e+00, 1.94e+01, 2.68e+00]    [1.89e+01, 1.94e+01, 1.54e+01, 1.56e+01]    
23000     [2.82e+01, 1.12e+01, 2.68e+00]    [0.00e+00, 1.52e+01, 2.68e+00]    [1.47e+01, 1.52e+01, 1.55e+01, 1.51e+01]    
24000     [2.69e+01, 3.26e+00, 2.68e+00]    [0.00e+00, 1.87e+01, 2.68e+00]    [1.82e+01, 1.87e+01, 1.51e+01, 1.51e+01]    
25000     [2.75e+01, 6.17e+00, 2.68e+00]    [0.00e+00, 1.95e+01, 2.68e+00]    [1.90e+01, 1.95e+01, 1.55e+01, 1.57e+01]    
26000     [2.80e+01, 1.11e+01, 2.67e+00]    [0.00e+00, 1.49e+01, 2.67e+00]    [1.46e+01, 1.49e+01, 1.62e+01, 1.58e+01]    
27000     [2.68e+01, 4.28e+00, 2.67e+00]    [0.00e+00, 1.88e+01, 2.67e+00]    [1.83e+01, 1.88e+01, 1.59e+01, 1.60e+01]    
28000     [2.74e+01, 7.08e+00, 2.67e+00]    [0.00e+00, 2.00e+01, 2.67e+00]    [1.94e+01, 2.00e+01, 1.60e+01, 1.62e+01]    
29000     [2.80e+01, 1.12e+01, 2.66e+00]    [0.00e+00, 1.49e+01, 2.66e+00]    [1.49e+01, 1.49e+01, 1.65e+01, 1.64e+01]    
30000     [2.60e+01, 3.34e+00, 2.66e+00]    [0.00e+00, 1.85e+01, 2.66e+00]    [1.77e+01, 1.85e+01, 1.60e+01, 1.58e+01]    

Best model at step 30000:
  train loss: 3.20e+01
  test loss: 2.11e+01
  test metric: [1.77e+01, 1.85e+01, 1.60e+01, 1.58e+01]

'train' took 69.199114 s


Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.213429 s

'compile' took 1.082413 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.06e+02, 4.13e+02, 2.69e+00]    [0.00e+00, 7.16e+02, 2.69e+00]    [7.16e+02, 7.16e+02, 1.26e+03, 1.26e+03]    
1000      [4.08e+01, 1.11e+01, 2.66e+00]    [0.00e+00, 4.60e+01, 2.66e+00]    [4.59e+01, 4.60e+01, 1.97e+01, 1.98e+01]    
2000      [3.47e+01, 1.08e+01, 2.65e+00]    [0.00e+00, 3.09e+01, 2.65e+00]    [3.03e+01, 3.09e+01, 1.61e+01, 1.54e+01]    
3000      [3.32e+01, 4.06e+00, 2.65e+00]    [0.00e+00, 2.61e+01, 2.65e+00]    [2.52e+01, 2.61e+01, 1.84e+01, 2.08e+01]    
4000      [3.26e+01, 1.94e+00, 2.66e+00]    [0.00e+00, 2.22e+01, 2.66e+00]    [2.10e+01, 2.22e+01, 1.58e+01, 1.85e+01]    
5000      [3.27e+01, 2.24e+00, 2.66e+00]    [0.00e+00, 2.01e+01, 2.66e+00]    [1.86e+01, 2.01e+01, 1.52e+01, 1.70e+01]    
6000      [3.40e+01, 7.33e+00, 2.66e+00]    [0.00e+00, 2.53e+01, 2.66e+00]    [2.15e+01, 2.53e+01, 2.31e+01, 2.89e+01]    
7000      [3.12e+01, 3.19e+00, 2.66e+00]    [0.00e+00, 2.17e+01, 2.66e+00]    [2.01e+01, 2.17e+01, 1.53e+01, 1.70e+01]    
8000      [3.33e+01, 6.73e+00, 2.66e+00]    [0.00e+00, 1.95e+01, 2.66e+00]    [2.31e+01, 1.95e+01, 1.77e+01, 1.66e+01]    
9000      [2.93e+01, 1.32e+00, 2.66e+00]    [0.00e+00, 2.31e+01, 2.66e+00]    [2.15e+01, 2.31e+01, 1.79e+01, 2.53e+01]    
10000     [2.89e+01, 1.58e+00, 2.67e+00]    [0.00e+00, 2.38e+01, 2.67e+00]    [2.21e+01, 2.38e+01, 1.88e+01, 2.68e+01]    
11000     [2.93e+01, 2.78e+00, 2.67e+00]    [0.00e+00, 2.09e+01, 2.67e+00]    [1.92e+01, 2.09e+01, 1.81e+01, 1.71e+01]    
12000     [2.80e+01, 1.14e+00, 2.67e+00]    [0.00e+00, 2.08e+01, 2.67e+00]    [1.90e+01, 2.08e+01, 1.75e+01, 1.79e+01]    
13000     [2.76e+01, 2.86e+00, 2.67e+00]    [0.00e+00, 2.15e+01, 2.67e+00]    [1.96e+01, 2.15e+01, 1.69e+01, 2.00e+01]    
14000     [2.73e+01, 2.20e+00, 2.67e+00]    [0.00e+00, 2.29e+01, 2.67e+00]    [2.09e+01, 2.29e+01, 1.71e+01, 2.19e+01]    
15000     [2.86e+01, 3.67e+00, 2.68e+00]    [0.00e+00, 2.13e+01, 2.68e+00]    [2.08e+01, 2.13e+01, 1.94e+01, 1.67e+01]    
16000     [2.89e+01, 5.56e+00, 2.68e+00]    [0.00e+00, 2.22e+01, 2.68e+00]    [2.14e+01, 2.22e+01, 1.96e+01, 1.67e+01]    
17000     [2.83e+01, 4.69e+00, 2.68e+00]    [0.00e+00, 2.24e+01, 2.68e+00]    [2.13e+01, 2.24e+01, 1.98e+01, 1.69e+01]    
18000     [2.78e+01, 4.76e+00, 2.68e+00]    [0.00e+00, 2.30e+01, 2.68e+00]    [2.14e+01, 2.30e+01, 2.00e+01, 1.70e+01]    
19000     [2.83e+01, 4.01e+00, 2.69e+00]    [0.00e+00, 2.14e+01, 2.69e+00]    [2.31e+01, 2.14e+01, 2.02e+01, 1.68e+01]    
20000     [3.04e+01, 7.89e+00, 2.69e+00]    [0.00e+00, 2.19e+01, 2.69e+00]    [2.57e+01, 2.19e+01, 2.12e+01, 1.78e+01]    
21000     [3.05e+01, 7.92e+00, 2.69e+00]    [0.00e+00, 3.19e+01, 2.69e+00]    [2.69e+01, 3.19e+01, 2.94e+01, 4.09e+01]    
22000     [2.52e+01, 2.40e+00, 2.69e+00]    [0.00e+00, 2.32e+01, 2.69e+00]    [2.11e+01, 2.32e+01, 1.77e+01, 2.00e+01]    
23000     [2.96e+01, 8.61e+00, 2.69e+00]    [0.00e+00, 2.83e+01, 2.69e+00]    [2.46e+01, 2.83e+01, 2.44e+01, 3.49e+01]    
24000     [2.58e+01, 3.93e+00, 2.70e+00]    [0.00e+00, 2.63e+01, 2.70e+00]    [2.43e+01, 2.63e+01, 1.76e+01, 2.36e+01]    
25000     [2.87e+01, 5.42e+00, 2.70e+00]    [0.00e+00, 2.08e+01, 2.70e+00]    [2.59e+01, 2.08e+01, 2.13e+01, 1.79e+01]    
26000     [2.49e+01, 9.66e-01, 2.70e+00]    [0.00e+00, 2.25e+01, 2.70e+00]    [2.11e+01, 2.25e+01, 1.99e+01, 1.63e+01]    
27000     [2.48e+01, 1.80e+00, 2.70e+00]    [0.00e+00, 2.29e+01, 2.70e+00]    [2.16e+01, 2.29e+01, 1.94e+01, 1.65e+01]    
28000     [2.94e+01, 7.58e+00, 2.70e+00]    [0.00e+00, 2.17e+01, 2.70e+00]    [2.71e+01, 2.17e+01, 2.18e+01, 1.87e+01]    
29000     [2.57e+01, 2.51e+00, 2.70e+00]    [0.00e+00, 2.58e+01, 2.70e+00]    [2.47e+01, 2.58e+01, 1.95e+01, 2.54e+01]    
30000     [2.47e+01, 2.97e+00, 2.70e+00]    [0.00e+00, 2.33e+01, 2.70e+00]    [2.25e+01, 2.33e+01, 1.77e+01, 1.94e+01]    

Best model at step 26000:
  train loss: 2.86e+01
  test loss: 2.52e+01
  test metric: [2.11e+01, 2.25e+01, 1.99e+01, 1.63e+01]

'train' took 64.577505 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.219338 s

'compile' took 1.001246 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.10e+02, 1.10e+02, 2.78e+00]    [0.00e+00, 7.17e+02, 2.78e+00]    [7.17e+02, 7.17e+02, 8.42e+02, 8.42e+02]    
1000      [3.34e+01, 5.77e+00, 2.79e+00]    [0.00e+00, 2.13e+01, 2.79e+00]    [1.81e+01, 2.13e+01, 1.76e+01, 2.66e+01]    
2000      [3.06e+01, 2.02e+00, 2.80e+00]    [0.00e+00, 3.73e+01, 2.80e+00]    [1.54e+01, 3.73e+01, 1.36e+01, 5.22e+01]    
3000      [2.85e+01, 1.37e-01, 2.81e+00]    [0.00e+00, 5.75e+01, 2.81e+00]    [1.63e+01, 5.75e+01, 9.37e+00, 8.18e+01]    
4000      [2.71e+01, 1.50e-01, 2.81e+00]    [0.00e+00, 7.00e+01, 2.81e+00]    [1.79e+01, 7.00e+01, 7.40e+00, 9.72e+01]    
5000      [2.62e+01, 2.56e-01, 2.81e+00]    [0.00e+00, 7.52e+01, 2.81e+00]    [1.80e+01, 7.52e+01, 8.03e+00, 1.03e+02]    
6000      [3.05e+01, 3.66e-01, 2.81e+00]    [0.00e+00, 5.98e+01, 2.81e+00]    [2.30e+01, 5.98e+01, 1.74e+01, 7.58e+01]    
7000      [3.11e+01, 4.56e-01, 2.81e+00]    [0.00e+00, 5.60e+01, 2.81e+00]    [2.53e+01, 5.60e+01, 1.95e+01, 6.65e+01]    
8000      [2.64e+01, 4.13e-01, 2.81e+00]    [0.00e+00, 6.17e+01, 2.81e+00]    [1.67e+01, 6.17e+01, 1.15e+01, 7.26e+01]    
9000      [2.63e+01, 2.12e-01, 2.81e+00]    [0.00e+00, 7.25e+01, 2.81e+00]    [1.62e+01, 7.25e+01, 1.44e+01, 8.82e+01]    
10000     [2.68e+01, 1.88e-01, 2.81e+00]    [0.00e+00, 4.89e+01, 2.81e+00]    [1.94e+01, 4.89e+01, 1.34e+01, 4.86e+01]    
11000     [2.46e+01, 1.03e-01, 2.81e+00]    [0.00e+00, 5.67e+01, 2.81e+00]    [1.44e+01, 5.67e+01, 1.07e+01, 5.89e+01]    
12000     [2.48e+01, 2.29e-01, 2.80e+00]    [0.00e+00, 4.18e+01, 2.80e+00]    [1.60e+01, 4.18e+01, 1.17e+01, 3.57e+01]    
13000     [2.51e+01, 3.51e-01, 2.80e+00]    [0.00e+00, 5.06e+01, 2.80e+00]    [1.45e+01, 5.06e+01, 1.24e+01, 4.61e+01]    
14000     [2.84e+01, 4.04e-01, 2.80e+00]    [0.00e+00, 5.60e+01, 2.80e+00]    [1.87e+01, 5.60e+01, 2.11e+01, 5.30e+01]    
15000     [2.40e+01, 1.63e-01, 2.80e+00]    [0.00e+00, 4.02e+01, 2.80e+00]    [1.50e+01, 4.02e+01, 1.12e+01, 2.98e+01]    
16000     [2.69e+01, 5.59e-01, 2.80e+00]    [0.00e+00, 4.14e+01, 2.80e+00]    [1.60e+01, 4.14e+01, 1.62e+01, 3.05e+01]    
17000     [2.28e+01, 3.07e-01, 2.79e+00]    [0.00e+00, 2.31e+01, 2.79e+00]    [1.40e+01, 2.31e+01, 8.99e+00, 1.95e+01]    
18000     [3.16e+01, 5.84e-01, 2.79e+00]    [0.00e+00, 4.35e+01, 2.79e+00]    [3.06e+01, 4.35e+01, 2.83e+01, 4.30e+01]    
19000     [2.52e+01, 2.46e-01, 2.79e+00]    [0.00e+00, 2.63e+01, 2.79e+00]    [1.59e+01, 2.63e+01, 1.51e+01, 2.03e+01]    
20000     [2.64e+01, 4.52e-01, 2.79e+00]    [0.00e+00, 2.52e+01, 2.79e+00]    [1.69e+01, 2.52e+01, 1.79e+01, 2.12e+01]    
21000     [2.67e+01, 5.57e-01, 2.78e+00]    [0.00e+00, 2.39e+01, 2.78e+00]    [1.83e+01, 2.39e+01, 2.02e+01, 2.39e+01]    
22000     [2.23e+01, 7.76e-02, 2.78e+00]    [0.00e+00, 4.15e+01, 2.78e+00]    [1.41e+01, 4.15e+01, 9.85e+00, 3.52e+01]    
23000     [2.47e+01, 2.20e-01, 2.78e+00]    [0.00e+00, 5.10e+01, 2.78e+00]    [1.87e+01, 5.10e+01, 1.13e+01, 4.79e+01]    
24000     [2.61e+01, 5.08e-01, 2.78e+00]    [0.00e+00, 3.30e+01, 2.78e+00]    [1.72e+01, 3.30e+01, 1.81e+01, 2.53e+01]    
25000     [2.52e+01, 4.05e-01, 2.77e+00]    [0.00e+00, 3.65e+01, 2.77e+00]    [1.79e+01, 3.65e+01, 1.78e+01, 2.56e+01]    
26000     [2.31e+01, 2.03e-01, 2.77e+00]    [0.00e+00, 4.34e+01, 2.77e+00]    [1.40e+01, 4.34e+01, 1.03e+01, 3.41e+01]    
27000     [2.29e+01, 9.83e-02, 2.77e+00]    [0.00e+00, 5.59e+01, 2.77e+00]    [1.60e+01, 5.59e+01, 9.82e+00, 4.93e+01]    
28000     [2.27e+01, 1.69e-01, 2.77e+00]    [0.00e+00, 4.64e+01, 2.77e+00]    [1.47e+01, 4.64e+01, 1.05e+01, 3.51e+01]    
29000     [2.41e+01, 2.74e-01, 2.77e+00]    [0.00e+00, 4.46e+01, 2.77e+00]    [1.49e+01, 4.46e+01, 1.31e+01, 3.19e+01]    
30000     [2.30e+01, 2.28e-01, 2.76e+00]    [0.00e+00, 4.83e+01, 2.76e+00]    [1.46e+01, 4.83e+01, 1.17e+01, 3.47e+01]    

Best model at step 22000:
  train loss: 2.51e+01
  test loss: 4.43e+01
  test metric: [1.41e+01, 4.15e+01, 9.85e+00, 3.52e+01]

'train' took 58.319770 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.202002 s

'compile' took 0.988032 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.71e+02, 9.73e+02, 2.69e+00]    [0.00e+00, 2.84e+02, 2.69e+00]    [2.84e+02, 2.84e+02, 3.58e+02, 3.58e+02]    
1000      [6.48e+01, 3.14e+01, 2.67e+00]    [0.00e+00, 6.73e+01, 2.67e+00]    [6.73e+01, 6.73e+01, 2.79e+01, 2.79e+01]    
2000      [5.25e+01, 1.40e+01, 2.66e+00]    [0.00e+00, 5.55e+01, 2.66e+00]    [5.54e+01, 5.55e+01, 1.91e+01, 1.89e+01]    
3000      [4.05e+01, 1.82e+01, 2.66e+00]    [0.00e+00, 4.23e+01, 2.66e+00]    [4.21e+01, 4.23e+01, 1.88e+01, 1.85e+01]    
4000      [3.39e+01, 5.25e+00, 2.66e+00]    [0.00e+00, 3.63e+01, 2.66e+00]    [3.60e+01, 3.63e+01, 1.44e+01, 1.39e+01]    
5000      [3.12e+01, 6.96e+00, 2.66e+00]    [0.00e+00, 3.18e+01, 2.66e+00]    [3.14e+01, 3.18e+01, 1.41e+01, 1.35e+01]    
6000      [2.99e+01, 8.89e+00, 2.66e+00]    [0.00e+00, 2.95e+01, 2.66e+00]    [2.90e+01, 2.95e+01, 1.41e+01, 1.34e+01]    
7000      [3.03e+01, 2.12e+01, 2.66e+00]    [0.00e+00, 2.85e+01, 2.66e+00]    [2.79e+01, 2.85e+01, 1.27e+01, 1.19e+01]    
8000      [2.88e+01, 1.47e+01, 2.66e+00]    [0.00e+00, 2.65e+01, 2.66e+00]    [2.58e+01, 2.65e+01, 1.43e+01, 1.34e+01]    
9000      [2.98e+01, 1.97e+01, 2.66e+00]    [0.00e+00, 2.59e+01, 2.66e+00]    [2.51e+01, 2.59e+01, 1.33e+01, 1.24e+01]    
10000     [2.95e+01, 2.49e+01, 2.66e+00]    [0.00e+00, 2.38e+01, 2.66e+00]    [2.37e+01, 2.38e+01, 1.36e+01, 1.40e+01]    
11000     [2.67e+01, 8.12e+00, 2.66e+00]    [0.00e+00, 2.36e+01, 2.66e+00]    [2.27e+01, 2.36e+01, 1.37e+01, 1.27e+01]    
12000     [2.73e+01, 1.35e+01, 2.66e+00]    [0.00e+00, 2.34e+01, 2.66e+00]    [2.24e+01, 2.34e+01, 1.39e+01, 1.29e+01]    
13000     [2.69e+01, 1.34e+01, 2.66e+00]    [0.00e+00, 2.27e+01, 2.66e+00]    [2.26e+01, 2.27e+01, 1.36e+01, 1.40e+01]    
14000     [2.84e+01, 1.96e+01, 2.66e+00]    [0.00e+00, 2.39e+01, 2.66e+00]    [2.26e+01, 2.39e+01, 1.31e+01, 1.20e+01]    
15000     [2.63e+01, 1.47e+01, 2.66e+00]    [0.00e+00, 2.27e+01, 2.66e+00]    [2.20e+01, 2.27e+01, 1.39e+01, 1.37e+01]    
16000     [2.56e+01, 1.08e+01, 2.66e+00]    [0.00e+00, 2.25e+01, 2.66e+00]    [2.18e+01, 2.25e+01, 1.39e+01, 1.38e+01]    
17000     [2.61e+01, 9.09e+00, 2.66e+00]    [0.00e+00, 2.28e+01, 2.66e+00]    [2.13e+01, 2.28e+01, 1.43e+01, 1.31e+01]    
18000     [2.59e+01, 6.91e+00, 2.66e+00]    [0.00e+00, 2.27e+01, 2.66e+00]    [2.15e+01, 2.27e+01, 1.40e+01, 1.35e+01]    
19000     [2.53e+01, 4.74e+00, 2.66e+00]    [0.00e+00, 2.30e+01, 2.66e+00]    [2.12e+01, 2.30e+01, 1.45e+01, 1.33e+01]    
20000     [2.76e+01, 1.77e+01, 2.66e+00]    [0.00e+00, 2.33e+01, 2.66e+00]    [2.11e+01, 2.33e+01, 1.43e+01, 1.28e+01]    
21000     [2.53e+01, 6.44e+00, 2.65e+00]    [0.00e+00, 2.31e+01, 2.65e+00]    [2.08e+01, 2.31e+01, 1.46e+01, 1.30e+01]    
22000     [2.73e+01, 1.73e+01, 2.65e+00]    [0.00e+00, 2.33e+01, 2.65e+00]    [2.08e+01, 2.33e+01, 1.44e+01, 1.27e+01]    
23000     [2.46e+01, 3.27e+00, 2.65e+00]    [0.00e+00, 2.25e+01, 2.65e+00]    [2.10e+01, 2.25e+01, 1.41e+01, 1.39e+01]    
24000     [2.91e+01, 3.15e+01, 2.65e+00]    [0.00e+00, 2.23e+01, 2.65e+00]    [2.10e+01, 2.23e+01, 1.42e+01, 1.44e+01]    
25000     [2.89e+01, 2.57e+01, 2.65e+00]    [0.00e+00, 2.37e+01, 2.65e+00]    [2.03e+01, 2.37e+01, 1.43e+01, 1.24e+01]    
26000     [2.45e+01, 3.54e+00, 2.65e+00]    [0.00e+00, 2.26e+01, 2.65e+00]    [2.06e+01, 2.26e+01, 1.42e+01, 1.40e+01]    
27000     [2.41e+01, 2.24e+00, 2.65e+00]    [0.00e+00, 2.30e+01, 2.65e+00]    [2.00e+01, 2.30e+01, 1.47e+01, 1.35e+01]    
28000     [2.47e+01, 3.63e+00, 2.65e+00]    [0.00e+00, 2.31e+01, 2.65e+00]    [2.02e+01, 2.31e+01, 1.44e+01, 1.35e+01]    
29000     [2.66e+01, 1.72e+01, 2.65e+00]    [0.00e+00, 2.40e+01, 2.65e+00]    [1.96e+01, 2.40e+01, 1.48e+01, 1.25e+01]    
30000     [2.68e+01, 2.12e+01, 2.65e+00]    [0.00e+00, 2.31e+01, 2.65e+00]    [2.01e+01, 2.31e+01, 1.44e+01, 1.38e+01]    

Best model at step 27000:
  train loss: 2.90e+01
  test loss: 2.57e+01
  test metric: [2.00e+01, 2.30e+01, 1.47e+01, 1.35e+01]

'train' took 64.357018 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.232423 s

'compile' took 1.349948 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.00e+02, 3.38e+02, 2.73e+00]    [0.00e+00, 1.74e+02, 2.73e+00]    [1.74e+02, 1.74e+02, 1.22e+02, 1.22e+02]    
1000      [5.41e+01, 3.58e+01, 2.69e+00]    [0.00e+00, 5.30e+01, 2.69e+00]    [5.29e+01, 5.30e+01, 2.79e+01, 2.78e+01]    
2000      [3.77e+01, 3.10e+01, 2.66e+00]    [0.00e+00, 2.95e+01, 2.66e+00]    [2.95e+01, 2.95e+01, 2.20e+01, 2.20e+01]    
3000      [3.31e+01, 2.12e+01, 2.64e+00]    [0.00e+00, 2.04e+01, 2.64e+00]    [2.04e+01, 2.04e+01, 1.70e+01, 1.70e+01]    
4000      [3.19e+01, 8.92e+00, 2.64e+00]    [0.00e+00, 2.37e+01, 2.64e+00]    [2.36e+01, 2.37e+01, 1.49e+01, 1.49e+01]    
5000      [2.93e+01, 1.76e+00, 2.63e+00]    [0.00e+00, 2.15e+01, 2.63e+00]    [2.14e+01, 2.15e+01, 1.52e+01, 1.51e+01]    
6000      [3.11e+01, 9.54e+00, 2.63e+00]    [0.00e+00, 2.33e+01, 2.63e+00]    [2.32e+01, 2.33e+01, 1.67e+01, 1.67e+01]    
7000      [2.82e+01, 1.24e+00, 2.62e+00]    [0.00e+00, 2.08e+01, 2.62e+00]    [2.06e+01, 2.08e+01, 1.63e+01, 1.63e+01]    
8000      [2.82e+01, 3.94e+00, 2.62e+00]    [0.00e+00, 2.14e+01, 2.62e+00]    [2.11e+01, 2.14e+01, 1.64e+01, 1.64e+01]    
9000      [3.01e+01, 1.33e+01, 2.62e+00]    [0.00e+00, 1.70e+01, 2.62e+00]    [1.69e+01, 1.70e+01, 1.72e+01, 1.71e+01]    
10000     [3.25e+01, 1.86e+01, 2.61e+00]    [0.00e+00, 2.51e+01, 2.61e+00]    [2.47e+01, 2.51e+01, 1.93e+01, 1.96e+01]    
11000     [2.82e+01, 8.87e+00, 2.61e+00]    [0.00e+00, 1.88e+01, 2.61e+00]    [1.84e+01, 1.88e+01, 1.59e+01, 1.58e+01]    
12000     [2.85e+01, 5.93e+00, 2.61e+00]    [0.00e+00, 1.75e+01, 2.61e+00]    [1.75e+01, 1.75e+01, 1.64e+01, 1.65e+01]    
13000     [3.29e+01, 2.42e+01, 2.60e+00]    [0.00e+00, 1.79e+01, 2.60e+00]    [1.84e+01, 1.79e+01, 1.58e+01, 1.61e+01]    
14000     [2.74e+01, 6.53e+00, 2.60e+00]    [0.00e+00, 1.79e+01, 2.60e+00]    [1.74e+01, 1.79e+01, 1.61e+01, 1.60e+01]    
15000     [2.90e+01, 1.30e+01, 2.60e+00]    [0.00e+00, 1.70e+01, 2.60e+00]    [1.65e+01, 1.70e+01, 1.65e+01, 1.61e+01]    
16000     [2.74e+01, 7.48e+00, 2.59e+00]    [0.00e+00, 1.83e+01, 2.59e+00]    [1.77e+01, 1.83e+01, 1.55e+01, 1.53e+01]    
17000     [2.87e+01, 1.08e+01, 2.59e+00]    [0.00e+00, 2.24e+01, 2.59e+00]    [2.17e+01, 2.24e+01, 1.70e+01, 1.75e+01]    
18000     [3.00e+01, 1.72e+01, 2.59e+00]    [0.00e+00, 1.62e+01, 2.59e+00]    [1.65e+01, 1.62e+01, 1.65e+01, 1.67e+01]    
19000     [2.96e+01, 1.60e+01, 2.58e+00]    [0.00e+00, 1.60e+01, 2.58e+00]    [1.61e+01, 1.60e+01, 1.67e+01, 1.67e+01]    
20000     [2.80e+01, 1.12e+01, 2.58e+00]    [0.00e+00, 1.72e+01, 2.58e+00]    [1.65e+01, 1.72e+01, 1.61e+01, 1.56e+01]    
21000     [3.01e+01, 1.93e+01, 2.58e+00]    [0.00e+00, 1.63e+01, 2.58e+00]    [1.70e+01, 1.63e+01, 1.60e+01, 1.65e+01]    
22000     [2.65e+01, 5.62e+00, 2.57e+00]    [0.00e+00, 2.06e+01, 2.57e+00]    [1.98e+01, 2.06e+01, 1.57e+01, 1.60e+01]    
23000     [2.74e+01, 8.01e+00, 2.56e+00]    [0.00e+00, 2.19e+01, 2.56e+00]    [2.11e+01, 2.19e+01, 1.58e+01, 1.62e+01]    
24000     [2.70e+01, 7.55e+00, 2.56e+00]    [0.00e+00, 2.14e+01, 2.56e+00]    [2.06e+01, 2.14e+01, 1.57e+01, 1.60e+01]    
25000     [2.78e+01, 9.64e+00, 2.55e+00]    [0.00e+00, 2.22e+01, 2.55e+00]    [2.14e+01, 2.22e+01, 1.60e+01, 1.64e+01]    
26000     [2.75e+01, 8.38e+00, 2.55e+00]    [0.00e+00, 2.21e+01, 2.55e+00]    [2.12e+01, 2.21e+01, 1.57e+01, 1.61e+01]    
27000     [2.72e+01, 7.33e+00, 2.54e+00]    [0.00e+00, 2.22e+01, 2.54e+00]    [2.13e+01, 2.22e+01, 1.53e+01, 1.57e+01]    
28000     [2.73e+01, 8.24e+00, 2.54e+00]    [0.00e+00, 2.20e+01, 2.54e+00]    [2.10e+01, 2.20e+01, 1.53e+01, 1.57e+01]    
29000     [2.53e+01, 2.20e+00, 2.53e+00]    [0.00e+00, 2.03e+01, 2.53e+00]    [1.94e+01, 2.03e+01, 1.45e+01, 1.45e+01]    
30000     [2.73e+01, 1.27e+01, 2.53e+00]    [0.00e+00, 1.69e+01, 2.53e+00]    [1.59e+01, 1.69e+01, 1.62e+01, 1.54e+01]    

Best model at step 29000:
  train loss: 3.01e+01
  test loss: 2.29e+01
  test metric: [1.94e+01, 2.03e+01, 1.45e+01, 1.45e+01]

'train' took 64.922048 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.222432 s

'compile' took 1.071549 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.00e+02, 8.60e+02, 2.71e+00]    [0.00e+00, 4.82e+02, 2.71e+00]    [4.82e+02, 4.82e+02, 1.17e+03, 1.17e+03]    
1000      [4.60e+01, 4.11e+00, 2.69e+00]    [0.00e+00, 6.39e+01, 2.69e+00]    [6.36e+01, 6.39e+01, 1.77e+01, 1.78e+01]    
2000      [3.47e+01, 4.90e+00, 2.69e+00]    [0.00e+00, 4.48e+01, 2.69e+00]    [4.43e+01, 4.48e+01, 1.80e+01, 1.77e+01]    
3000      [3.10e+01, 1.69e+00, 2.68e+00]    [0.00e+00, 3.78e+01, 2.68e+00]    [3.70e+01, 3.78e+01, 1.58e+01, 1.55e+01]    
4000      [3.08e+01, 6.95e+00, 2.68e+00]    [0.00e+00, 3.58e+01, 2.68e+00]    [3.48e+01, 3.58e+01, 1.60e+01, 1.79e+01]    
5000      [2.85e+01, 5.06e+00, 2.67e+00]    [0.00e+00, 3.02e+01, 2.67e+00]    [2.89e+01, 3.02e+01, 1.43e+01, 1.40e+01]    
6000      [2.93e+01, 7.62e+00, 2.67e+00]    [0.00e+00, 3.19e+01, 2.67e+00]    [3.02e+01, 3.19e+01, 1.49e+01, 1.74e+01]    
7000      [2.69e+01, 2.16e+00, 2.67e+00]    [0.00e+00, 3.05e+01, 2.67e+00]    [2.85e+01, 3.05e+01, 1.45e+01, 1.53e+01]    
8000      [2.83e+01, 7.37e+00, 2.67e+00]    [0.00e+00, 3.24e+01, 2.67e+00]    [3.01e+01, 3.24e+01, 1.49e+01, 1.90e+01]    
9000      [2.69e+01, 7.64e+00, 2.67e+00]    [0.00e+00, 2.63e+01, 2.67e+00]    [2.79e+01, 2.63e+01, 1.48e+01, 1.58e+01]    
10000     [2.84e+01, 9.44e+00, 2.67e+00]    [0.00e+00, 3.35e+01, 2.67e+00]    [3.06e+01, 3.35e+01, 1.60e+01, 2.26e+01]    
11000     [2.73e+01, 9.35e+00, 2.67e+00]    [0.00e+00, 2.52e+01, 2.67e+00]    [2.92e+01, 2.52e+01, 1.58e+01, 1.63e+01]    
12000     [2.58e+01, 3.79e+00, 2.67e+00]    [0.00e+00, 3.07e+01, 2.67e+00]    [2.74e+01, 3.07e+01, 1.50e+01, 1.90e+01]    
13000     [2.63e+01, 7.34e+00, 2.66e+00]    [0.00e+00, 2.46e+01, 2.66e+00]    [2.76e+01, 2.46e+01, 1.54e+01, 1.68e+01]    
14000     [2.55e+01, 3.15e+00, 2.66e+00]    [0.00e+00, 3.11e+01, 2.66e+00]    [2.76e+01, 3.11e+01, 1.51e+01, 2.02e+01]    
15000     [2.70e+01, 1.03e+01, 2.66e+00]    [0.00e+00, 2.40e+01, 2.66e+00]    [2.78e+01, 2.40e+01, 1.61e+01, 1.75e+01]    
16000     [2.52e+01, 5.89e+00, 2.66e+00]    [0.00e+00, 2.63e+01, 2.66e+00]    [2.50e+01, 2.63e+01, 1.65e+01, 1.63e+01]    
17000     [2.58e+01, 5.10e+00, 2.65e+00]    [0.00e+00, 3.24e+01, 2.65e+00]    [2.86e+01, 3.24e+01, 1.65e+01, 2.41e+01]    
18000     [2.54e+01, 7.15e+00, 2.65e+00]    [0.00e+00, 2.62e+01, 2.65e+00]    [2.51e+01, 2.62e+01, 1.65e+01, 1.64e+01]    
19000     [2.50e+01, 4.21e+00, 2.65e+00]    [0.00e+00, 3.02e+01, 2.65e+00]    [2.62e+01, 3.02e+01, 1.64e+01, 2.16e+01]    
20000     [2.52e+01, 6.85e+00, 2.65e+00]    [0.00e+00, 2.59e+01, 2.65e+00]    [2.51e+01, 2.59e+01, 1.66e+01, 1.66e+01]    
21000     [2.39e+01, 2.62e+00, 2.64e+00]    [0.00e+00, 2.77e+01, 2.64e+00]    [2.36e+01, 2.77e+01, 1.79e+01, 1.76e+01]    
22000     [2.40e+01, 3.20e+00, 2.64e+00]    [0.00e+00, 2.76e+01, 2.64e+00]    [2.34e+01, 2.76e+01, 1.80e+01, 1.81e+01]    
23000     [2.39e+01, 2.66e+00, 2.64e+00]    [0.00e+00, 3.04e+01, 2.64e+00]    [2.62e+01, 3.04e+01, 1.65e+01, 2.19e+01]    
24000     [2.49e+01, 5.99e+00, 2.64e+00]    [0.00e+00, 3.25e+01, 2.64e+00]    [2.82e+01, 3.25e+01, 1.81e+01, 2.64e+01]    
25000     [2.57e+01, 7.71e+00, 2.64e+00]    [0.00e+00, 2.58e+01, 2.64e+00]    [2.50e+01, 2.58e+01, 1.69e+01, 1.70e+01]    
26000     [2.54e+01, 8.42e+00, 2.63e+00]    [0.00e+00, 3.25e+01, 2.63e+00]    [2.81e+01, 3.25e+01, 1.91e+01, 2.77e+01]    
27000     [2.42e+01, 5.89e+00, 2.63e+00]    [0.00e+00, 3.04e+01, 2.63e+00]    [2.60e+01, 3.04e+01, 1.75e+01, 2.35e+01]    
28000     [2.45e+01, 6.04e+00, 2.63e+00]    [0.00e+00, 3.25e+01, 2.63e+00]    [2.81e+01, 3.25e+01, 1.85e+01, 2.67e+01]    
29000     [2.44e+01, 6.53e+00, 2.63e+00]    [0.00e+00, 3.16e+01, 2.63e+00]    [2.72e+01, 3.16e+01, 1.82e+01, 2.58e+01]    
30000     [2.30e+01, 2.42e+00, 2.62e+00]    [0.00e+00, 3.02e+01, 2.62e+00]    [2.57e+01, 3.02e+01, 1.72e+01, 2.17e+01]    

Best model at step 30000:
  train loss: 2.81e+01
  test loss: 3.28e+01
  test metric: [2.57e+01, 3.02e+01, 1.72e+01, 2.17e+01]

'train' took 54.546718 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.203433 s

'compile' took 0.970771 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.14e+02, 4.83e+02, 2.69e+00]    [0.00e+00, 3.68e+02, 2.69e+00]    [3.68e+02, 3.68e+02, 7.29e+02, 7.29e+02]    
1000      [4.07e+01, 5.55e+00, 2.68e+00]    [0.00e+00, 4.38e+01, 2.68e+00]    [4.23e+01, 4.38e+01, 2.20e+01, 1.98e+01]    
2000      [3.10e+01, 2.78e+00, 2.68e+00]    [0.00e+00, 3.53e+01, 2.68e+00]    [3.32e+01, 3.53e+01, 1.45e+01, 1.52e+01]    
3000      [3.08e+01, 7.32e+00, 2.68e+00]    [0.00e+00, 3.53e+01, 2.68e+00]    [3.21e+01, 3.53e+01, 1.66e+01, 2.21e+01]    
4000      [2.71e+01, 2.84e+00, 2.67e+00]    [0.00e+00, 2.89e+01, 2.67e+00]    [2.47e+01, 2.89e+01, 1.42e+01, 1.34e+01]    
5000      [2.65e+01, 7.48e-01, 2.67e+00]    [0.00e+00, 2.57e+01, 2.67e+00]    [2.26e+01, 2.57e+01, 1.45e+01, 1.39e+01]    
6000      [2.64e+01, 1.84e+00, 2.67e+00]    [0.00e+00, 2.85e+01, 2.67e+00]    [2.29e+01, 2.85e+01, 1.33e+01, 1.67e+01]    
7000      [2.93e+01, 1.02e+01, 2.66e+00]    [0.00e+00, 3.24e+01, 2.66e+00]    [2.71e+01, 3.24e+01, 1.79e+01, 3.05e+01]    
8000      [2.54e+01, 5.67e+00, 2.66e+00]    [0.00e+00, 2.48e+01, 2.66e+00]    [2.15e+01, 2.48e+01, 1.35e+01, 1.34e+01]    
9000      [2.68e+01, 5.63e+00, 2.66e+00]    [0.00e+00, 3.03e+01, 2.66e+00]    [2.42e+01, 3.03e+01, 1.46e+01, 2.56e+01]    
10000     [2.54e+01, 6.61e+00, 2.65e+00]    [0.00e+00, 2.48e+01, 2.65e+00]    [2.14e+01, 2.48e+01, 1.36e+01, 1.36e+01]    
11000     [2.64e+01, 5.32e+00, 2.65e+00]    [0.00e+00, 3.04e+01, 2.65e+00]    [2.39e+01, 3.04e+01, 1.47e+01, 2.57e+01]    
12000     [2.61e+01, 8.44e+00, 2.65e+00]    [0.00e+00, 2.38e+01, 2.65e+00]    [2.19e+01, 2.38e+01, 1.37e+01, 1.33e+01]    
13000     [2.60e+01, 4.68e+00, 2.64e+00]    [0.00e+00, 2.99e+01, 2.64e+00]    [2.34e+01, 2.99e+01, 1.49e+01, 2.61e+01]    
14000     [2.65e+01, 6.20e+00, 2.64e+00]    [0.00e+00, 3.05e+01, 2.64e+00]    [2.42e+01, 3.05e+01, 1.59e+01, 2.83e+01]    
15000     [2.70e+01, 1.11e+01, 2.64e+00]    [0.00e+00, 2.21e+01, 2.64e+00]    [2.31e+01, 2.21e+01, 1.49e+01, 1.33e+01]    
16000     [2.59e+01, 5.25e+00, 2.63e+00]    [0.00e+00, 3.01e+01, 2.63e+00]    [2.34e+01, 3.01e+01, 1.56e+01, 2.72e+01]    
17000     [2.52e+01, 4.02e+00, 2.63e+00]    [0.00e+00, 2.89e+01, 2.63e+00]    [2.23e+01, 2.89e+01, 1.48e+01, 2.49e+01]    
18000     [2.35e+01, 3.23e+00, 2.63e+00]    [0.00e+00, 2.54e+01, 2.63e+00]    [1.94e+01, 2.54e+01, 1.53e+01, 1.76e+01]    
19000     [2.50e+01, 4.99e+00, 2.63e+00]    [0.00e+00, 2.91e+01, 2.63e+00]    [2.21e+01, 2.91e+01, 1.49e+01, 2.56e+01]    
20000     [2.46e+01, 4.43e+00, 2.62e+00]    [0.00e+00, 2.91e+01, 2.62e+00]    [2.22e+01, 2.91e+01, 1.48e+01, 2.61e+01]    
21000     [2.76e+01, 1.20e+01, 2.62e+00]    [0.00e+00, 3.24e+01, 2.62e+00]    [2.55e+01, 3.24e+01, 1.89e+01, 3.44e+01]    
22000     [2.35e+01, 2.66e+00, 2.62e+00]    [0.00e+00, 2.56e+01, 2.62e+00]    [2.00e+01, 2.56e+01, 1.52e+01, 1.86e+01]    
23000     [2.40e+01, 4.10e+00, 2.62e+00]    [0.00e+00, 2.50e+01, 2.62e+00]    [2.07e+01, 2.50e+01, 1.49e+01, 1.74e+01]    
24000     [2.54e+01, 7.35e+00, 2.61e+00]    [0.00e+00, 3.09e+01, 2.61e+00]    [2.39e+01, 3.09e+01, 1.62e+01, 3.02e+01]    
25000     [2.30e+01, 1.58e+00, 2.61e+00]    [0.00e+00, 2.66e+01, 2.61e+00]    [1.95e+01, 2.66e+01, 1.62e+01, 2.00e+01]    
26000     [2.31e+01, 5.82e-01, 2.61e+00]    [0.00e+00, 2.82e+01, 2.61e+00]    [2.05e+01, 2.82e+01, 1.54e+01, 2.18e+01]    
27000     [2.28e+01, 5.81e-01, 2.60e+00]    [0.00e+00, 2.64e+01, 2.60e+00]    [1.94e+01, 2.64e+01, 1.63e+01, 2.04e+01]    
28000     [2.30e+01, 2.24e+00, 2.60e+00]    [0.00e+00, 2.59e+01, 2.60e+00]    [2.01e+01, 2.59e+01, 1.58e+01, 1.85e+01]    
29000     [2.39e+01, 3.87e+00, 2.60e+00]    [0.00e+00, 3.01e+01, 2.60e+00]    [2.25e+01, 3.01e+01, 1.51e+01, 2.54e+01]    
30000     [2.48e+01, 6.12e+00, 2.59e+00]    [0.00e+00, 2.37e+01, 2.59e+00]    [2.23e+01, 2.37e+01, 1.56e+01, 1.57e+01]    

Best model at step 27000:
  train loss: 2.60e+01
  test loss: 2.90e+01
  test metric: [1.94e+01, 2.64e+01, 1.63e+01, 2.04e+01]

'train' took 54.618826 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[19.90373030138056, 91.91479744579519, 18.530744323287553, 18.476624499440806, 22.473092118026393, 41.482491879795546, 23.046504888446453, 20.33829623076823, 30.166723161903775, 26.351958565026354]
sigma_y 4 31.268496341387078 21.27711343804123
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.213978 s

'compile' took 0.965285 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.95e+02, 1.35e+03, 2.91e+00]    [0.00e+00, 2.17e+02, 2.91e+00]    [2.17e+02, 2.17e+02, 2.10e+02, 2.10e+02]    
1000      [5.32e+01, 3.72e+01, 2.88e+00]    [0.00e+00, 5.39e+01, 2.88e+00]    [5.39e+01, 5.39e+01, 2.37e+01, 2.36e+01]    
2000      [3.74e+01, 4.25e+01, 2.88e+00]    [0.00e+00, 3.37e+01, 2.88e+00]    [3.38e+01, 3.37e+01, 1.81e+01, 1.79e+01]    
3000      [3.12e+01, 2.76e+01, 2.88e+00]    [0.00e+00, 2.39e+01, 2.88e+00]    [2.38e+01, 2.39e+01, 1.49e+01, 1.48e+01]    
4000      [2.80e+01, 1.53e+01, 2.88e+00]    [0.00e+00, 1.71e+01, 2.88e+00]    [1.69e+01, 1.71e+01, 1.64e+01, 1.63e+01]    
5000      [2.85e+01, 6.45e+00, 2.88e+00]    [0.00e+00, 1.55e+01, 2.88e+00]    [1.49e+01, 1.55e+01, 1.51e+01, 1.46e+01]    
6000      [2.90e+01, 1.18e+01, 2.87e+00]    [0.00e+00, 1.45e+01, 2.87e+00]    [1.40e+01, 1.45e+01, 1.40e+01, 1.36e+01]    
7000      [2.85e+01, 3.47e+00, 2.87e+00]    [0.00e+00, 1.44e+01, 2.87e+00]    [1.39e+01, 1.44e+01, 1.39e+01, 1.34e+01]    
8000      [2.89e+01, 5.29e+00, 2.87e+00]    [0.00e+00, 1.43e+01, 2.87e+00]    [1.38e+01, 1.43e+01, 1.42e+01, 1.36e+01]    
9000      [2.89e+01, 6.71e+00, 2.87e+00]    [0.00e+00, 1.44e+01, 2.87e+00]    [1.39e+01, 1.44e+01, 1.47e+01, 1.41e+01]    
10000     [3.22e+01, 2.34e+01, 2.87e+00]    [0.00e+00, 1.54e+01, 2.87e+00]    [1.53e+01, 1.54e+01, 1.46e+01, 1.42e+01]    
11000     [2.85e+01, 7.69e+00, 2.86e+00]    [0.00e+00, 1.44e+01, 2.86e+00]    [1.48e+01, 1.44e+01, 1.48e+01, 1.49e+01]    
12000     [2.99e+01, 1.75e+01, 2.86e+00]    [0.00e+00, 1.55e+01, 2.86e+00]    [1.52e+01, 1.55e+01, 1.52e+01, 1.46e+01]    
13000     [3.02e+01, 1.46e+01, 2.86e+00]    [0.00e+00, 1.51e+01, 2.86e+00]    [1.52e+01, 1.51e+01, 1.53e+01, 1.52e+01]    
14000     [2.79e+01, 1.03e+01, 2.85e+00]    [0.00e+00, 1.58e+01, 2.85e+00]    [1.54e+01, 1.58e+01, 1.56e+01, 1.49e+01]    
15000     [2.84e+01, 1.34e+01, 2.85e+00]    [0.00e+00, 1.58e+01, 2.85e+00]    [1.55e+01, 1.58e+01, 1.56e+01, 1.50e+01]    
16000     [2.98e+01, 1.37e+01, 2.85e+00]    [0.00e+00, 1.54e+01, 2.85e+00]    [1.56e+01, 1.54e+01, 1.54e+01, 1.52e+01]    
17000     [3.22e+01, 2.38e+01, 2.84e+00]    [0.00e+00, 1.54e+01, 2.84e+00]    [1.55e+01, 1.54e+01, 1.55e+01, 1.54e+01]    
18000     [3.18e+01, 2.32e+01, 2.84e+00]    [0.00e+00, 1.53e+01, 2.84e+00]    [1.55e+01, 1.53e+01, 1.55e+01, 1.55e+01]    
19000     [2.79e+01, 1.33e+01, 2.84e+00]    [0.00e+00, 1.58e+01, 2.84e+00]    [1.58e+01, 1.58e+01, 1.57e+01, 1.53e+01]    
20000     [2.91e+01, 1.22e+01, 2.83e+00]    [0.00e+00, 1.57e+01, 2.83e+00]    [1.58e+01, 1.57e+01, 1.55e+01, 1.53e+01]    
21000     [2.88e+01, 1.64e+01, 2.83e+00]    [0.00e+00, 1.61e+01, 2.83e+00]    [1.60e+01, 1.61e+01, 1.57e+01, 1.52e+01]    
22000     [2.72e+01, 6.55e+00, 2.83e+00]    [0.00e+00, 1.59e+01, 2.83e+00]    [1.60e+01, 1.59e+01, 1.54e+01, 1.53e+01]    
23000     [2.79e+01, 9.45e+00, 2.82e+00]    [0.00e+00, 1.59e+01, 2.82e+00]    [1.61e+01, 1.59e+01, 1.53e+01, 1.53e+01]    
24000     [2.90e+01, 1.38e+01, 2.82e+00]    [0.00e+00, 1.60e+01, 2.82e+00]    [1.61e+01, 1.60e+01, 1.53e+01, 1.52e+01]    
25000     [2.65e+01, 8.21e+00, 2.82e+00]    [0.00e+00, 1.60e+01, 2.82e+00]    [1.61e+01, 1.60e+01, 1.55e+01, 1.52e+01]    
26000     [2.67e+01, 5.70e+00, 2.81e+00]    [0.00e+00, 1.60e+01, 2.81e+00]    [1.62e+01, 1.60e+01, 1.53e+01, 1.51e+01]    
27000     [2.67e+01, 6.11e+00, 2.81e+00]    [0.00e+00, 1.60e+01, 2.81e+00]    [1.61e+01, 1.60e+01, 1.52e+01, 1.51e+01]    
28000     [2.63e+01, 1.03e+01, 2.81e+00]    [0.00e+00, 1.61e+01, 2.81e+00]    [1.62e+01, 1.61e+01, 1.53e+01, 1.51e+01]    
29000     [2.79e+01, 1.06e+01, 2.80e+00]    [0.00e+00, 1.58e+01, 2.80e+00]    [1.60e+01, 1.58e+01, 1.52e+01, 1.51e+01]    
30000     [2.59e+01, 8.89e+00, 2.80e+00]    [0.00e+00, 1.58e+01, 2.80e+00]    [1.59e+01, 1.58e+01, 1.54e+01, 1.52e+01]    

Best model at step 7000:
  train loss: 3.48e+01
  test loss: 1.72e+01
  test metric: [1.39e+01, 1.44e+01, 1.39e+01, 1.34e+01]

'train' took 55.361048 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.221408 s

'compile' took 0.969093 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.52e+02, 1.14e+02, 2.72e+00]    [0.00e+00, 3.42e+02, 2.72e+00]    [3.42e+02, 3.42e+02, 2.57e+02, 2.57e+02]    
1000      [3.57e+01, 9.87e+00, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [2.23e+01, 2.30e+01, 2.99e+01, 2.95e+01]    
2000      [2.93e+01, 3.15e+00, 2.74e+00]    [0.00e+00, 5.47e+01, 2.74e+00]    [1.31e+01, 5.47e+01, 8.76e+00, 7.01e+01]    
3000      [2.89e+01, 1.06e+00, 2.77e+00]    [0.00e+00, 1.30e+02, 2.77e+00]    [1.33e+01, 1.30e+02, 1.31e+01, 1.84e+02]    
4000      [2.53e+01, 6.68e-02, 2.79e+00]    [0.00e+00, 1.97e+02, 2.79e+00]    [1.30e+01, 1.97e+02, 9.99e+00, 2.84e+02]    
5000      [2.45e+01, 5.75e-02, 2.80e+00]    [0.00e+00, 2.30e+02, 2.80e+00]    [1.51e+01, 2.30e+02, 1.07e+01, 3.31e+02]    
6000      [2.57e+01, 4.38e-01, 2.80e+00]    [0.00e+00, 2.31e+02, 2.80e+00]    [1.58e+01, 2.31e+02, 1.23e+01, 3.28e+02]    
7000      [2.42e+01, 1.99e-01, 2.79e+00]    [0.00e+00, 2.48e+02, 2.79e+00]    [1.54e+01, 2.48e+02, 1.16e+01, 3.53e+02]    
8000      [2.63e+01, 3.34e-01, 2.79e+00]    [0.00e+00, 2.58e+02, 2.79e+00]    [1.97e+01, 2.58e+02, 1.41e+01, 3.67e+02]    
9000      [2.33e+01, 1.37e-01, 2.79e+00]    [0.00e+00, 2.57e+02, 2.79e+00]    [1.35e+01, 2.57e+02, 1.15e+01, 3.65e+02]    
10000     [2.41e+01, 1.69e-01, 2.79e+00]    [0.00e+00, 2.64e+02, 2.79e+00]    [1.64e+01, 2.64e+02, 1.14e+01, 3.75e+02]    
11000     [2.37e+01, 2.34e-01, 2.78e+00]    [0.00e+00, 2.60e+02, 2.78e+00]    [1.46e+01, 2.60e+02, 1.04e+01, 3.68e+02]    
12000     [2.57e+01, 4.21e-01, 2.78e+00]    [0.00e+00, 2.78e+02, 2.78e+00]    [2.02e+01, 2.78e+02, 1.34e+01, 3.94e+02]    
13000     [2.27e+01, 1.78e-01, 2.78e+00]    [0.00e+00, 2.76e+02, 2.78e+00]    [1.42e+01, 2.76e+02, 1.08e+01, 3.92e+02]    
14000     [2.34e+01, 4.78e-01, 2.77e+00]    [0.00e+00, 2.81e+02, 2.77e+00]    [1.53e+01, 2.81e+02, 1.07e+01, 3.99e+02]    
15000     [2.42e+01, 4.01e-01, 2.77e+00]    [0.00e+00, 2.75e+02, 2.77e+00]    [1.41e+01, 2.75e+02, 1.18e+01, 3.90e+02]    
16000     [2.53e+01, 5.53e-01, 2.77e+00]    [0.00e+00, 2.91e+02, 2.77e+00]    [2.02e+01, 2.91e+02, 1.27e+01, 4.15e+02]    
17000     [2.35e+01, 2.82e-01, 2.77e+00]    [0.00e+00, 2.89e+02, 2.77e+00]    [1.66e+01, 2.89e+02, 1.11e+01, 4.11e+02]    
18000     [2.28e+01, 1.71e-01, 2.76e+00]    [0.00e+00, 2.89e+02, 2.76e+00]    [1.50e+01, 2.89e+02, 1.14e+01, 4.11e+02]    
19000     [2.38e+01, 2.12e-01, 2.76e+00]    [0.00e+00, 2.82e+02, 2.76e+00]    [1.47e+01, 2.82e+02, 1.23e+01, 4.02e+02]    
20000     [2.21e+01, 3.54e-02, 2.76e+00]    [0.00e+00, 2.89e+02, 2.76e+00]    [1.35e+01, 2.89e+02, 1.04e+01, 4.13e+02]    
21000     [2.68e+01, 7.51e-01, 2.76e+00]    [0.00e+00, 2.79e+02, 2.76e+00]    [1.75e+01, 2.79e+02, 1.72e+01, 3.98e+02]    
22000     [2.40e+01, 5.05e-01, 2.75e+00]    [0.00e+00, 3.00e+02, 2.75e+00]    [1.88e+01, 3.00e+02, 1.18e+01, 4.29e+02]    
23000     [2.71e+01, 8.25e-01, 2.75e+00]    [0.00e+00, 2.82e+02, 2.75e+00]    [1.83e+01, 2.82e+02, 1.83e+01, 4.01e+02]    
24000     [2.19e+01, 1.44e-01, 2.75e+00]    [0.00e+00, 2.96e+02, 2.75e+00]    [1.49e+01, 2.96e+02, 1.04e+01, 4.23e+02]    
25000     [2.15e+01, 1.32e-01, 2.75e+00]    [0.00e+00, 2.98e+02, 2.75e+00]    [1.41e+01, 2.98e+02, 1.06e+01, 4.26e+02]    
26000     [2.30e+01, 3.54e-01, 2.75e+00]    [0.00e+00, 3.03e+02, 2.75e+00]    [1.73e+01, 3.03e+02, 1.08e+01, 4.34e+02]    
27000     [2.23e+01, 3.10e-01, 2.74e+00]    [0.00e+00, 3.04e+02, 2.74e+00]    [1.60e+01, 3.04e+02, 1.05e+01, 4.36e+02]    
28000     [2.16e+01, 2.08e-01, 2.74e+00]    [0.00e+00, 3.03e+02, 2.74e+00]    [1.30e+01, 3.03e+02, 1.08e+01, 4.35e+02]    
29000     [2.35e+01, 3.29e-01, 2.74e+00]    [0.00e+00, 2.97e+02, 2.74e+00]    [1.48e+01, 2.97e+02, 1.28e+01, 4.27e+02]    
30000     [2.28e+01, 3.21e-01, 2.74e+00]    [0.00e+00, 2.99e+02, 2.74e+00]    [1.48e+01, 2.99e+02, 1.15e+01, 4.30e+02]    

Best model at step 25000:
  train loss: 2.44e+01
  test loss: 3.00e+02
  test metric: [1.41e+01, 2.98e+02, 1.06e+01, 4.26e+02]

'train' took 60.175201 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.234107 s

'compile' took 0.995551 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.48e+02, 3.87e+02, 2.65e+00]    [0.00e+00, 1.63e+02, 2.65e+00]    [1.63e+02, 1.63e+02, 1.40e+02, 1.40e+02]    
1000      [5.30e+01, 2.93e+01, 2.61e+00]    [0.00e+00, 5.40e+01, 2.61e+00]    [5.41e+01, 5.40e+01, 1.86e+01, 1.87e+01]    
2000      [3.65e+01, 9.84e+00, 2.60e+00]    [0.00e+00, 3.17e+01, 2.60e+00]    [3.19e+01, 3.17e+01, 1.73e+01, 1.77e+01]    
3000      [3.33e+01, 1.44e+01, 2.60e+00]    [0.00e+00, 2.39e+01, 2.60e+00]    [2.45e+01, 2.39e+01, 1.23e+01, 1.23e+01]    
4000      [3.18e+01, 6.86e+00, 2.60e+00]    [0.00e+00, 1.98e+01, 2.60e+00]    [2.05e+01, 1.98e+01, 1.12e+01, 1.15e+01]    
5000      [3.13e+01, 8.23e+00, 2.59e+00]    [0.00e+00, 1.82e+01, 2.59e+00]    [1.73e+01, 1.82e+01, 1.29e+01, 1.20e+01]    
6000      [3.02e+01, 1.51e+00, 2.59e+00]    [0.00e+00, 1.68e+01, 2.59e+00]    [1.78e+01, 1.68e+01, 1.24e+01, 1.30e+01]    
7000      [2.92e+01, 5.11e+00, 2.59e+00]    [0.00e+00, 1.64e+01, 2.59e+00]    [1.67e+01, 1.64e+01, 1.34e+01, 1.36e+01]    
8000      [3.24e+01, 1.83e+01, 2.59e+00]    [0.00e+00, 1.83e+01, 2.59e+00]    [1.73e+01, 1.83e+01, 1.34e+01, 1.34e+01]    
9000      [2.97e+01, 1.07e+01, 2.59e+00]    [0.00e+00, 1.84e+01, 2.59e+00]    [1.97e+01, 1.84e+01, 1.44e+01, 1.35e+01]    
10000     [2.83e+01, 4.75e+00, 2.59e+00]    [0.00e+00, 1.78e+01, 2.59e+00]    [1.93e+01, 1.78e+01, 1.33e+01, 1.30e+01]    
11000     [2.87e+01, 8.36e+00, 2.59e+00]    [0.00e+00, 1.84e+01, 2.59e+00]    [1.99e+01, 1.84e+01, 1.42e+01, 1.33e+01]    
12000     [2.74e+01, 6.99e-01, 2.58e+00]    [0.00e+00, 1.75e+01, 2.58e+00]    [1.91e+01, 1.75e+01, 1.27e+01, 1.28e+01]    
13000     [2.72e+01, 6.10e+00, 2.58e+00]    [0.00e+00, 1.63e+01, 2.58e+00]    [1.80e+01, 1.63e+01, 1.29e+01, 1.36e+01]    
14000     [2.68e+01, 4.89e+00, 2.58e+00]    [0.00e+00, 1.65e+01, 2.58e+00]    [1.82e+01, 1.65e+01, 1.30e+01, 1.35e+01]    
15000     [2.64e+01, 2.26e+00, 2.58e+00]    [0.00e+00, 1.73e+01, 2.58e+00]    [1.90e+01, 1.73e+01, 1.31e+01, 1.31e+01]    
16000     [2.79e+01, 8.64e+00, 2.57e+00]    [0.00e+00, 1.89e+01, 2.57e+00]    [2.06e+01, 1.89e+01, 1.50e+01, 1.36e+01]    
17000     [2.67e+01, 4.42e+00, 2.57e+00]    [0.00e+00, 1.83e+01, 2.57e+00]    [2.00e+01, 1.83e+01, 1.41e+01, 1.31e+01]    
18000     [2.67e+01, 7.17e+00, 2.57e+00]    [0.00e+00, 1.64e+01, 2.57e+00]    [1.81e+01, 1.64e+01, 1.28e+01, 1.36e+01]    
19000     [2.68e+01, 8.02e+00, 2.56e+00]    [0.00e+00, 1.63e+01, 2.56e+00]    [1.80e+01, 1.63e+01, 1.28e+01, 1.37e+01]    
20000     [2.84e+01, 1.30e+01, 2.56e+00]    [0.00e+00, 1.63e+01, 2.56e+00]    [1.69e+01, 1.63e+01, 1.34e+01, 1.37e+01]    
21000     [2.82e+01, 1.38e+01, 2.56e+00]    [0.00e+00, 1.67e+01, 2.56e+00]    [1.71e+01, 1.67e+01, 1.33e+01, 1.36e+01]    
22000     [2.58e+01, 4.16e+00, 2.56e+00]    [0.00e+00, 1.70e+01, 2.56e+00]    [1.88e+01, 1.70e+01, 1.27e+01, 1.31e+01]    
23000     [2.62e+01, 4.30e+00, 2.55e+00]    [0.00e+00, 1.81e+01, 2.55e+00]    [1.99e+01, 1.81e+01, 1.37e+01, 1.30e+01]    
24000     [2.57e+01, 1.04e+00, 2.55e+00]    [0.00e+00, 1.77e+01, 2.55e+00]    [1.94e+01, 1.77e+01, 1.27e+01, 1.27e+01]    
25000     [2.64e+01, 5.52e+00, 2.55e+00]    [0.00e+00, 1.84e+01, 2.55e+00]    [2.01e+01, 1.84e+01, 1.37e+01, 1.29e+01]    
26000     [2.60e+01, 4.76e+00, 2.55e+00]    [0.00e+00, 1.81e+01, 2.55e+00]    [1.98e+01, 1.81e+01, 1.37e+01, 1.30e+01]    
27000     [2.59e+01, 3.99e+00, 2.54e+00]    [0.00e+00, 1.84e+01, 2.54e+00]    [2.00e+01, 1.84e+01, 1.35e+01, 1.28e+01]    
28000     [2.58e+01, 6.20e+00, 2.54e+00]    [0.00e+00, 1.63e+01, 2.54e+00]    [1.79e+01, 1.63e+01, 1.29e+01, 1.36e+01]    
29000     [2.57e+01, 3.73e+00, 2.54e+00]    [0.00e+00, 1.78e+01, 2.54e+00]    [1.93e+01, 1.78e+01, 1.37e+01, 1.30e+01]    
30000     [2.57e+01, 4.69e+00, 2.53e+00]    [0.00e+00, 1.81e+01, 2.53e+00]    [1.97e+01, 1.81e+01, 1.36e+01, 1.30e+01]    

Best model at step 24000:
  train loss: 2.93e+01
  test loss: 2.03e+01
  test metric: [1.94e+01, 1.77e+01, 1.27e+01, 1.27e+01]

'train' took 55.594651 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.243380 s

'compile' took 1.035782 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.16e+02, 2.47e+02, 2.74e+00]    [0.00e+00, 9.93e+01, 2.74e+00]    [9.93e+01, 9.93e+01, 4.98e+01, 4.98e+01]    
1000      [4.08e+01, 1.58e+01, 2.73e+00]    [0.00e+00, 4.87e+01, 2.73e+00]    [4.91e+01, 4.87e+01, 2.97e+01, 3.00e+01]    
2000      [3.59e+01, 1.21e+01, 2.72e+00]    [0.00e+00, 3.63e+01, 2.72e+00]    [3.65e+01, 3.63e+01, 2.58e+01, 2.60e+01]    
3000      [3.51e+01, 7.50e+00, 2.72e+00]    [0.00e+00, 2.93e+01, 2.72e+00]    [2.95e+01, 2.93e+01, 2.42e+01, 2.43e+01]    
4000      [3.27e+01, 5.58e+00, 2.71e+00]    [0.00e+00, 2.56e+01, 2.71e+00]    [2.54e+01, 2.56e+01, 2.19e+01, 2.18e+01]    
5000      [3.09e+01, 5.09e+00, 2.69e+00]    [0.00e+00, 2.42e+01, 2.69e+00]    [2.41e+01, 2.42e+01, 1.91e+01, 1.90e+01]    
6000      [3.02e+01, 4.61e+00, 2.68e+00]    [0.00e+00, 2.15e+01, 2.68e+00]    [2.15e+01, 2.15e+01, 1.91e+01, 1.91e+01]    
7000      [2.96e+01, 2.48e+00, 2.68e+00]    [0.00e+00, 1.98e+01, 2.68e+00]    [1.99e+01, 1.98e+01, 1.81e+01, 1.81e+01]    
8000      [3.02e+01, 9.70e+00, 2.67e+00]    [0.00e+00, 2.05e+01, 2.67e+00]    [2.06e+01, 2.05e+01, 1.74e+01, 1.74e+01]    
9000      [2.94e+01, 8.00e+00, 2.66e+00]    [0.00e+00, 2.03e+01, 2.66e+00]    [2.04e+01, 2.03e+01, 1.71e+01, 1.71e+01]    
10000     [2.80e+01, 2.87e+00, 2.66e+00]    [0.00e+00, 1.83e+01, 2.66e+00]    [1.84e+01, 1.83e+01, 1.77e+01, 1.78e+01]    
11000     [2.85e+01, 5.58e+00, 2.66e+00]    [0.00e+00, 1.76e+01, 2.66e+00]    [1.74e+01, 1.76e+01, 1.79e+01, 1.77e+01]    
12000     [2.92e+01, 9.28e+00, 2.65e+00]    [0.00e+00, 2.02e+01, 2.65e+00]    [2.03e+01, 2.02e+01, 1.68e+01, 1.67e+01]    
13000     [3.08e+01, 1.24e+01, 2.65e+00]    [0.00e+00, 1.80e+01, 2.65e+00]    [1.78e+01, 1.80e+01, 1.70e+01, 1.69e+01]    
14000     [2.89e+01, 9.85e+00, 2.65e+00]    [0.00e+00, 1.97e+01, 2.65e+00]    [1.99e+01, 1.97e+01, 1.70e+01, 1.69e+01]    
15000     [2.80e+01, 5.21e+00, 2.64e+00]    [0.00e+00, 1.70e+01, 2.64e+00]    [1.71e+01, 1.70e+01, 1.76e+01, 1.77e+01]    
16000     [2.85e+01, 6.58e+00, 2.64e+00]    [0.00e+00, 2.07e+01, 2.64e+00]    [2.08e+01, 2.07e+01, 1.59e+01, 1.58e+01]    
17000     [2.83e+01, 7.97e+00, 2.64e+00]    [0.00e+00, 1.94e+01, 2.64e+00]    [1.97e+01, 1.94e+01, 1.71e+01, 1.71e+01]    
18000     [2.93e+01, 1.11e+01, 2.64e+00]    [0.00e+00, 2.00e+01, 2.64e+00]    [2.03e+01, 2.00e+01, 1.74e+01, 1.73e+01]    
19000     [2.84e+01, 8.56e+00, 2.64e+00]    [0.00e+00, 1.69e+01, 2.64e+00]    [1.67e+01, 1.69e+01, 1.79e+01, 1.77e+01]    
20000     [2.81e+01, 9.04e+00, 2.63e+00]    [0.00e+00, 1.70e+01, 2.63e+00]    [1.67e+01, 1.70e+01, 1.80e+01, 1.78e+01]    
21000     [3.06e+01, 1.59e+01, 2.63e+00]    [0.00e+00, 1.83e+01, 2.63e+00]    [1.81e+01, 1.83e+01, 1.73e+01, 1.73e+01]    
22000     [2.70e+01, 6.28e+00, 2.63e+00]    [0.00e+00, 1.92e+01, 2.63e+00]    [1.94e+01, 1.92e+01, 1.74e+01, 1.74e+01]    
23000     [2.66e+01, 4.38e+00, 2.63e+00]    [0.00e+00, 1.91e+01, 2.63e+00]    [1.93e+01, 1.91e+01, 1.75e+01, 1.75e+01]    
24000     [2.88e+01, 1.16e+01, 2.63e+00]    [0.00e+00, 2.05e+01, 2.63e+00]    [2.08e+01, 2.05e+01, 1.79e+01, 1.77e+01]    
25000     [2.63e+01, 5.23e+00, 2.63e+00]    [0.00e+00, 1.72e+01, 2.63e+00]    [1.75e+01, 1.72e+01, 1.82e+01, 1.84e+01]    
26000     [2.86e+01, 1.12e+01, 2.63e+00]    [0.00e+00, 2.06e+01, 2.63e+00]    [2.09e+01, 2.06e+01, 1.78e+01, 1.77e+01]    
27000     [2.80e+01, 1.07e+01, 2.63e+00]    [0.00e+00, 1.78e+01, 2.63e+00]    [1.76e+01, 1.78e+01, 1.82e+01, 1.81e+01]    
28000     [2.60e+01, 4.92e+00, 2.62e+00]    [0.00e+00, 1.75e+01, 2.62e+00]    [1.78e+01, 1.75e+01, 1.81e+01, 1.83e+01]    
29000     [2.74e+01, 8.68e+00, 2.62e+00]    [0.00e+00, 2.02e+01, 2.62e+00]    [2.05e+01, 2.02e+01, 1.76e+01, 1.76e+01]    
30000     [2.89e+01, 1.38e+01, 2.62e+00]    [0.00e+00, 1.85e+01, 2.62e+00]    [1.83e+01, 1.85e+01, 1.77e+01, 1.76e+01]    

Best model at step 28000:
  train loss: 3.35e+01
  test loss: 2.01e+01
  test metric: [1.78e+01, 1.75e+01, 1.81e+01, 1.83e+01]

'train' took 54.519293 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.202353 s

'compile' took 0.973794 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.22e+02, 1.59e+02, 2.82e+00]    [0.00e+00, 3.17e+02, 2.82e+00]    [3.17e+02, 3.17e+02, 4.66e+02, 4.66e+02]    
1000      [3.35e+01, 2.33e+01, 2.80e+00]    [0.00e+00, 3.07e+01, 2.80e+00]    [3.26e+01, 3.07e+01, 1.64e+01, 1.84e+01]    
2000      [2.88e+01, 1.15e+01, 2.79e+00]    [0.00e+00, 2.27e+01, 2.79e+00]    [1.82e+01, 2.27e+01, 1.33e+01, 2.02e+01]    
3000      [2.96e+01, 7.51e+00, 2.79e+00]    [0.00e+00, 2.10e+01, 2.79e+00]    [1.40e+01, 2.10e+01, 1.27e+01, 1.94e+01]    
4000      [3.00e+01, 7.09e+00, 2.79e+00]    [0.00e+00, 2.58e+01, 2.79e+00]    [1.70e+01, 2.58e+01, 1.16e+01, 2.66e+01]    
5000      [2.92e+01, 4.51e+00, 2.79e+00]    [0.00e+00, 2.88e+01, 2.79e+00]    [1.76e+01, 2.88e+01, 1.20e+01, 3.49e+01]    
6000      [3.21e+01, 7.64e+00, 2.79e+00]    [0.00e+00, 2.32e+01, 2.79e+00]    [2.03e+01, 2.32e+01, 1.62e+01, 2.56e+01]    
7000      [3.25e+01, 5.93e+00, 2.79e+00]    [0.00e+00, 2.18e+01, 2.79e+00]    [2.00e+01, 2.18e+01, 1.80e+01, 2.54e+01]    
8000      [3.03e+01, 3.78e+00, 2.80e+00]    [0.00e+00, 2.49e+01, 2.80e+00]    [1.64e+01, 2.49e+01, 1.62e+01, 3.25e+01]    
9000      [3.01e+01, 4.72e+00, 2.80e+00]    [0.00e+00, 3.21e+01, 2.80e+00]    [1.69e+01, 3.21e+01, 1.50e+01, 4.48e+01]    
10000     [2.83e+01, 1.61e+00, 2.80e+00]    [0.00e+00, 2.74e+01, 2.80e+00]    [1.65e+01, 2.74e+01, 1.44e+01, 3.75e+01]    
11000     [2.98e+01, 3.33e+00, 2.80e+00]    [0.00e+00, 2.50e+01, 2.80e+00]    [1.65e+01, 2.50e+01, 1.59e+01, 3.40e+01]    
12000     [2.73e+01, 7.89e-01, 2.80e+00]    [0.00e+00, 2.83e+01, 2.80e+00]    [1.71e+01, 2.83e+01, 1.38e+01, 3.76e+01]    
13000     [3.01e+01, 6.21e+00, 2.80e+00]    [0.00e+00, 3.44e+01, 2.80e+00]    [1.83e+01, 3.44e+01, 1.66e+01, 4.74e+01]    
14000     [2.73e+01, 1.40e+00, 2.80e+00]    [0.00e+00, 2.67e+01, 2.80e+00]    [1.69e+01, 2.67e+01, 1.54e+01, 3.39e+01]    
15000     [2.95e+01, 3.16e+00, 2.80e+00]    [0.00e+00, 2.25e+01, 2.80e+00]    [2.10e+01, 2.25e+01, 1.61e+01, 2.76e+01]    
16000     [2.92e+01, 4.40e+00, 2.80e+00]    [0.00e+00, 3.79e+01, 2.80e+00]    [2.16e+01, 3.79e+01, 1.91e+01, 5.10e+01]    
17000     [2.63e+01, 2.25e+00, 2.80e+00]    [0.00e+00, 3.00e+01, 2.80e+00]    [1.79e+01, 3.00e+01, 1.41e+01, 3.77e+01]    
18000     [2.65e+01, 2.80e+00, 2.80e+00]    [0.00e+00, 3.16e+01, 2.80e+00]    [1.89e+01, 3.16e+01, 1.41e+01, 4.00e+01]    
19000     [2.67e+01, 2.13e+00, 2.80e+00]    [0.00e+00, 2.55e+01, 2.80e+00]    [1.80e+01, 2.55e+01, 1.63e+01, 3.10e+01]    
20000     [2.75e+01, 2.85e+00, 2.80e+00]    [0.00e+00, 3.58e+01, 2.80e+00]    [2.13e+01, 3.58e+01, 1.75e+01, 4.72e+01]    
21000     [2.89e+01, 3.69e+00, 2.79e+00]    [0.00e+00, 2.16e+01, 2.79e+00]    [2.32e+01, 2.16e+01, 1.77e+01, 2.40e+01]    
22000     [3.27e+01, 9.08e+00, 2.79e+00]    [0.00e+00, 4.48e+01, 2.79e+00]    [2.60e+01, 4.48e+01, 2.96e+01, 6.13e+01]    
23000     [2.69e+01, 3.08e+00, 2.79e+00]    [0.00e+00, 2.55e+01, 2.79e+00]    [2.02e+01, 2.55e+01, 1.60e+01, 2.99e+01]    
24000     [2.49e+01, 5.67e-01, 2.79e+00]    [0.00e+00, 3.15e+01, 2.79e+00]    [2.03e+01, 3.15e+01, 1.46e+01, 3.93e+01]    
25000     [2.45e+01, 9.71e-01, 2.79e+00]    [0.00e+00, 2.98e+01, 2.79e+00]    [1.95e+01, 2.98e+01, 1.47e+01, 3.55e+01]    
26000     [2.69e+01, 4.04e+00, 2.79e+00]    [0.00e+00, 3.61e+01, 2.79e+00]    [2.24e+01, 3.61e+01, 1.74e+01, 4.53e+01]    
27000     [2.50e+01, 1.97e+00, 2.79e+00]    [0.00e+00, 3.11e+01, 2.79e+00]    [1.99e+01, 3.11e+01, 1.50e+01, 3.87e+01]    
28000     [2.60e+01, 3.63e+00, 2.78e+00]    [0.00e+00, 3.25e+01, 2.78e+00]    [2.02e+01, 3.25e+01, 1.59e+01, 4.08e+01]    
29000     [2.85e+01, 6.07e+00, 2.78e+00]    [0.00e+00, 3.79e+01, 2.78e+00]    [2.20e+01, 3.79e+01, 2.18e+01, 4.93e+01]    
30000     [2.45e+01, 1.31e+00, 2.78e+00]    [0.00e+00, 2.61e+01, 2.78e+00]    [1.84e+01, 2.61e+01, 1.63e+01, 3.00e+01]    

Best model at step 25000:
  train loss: 2.82e+01
  test loss: 3.26e+01
  test metric: [1.95e+01, 2.98e+01, 1.47e+01, 3.55e+01]

'train' took 55.004602 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.188604 s

'compile' took 0.814634 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.16e+02, 8.76e+01, 2.99e+00]    [0.00e+00, 5.61e+02, 2.99e+00]    [5.61e+02, 5.61e+02, 4.33e+02, 4.33e+02]    
1000      [3.44e+01, 5.89e+00, 3.00e+00]    [0.00e+00, 2.47e+01, 3.00e+00]    [1.60e+01, 2.47e+01, 1.09e+01, 3.15e+01]    
2000      [3.32e+01, 6.79e-01, 3.02e+00]    [0.00e+00, 7.63e+01, 3.02e+00]    [9.68e+00, 7.63e+01, 1.63e+01, 1.04e+02]    
3000      [2.92e+01, 1.96e-01, 3.02e+00]    [0.00e+00, 7.94e+01, 3.02e+00]    [1.04e+01, 7.94e+01, 9.80e+00, 1.06e+02]    
4000      [2.87e+01, 1.64e-01, 3.02e+00]    [0.00e+00, 8.78e+01, 3.02e+00]    [1.37e+01, 8.78e+01, 1.12e+01, 1.16e+02]    
5000      [2.72e+01, 2.64e-01, 3.02e+00]    [0.00e+00, 1.07e+02, 3.02e+00]    [1.19e+01, 1.07e+02, 1.02e+01, 1.44e+02]    
6000      [2.63e+01, 2.21e-01, 3.02e+00]    [0.00e+00, 9.76e+01, 3.02e+00]    [1.39e+01, 9.76e+01, 9.48e+00, 1.29e+02]    
7000      [2.43e+01, 1.73e-01, 3.02e+00]    [0.00e+00, 9.67e+01, 3.02e+00]    [1.12e+01, 9.67e+01, 8.04e+00, 1.26e+02]    
8000      [2.44e+01, 2.20e-01, 3.02e+00]    [0.00e+00, 8.37e+01, 3.02e+00]    [1.16e+01, 8.37e+01, 8.95e+00, 1.06e+02]    
9000      [2.84e+01, 4.70e-01, 3.01e+00]    [0.00e+00, 9.42e+01, 3.01e+00]    [1.57e+01, 9.42e+01, 1.89e+01, 1.24e+02]    
10000     [2.78e+01, 4.80e-01, 3.01e+00]    [0.00e+00, 8.79e+01, 3.01e+00]    [1.56e+01, 8.79e+01, 1.91e+01, 1.14e+02]    
11000     [2.53e+01, 5.17e-01, 3.01e+00]    [0.00e+00, 7.85e+01, 3.01e+00]    [1.01e+01, 7.85e+01, 1.10e+01, 8.82e+01]    
12000     [2.38e+01, 1.08e-01, 3.01e+00]    [0.00e+00, 7.00e+01, 3.01e+00]    [1.35e+01, 7.00e+01, 9.62e+00, 6.09e+01]    
13000     [2.47e+01, 2.63e-01, 3.00e+00]    [0.00e+00, 7.53e+01, 3.00e+00]    [1.17e+01, 7.53e+01, 1.11e+01, 6.85e+01]    
14000     [2.39e+01, 2.98e-01, 3.00e+00]    [0.00e+00, 7.03e+01, 3.00e+00]    [1.03e+01, 7.03e+01, 9.42e+00, 5.73e+01]    
15000     [2.24e+01, 1.76e-01, 3.00e+00]    [0.00e+00, 6.35e+01, 3.00e+00]    [1.11e+01, 6.35e+01, 8.41e+00, 4.70e+01]    
16000     [2.61e+01, 4.81e-01, 3.00e+00]    [0.00e+00, 7.43e+01, 3.00e+00]    [1.46e+01, 7.43e+01, 1.79e+01, 5.97e+01]    
17000     [2.39e+01, 1.55e-01, 3.00e+00]    [0.00e+00, 6.40e+01, 3.00e+00]    [1.56e+01, 6.40e+01, 1.05e+01, 4.04e+01]    
18000     [2.22e+01, 2.80e-02, 3.00e+00]    [0.00e+00, 6.88e+01, 3.00e+00]    [1.11e+01, 6.88e+01, 8.55e+00, 4.45e+01]    
19000     [2.32e+01, 1.68e-01, 3.00e+00]    [0.00e+00, 6.75e+01, 3.00e+00]    [1.42e+01, 6.75e+01, 9.12e+00, 4.40e+01]    
20000     [2.31e+01, 1.08e-01, 3.00e+00]    [0.00e+00, 7.15e+01, 3.00e+00]    [1.15e+01, 7.15e+01, 1.10e+01, 4.48e+01]    
21000     [2.22e+01, 3.35e-01, 3.00e+00]    [0.00e+00, 7.19e+01, 3.00e+00]    [1.11e+01, 7.19e+01, 8.67e+00, 4.55e+01]    
22000     [2.28e+01, 2.85e-01, 3.00e+00]    [0.00e+00, 7.38e+01, 3.00e+00]    [1.20e+01, 7.38e+01, 1.11e+01, 4.66e+01]    
23000     [2.39e+01, 3.76e-01, 3.00e+00]    [0.00e+00, 7.87e+01, 3.00e+00]    [1.67e+01, 7.87e+01, 1.10e+01, 5.67e+01]    
24000     [2.29e+01, 4.29e-01, 2.99e+00]    [0.00e+00, 7.79e+01, 2.99e+00]    [1.23e+01, 7.79e+01, 1.22e+01, 5.18e+01]    
25000     [2.37e+01, 2.00e-01, 2.99e+00]    [0.00e+00, 8.70e+01, 2.99e+00]    [1.75e+01, 8.70e+01, 1.26e+01, 5.76e+01]    
26000     [2.38e+01, 3.38e-01, 2.99e+00]    [0.00e+00, 9.01e+01, 2.99e+00]    [1.75e+01, 9.01e+01, 1.26e+01, 5.92e+01]    
27000     [2.20e+01, 2.20e-01, 2.99e+00]    [0.00e+00, 8.77e+01, 2.99e+00]    [1.34e+01, 8.77e+01, 8.53e+00, 6.18e+01]    
28000     [2.11e+01, 4.43e-01, 2.99e+00]    [0.00e+00, 8.84e+01, 2.99e+00]    [1.10e+01, 8.84e+01, 7.61e+00, 5.99e+01]    
29000     [2.22e+01, 2.38e-01, 2.99e+00]    [0.00e+00, 8.42e+01, 2.99e+00]    [1.08e+01, 8.42e+01, 9.86e+00, 6.32e+01]    
30000     [2.30e+01, 2.60e-01, 2.99e+00]    [0.00e+00, 1.03e+02, 2.99e+00]    [1.67e+01, 1.03e+02, 1.09e+01, 6.78e+01]    

Best model at step 28000:
  train loss: 2.45e+01
  test loss: 9.14e+01
  test metric: [1.10e+01, 8.84e+01, 7.61e+00, 5.99e+01]

'train' took 56.166171 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.226340 s

'compile' took 1.000033 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.76e+02, 3.86e+02, 2.77e+00]    [0.00e+00, 1.14e+02, 2.77e+00]    [1.14e+02, 1.14e+02, 8.46e+01, 8.46e+01]    
1000      [5.42e+01, 8.06e+00, 2.73e+00]    [0.00e+00, 6.65e+01, 2.73e+00]    [6.65e+01, 6.65e+01, 1.69e+01, 1.69e+01]    
2000      [4.32e+01, 6.30e+00, 2.72e+00]    [0.00e+00, 5.25e+01, 2.72e+00]    [5.24e+01, 5.25e+01, 1.59e+01, 1.59e+01]    
3000      [3.50e+01, 7.41e+00, 2.71e+00]    [0.00e+00, 4.19e+01, 2.71e+00]    [4.19e+01, 4.19e+01, 1.29e+01, 1.29e+01]    
4000      [3.30e+01, 1.47e+01, 2.71e+00]    [0.00e+00, 3.51e+01, 2.71e+00]    [3.51e+01, 3.51e+01, 1.17e+01, 1.16e+01]    
5000      [3.27e+01, 2.03e+01, 2.70e+00]    [0.00e+00, 3.13e+01, 2.70e+00]    [3.13e+01, 3.13e+01, 1.09e+01, 1.08e+01]    
6000      [3.06e+01, 1.52e+01, 2.70e+00]    [0.00e+00, 2.97e+01, 2.70e+00]    [2.97e+01, 2.97e+01, 1.18e+01, 1.17e+01]    
7000      [3.11e+01, 2.22e+01, 2.70e+00]    [0.00e+00, 2.87e+01, 2.70e+00]    [2.87e+01, 2.87e+01, 1.21e+01, 1.19e+01]    
8000      [2.92e+01, 1.18e+01, 2.69e+00]    [0.00e+00, 2.70e+01, 2.69e+00]    [2.69e+01, 2.70e+01, 1.38e+01, 1.37e+01]    
9000      [2.65e+01, 2.64e+00, 2.69e+00]    [0.00e+00, 2.58e+01, 2.69e+00]    [2.57e+01, 2.58e+01, 1.41e+01, 1.40e+01]    
10000     [2.72e+01, 1.09e+01, 2.69e+00]    [0.00e+00, 2.55e+01, 2.69e+00]    [2.54e+01, 2.55e+01, 1.35e+01, 1.34e+01]    
11000     [2.77e+01, 1.11e+01, 2.69e+00]    [0.00e+00, 2.47e+01, 2.69e+00]    [2.45e+01, 2.47e+01, 1.45e+01, 1.44e+01]    
12000     [2.76e+01, 1.13e+01, 2.68e+00]    [0.00e+00, 2.43e+01, 2.68e+00]    [2.41e+01, 2.43e+01, 1.46e+01, 1.45e+01]    
13000     [2.90e+01, 1.86e+01, 2.68e+00]    [0.00e+00, 2.37e+01, 2.68e+00]    [2.34e+01, 2.37e+01, 1.53e+01, 1.51e+01]    
14000     [2.51e+01, 8.94e-01, 2.68e+00]    [0.00e+00, 2.44e+01, 2.68e+00]    [2.42e+01, 2.44e+01, 1.42e+01, 1.41e+01]    
15000     [2.55e+01, 5.38e+00, 2.68e+00]    [0.00e+00, 2.37e+01, 2.68e+00]    [2.34e+01, 2.37e+01, 1.51e+01, 1.49e+01]    
16000     [2.68e+01, 1.15e+01, 2.68e+00]    [0.00e+00, 2.38e+01, 2.68e+00]    [2.35e+01, 2.38e+01, 1.52e+01, 1.50e+01]    
17000     [2.75e+01, 1.62e+01, 2.68e+00]    [0.00e+00, 2.42e+01, 2.68e+00]    [2.38e+01, 2.42e+01, 1.47e+01, 1.45e+01]    
18000     [2.77e+01, 1.76e+01, 2.68e+00]    [0.00e+00, 2.42e+01, 2.68e+00]    [2.38e+01, 2.42e+01, 1.48e+01, 1.46e+01]    
19000     [2.49e+01, 5.04e+00, 2.67e+00]    [0.00e+00, 2.37e+01, 2.67e+00]    [2.33e+01, 2.37e+01, 1.56e+01, 1.54e+01]    
20000     [2.69e+01, 1.48e+01, 2.67e+00]    [0.00e+00, 2.39e+01, 2.67e+00]    [2.35e+01, 2.39e+01, 1.53e+01, 1.52e+01]    
21000     [2.52e+01, 7.24e+00, 2.67e+00]    [0.00e+00, 2.34e+01, 2.67e+00]    [2.29e+01, 2.34e+01, 1.62e+01, 1.59e+01]    
22000     [2.47e+01, 5.77e+00, 2.67e+00]    [0.00e+00, 2.33e+01, 2.67e+00]    [2.28e+01, 2.33e+01, 1.65e+01, 1.62e+01]    
23000     [2.58e+01, 1.07e+01, 2.67e+00]    [0.00e+00, 2.35e+01, 2.67e+00]    [2.30e+01, 2.35e+01, 1.63e+01, 1.61e+01]    
24000     [2.66e+01, 1.43e+01, 2.66e+00]    [0.00e+00, 2.30e+01, 2.66e+00]    [2.24e+01, 2.30e+01, 1.73e+01, 1.69e+01]    
25000     [2.51e+01, 8.86e+00, 2.66e+00]    [0.00e+00, 2.30e+01, 2.66e+00]    [2.24e+01, 2.30e+01, 1.74e+01, 1.70e+01]    
26000     [2.47e+01, 7.39e+00, 2.66e+00]    [0.00e+00, 2.30e+01, 2.66e+00]    [2.23e+01, 2.30e+01, 1.76e+01, 1.73e+01]    
27000     [2.64e+01, 1.43e+01, 2.66e+00]    [0.00e+00, 2.28e+01, 2.66e+00]    [2.21e+01, 2.28e+01, 1.80e+01, 1.77e+01]    
28000     [2.45e+01, 7.46e+00, 2.66e+00]    [0.00e+00, 2.27e+01, 2.66e+00]    [2.19e+01, 2.27e+01, 1.83e+01, 1.79e+01]    
29000     [2.44e+01, 6.81e+00, 2.66e+00]    [0.00e+00, 2.28e+01, 2.66e+00]    [2.20e+01, 2.28e+01, 1.82e+01, 1.79e+01]    
30000     [2.42e+01, 6.51e+00, 2.66e+00]    [0.00e+00, 2.26e+01, 2.66e+00]    [2.18e+01, 2.26e+01, 1.85e+01, 1.82e+01]    

Best model at step 14000:
  train loss: 2.87e+01
  test loss: 2.71e+01
  test metric: [2.42e+01, 2.44e+01, 1.42e+01, 1.41e+01]

'train' took 59.055678 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.187810 s

'compile' took 0.948030 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.56e+02, 5.57e+02, 2.68e+00]    [0.00e+00, 3.50e+02, 2.68e+00]    [3.50e+02, 3.50e+02, 3.63e+02, 3.63e+02]    
1000      [3.88e+01, 2.60e+01, 2.67e+00]    [0.00e+00, 4.68e+01, 2.67e+00]    [4.66e+01, 4.68e+01, 1.61e+01, 1.63e+01]    
2000      [3.14e+01, 1.71e+01, 2.66e+00]    [0.00e+00, 3.00e+01, 2.66e+00]    [2.97e+01, 3.00e+01, 1.50e+01, 1.50e+01]    
3000      [3.05e+01, 1.17e+01, 2.65e+00]    [0.00e+00, 1.96e+01, 2.65e+00]    [1.95e+01, 1.96e+01, 1.21e+01, 1.23e+01]    
4000      [3.02e+01, 1.04e+01, 2.64e+00]    [0.00e+00, 2.07e+01, 2.64e+00]    [2.04e+01, 2.07e+01, 1.33e+01, 1.35e+01]    
5000      [2.91e+01, 3.62e+00, 2.64e+00]    [0.00e+00, 1.73e+01, 2.64e+00]    [1.69e+01, 1.73e+01, 1.25e+01, 1.24e+01]    
6000      [2.93e+01, 5.03e+00, 2.63e+00]    [0.00e+00, 1.77e+01, 2.63e+00]    [1.74e+01, 1.77e+01, 1.33e+01, 1.34e+01]    
7000      [3.04e+01, 9.90e+00, 2.63e+00]    [0.00e+00, 1.57e+01, 2.63e+00]    [1.52e+01, 1.57e+01, 1.45e+01, 1.44e+01]    
8000      [3.00e+01, 7.26e+00, 2.63e+00]    [0.00e+00, 2.38e+01, 2.63e+00]    [2.33e+01, 2.38e+01, 1.63e+01, 1.70e+01]    
9000      [2.80e+01, 3.71e+00, 2.63e+00]    [0.00e+00, 2.27e+01, 2.63e+00]    [2.21e+01, 2.27e+01, 1.50e+01, 1.57e+01]    
10000     [3.02e+01, 1.33e+01, 2.63e+00]    [0.00e+00, 1.61e+01, 2.63e+00]    [1.55e+01, 1.61e+01, 1.55e+01, 1.51e+01]    
11000     [2.66e+01, 3.95e-01, 2.63e+00]    [0.00e+00, 2.18e+01, 2.63e+00]    [2.11e+01, 2.18e+01, 1.42e+01, 1.48e+01]    
12000     [2.68e+01, 3.17e+00, 2.62e+00]    [0.00e+00, 2.01e+01, 2.62e+00]    [1.93e+01, 2.01e+01, 1.44e+01, 1.45e+01]    
13000     [2.81e+01, 6.31e+00, 2.62e+00]    [0.00e+00, 2.44e+01, 2.62e+00]    [2.37e+01, 2.44e+01, 1.54e+01, 1.66e+01]    
14000     [2.68e+01, 3.87e+00, 2.62e+00]    [0.00e+00, 1.96e+01, 2.62e+00]    [1.90e+01, 1.96e+01, 1.44e+01, 1.46e+01]    
15000     [2.72e+01, 5.74e+00, 2.62e+00]    [0.00e+00, 2.35e+01, 2.62e+00]    [2.25e+01, 2.35e+01, 1.48e+01, 1.55e+01]    
16000     [2.63e+01, 3.75e+00, 2.62e+00]    [0.00e+00, 2.02e+01, 2.62e+00]    [1.95e+01, 2.02e+01, 1.38e+01, 1.41e+01]    
17000     [2.82e+01, 8.57e+00, 2.62e+00]    [0.00e+00, 2.47e+01, 2.62e+00]    [2.37e+01, 2.47e+01, 1.49e+01, 1.58e+01]    
18000     [2.71e+01, 5.57e+00, 2.61e+00]    [0.00e+00, 1.89e+01, 2.61e+00]    [1.80e+01, 1.89e+01, 1.45e+01, 1.40e+01]    
19000     [2.71e+01, 6.24e+00, 2.61e+00]    [0.00e+00, 1.85e+01, 2.61e+00]    [1.79e+01, 1.85e+01, 1.45e+01, 1.44e+01]    
20000     [2.71e+01, 6.27e+00, 2.61e+00]    [0.00e+00, 2.36e+01, 2.61e+00]    [2.29e+01, 2.36e+01, 1.40e+01, 1.53e+01]    
21000     [2.69e+01, 5.45e+00, 2.61e+00]    [0.00e+00, 1.86e+01, 2.61e+00]    [1.75e+01, 1.86e+01, 1.47e+01, 1.40e+01]    
22000     [2.74e+01, 6.96e+00, 2.61e+00]    [0.00e+00, 1.77e+01, 2.61e+00]    [1.71e+01, 1.77e+01, 1.50e+01, 1.47e+01]    
23000     [2.75e+01, 8.81e+00, 2.60e+00]    [0.00e+00, 2.37e+01, 2.60e+00]    [2.27e+01, 2.37e+01, 1.42e+01, 1.52e+01]    
24000     [2.66e+01, 4.96e+00, 2.60e+00]    [0.00e+00, 1.82e+01, 2.60e+00]    [1.76e+01, 1.82e+01, 1.44e+01, 1.43e+01]    
25000     [2.67e+01, 6.09e+00, 2.60e+00]    [0.00e+00, 1.81e+01, 2.60e+00]    [1.75e+01, 1.81e+01, 1.45e+01, 1.42e+01]    
26000     [2.68e+01, 8.11e+00, 2.60e+00]    [0.00e+00, 2.29e+01, 2.60e+00]    [2.21e+01, 2.29e+01, 1.35e+01, 1.47e+01]    
27000     [2.58e+01, 2.98e+00, 2.60e+00]    [0.00e+00, 1.86e+01, 2.60e+00]    [1.80e+01, 1.86e+01, 1.40e+01, 1.40e+01]    
28000     [2.66e+01, 5.74e+00, 2.59e+00]    [0.00e+00, 1.77e+01, 2.59e+00]    [1.71e+01, 1.77e+01, 1.48e+01, 1.43e+01]    
29000     [2.72e+01, 8.76e+00, 2.59e+00]    [0.00e+00, 2.33e+01, 2.59e+00]    [2.26e+01, 2.33e+01, 1.33e+01, 1.49e+01]    
30000     [2.54e+01, 2.94e+00, 2.59e+00]    [0.00e+00, 1.89e+01, 2.59e+00]    [1.83e+01, 1.89e+01, 1.37e+01, 1.36e+01]    

Best model at step 11000:
  train loss: 2.96e+01
  test loss: 2.45e+01
  test metric: [2.11e+01, 2.18e+01, 1.42e+01, 1.48e+01]

'train' took 55.731861 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.203614 s

'compile' took 1.020367 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.73e+02, 8.55e+02, 2.76e+00]    [0.00e+00, 7.41e+02, 2.76e+00]    [7.41e+02, 7.41e+02, 1.85e+03, 1.85e+03]    
1000      [4.39e+01, 4.64e+00, 2.75e+00]    [0.00e+00, 5.73e+01, 2.75e+00]    [5.78e+01, 5.73e+01, 1.85e+01, 1.96e+01]    
2000      [3.45e+01, 9.60e+00, 2.75e+00]    [0.00e+00, 4.59e+01, 2.75e+00]    [4.72e+01, 4.59e+01, 1.52e+01, 1.53e+01]    
3000      [2.90e+01, 1.12e+00, 2.74e+00]    [0.00e+00, 3.68e+01, 2.74e+00]    [3.55e+01, 3.68e+01, 1.88e+01, 1.68e+01]    
4000      [2.74e+01, 2.28e+00, 2.74e+00]    [0.00e+00, 3.25e+01, 2.74e+00]    [3.34e+01, 3.25e+01, 1.59e+01, 1.70e+01]    
5000      [2.68e+01, 2.10e+00, 2.74e+00]    [0.00e+00, 3.01e+01, 2.74e+00]    [3.08e+01, 3.01e+01, 1.54e+01, 1.61e+01]    
6000      [2.69e+01, 4.44e+00, 2.74e+00]    [0.00e+00, 2.67e+01, 2.74e+00]    [2.98e+01, 2.67e+01, 1.48e+01, 1.74e+01]    
7000      [2.66e+01, 4.92e+00, 2.74e+00]    [0.00e+00, 2.49e+01, 2.74e+00]    [2.84e+01, 2.49e+01, 1.53e+01, 1.68e+01]    
8000      [2.77e+01, 7.79e+00, 2.74e+00]    [0.00e+00, 2.64e+01, 2.74e+00]    [3.00e+01, 2.64e+01, 1.65e+01, 1.48e+01]    
9000      [2.62e+01, 5.12e+00, 2.73e+00]    [0.00e+00, 2.44e+01, 2.73e+00]    [2.73e+01, 2.44e+01, 1.43e+01, 1.62e+01]    
10000     [2.61e+01, 6.08e+00, 2.73e+00]    [0.00e+00, 2.41e+01, 2.73e+00]    [2.71e+01, 2.41e+01, 1.44e+01, 1.62e+01]    
11000     [2.54e+01, 3.42e+00, 2.73e+00]    [0.00e+00, 2.93e+01, 2.73e+00]    [2.56e+01, 2.93e+01, 1.47e+01, 1.73e+01]    
12000     [2.48e+01, 1.40e+00, 2.73e+00]    [0.00e+00, 2.77e+01, 2.73e+00]    [2.39e+01, 2.77e+01, 1.63e+01, 1.58e+01]    
13000     [2.52e+01, 4.71e+00, 2.73e+00]    [0.00e+00, 2.44e+01, 2.73e+00]    [2.67e+01, 2.44e+01, 1.47e+01, 1.60e+01]    
14000     [2.73e+01, 6.75e+00, 2.73e+00]    [0.00e+00, 3.06e+01, 2.73e+00]    [2.69e+01, 3.06e+01, 1.57e+01, 2.08e+01]    
15000     [2.56e+01, 6.24e+00, 2.73e+00]    [0.00e+00, 2.40e+01, 2.73e+00]    [2.74e+01, 2.40e+01, 1.50e+01, 1.69e+01]    
16000     [2.67e+01, 5.29e+00, 2.73e+00]    [0.00e+00, 3.12e+01, 2.73e+00]    [2.74e+01, 3.12e+01, 1.60e+01, 2.16e+01]    
17000     [2.58e+01, 6.47e+00, 2.73e+00]    [0.00e+00, 2.36e+01, 2.73e+00]    [2.77e+01, 2.36e+01, 1.54e+01, 1.74e+01]    
18000     [2.70e+01, 9.58e+00, 2.73e+00]    [0.00e+00, 2.56e+01, 2.73e+00]    [2.97e+01, 2.56e+01, 1.77e+01, 1.57e+01]    
19000     [2.42e+01, 1.85e+00, 2.73e+00]    [0.00e+00, 2.76e+01, 2.73e+00]    [2.37e+01, 2.76e+01, 1.71e+01, 1.68e+01]    
20000     [2.49e+01, 5.13e+00, 2.73e+00]    [0.00e+00, 2.30e+01, 2.73e+00]    [2.72e+01, 2.30e+01, 1.64e+01, 1.81e+01]    
21000     [2.53e+01, 4.53e+00, 2.73e+00]    [0.00e+00, 3.04e+01, 2.73e+00]    [2.64e+01, 3.04e+01, 1.61e+01, 2.07e+01]    
22000     [2.54e+01, 4.29e+00, 2.72e+00]    [0.00e+00, 2.98e+01, 2.72e+00]    [2.58e+01, 2.98e+01, 1.63e+01, 2.04e+01]    
23000     [2.50e+01, 5.97e+00, 2.72e+00]    [0.00e+00, 2.36e+01, 2.72e+00]    [2.70e+01, 2.36e+01, 1.59e+01, 1.76e+01]    
24000     [2.45e+01, 4.40e+00, 2.72e+00]    [0.00e+00, 2.32e+01, 2.72e+00]    [2.72e+01, 2.32e+01, 1.63e+01, 1.80e+01]    
25000     [2.39e+01, 2.37e+00, 2.72e+00]    [0.00e+00, 2.82e+01, 2.72e+00]    [2.41e+01, 2.82e+01, 1.70e+01, 1.84e+01]    
26000     [2.41e+01, 3.26e+00, 2.72e+00]    [0.00e+00, 2.87e+01, 2.72e+00]    [2.45e+01, 2.87e+01, 1.68e+01, 1.92e+01]    
27000     [2.33e+01, 2.62e+00, 2.72e+00]    [0.00e+00, 2.51e+01, 2.72e+00]    [2.47e+01, 2.51e+01, 1.67e+01, 1.69e+01]    
28000     [2.52e+01, 6.41e+00, 2.72e+00]    [0.00e+00, 2.36e+01, 2.72e+00]    [2.70e+01, 2.36e+01, 1.64e+01, 1.77e+01]    
29000     [2.33e+01, 3.43e+00, 2.72e+00]    [0.00e+00, 2.57e+01, 2.72e+00]    [2.36e+01, 2.57e+01, 1.74e+01, 1.72e+01]    
30000     [2.57e+01, 7.90e+00, 2.72e+00]    [0.00e+00, 2.34e+01, 2.72e+00]    [2.79e+01, 2.34e+01, 1.71e+01, 1.81e+01]    

Best model at step 27000:
  train loss: 2.86e+01
  test loss: 2.78e+01
  test metric: [2.47e+01, 2.51e+01, 1.67e+01, 1.69e+01]

'train' took 52.922975 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.208099 s

'compile' took 0.995998 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.26e+02, 9.26e+02, 2.74e+00]    [0.00e+00, 6.99e+02, 2.74e+00]    [6.99e+02, 6.99e+02, 1.65e+03, 1.65e+03]    
1000      [4.58e+01, 1.80e+01, 2.73e+00]    [0.00e+00, 5.41e+01, 2.73e+00]    [5.15e+01, 5.41e+01, 2.00e+01, 2.09e+01]    
2000      [3.00e+01, 2.64e+00, 2.74e+00]    [0.00e+00, 4.03e+01, 2.74e+00]    [3.61e+01, 4.03e+01, 1.41e+01, 2.30e+01]    
3000      [2.92e+01, 7.07e+00, 2.73e+00]    [0.00e+00, 2.85e+01, 2.73e+00]    [2.73e+01, 2.85e+01, 1.34e+01, 1.23e+01]    
4000      [2.67e+01, 4.14e+00, 2.73e+00]    [0.00e+00, 3.35e+01, 2.73e+00]    [2.93e+01, 3.35e+01, 1.46e+01, 2.31e+01]    
5000      [2.90e+01, 9.46e+00, 2.72e+00]    [0.00e+00, 2.32e+01, 2.72e+00]    [2.57e+01, 2.32e+01, 1.30e+01, 1.39e+01]    
6000      [2.51e+01, 2.03e+00, 2.71e+00]    [0.00e+00, 3.05e+01, 2.71e+00]    [2.60e+01, 3.05e+01, 1.22e+01, 1.80e+01]    
7000      [2.56e+01, 3.61e+00, 2.71e+00]    [0.00e+00, 3.12e+01, 2.71e+00]    [2.68e+01, 3.12e+01, 1.28e+01, 2.03e+01]    
8000      [2.69e+01, 7.32e+00, 2.71e+00]    [0.00e+00, 2.39e+01, 2.71e+00]    [2.40e+01, 2.39e+01, 1.24e+01, 1.22e+01]    
9000      [2.77e+01, 7.75e+00, 2.70e+00]    [0.00e+00, 3.35e+01, 2.70e+00]    [2.90e+01, 3.35e+01, 1.69e+01, 2.66e+01]    
10000     [2.65e+01, 7.44e+00, 2.70e+00]    [0.00e+00, 2.40e+01, 2.70e+00]    [2.35e+01, 2.40e+01, 1.24e+01, 1.19e+01]    
11000     [2.50e+01, 3.01e+00, 2.69e+00]    [0.00e+00, 3.08e+01, 2.69e+00]    [2.63e+01, 3.08e+01, 1.26e+01, 2.04e+01]    
12000     [2.59e+01, 5.27e+00, 2.69e+00]    [0.00e+00, 3.13e+01, 2.69e+00]    [2.68e+01, 3.13e+01, 1.43e+01, 2.29e+01]    
13000     [2.40e+01, 2.17e+00, 2.69e+00]    [0.00e+00, 2.74e+01, 2.69e+00]    [2.29e+01, 2.74e+01, 1.27e+01, 1.58e+01]    
14000     [2.54e+01, 5.46e+00, 2.68e+00]    [0.00e+00, 2.33e+01, 2.68e+00]    [2.25e+01, 2.33e+01, 1.29e+01, 1.26e+01]    
15000     [2.72e+01, 7.99e+00, 2.68e+00]    [0.00e+00, 3.36e+01, 2.68e+00]    [2.90e+01, 3.36e+01, 1.81e+01, 2.84e+01]    
16000     [2.47e+01, 4.91e+00, 2.67e+00]    [0.00e+00, 2.41e+01, 2.67e+00]    [2.21e+01, 2.41e+01, 1.33e+01, 1.27e+01]    
17000     [2.39e+01, 2.62e+00, 2.67e+00]    [0.00e+00, 2.92e+01, 2.67e+00]    [2.45e+01, 2.92e+01, 1.27e+01, 1.90e+01]    
18000     [2.55e+01, 5.71e+00, 2.67e+00]    [0.00e+00, 3.25e+01, 2.67e+00]    [2.78e+01, 3.25e+01, 1.52e+01, 2.48e+01]    
19000     [2.69e+01, 9.13e+00, 2.66e+00]    [0.00e+00, 3.18e+01, 2.66e+00]    [2.71e+01, 3.18e+01, 1.60e+01, 2.55e+01]    
20000     [2.40e+01, 3.69e+00, 2.66e+00]    [0.00e+00, 2.47e+01, 2.66e+00]    [2.17e+01, 2.47e+01, 1.40e+01, 1.34e+01]    
21000     [2.43e+01, 3.89e+00, 2.65e+00]    [0.00e+00, 2.39e+01, 2.65e+00]    [2.21e+01, 2.39e+01, 1.38e+01, 1.35e+01]    
22000     [2.41e+01, 4.22e+00, 2.65e+00]    [0.00e+00, 2.96e+01, 2.65e+00]    [2.47e+01, 2.96e+01, 1.37e+01, 2.10e+01]    
23000     [2.71e+01, 9.09e+00, 2.65e+00]    [0.00e+00, 3.42e+01, 2.65e+00]    [2.94e+01, 3.42e+01, 1.98e+01, 3.07e+01]    
24000     [2.48e+01, 5.44e+00, 2.64e+00]    [0.00e+00, 3.15e+01, 2.64e+00]    [2.66e+01, 3.15e+01, 1.46e+01, 2.35e+01]    
25000     [2.40e+01, 4.76e+00, 2.64e+00]    [0.00e+00, 2.98e+01, 2.64e+00]    [2.49e+01, 2.98e+01, 1.38e+01, 2.11e+01]    
26000     [2.58e+01, 7.77e+00, 2.63e+00]    [0.00e+00, 2.30e+01, 2.63e+00]    [2.37e+01, 2.30e+01, 1.37e+01, 1.38e+01]    
27000     [2.35e+01, 3.89e+00, 2.63e+00]    [0.00e+00, 3.00e+01, 2.63e+00]    [2.50e+01, 3.00e+01, 1.33e+01, 2.01e+01]    
28000     [2.91e+01, 1.30e+01, 2.63e+00]    [0.00e+00, 2.33e+01, 2.63e+00]    [2.77e+01, 2.33e+01, 2.01e+01, 1.46e+01]    
29000     [2.62e+01, 8.44e+00, 2.62e+00]    [0.00e+00, 2.23e+01, 2.62e+00]    [2.43e+01, 2.23e+01, 1.43e+01, 1.46e+01]    
30000     [2.26e+01, 1.50e+00, 2.62e+00]    [0.00e+00, 2.59e+01, 2.62e+00]    [2.08e+01, 2.59e+01, 1.59e+01, 1.56e+01]    

Best model at step 30000:
  train loss: 2.68e+01
  test loss: 2.85e+01
  test metric: [2.08e+01, 2.59e+01, 1.59e+01, 1.56e+01]

'train' took 58.343799 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[14.359377524238731, 297.5095550042416, 17.729696581275075, 17.474699478864203, 29.795735851695568, 88.42518473265957, 24.43977381976244, 21.83166980449509, 25.11427889501486, 25.926048831568203]
sigma_y 5 56.260602052381536 82.92701092206198
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.219281 s

'compile' took 0.949355 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.21e+02, 6.63e+02, 2.65e+00]    [0.00e+00, 2.59e+02, 2.65e+00]    [2.59e+02, 2.59e+02, 1.77e+02, 1.77e+02]    
1000      [5.63e+01, 3.70e+01, 2.64e+00]    [0.00e+00, 6.12e+01, 2.64e+00]    [6.12e+01, 6.12e+01, 1.74e+01, 1.73e+01]    
2000      [3.66e+01, 3.19e+01, 2.67e+00]    [0.00e+00, 3.10e+01, 2.67e+00]    [3.11e+01, 3.10e+01, 9.79e+00, 9.55e+00]    
3000      [3.15e+01, 2.31e+01, 2.69e+00]    [0.00e+00, 1.69e+01, 2.69e+00]    [1.69e+01, 1.69e+01, 1.20e+01, 1.17e+01]    
4000      [2.90e+01, 1.26e+01, 2.70e+00]    [0.00e+00, 1.52e+01, 2.70e+00]    [1.52e+01, 1.52e+01, 8.76e+00, 8.61e+00]    
5000      [3.11e+01, 1.68e+01, 2.70e+00]    [0.00e+00, 1.34e+01, 2.70e+00]    [1.34e+01, 1.34e+01, 9.49e+00, 9.56e+00]    
6000      [2.92e+01, 4.38e+00, 2.71e+00]    [0.00e+00, 1.35e+01, 2.71e+00]    [1.33e+01, 1.35e+01, 1.15e+01, 1.15e+01]    
7000      [3.04e+01, 1.45e+01, 2.71e+00]    [0.00e+00, 1.44e+01, 2.71e+00]    [1.42e+01, 1.44e+01, 1.15e+01, 1.15e+01]    
8000      [3.02e+01, 1.29e+01, 2.71e+00]    [0.00e+00, 1.35e+01, 2.71e+00]    [1.35e+01, 1.35e+01, 1.17e+01, 1.19e+01]    
9000      [3.07e+01, 1.79e+01, 2.70e+00]    [0.00e+00, 1.51e+01, 2.70e+00]    [1.48e+01, 1.51e+01, 1.18e+01, 1.17e+01]    
10000     [2.90e+01, 9.33e+00, 2.70e+00]    [0.00e+00, 1.49e+01, 2.70e+00]    [1.50e+01, 1.49e+01, 1.11e+01, 1.15e+01]    
11000     [3.06e+01, 1.63e+01, 2.70e+00]    [0.00e+00, 1.49e+01, 2.70e+00]    [1.50e+01, 1.49e+01, 1.08e+01, 1.11e+01]    
12000     [2.82e+01, 3.82e+00, 2.69e+00]    [0.00e+00, 1.44e+01, 2.69e+00]    [1.45e+01, 1.44e+01, 1.18e+01, 1.21e+01]    
13000     [2.78e+01, 2.82e+00, 2.69e+00]    [0.00e+00, 1.61e+01, 2.69e+00]    [1.63e+01, 1.61e+01, 1.06e+01, 1.10e+01]    
14000     [2.87e+01, 8.98e+00, 2.69e+00]    [0.00e+00, 1.55e+01, 2.69e+00]    [1.56e+01, 1.55e+01, 1.09e+01, 1.11e+01]    
15000     [2.78e+01, 1.01e+01, 2.69e+00]    [0.00e+00, 1.64e+01, 2.69e+00]    [1.66e+01, 1.64e+01, 1.06e+01, 1.10e+01]    
16000     [2.78e+01, 5.38e+00, 2.69e+00]    [0.00e+00, 1.63e+01, 2.69e+00]    [1.65e+01, 1.63e+01, 1.06e+01, 1.09e+01]    
17000     [2.79e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 1.67e+01, 2.69e+00]    [1.69e+01, 1.67e+01, 1.11e+01, 1.14e+01]    
18000     [2.90e+01, 1.53e+01, 2.68e+00]    [0.00e+00, 1.73e+01, 2.68e+00]    [1.75e+01, 1.73e+01, 1.08e+01, 1.11e+01]    
19000     [2.95e+01, 1.23e+01, 2.68e+00]    [0.00e+00, 1.71e+01, 2.68e+00]    [1.73e+01, 1.71e+01, 1.07e+01, 1.08e+01]    
20000     [2.82e+01, 7.66e+00, 2.68e+00]    [0.00e+00, 1.71e+01, 2.68e+00]    [1.74e+01, 1.71e+01, 1.11e+01, 1.12e+01]    
21000     [2.72e+01, 1.73e+00, 2.68e+00]    [0.00e+00, 1.74e+01, 2.68e+00]    [1.77e+01, 1.74e+01, 1.13e+01, 1.15e+01]    
22000     [2.72e+01, 4.10e+00, 2.68e+00]    [0.00e+00, 1.80e+01, 2.68e+00]    [1.82e+01, 1.80e+01, 1.09e+01, 1.10e+01]    
23000     [2.72e+01, 4.19e+00, 2.68e+00]    [0.00e+00, 1.83e+01, 2.68e+00]    [1.86e+01, 1.83e+01, 1.08e+01, 1.08e+01]    
24000     [2.72e+01, 4.60e+00, 2.68e+00]    [0.00e+00, 1.82e+01, 2.68e+00]    [1.85e+01, 1.82e+01, 1.09e+01, 1.10e+01]    
25000     [2.86e+01, 9.30e+00, 2.67e+00]    [0.00e+00, 1.81e+01, 2.67e+00]    [1.84e+01, 1.81e+01, 1.08e+01, 1.08e+01]    
26000     [2.74e+01, 7.74e+00, 2.67e+00]    [0.00e+00, 1.80e+01, 2.67e+00]    [1.82e+01, 1.80e+01, 1.12e+01, 1.10e+01]    
27000     [2.72e+01, 1.07e+01, 2.67e+00]    [0.00e+00, 1.84e+01, 2.67e+00]    [1.87e+01, 1.84e+01, 1.06e+01, 1.08e+01]    
28000     [2.75e+01, 6.86e+00, 2.67e+00]    [0.00e+00, 1.81e+01, 2.67e+00]    [1.84e+01, 1.81e+01, 1.07e+01, 1.07e+01]    
29000     [2.65e+01, 2.95e+00, 2.66e+00]    [0.00e+00, 1.85e+01, 2.66e+00]    [1.88e+01, 1.85e+01, 1.02e+01, 1.02e+01]    
30000     [2.68e+01, 1.08e+01, 2.66e+00]    [0.00e+00, 1.82e+01, 2.66e+00]    [1.86e+01, 1.82e+01, 1.06e+01, 1.07e+01]    

Best model at step 21000:
  train loss: 3.16e+01
  test loss: 2.01e+01
  test metric: [1.77e+01, 1.74e+01, 1.13e+01, 1.15e+01]

'train' took 59.775891 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.239506 s

'compile' took 1.011071 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [8.08e+02, 1.16e+02, 2.66e+00]    [0.00e+00, 1.97e+03, 2.66e+00]    [1.97e+03, 1.97e+03, 2.80e+03, 2.80e+03]    
1000      [2.93e+01, 1.37e+01, 2.64e+00]    [0.00e+00, 1.77e+01, 2.64e+00]    [1.79e+01, 1.77e+01, 1.43e+01, 2.57e+01]    
2000      [2.79e+01, 9.51e+00, 2.63e+00]    [0.00e+00, 1.23e+01, 2.63e+00]    [1.20e+01, 1.23e+01, 1.11e+01, 9.23e+00]    
3000      [2.79e+01, 5.46e+00, 2.65e+00]    [0.00e+00, 6.39e+01, 2.65e+00]    [1.01e+01, 6.39e+01, 1.07e+01, 8.05e+01]    
4000      [2.73e+01, 1.72e+00, 2.68e+00]    [0.00e+00, 1.80e+02, 2.68e+00]    [9.58e+00, 1.80e+02, 1.01e+01, 2.53e+02]    
5000      [2.73e+01, 1.38e+00, 2.68e+00]    [0.00e+00, 2.23e+02, 2.68e+00]    [1.09e+01, 2.23e+02, 1.01e+01, 3.18e+02]    
6000      [2.68e+01, 1.03e+00, 2.68e+00]    [0.00e+00, 2.48e+02, 2.68e+00]    [1.11e+01, 2.48e+02, 9.05e+00, 3.54e+02]    
7000      [2.70e+01, 2.96e-01, 2.68e+00]    [0.00e+00, 2.80e+02, 2.68e+00]    [1.23e+01, 2.80e+02, 8.64e+00, 4.04e+02]    
8000      [2.84e+01, 5.14e-01, 2.68e+00]    [0.00e+00, 2.94e+02, 2.68e+00]    [1.26e+01, 2.94e+02, 1.37e+01, 4.25e+02]    
9000      [2.64e+01, 1.84e-01, 2.68e+00]    [0.00e+00, 3.22e+02, 2.68e+00]    [1.11e+01, 3.22e+02, 9.01e+00, 4.67e+02]    
10000     [2.76e+01, 3.99e-01, 2.68e+00]    [0.00e+00, 3.37e+02, 2.68e+00]    [1.28e+01, 3.37e+02, 1.39e+01, 4.88e+02]    
11000     [2.70e+01, 2.81e-01, 2.68e+00]    [0.00e+00, 3.72e+02, 2.68e+00]    [1.62e+01, 3.72e+02, 1.03e+01, 5.42e+02]    
12000     [2.56e+01, 1.88e-01, 2.68e+00]    [0.00e+00, 3.86e+02, 2.68e+00]    [1.36e+01, 3.86e+02, 9.16e+00, 5.62e+02]    
13000     [2.67e+01, 3.60e-01, 2.68e+00]    [0.00e+00, 3.91e+02, 2.68e+00]    [1.29e+01, 3.91e+02, 1.30e+01, 5.68e+02]    
14000     [2.43e+01, 1.30e-01, 2.67e+00]    [0.00e+00, 4.18e+02, 2.67e+00]    [1.18e+01, 4.18e+02, 1.05e+01, 6.07e+02]    
15000     [2.47e+01, 2.54e-01, 2.67e+00]    [0.00e+00, 4.30e+02, 2.67e+00]    [1.33e+01, 4.30e+02, 1.05e+01, 6.24e+02]    
16000     [2.43e+01, 2.41e-01, 2.67e+00]    [0.00e+00, 4.54e+02, 2.67e+00]    [1.36e+01, 4.54e+02, 1.07e+01, 6.59e+02]    
17000     [2.74e+01, 7.27e-01, 2.67e+00]    [0.00e+00, 4.52e+02, 2.67e+00]    [1.61e+01, 4.52e+02, 1.35e+01, 6.57e+02]    
18000     [2.38e+01, 1.51e-01, 2.67e+00]    [0.00e+00, 4.74e+02, 2.67e+00]    [1.32e+01, 4.74e+02, 1.11e+01, 6.87e+02]    
19000     [2.44e+01, 2.53e-01, 2.66e+00]    [0.00e+00, 4.74e+02, 2.66e+00]    [1.22e+01, 4.74e+02, 1.12e+01, 6.86e+02]    
20000     [2.52e+01, 3.72e-01, 2.66e+00]    [0.00e+00, 4.74e+02, 2.66e+00]    [1.36e+01, 4.74e+02, 1.29e+01, 6.85e+02]    
21000     [2.78e+01, 8.40e-01, 2.66e+00]    [0.00e+00, 4.69e+02, 2.66e+00]    [1.74e+01, 4.69e+02, 1.55e+01, 6.78e+02]    
22000     [2.47e+01, 3.28e-01, 2.65e+00]    [0.00e+00, 4.82e+02, 2.65e+00]    [1.20e+01, 4.82e+02, 1.19e+01, 6.96e+02]    
23000     [2.42e+01, 1.97e-01, 2.65e+00]    [0.00e+00, 4.94e+02, 2.65e+00]    [1.52e+01, 4.94e+02, 1.03e+01, 7.11e+02]    
24000     [2.66e+01, 5.02e-01, 2.65e+00]    [0.00e+00, 5.02e+02, 2.65e+00]    [2.09e+01, 5.02e+02, 1.25e+01, 7.22e+02]    
25000     [2.44e+01, 3.17e-01, 2.65e+00]    [0.00e+00, 4.85e+02, 2.65e+00]    [1.23e+01, 4.85e+02, 1.18e+01, 6.98e+02]    
26000     [2.47e+01, 3.61e-01, 2.64e+00]    [0.00e+00, 4.98e+02, 2.64e+00]    [1.68e+01, 4.98e+02, 1.08e+01, 7.16e+02]    
27000     [2.37e+01, 4.22e-01, 2.64e+00]    [0.00e+00, 4.88e+02, 2.64e+00]    [1.12e+01, 4.88e+02, 1.13e+01, 7.02e+02]    
28000     [2.47e+01, 3.73e-01, 2.64e+00]    [0.00e+00, 4.82e+02, 2.64e+00]    [1.43e+01, 4.82e+02, 1.39e+01, 6.90e+02]    
29000     [2.71e+01, 6.74e-01, 2.64e+00]    [0.00e+00, 4.72e+02, 2.64e+00]    [1.88e+01, 4.72e+02, 1.66e+01, 6.76e+02]    
30000     [2.59e+01, 5.94e-01, 2.64e+00]    [0.00e+00, 4.76e+02, 2.64e+00]    [1.40e+01, 4.76e+02, 1.31e+01, 6.85e+02]    

Best model at step 18000:
  train loss: 2.66e+01
  test loss: 4.77e+02
  test metric: [1.32e+01, 4.74e+02, 1.11e+01, 6.87e+02]

'train' took 54.467518 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.200262 s

'compile' took 0.950899 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.57e+02, 7.07e+02, 2.72e+00]    [0.00e+00, 2.33e+02, 2.72e+00]    [2.33e+02, 2.33e+02, 3.73e+02, 3.73e+02]    
1000      [4.07e+01, 2.70e+01, 2.70e+00]    [0.00e+00, 4.90e+01, 2.70e+00]    [4.91e+01, 4.90e+01, 1.65e+01, 1.65e+01]    
2000      [2.94e+01, 1.40e+01, 2.70e+00]    [0.00e+00, 2.63e+01, 2.70e+00]    [2.65e+01, 2.63e+01, 1.88e+01, 1.89e+01]    
3000      [2.89e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 2.16e+01, 2.69e+00]    [2.15e+01, 2.16e+01, 1.66e+01, 1.64e+01]    
4000      [2.73e+01, 4.24e+00, 2.69e+00]    [0.00e+00, 2.25e+01, 2.69e+00]    [2.24e+01, 2.25e+01, 1.60e+01, 1.59e+01]    
5000      [3.14e+01, 1.43e+01, 2.69e+00]    [0.00e+00, 2.65e+01, 2.69e+00]    [2.63e+01, 2.65e+01, 1.87e+01, 1.86e+01]    
6000      [2.86e+01, 6.87e+00, 2.69e+00]    [0.00e+00, 2.34e+01, 2.69e+00]    [2.27e+01, 2.34e+01, 1.66e+01, 1.60e+01]    
7000      [2.84e+01, 7.29e+00, 2.69e+00]    [0.00e+00, 2.27e+01, 2.69e+00]    [2.21e+01, 2.27e+01, 1.72e+01, 1.66e+01]    
8000      [2.77e+01, 5.53e+00, 2.69e+00]    [0.00e+00, 2.28e+01, 2.69e+00]    [2.21e+01, 2.28e+01, 1.70e+01, 1.63e+01]    
9000      [2.69e+01, 3.78e+00, 2.68e+00]    [0.00e+00, 2.50e+01, 2.68e+00]    [2.43e+01, 2.50e+01, 1.65e+01, 1.58e+01]    
10000     [2.79e+01, 7.64e+00, 2.68e+00]    [0.00e+00, 2.18e+01, 2.68e+00]    [2.11e+01, 2.18e+01, 1.67e+01, 1.60e+01]    
11000     [2.95e+01, 1.18e+01, 2.68e+00]    [0.00e+00, 2.01e+01, 2.68e+00]    [1.94e+01, 2.01e+01, 1.79e+01, 1.73e+01]    
12000     [2.64e+01, 3.92e+00, 2.68e+00]    [0.00e+00, 2.33e+01, 2.68e+00]    [2.26e+01, 2.33e+01, 1.67e+01, 1.59e+01]    
13000     [2.63e+01, 3.82e+00, 2.67e+00]    [0.00e+00, 2.34e+01, 2.67e+00]    [2.28e+01, 2.34e+01, 1.61e+01, 1.54e+01]    
14000     [2.90e+01, 1.22e+01, 2.67e+00]    [0.00e+00, 1.92e+01, 2.67e+00]    [1.87e+01, 1.92e+01, 1.78e+01, 1.73e+01]    
15000     [2.77e+01, 8.78e+00, 2.67e+00]    [0.00e+00, 2.37e+01, 2.67e+00]    [2.32e+01, 2.37e+01, 1.65e+01, 1.59e+01]    
16000     [2.66e+01, 6.04e+00, 2.66e+00]    [0.00e+00, 2.01e+01, 2.66e+00]    [1.96e+01, 2.01e+01, 1.64e+01, 1.59e+01]    
17000     [2.77e+01, 8.48e+00, 2.66e+00]    [0.00e+00, 2.39e+01, 2.66e+00]    [2.35e+01, 2.39e+01, 1.54e+01, 1.50e+01]    
18000     [2.59e+01, 4.60e+00, 2.65e+00]    [0.00e+00, 2.23e+01, 2.65e+00]    [2.19e+01, 2.23e+01, 1.55e+01, 1.49e+01]    
19000     [2.54e+01, 2.52e+00, 2.65e+00]    [0.00e+00, 2.07e+01, 2.65e+00]    [2.03e+01, 2.07e+01, 1.59e+01, 1.53e+01]    
20000     [2.61e+01, 5.03e+00, 2.64e+00]    [0.00e+00, 1.98e+01, 2.64e+00]    [1.95e+01, 1.98e+01, 1.61e+01, 1.57e+01]    
21000     [2.69e+01, 8.41e+00, 2.64e+00]    [0.00e+00, 2.29e+01, 2.64e+00]    [2.25e+01, 2.29e+01, 1.55e+01, 1.49e+01]    
22000     [2.65e+01, 7.54e+00, 2.64e+00]    [0.00e+00, 2.20e+01, 2.64e+00]    [2.16e+01, 2.20e+01, 1.57e+01, 1.51e+01]    
23000     [2.61e+01, 6.72e+00, 2.63e+00]    [0.00e+00, 2.23e+01, 2.63e+00]    [2.19e+01, 2.23e+01, 1.51e+01, 1.45e+01]    
24000     [2.59e+01, 5.25e+00, 2.63e+00]    [0.00e+00, 1.92e+01, 2.63e+00]    [1.90e+01, 1.92e+01, 1.61e+01, 1.57e+01]    
25000     [2.72e+01, 9.62e+00, 2.63e+00]    [0.00e+00, 1.95e+01, 2.63e+00]    [1.90e+01, 1.95e+01, 1.63e+01, 1.57e+01]    
26000     [2.50e+01, 2.72e+00, 2.62e+00]    [0.00e+00, 2.14e+01, 2.62e+00]    [2.10e+01, 2.14e+01, 1.46e+01, 1.40e+01]    
27000     [2.47e+01, 1.33e+00, 2.62e+00]    [0.00e+00, 2.02e+01, 2.62e+00]    [2.00e+01, 2.02e+01, 1.53e+01, 1.48e+01]    
28000     [2.49e+01, 4.47e+00, 2.62e+00]    [0.00e+00, 2.11e+01, 2.62e+00]    [2.08e+01, 2.11e+01, 1.49e+01, 1.42e+01]    
29000     [2.52e+01, 4.34e+00, 2.61e+00]    [0.00e+00, 1.90e+01, 2.61e+00]    [1.88e+01, 1.90e+01, 1.58e+01, 1.53e+01]    
30000     [2.50e+01, 4.82e+00, 2.61e+00]    [0.00e+00, 2.11e+01, 2.61e+00]    [2.08e+01, 2.11e+01, 1.49e+01, 1.42e+01]    

Best model at step 27000:
  train loss: 2.87e+01
  test loss: 2.28e+01
  test metric: [2.00e+01, 2.02e+01, 1.53e+01, 1.48e+01]

'train' took 59.794842 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.179926 s

'compile' took 0.856641 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.51e+02, 4.80e+02, 2.69e+00]    [0.00e+00, 2.96e+02, 2.69e+00]    [2.96e+02, 2.96e+02, 2.66e+02, 2.66e+02]    
1000      [4.03e+01, 1.67e+01, 2.65e+00]    [0.00e+00, 6.07e+01, 2.65e+00]    [6.06e+01, 6.07e+01, 2.38e+01, 2.38e+01]    
2000      [3.54e+01, 2.04e+01, 2.62e+00]    [0.00e+00, 4.90e+01, 2.62e+00]    [4.89e+01, 4.90e+01, 1.73e+01, 1.74e+01]    
3000      [3.28e+01, 1.22e+01, 2.59e+00]    [0.00e+00, 3.85e+01, 2.59e+00]    [3.84e+01, 3.85e+01, 1.96e+01, 1.96e+01]    
4000      [3.17e+01, 4.33e+00, 2.57e+00]    [0.00e+00, 3.31e+01, 2.57e+00]    [3.30e+01, 3.31e+01, 2.01e+01, 2.01e+01]    
5000      [3.21e+01, 5.49e+00, 2.56e+00]    [0.00e+00, 3.22e+01, 2.56e+00]    [3.22e+01, 3.22e+01, 2.14e+01, 2.14e+01]    
6000      [3.36e+01, 1.09e+01, 2.55e+00]    [0.00e+00, 2.82e+01, 2.55e+00]    [2.83e+01, 2.82e+01, 2.00e+01, 2.01e+01]    
7000      [3.35e+01, 1.20e+01, 2.55e+00]    [0.00e+00, 2.61e+01, 2.55e+00]    [2.63e+01, 2.61e+01, 2.10e+01, 2.11e+01]    
8000      [3.58e+01, 1.86e+01, 2.54e+00]    [0.00e+00, 2.36e+01, 2.54e+00]    [2.37e+01, 2.36e+01, 2.20e+01, 2.20e+01]    
9000      [3.03e+01, 4.55e+00, 2.54e+00]    [0.00e+00, 2.94e+01, 2.54e+00]    [2.99e+01, 2.94e+01, 2.15e+01, 2.14e+01]    
10000     [2.98e+01, 4.56e+00, 2.54e+00]    [0.00e+00, 2.66e+01, 2.54e+00]    [2.72e+01, 2.66e+01, 2.05e+01, 2.05e+01]    
11000     [3.15e+01, 9.34e+00, 2.54e+00]    [0.00e+00, 2.53e+01, 2.54e+00]    [2.59e+01, 2.53e+01, 1.99e+01, 2.00e+01]    
12000     [2.95e+01, 2.75e+00, 2.54e+00]    [0.00e+00, 2.81e+01, 2.54e+00]    [2.88e+01, 2.81e+01, 1.91e+01, 1.91e+01]    
13000     [3.18e+01, 1.23e+01, 2.54e+00]    [0.00e+00, 2.44e+01, 2.54e+00]    [2.51e+01, 2.44e+01, 2.00e+01, 2.02e+01]    
14000     [2.84e+01, 5.80e-01, 2.53e+00]    [0.00e+00, 2.68e+01, 2.53e+00]    [2.76e+01, 2.68e+01, 2.06e+01, 2.04e+01]    
15000     [2.94e+01, 5.84e+00, 2.53e+00]    [0.00e+00, 2.50e+01, 2.53e+00]    [2.59e+01, 2.50e+01, 1.96e+01, 1.96e+01]    
16000     [3.18e+01, 1.21e+01, 2.53e+00]    [0.00e+00, 2.93e+01, 2.53e+00]    [3.03e+01, 2.93e+01, 2.31e+01, 2.27e+01]    
17000     [2.85e+01, 4.46e+00, 2.53e+00]    [0.00e+00, 2.51e+01, 2.53e+00]    [2.57e+01, 2.51e+01, 1.99e+01, 1.95e+01]    
18000     [2.81e+01, 2.83e+00, 2.53e+00]    [0.00e+00, 2.54e+01, 2.53e+00]    [2.65e+01, 2.54e+01, 1.93e+01, 1.94e+01]    
19000     [2.92e+01, 6.79e+00, 2.53e+00]    [0.00e+00, 2.71e+01, 2.53e+00]    [2.82e+01, 2.71e+01, 2.14e+01, 2.10e+01]    
20000     [3.08e+01, 1.13e+01, 2.53e+00]    [0.00e+00, 2.84e+01, 2.53e+00]    [2.91e+01, 2.84e+01, 2.29e+01, 2.20e+01]    
21000     [2.73e+01, 3.12e+00, 2.53e+00]    [0.00e+00, 2.50e+01, 2.53e+00]    [2.56e+01, 2.50e+01, 1.97e+01, 1.90e+01]    
22000     [2.94e+01, 9.01e+00, 2.53e+00]    [0.00e+00, 2.33e+01, 2.53e+00]    [2.40e+01, 2.33e+01, 1.88e+01, 1.84e+01]    
23000     [2.77e+01, 4.10e+00, 2.52e+00]    [0.00e+00, 2.62e+01, 2.52e+00]    [2.68e+01, 2.62e+01, 2.04e+01, 1.95e+01]    
24000     [2.71e+01, 2.57e+00, 2.52e+00]    [0.00e+00, 2.49e+01, 2.52e+00]    [2.63e+01, 2.49e+01, 1.85e+01, 1.86e+01]    
25000     [2.73e+01, 2.90e+00, 2.52e+00]    [0.00e+00, 2.57e+01, 2.52e+00]    [2.71e+01, 2.57e+01, 1.94e+01, 1.93e+01]    
26000     [2.72e+01, 4.10e+00, 2.52e+00]    [0.00e+00, 2.57e+01, 2.52e+00]    [2.65e+01, 2.57e+01, 1.99e+01, 1.91e+01]    
27000     [2.71e+01, 2.29e+00, 2.52e+00]    [0.00e+00, 2.52e+01, 2.52e+00]    [2.58e+01, 2.52e+01, 1.91e+01, 1.83e+01]    
28000     [2.81e+01, 6.81e+00, 2.52e+00]    [0.00e+00, 2.62e+01, 2.52e+00]    [2.72e+01, 2.62e+01, 2.03e+01, 1.95e+01]    
29000     [2.80e+01, 7.35e+00, 2.52e+00]    [0.00e+00, 2.30e+01, 2.52e+00]    [2.39e+01, 2.30e+01, 1.78e+01, 1.73e+01]    
30000     [2.82e+01, 7.31e+00, 2.52e+00]    [0.00e+00, 2.37e+01, 2.52e+00]    [2.56e+01, 2.37e+01, 1.60e+01, 1.65e+01]    

Best model at step 14000:
  train loss: 3.15e+01
  test loss: 2.93e+01
  test metric: [2.76e+01, 2.68e+01, 2.06e+01, 2.04e+01]

'train' took 51.619271 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.199452 s

'compile' took 0.827903 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.85e+02, 2.54e+02, 2.72e+00]    [0.00e+00, 4.28e+02, 2.72e+00]    [4.28e+02, 4.28e+02, 6.06e+02, 6.06e+02]    
1000      [3.30e+01, 1.84e+01, 2.70e+00]    [0.00e+00, 3.28e+01, 2.70e+00]    [3.14e+01, 3.28e+01, 1.42e+01, 1.21e+01]    
2000      [2.94e+01, 1.07e+01, 2.70e+00]    [0.00e+00, 2.64e+01, 2.70e+00]    [1.79e+01, 2.64e+01, 1.03e+01, 2.22e+01]    
3000      [2.88e+01, 8.97e+00, 2.70e+00]    [0.00e+00, 2.11e+01, 2.70e+00]    [1.43e+01, 2.11e+01, 8.81e+00, 2.14e+01]    
4000      [3.26e+01, 7.22e+00, 2.71e+00]    [0.00e+00, 3.20e+01, 2.71e+00]    [1.92e+01, 3.20e+01, 1.85e+01, 4.14e+01]    
5000      [2.86e+01, 3.83e+00, 2.71e+00]    [0.00e+00, 2.23e+01, 2.71e+00]    [1.69e+01, 2.23e+01, 1.23e+01, 2.50e+01]    
6000      [2.80e+01, 3.25e+00, 2.71e+00]    [0.00e+00, 2.21e+01, 2.71e+00]    [1.69e+01, 2.21e+01, 1.37e+01, 2.30e+01]    
7000      [2.87e+01, 3.44e+00, 2.71e+00]    [0.00e+00, 2.60e+01, 2.71e+00]    [1.99e+01, 2.60e+01, 1.24e+01, 3.50e+01]    
8000      [2.76e+01, 2.49e+00, 2.71e+00]    [0.00e+00, 2.37e+01, 2.71e+00]    [1.88e+01, 2.37e+01, 1.14e+01, 3.05e+01]    
9000      [2.77e+01, 2.42e+00, 2.71e+00]    [0.00e+00, 2.50e+01, 2.71e+00]    [1.97e+01, 2.50e+01, 1.26e+01, 3.33e+01]    
10000     [2.84e+01, 3.58e+00, 2.72e+00]    [0.00e+00, 2.07e+01, 2.72e+00]    [2.01e+01, 2.07e+01, 1.55e+01, 1.73e+01]    
11000     [2.79e+01, 2.41e+00, 2.72e+00]    [0.00e+00, 1.97e+01, 2.72e+00]    [2.10e+01, 1.97e+01, 1.53e+01, 1.62e+01]    
12000     [2.66e+01, 2.15e+00, 2.72e+00]    [0.00e+00, 2.10e+01, 2.72e+00]    [1.79e+01, 2.10e+01, 1.53e+01, 2.20e+01]    
13000     [2.61e+01, 1.28e+00, 2.73e+00]    [0.00e+00, 2.22e+01, 2.73e+00]    [1.89e+01, 2.22e+01, 1.28e+01, 2.73e+01]    
14000     [2.66e+01, 1.45e+00, 2.73e+00]    [0.00e+00, 1.87e+01, 2.73e+00]    [2.07e+01, 1.87e+01, 1.52e+01, 1.56e+01]    
15000     [2.86e+01, 2.55e+00, 2.73e+00]    [0.00e+00, 1.65e+01, 2.73e+00]    [2.61e+01, 1.65e+01, 1.97e+01, 1.20e+01]    
16000     [2.48e+01, 1.00e+00, 2.73e+00]    [0.00e+00, 2.00e+01, 2.73e+00]    [1.76e+01, 2.00e+01, 1.60e+01, 1.98e+01]    
17000     [2.96e+01, 4.48e+00, 2.73e+00]    [0.00e+00, 3.41e+01, 2.73e+00]    [2.31e+01, 3.41e+01, 2.43e+01, 4.23e+01]    
18000     [2.45e+01, 9.22e-01, 2.73e+00]    [0.00e+00, 2.02e+01, 2.73e+00]    [1.85e+01, 2.02e+01, 1.65e+01, 1.85e+01]    
19000     [2.57e+01, 2.43e+00, 2.73e+00]    [0.00e+00, 2.57e+01, 2.73e+00]    [1.92e+01, 2.57e+01, 1.53e+01, 2.56e+01]    
20000     [2.42e+01, 1.61e+00, 2.73e+00]    [0.00e+00, 2.05e+01, 2.73e+00]    [1.85e+01, 2.05e+01, 1.72e+01, 1.88e+01]    
21000     [2.37e+01, 7.89e-01, 2.73e+00]    [0.00e+00, 2.16e+01, 2.73e+00]    [1.85e+01, 2.16e+01, 1.71e+01, 1.98e+01]    
22000     [2.74e+01, 3.77e+00, 2.73e+00]    [0.00e+00, 3.27e+01, 2.73e+00]    [2.23e+01, 3.27e+01, 2.16e+01, 3.62e+01]    
23000     [2.36e+01, 1.08e+00, 2.72e+00]    [0.00e+00, 2.35e+01, 2.72e+00]    [1.94e+01, 2.35e+01, 1.64e+01, 2.15e+01]    
24000     [2.57e+01, 2.40e+00, 2.72e+00]    [0.00e+00, 1.86e+01, 2.72e+00]    [2.27e+01, 1.86e+01, 1.75e+01, 1.26e+01]    
25000     [2.68e+01, 3.70e+00, 2.72e+00]    [0.00e+00, 3.20e+01, 2.72e+00]    [2.19e+01, 3.20e+01, 2.08e+01, 3.32e+01]    
26000     [2.37e+01, 1.36e+00, 2.72e+00]    [0.00e+00, 2.37e+01, 2.72e+00]    [1.88e+01, 2.37e+01, 1.68e+01, 2.02e+01]    
27000     [2.87e+01, 5.82e+00, 2.71e+00]    [0.00e+00, 1.99e+01, 2.71e+00]    [2.92e+01, 1.99e+01, 2.27e+01, 1.20e+01]    
28000     [2.44e+01, 1.64e+00, 2.71e+00]    [0.00e+00, 2.78e+01, 2.71e+00]    [2.05e+01, 2.78e+01, 1.69e+01, 2.59e+01]    
29000     [2.57e+01, 3.67e+00, 2.71e+00]    [0.00e+00, 1.84e+01, 2.71e+00]    [2.32e+01, 1.84e+01, 1.71e+01, 1.28e+01]    
30000     [2.38e+01, 6.48e-01, 2.71e+00]    [0.00e+00, 1.85e+01, 2.71e+00]    [2.12e+01, 1.85e+01, 1.65e+01, 1.47e+01]    

Best model at step 30000:
  train loss: 2.72e+01
  test loss: 2.12e+01
  test metric: [2.12e+01, 1.85e+01, 1.65e+01, 1.47e+01]

'train' took 47.156098 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.188184 s

'compile' took 0.824644 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.50e+02, 1.06e+02, 2.76e+00]    [0.00e+00, 2.46e+02, 2.76e+00]    [2.46e+02, 2.46e+02, 2.39e+02, 2.39e+02]    
1000      [3.09e+01, 1.15e+01, 2.78e+00]    [0.00e+00, 2.40e+01, 2.78e+00]    [1.57e+01, 2.40e+01, 1.05e+01, 1.87e+01]    
2000      [3.07e+01, 3.15e+00, 2.79e+00]    [0.00e+00, 1.90e+01, 2.79e+00]    [1.27e+01, 1.90e+01, 1.26e+01, 1.00e+01]    
3000      [3.03e+01, 8.72e-01, 2.81e+00]    [0.00e+00, 2.58e+01, 2.81e+00]    [1.29e+01, 2.58e+01, 1.50e+01, 1.70e+01]    
4000      [3.05e+01, 5.07e-01, 2.82e+00]    [0.00e+00, 2.99e+01, 2.82e+00]    [1.40e+01, 2.99e+01, 1.85e+01, 2.30e+01]    
5000      [2.70e+01, 2.47e-01, 2.82e+00]    [0.00e+00, 5.04e+01, 2.82e+00]    [1.32e+01, 5.04e+01, 9.29e+00, 2.93e+01]    
6000      [2.60e+01, 1.23e-01, 2.82e+00]    [0.00e+00, 4.35e+01, 2.82e+00]    [1.32e+01, 4.35e+01, 9.57e+00, 2.61e+01]    
7000      [2.60e+01, 1.42e-01, 2.82e+00]    [0.00e+00, 4.86e+01, 2.82e+00]    [1.22e+01, 4.86e+01, 9.10e+00, 2.81e+01]    
8000      [2.53e+01, 1.58e-01, 2.81e+00]    [0.00e+00, 5.66e+01, 2.81e+00]    [1.36e+01, 5.66e+01, 8.16e+00, 3.40e+01]    
9000      [2.55e+01, 1.33e-01, 2.81e+00]    [0.00e+00, 6.04e+01, 2.81e+00]    [1.30e+01, 6.04e+01, 9.80e+00, 3.80e+01]    
10000     [2.54e+01, 2.05e-01, 2.81e+00]    [0.00e+00, 7.48e+01, 2.81e+00]    [1.41e+01, 7.48e+01, 8.69e+00, 5.48e+01]    
11000     [2.43e+01, 1.18e-01, 2.81e+00]    [0.00e+00, 7.05e+01, 2.81e+00]    [1.26e+01, 7.05e+01, 7.22e+00, 5.03e+01]    
12000     [2.69e+01, 3.59e-01, 2.80e+00]    [0.00e+00, 8.39e+01, 2.80e+00]    [1.77e+01, 8.39e+01, 1.14e+01, 7.09e+01]    
13000     [2.55e+01, 2.97e-01, 2.80e+00]    [0.00e+00, 7.03e+01, 2.80e+00]    [1.26e+01, 7.03e+01, 1.06e+01, 5.10e+01]    
14000     [2.36e+01, 7.51e-02, 2.80e+00]    [0.00e+00, 7.84e+01, 2.80e+00]    [1.14e+01, 7.84e+01, 7.80e+00, 6.30e+01]    
15000     [2.61e+01, 1.85e-01, 2.80e+00]    [0.00e+00, 6.96e+01, 2.80e+00]    [1.35e+01, 6.96e+01, 1.50e+01, 5.11e+01]    
16000     [2.45e+01, 1.70e-01, 2.79e+00]    [0.00e+00, 8.48e+01, 2.79e+00]    [1.30e+01, 8.48e+01, 9.04e+00, 7.22e+01]    
17000     [2.31e+01, 1.09e-01, 2.79e+00]    [0.00e+00, 8.30e+01, 2.79e+00]    [1.18e+01, 8.30e+01, 7.29e+00, 7.11e+01]    
18000     [2.38e+01, 1.22e-01, 2.79e+00]    [0.00e+00, 8.08e+01, 2.79e+00]    [1.23e+01, 8.08e+01, 8.00e+00, 6.80e+01]    
19000     [2.45e+01, 1.54e-01, 2.79e+00]    [0.00e+00, 8.01e+01, 2.79e+00]    [1.20e+01, 8.01e+01, 9.69e+00, 6.88e+01]    
20000     [2.47e+01, 1.98e-01, 2.78e+00]    [0.00e+00, 9.04e+01, 2.78e+00]    [1.43e+01, 9.04e+01, 8.85e+00, 8.60e+01]    
21000     [2.37e+01, 1.84e-01, 2.78e+00]    [0.00e+00, 8.85e+01, 2.78e+00]    [1.22e+01, 8.85e+01, 8.10e+00, 8.55e+01]    
22000     [2.62e+01, 2.62e-01, 2.78e+00]    [0.00e+00, 7.38e+01, 2.78e+00]    [1.48e+01, 7.38e+01, 1.70e+01, 6.32e+01]    
23000     [2.32e+01, 2.46e-01, 2.77e+00]    [0.00e+00, 8.06e+01, 2.77e+00]    [1.08e+01, 8.06e+01, 7.92e+00, 7.67e+01]    
24000     [2.30e+01, 1.82e-01, 2.77e+00]    [0.00e+00, 7.87e+01, 2.77e+00]    [1.08e+01, 7.87e+01, 7.77e+00, 7.59e+01]    
25000     [2.29e+01, 1.54e-01, 2.77e+00]    [0.00e+00, 7.66e+01, 2.77e+00]    [1.11e+01, 7.66e+01, 7.86e+00, 7.45e+01]    
26000     [2.39e+01, 2.04e-01, 2.76e+00]    [0.00e+00, 8.14e+01, 2.76e+00]    [1.36e+01, 8.14e+01, 8.40e+00, 8.34e+01]    
27000     [2.31e+01, 1.18e-01, 2.76e+00]    [0.00e+00, 7.83e+01, 2.76e+00]    [1.19e+01, 7.83e+01, 8.83e+00, 8.00e+01]    
28000     [2.38e+01, 2.34e-01, 2.76e+00]    [0.00e+00, 6.82e+01, 2.76e+00]    [1.16e+01, 6.82e+01, 1.02e+01, 6.75e+01]    
29000     [2.47e+01, 2.69e-01, 2.75e+00]    [0.00e+00, 8.06e+01, 2.75e+00]    [1.54e+01, 8.06e+01, 9.65e+00, 8.48e+01]    
30000     [2.43e+01, 2.70e-01, 2.75e+00]    [0.00e+00, 6.54e+01, 2.75e+00]    [1.10e+01, 6.54e+01, 1.02e+01, 6.86e+01]    

Best model at step 25000:
  train loss: 2.58e+01
  test loss: 7.94e+01
  test metric: [1.11e+01, 7.66e+01, 7.86e+00, 7.45e+01]

'train' took 49.078203 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.169710 s

'compile' took 0.855407 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.18e+02, 8.43e+02, 2.72e+00]    [0.00e+00, 9.08e+01, 2.72e+00]    [9.08e+01, 9.08e+01, 5.08e+01, 5.08e+01]    
1000      [6.35e+01, 3.47e+01, 2.71e+00]    [0.00e+00, 6.86e+01, 2.71e+00]    [6.85e+01, 6.86e+01, 2.49e+01, 2.46e+01]    
2000      [4.37e+01, 1.25e+01, 2.70e+00]    [0.00e+00, 4.86e+01, 2.70e+00]    [4.87e+01, 4.86e+01, 1.93e+01, 1.97e+01]    
3000      [3.18e+01, 1.34e+01, 2.70e+00]    [0.00e+00, 3.28e+01, 2.70e+00]    [3.28e+01, 3.28e+01, 1.78e+01, 1.79e+01]    
4000      [2.90e+01, 1.30e+01, 2.70e+00]    [0.00e+00, 2.58e+01, 2.70e+00]    [2.56e+01, 2.58e+01, 1.59e+01, 1.56e+01]    
5000      [2.82e+01, 8.23e+00, 2.69e+00]    [0.00e+00, 2.34e+01, 2.69e+00]    [2.32e+01, 2.34e+01, 1.34e+01, 1.32e+01]    
6000      [3.00e+01, 1.78e+01, 2.69e+00]    [0.00e+00, 2.23e+01, 2.69e+00]    [2.22e+01, 2.23e+01, 1.28e+01, 1.27e+01]    
7000      [2.82e+01, 1.10e+01, 2.69e+00]    [0.00e+00, 2.24e+01, 2.69e+00]    [2.22e+01, 2.24e+01, 1.27e+01, 1.25e+01]    
8000      [3.30e+01, 2.80e+01, 2.68e+00]    [0.00e+00, 2.20e+01, 2.68e+00]    [2.19e+01, 2.20e+01, 1.31e+01, 1.30e+01]    
9000      [2.65e+01, 4.21e+00, 2.68e+00]    [0.00e+00, 2.27e+01, 2.68e+00]    [2.25e+01, 2.27e+01, 1.33e+01, 1.32e+01]    
10000     [2.70e+01, 8.56e+00, 2.67e+00]    [0.00e+00, 2.27e+01, 2.67e+00]    [2.25e+01, 2.27e+01, 1.26e+01, 1.25e+01]    
11000     [2.53e+01, 4.47e+00, 2.67e+00]    [0.00e+00, 2.33e+01, 2.67e+00]    [2.31e+01, 2.33e+01, 1.23e+01, 1.22e+01]    
12000     [2.50e+01, 2.76e+00, 2.67e+00]    [0.00e+00, 2.29e+01, 2.67e+00]    [2.27e+01, 2.29e+01, 1.26e+01, 1.25e+01]    
13000     [2.75e+01, 1.10e+01, 2.66e+00]    [0.00e+00, 2.20e+01, 2.66e+00]    [2.18e+01, 2.20e+01, 1.33e+01, 1.32e+01]    
14000     [2.53e+01, 4.31e+00, 2.66e+00]    [0.00e+00, 2.25e+01, 2.66e+00]    [2.23e+01, 2.25e+01, 1.29e+01, 1.27e+01]    
15000     [2.72e+01, 1.42e+01, 2.66e+00]    [0.00e+00, 2.26e+01, 2.66e+00]    [2.24e+01, 2.26e+01, 1.29e+01, 1.28e+01]    
16000     [2.51e+01, 6.44e+00, 2.65e+00]    [0.00e+00, 2.24e+01, 2.65e+00]    [2.22e+01, 2.24e+01, 1.30e+01, 1.29e+01]    
17000     [2.70e+01, 1.00e+01, 2.65e+00]    [0.00e+00, 2.15e+01, 2.65e+00]    [2.13e+01, 2.15e+01, 1.36e+01, 1.35e+01]    
18000     [2.55e+01, 5.98e+00, 2.65e+00]    [0.00e+00, 2.15e+01, 2.65e+00]    [2.13e+01, 2.15e+01, 1.35e+01, 1.34e+01]    
19000     [2.52e+01, 7.82e+00, 2.64e+00]    [0.00e+00, 2.20e+01, 2.64e+00]    [2.18e+01, 2.20e+01, 1.33e+01, 1.32e+01]    
20000     [2.45e+01, 2.57e+00, 2.64e+00]    [0.00e+00, 2.13e+01, 2.64e+00]    [2.11e+01, 2.13e+01, 1.39e+01, 1.38e+01]    
21000     [2.47e+01, 2.47e+00, 2.64e+00]    [0.00e+00, 2.13e+01, 2.64e+00]    [2.11e+01, 2.13e+01, 1.39e+01, 1.38e+01]    
22000     [2.57e+01, 7.66e+00, 2.63e+00]    [0.00e+00, 2.08e+01, 2.63e+00]    [2.07e+01, 2.08e+01, 1.42e+01, 1.41e+01]    
23000     [2.99e+01, 2.44e+01, 2.63e+00]    [0.00e+00, 2.21e+01, 2.63e+00]    [2.20e+01, 2.21e+01, 1.34e+01, 1.33e+01]    
24000     [2.42e+01, 1.73e+00, 2.63e+00]    [0.00e+00, 2.09e+01, 2.63e+00]    [2.07e+01, 2.09e+01, 1.42e+01, 1.41e+01]    
25000     [2.49e+01, 7.60e+00, 2.63e+00]    [0.00e+00, 2.14e+01, 2.63e+00]    [2.13e+01, 2.14e+01, 1.37e+01, 1.37e+01]    
26000     [2.48e+01, 7.77e+00, 2.62e+00]    [0.00e+00, 2.12e+01, 2.62e+00]    [2.11e+01, 2.12e+01, 1.39e+01, 1.38e+01]    
27000     [2.53e+01, 7.66e+00, 2.62e+00]    [0.00e+00, 2.11e+01, 2.62e+00]    [2.10e+01, 2.11e+01, 1.41e+01, 1.40e+01]    
28000     [2.54e+01, 8.00e+00, 2.62e+00]    [0.00e+00, 2.07e+01, 2.62e+00]    [2.05e+01, 2.07e+01, 1.41e+01, 1.41e+01]    
29000     [2.58e+01, 8.59e+00, 2.61e+00]    [0.00e+00, 2.04e+01, 2.61e+00]    [2.03e+01, 2.04e+01, 1.44e+01, 1.43e+01]    
30000     [2.47e+01, 5.71e+00, 2.61e+00]    [0.00e+00, 2.04e+01, 2.61e+00]    [2.04e+01, 2.04e+01, 1.44e+01, 1.45e+01]    

Best model at step 24000:
  train loss: 2.86e+01
  test loss: 2.35e+01
  test metric: [2.07e+01, 2.09e+01, 1.42e+01, 1.41e+01]

'train' took 47.251758 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.171424 s

'compile' took 0.823519 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.21e+02, 5.25e+02, 2.56e+00]    [0.00e+00, 2.85e+02, 2.56e+00]    [2.85e+02, 2.85e+02, 4.67e+02, 4.67e+02]    
1000      [4.59e+01, 2.38e+01, 2.53e+00]    [0.00e+00, 6.07e+01, 2.53e+00]    [6.07e+01, 6.07e+01, 1.81e+01, 1.81e+01]    
2000      [3.54e+01, 1.46e+01, 2.52e+00]    [0.00e+00, 3.93e+01, 2.52e+00]    [3.94e+01, 3.93e+01, 2.00e+01, 2.00e+01]    
3000      [3.35e+01, 1.28e+01, 2.52e+00]    [0.00e+00, 3.05e+01, 2.52e+00]    [3.07e+01, 3.05e+01, 1.68e+01, 1.68e+01]    
4000      [3.37e+01, 1.63e+01, 2.51e+00]    [0.00e+00, 3.11e+01, 2.51e+00]    [3.13e+01, 3.11e+01, 1.75e+01, 1.70e+01]    
5000      [3.06e+01, 2.24e+00, 2.51e+00]    [0.00e+00, 2.61e+01, 2.51e+00]    [2.64e+01, 2.61e+01, 1.54e+01, 1.53e+01]    
6000      [3.00e+01, 3.56e+00, 2.51e+00]    [0.00e+00, 2.46e+01, 2.51e+00]    [2.50e+01, 2.46e+01, 1.61e+01, 1.60e+01]    
7000      [2.92e+01, 2.03e+00, 2.51e+00]    [0.00e+00, 2.28e+01, 2.51e+00]    [2.33e+01, 2.28e+01, 1.67e+01, 1.67e+01]    
8000      [2.97e+01, 6.21e+00, 2.51e+00]    [0.00e+00, 2.09e+01, 2.51e+00]    [2.14e+01, 2.09e+01, 1.73e+01, 1.77e+01]    
9000      [2.83e+01, 4.44e+00, 2.51e+00]    [0.00e+00, 2.07e+01, 2.51e+00]    [2.13e+01, 2.07e+01, 1.75e+01, 1.78e+01]    
10000     [2.74e+01, 3.78e+00, 2.51e+00]    [0.00e+00, 2.13e+01, 2.51e+00]    [2.19e+01, 2.13e+01, 1.69e+01, 1.72e+01]    
11000     [2.90e+01, 7.95e+00, 2.51e+00]    [0.00e+00, 2.27e+01, 2.51e+00]    [2.34e+01, 2.27e+01, 1.68e+01, 1.65e+01]    
12000     [2.97e+01, 9.91e+00, 2.51e+00]    [0.00e+00, 2.31e+01, 2.51e+00]    [2.39e+01, 2.31e+01, 1.69e+01, 1.64e+01]    
13000     [2.98e+01, 1.17e+01, 2.51e+00]    [0.00e+00, 2.27e+01, 2.51e+00]    [2.35e+01, 2.27e+01, 1.70e+01, 1.66e+01]    
14000     [2.68e+01, 4.51e+00, 2.51e+00]    [0.00e+00, 2.17e+01, 2.51e+00]    [2.25e+01, 2.17e+01, 1.64e+01, 1.68e+01]    
15000     [2.71e+01, 3.01e+00, 2.50e+00]    [0.00e+00, 2.16e+01, 2.50e+00]    [2.25e+01, 2.16e+01, 1.66e+01, 1.68e+01]    
16000     [2.94e+01, 1.04e+01, 2.50e+00]    [0.00e+00, 2.27e+01, 2.50e+00]    [2.38e+01, 2.27e+01, 1.65e+01, 1.64e+01]    
17000     [2.85e+01, 7.58e+00, 2.50e+00]    [0.00e+00, 2.28e+01, 2.50e+00]    [2.39e+01, 2.28e+01, 1.60e+01, 1.61e+01]    
18000     [2.83e+01, 1.17e+01, 2.50e+00]    [0.00e+00, 2.13e+01, 2.50e+00]    [2.12e+01, 2.13e+01, 1.73e+01, 1.72e+01]    
19000     [2.67e+01, 7.14e+00, 2.49e+00]    [0.00e+00, 2.05e+01, 2.49e+00]    [2.15e+01, 2.05e+01, 1.67e+01, 1.78e+01]    
20000     [2.74e+01, 8.55e+00, 2.49e+00]    [0.00e+00, 2.05e+01, 2.49e+00]    [2.09e+01, 2.05e+01, 1.72e+01, 1.76e+01]    
21000     [2.66e+01, 3.03e+00, 2.49e+00]    [0.00e+00, 2.15e+01, 2.49e+00]    [2.28e+01, 2.15e+01, 1.56e+01, 1.62e+01]    
22000     [2.59e+01, 9.51e-01, 2.48e+00]    [0.00e+00, 2.10e+01, 2.48e+00]    [2.22e+01, 2.10e+01, 1.59e+01, 1.66e+01]    
23000     [2.84e+01, 8.42e+00, 2.48e+00]    [0.00e+00, 2.24e+01, 2.48e+00]    [2.36e+01, 2.24e+01, 1.55e+01, 1.56e+01]    
24000     [2.77e+01, 1.09e+01, 2.48e+00]    [0.00e+00, 2.03e+01, 2.48e+00]    [2.09e+01, 2.03e+01, 1.67e+01, 1.73e+01]    
25000     [2.75e+01, 1.09e+01, 2.47e+00]    [0.00e+00, 2.07e+01, 2.47e+00]    [2.12e+01, 2.07e+01, 1.63e+01, 1.70e+01]    
26000     [2.79e+01, 1.11e+01, 2.47e+00]    [0.00e+00, 2.04e+01, 2.47e+00]    [2.02e+01, 2.04e+01, 1.71e+01, 1.71e+01]    
27000     [2.69e+01, 5.32e+00, 2.47e+00]    [0.00e+00, 2.18e+01, 2.47e+00]    [2.30e+01, 2.18e+01, 1.51e+01, 1.54e+01]    
28000     [2.58e+01, 4.62e+00, 2.46e+00]    [0.00e+00, 1.97e+01, 2.46e+00]    [2.10e+01, 1.97e+01, 1.60e+01, 1.73e+01]    
29000     [2.81e+01, 8.41e+00, 2.46e+00]    [0.00e+00, 2.23e+01, 2.46e+00]    [2.36e+01, 2.23e+01, 1.48e+01, 1.49e+01]    
30000     [2.79e+01, 8.47e+00, 2.46e+00]    [0.00e+00, 2.24e+01, 2.46e+00]    [2.37e+01, 2.24e+01, 1.46e+01, 1.47e+01]    

Best model at step 22000:
  train loss: 2.93e+01
  test loss: 2.35e+01
  test metric: [2.22e+01, 2.10e+01, 1.59e+01, 1.66e+01]

'train' took 48.056650 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.172442 s

'compile' took 0.808860 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.02e+02, 5.98e+02, 2.69e+00]    [0.00e+00, 6.21e+02, 2.69e+00]    [6.21e+02, 6.21e+02, 1.39e+03, 1.39e+03]    
1000      [4.45e+01, 1.98e+01, 2.67e+00]    [0.00e+00, 5.53e+01, 2.67e+00]    [5.74e+01, 5.53e+01, 1.75e+01, 2.20e+01]    
2000      [3.14e+01, 8.67e+00, 2.68e+00]    [0.00e+00, 3.75e+01, 2.68e+00]    [3.50e+01, 3.75e+01, 1.68e+01, 1.21e+01]    
3000      [3.08e+01, 1.12e+01, 2.68e+00]    [0.00e+00, 2.79e+01, 2.68e+00]    [3.26e+01, 2.79e+01, 1.29e+01, 1.33e+01]    
4000      [2.81e+01, 4.55e+00, 2.68e+00]    [0.00e+00, 2.92e+01, 2.68e+00]    [2.41e+01, 2.92e+01, 1.37e+01, 1.60e+01]    
5000      [3.01e+01, 7.93e+00, 2.68e+00]    [0.00e+00, 2.15e+01, 2.68e+00]    [2.81e+01, 2.15e+01, 1.61e+01, 1.38e+01]    
6000      [2.75e+01, 2.24e+00, 2.68e+00]    [0.00e+00, 2.97e+01, 2.68e+00]    [2.29e+01, 2.97e+01, 1.33e+01, 2.21e+01]    
7000      [2.80e+01, 4.96e+00, 2.67e+00]    [0.00e+00, 2.54e+01, 2.67e+00]    [2.36e+01, 2.54e+01, 1.26e+01, 1.46e+01]    
8000      [2.63e+01, 2.30e+00, 2.67e+00]    [0.00e+00, 3.32e+01, 2.67e+00]    [2.51e+01, 3.32e+01, 1.33e+01, 2.96e+01]    
9000      [2.68e+01, 4.17e+00, 2.67e+00]    [0.00e+00, 3.42e+01, 2.67e+00]    [2.59e+01, 3.42e+01, 1.43e+01, 3.25e+01]    
10000     [2.56e+01, 1.64e+00, 2.67e+00]    [0.00e+00, 2.87e+01, 2.67e+00]    [2.10e+01, 2.87e+01, 1.47e+01, 2.05e+01]    
11000     [2.84e+01, 7.93e+00, 2.67e+00]    [0.00e+00, 3.79e+01, 2.67e+00]    [2.92e+01, 3.79e+01, 1.99e+01, 4.17e+01]    
12000     [2.52e+01, 3.10e+00, 2.67e+00]    [0.00e+00, 3.22e+01, 2.67e+00]    [2.35e+01, 3.22e+01, 1.33e+01, 2.82e+01]    
13000     [2.49e+01, 2.17e+00, 2.67e+00]    [0.00e+00, 3.20e+01, 2.67e+00]    [2.32e+01, 3.20e+01, 1.31e+01, 2.70e+01]    
14000     [2.49e+01, 2.55e+00, 2.67e+00]    [0.00e+00, 3.39e+01, 2.67e+00]    [2.50e+01, 3.39e+01, 1.37e+01, 3.18e+01]    
15000     [2.42e+01, 1.31e+00, 2.67e+00]    [0.00e+00, 2.96e+01, 2.67e+00]    [2.07e+01, 2.96e+01, 1.54e+01, 2.24e+01]    
16000     [2.54e+01, 3.76e+00, 2.67e+00]    [0.00e+00, 2.70e+01, 2.67e+00]    [2.29e+01, 2.70e+01, 1.35e+01, 1.76e+01]    
17000     [2.57e+01, 4.19e+00, 2.66e+00]    [0.00e+00, 2.69e+01, 2.66e+00]    [2.30e+01, 2.69e+01, 1.35e+01, 1.73e+01]    
18000     [2.46e+01, 3.11e+00, 2.66e+00]    [0.00e+00, 3.32e+01, 2.66e+00]    [2.46e+01, 3.32e+01, 1.39e+01, 3.05e+01]    
19000     [2.40e+01, 2.04e+00, 2.66e+00]    [0.00e+00, 2.81e+01, 2.66e+00]    [2.15e+01, 2.81e+01, 1.48e+01, 1.98e+01]    
20000     [2.41e+01, 2.36e+00, 2.66e+00]    [0.00e+00, 2.79e+01, 2.66e+00]    [2.17e+01, 2.79e+01, 1.47e+01, 1.94e+01]    
21000     [2.32e+01, 6.09e-01, 2.66e+00]    [0.00e+00, 2.93e+01, 2.66e+00]    [2.12e+01, 2.93e+01, 1.53e+01, 2.21e+01]    
22000     [2.37e+01, 2.41e+00, 2.66e+00]    [0.00e+00, 2.88e+01, 2.66e+00]    [2.09e+01, 2.88e+01, 1.57e+01, 2.17e+01]    
23000     [2.51e+01, 4.83e+00, 2.65e+00]    [0.00e+00, 2.51e+01, 2.65e+00]    [2.35e+01, 2.51e+01, 1.42e+01, 1.57e+01]    
24000     [2.41e+01, 2.89e+00, 2.65e+00]    [0.00e+00, 3.18e+01, 2.65e+00]    [2.42e+01, 3.18e+01, 1.43e+01, 2.76e+01]    
25000     [2.52e+01, 4.69e+00, 2.65e+00]    [0.00e+00, 2.57e+01, 2.65e+00]    [2.36e+01, 2.57e+01, 1.37e+01, 1.53e+01]    
26000     [2.49e+01, 4.54e+00, 2.65e+00]    [0.00e+00, 2.51e+01, 2.65e+00]    [2.41e+01, 2.51e+01, 1.35e+01, 1.46e+01]    
27000     [2.48e+01, 4.10e+00, 2.65e+00]    [0.00e+00, 2.59e+01, 2.65e+00]    [2.38e+01, 2.59e+01, 1.33e+01, 1.48e+01]    
28000     [2.41e+01, 3.27e+00, 2.65e+00]    [0.00e+00, 2.63e+01, 2.65e+00]    [2.35e+01, 2.63e+01, 1.32e+01, 1.50e+01]    
29000     [2.50e+01, 5.02e+00, 2.65e+00]    [0.00e+00, 3.32e+01, 2.65e+00]    [2.61e+01, 3.32e+01, 1.44e+01, 2.91e+01]    
30000     [2.38e+01, 3.18e+00, 2.64e+00]    [0.00e+00, 3.20e+01, 2.64e+00]    [2.49e+01, 3.20e+01, 1.30e+01, 2.52e+01]    

Best model at step 21000:
  train loss: 2.64e+01
  test loss: 3.19e+01
  test metric: [2.12e+01, 2.93e+01, 1.53e+01, 2.21e+01]

'train' took 46.480973 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.187960 s

'compile' took 0.818464 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.52e+02, 2.57e+02, 2.81e+00]    [0.00e+00, 4.18e+02, 2.81e+00]    [4.18e+02, 4.18e+02, 8.16e+02, 8.16e+02]    
1000      [3.69e+01, 1.28e+01, 2.80e+00]    [0.00e+00, 3.91e+01, 2.80e+00]    [3.77e+01, 3.91e+01, 2.19e+01, 2.02e+01]    
2000      [3.07e+01, 5.31e+00, 2.81e+00]    [0.00e+00, 2.84e+01, 2.81e+00]    [2.66e+01, 2.84e+01, 1.49e+01, 1.58e+01]    
3000      [3.16e+01, 1.42e+01, 2.81e+00]    [0.00e+00, 2.27e+01, 2.81e+00]    [2.54e+01, 2.27e+01, 1.41e+01, 1.38e+01]    
4000      [2.83e+01, 5.53e+00, 2.81e+00]    [0.00e+00, 2.36e+01, 2.81e+00]    [2.19e+01, 2.36e+01, 1.44e+01, 1.70e+01]    
5000      [2.80e+01, 5.21e+00, 2.81e+00]    [0.00e+00, 1.68e+01, 2.81e+00]    [2.00e+01, 1.68e+01, 1.55e+01, 1.67e+01]    
6000      [2.78e+01, 5.29e+00, 2.81e+00]    [0.00e+00, 2.29e+01, 2.81e+00]    [2.08e+01, 2.29e+01, 1.62e+01, 1.91e+01]    
7000      [2.94e+01, 8.25e+00, 2.81e+00]    [0.00e+00, 2.56e+01, 2.81e+00]    [2.34e+01, 2.56e+01, 1.87e+01, 2.36e+01]    
8000      [2.88e+01, 8.23e+00, 2.82e+00]    [0.00e+00, 1.75e+01, 2.82e+00]    [2.10e+01, 1.75e+01, 1.59e+01, 1.68e+01]    
9000      [2.76e+01, 6.03e+00, 2.82e+00]    [0.00e+00, 2.20e+01, 2.82e+00]    [2.01e+01, 2.20e+01, 1.67e+01, 1.86e+01]    
10000     [2.66e+01, 3.02e+00, 2.82e+00]    [0.00e+00, 2.12e+01, 2.82e+00]    [1.95e+01, 2.12e+01, 1.66e+01, 1.73e+01]    
11000     [2.62e+01, 1.60e+00, 2.83e+00]    [0.00e+00, 1.97e+01, 2.83e+00]    [1.84e+01, 1.97e+01, 1.75e+01, 1.69e+01]    
12000     [2.59e+01, 1.60e+00, 2.83e+00]    [0.00e+00, 1.87e+01, 2.83e+00]    [1.79e+01, 1.87e+01, 1.82e+01, 1.68e+01]    
13000     [3.13e+01, 1.18e+01, 2.83e+00]    [0.00e+00, 2.15e+01, 2.83e+00]    [2.43e+01, 2.15e+01, 1.90e+01, 1.73e+01]    
14000     [2.70e+01, 4.16e+00, 2.84e+00]    [0.00e+00, 1.78e+01, 2.84e+00]    [2.06e+01, 1.78e+01, 1.74e+01, 1.76e+01]    
15000     [2.80e+01, 6.22e+00, 2.84e+00]    [0.00e+00, 2.53e+01, 2.84e+00]    [2.49e+01, 2.53e+01, 2.00e+01, 2.17e+01]    
16000     [2.67e+01, 4.18e+00, 2.84e+00]    [0.00e+00, 2.33e+01, 2.84e+00]    [2.31e+01, 2.33e+01, 1.87e+01, 1.93e+01]    
17000     [2.79e+01, 7.88e+00, 2.84e+00]    [0.00e+00, 2.31e+01, 2.84e+00]    [2.31e+01, 2.31e+01, 1.95e+01, 1.96e+01]    
18000     [2.67e+01, 5.51e+00, 2.84e+00]    [0.00e+00, 2.22e+01, 2.84e+00]    [2.25e+01, 2.22e+01, 1.87e+01, 1.84e+01]    
19000     [2.59e+01, 4.36e+00, 2.84e+00]    [0.00e+00, 2.14e+01, 2.84e+00]    [2.17e+01, 2.14e+01, 1.82e+01, 1.73e+01]    
20000     [2.44e+01, 1.18e+00, 2.84e+00]    [0.00e+00, 1.89e+01, 2.84e+00]    [1.97e+01, 1.89e+01, 1.82e+01, 1.68e+01]    
21000     [2.52e+01, 3.42e+00, 2.84e+00]    [0.00e+00, 1.97e+01, 2.84e+00]    [2.07e+01, 1.97e+01, 1.80e+01, 1.67e+01]    
22000     [2.67e+01, 5.46e+00, 2.84e+00]    [0.00e+00, 2.34e+01, 2.84e+00]    [2.42e+01, 2.34e+01, 1.98e+01, 1.81e+01]    
23000     [2.63e+01, 5.45e+00, 2.84e+00]    [0.00e+00, 2.18e+01, 2.84e+00]    [2.20e+01, 2.18e+01, 1.81e+01, 1.74e+01]    
24000     [2.55e+01, 4.26e+00, 2.84e+00]    [0.00e+00, 2.13e+01, 2.84e+00]    [2.14e+01, 2.13e+01, 1.78e+01, 1.66e+01]    
25000     [2.51e+01, 4.22e+00, 2.84e+00]    [0.00e+00, 1.95e+01, 2.84e+00]    [2.09e+01, 1.95e+01, 1.81e+01, 1.63e+01]    
26000     [2.77e+01, 8.77e+00, 2.84e+00]    [0.00e+00, 2.29e+01, 2.84e+00]    [2.40e+01, 2.29e+01, 2.05e+01, 1.80e+01]    
27000     [2.48e+01, 3.87e+00, 2.84e+00]    [0.00e+00, 1.98e+01, 2.84e+00]    [1.95e+01, 1.98e+01, 1.82e+01, 1.58e+01]    
28000     [2.58e+01, 5.64e+00, 2.84e+00]    [0.00e+00, 2.08e+01, 2.84e+00]    [2.05e+01, 2.08e+01, 1.78e+01, 1.59e+01]    
29000     [2.31e+01, 1.06e+00, 2.84e+00]    [0.00e+00, 1.89e+01, 2.84e+00]    [1.84e+01, 1.89e+01, 1.89e+01, 1.59e+01]    
30000     [2.52e+01, 4.99e+00, 2.84e+00]    [0.00e+00, 1.99e+01, 2.84e+00]    [2.16e+01, 1.99e+01, 1.82e+01, 1.57e+01]    

Best model at step 29000:
  train loss: 2.71e+01
  test loss: 2.18e+01
  test metric: [1.84e+01, 1.89e+01, 1.89e+01, 1.59e+01]

'train' took 45.409308 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[17.41217529713056, 474.46528823906857, 20.15925249238969, 26.800829978829285, 18.45291317965288, 76.60472879903315, 20.89821195623434, 20.997389302295968, 29.271591150313597, 18.91512061387469]
sigma_y 6 72.39775010088228 135.06733849463612
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.201601 s

'compile' took 0.808345 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.31e+02, 1.01e+03, 2.70e+00]    [0.00e+00, 1.84e+02, 2.70e+00]    [1.84e+02, 1.84e+02, 1.76e+02, 1.76e+02]    
1000      [3.71e+01, 3.19e+01, 2.68e+00]    [0.00e+00, 4.40e+01, 2.68e+00]    [4.40e+01, 4.40e+01, 1.47e+01, 1.47e+01]    
2000      [3.17e+01, 2.06e+01, 2.69e+00]    [0.00e+00, 2.91e+01, 2.69e+00]    [2.91e+01, 2.91e+01, 1.38e+01, 1.37e+01]    
3000      [3.11e+01, 2.26e+01, 2.68e+00]    [0.00e+00, 2.49e+01, 2.68e+00]    [2.50e+01, 2.49e+01, 1.18e+01, 1.19e+01]    
4000      [3.40e+01, 3.12e+01, 2.68e+00]    [0.00e+00, 2.17e+01, 2.68e+00]    [2.20e+01, 2.17e+01, 1.14e+01, 1.15e+01]    
5000      [2.86e+01, 1.18e+01, 2.69e+00]    [0.00e+00, 1.64e+01, 2.69e+00]    [1.66e+01, 1.64e+01, 1.33e+01, 1.32e+01]    
6000      [2.98e+01, 1.60e+01, 2.69e+00]    [0.00e+00, 1.62e+01, 2.69e+00]    [1.65e+01, 1.62e+01, 1.22e+01, 1.23e+01]    
7000      [2.80e+01, 8.21e+00, 2.69e+00]    [0.00e+00, 1.62e+01, 2.69e+00]    [1.66e+01, 1.62e+01, 1.24e+01, 1.25e+01]    
8000      [2.81e+01, 7.82e+00, 2.70e+00]    [0.00e+00, 1.59e+01, 2.70e+00]    [1.64e+01, 1.59e+01, 1.29e+01, 1.32e+01]    
9000      [2.94e+01, 1.34e+01, 2.70e+00]    [0.00e+00, 1.52e+01, 2.70e+00]    [1.58e+01, 1.52e+01, 1.38e+01, 1.43e+01]    
10000     [2.90e+01, 1.20e+01, 2.70e+00]    [0.00e+00, 1.50e+01, 2.70e+00]    [1.57e+01, 1.50e+01, 1.45e+01, 1.52e+01]    
11000     [3.06e+01, 1.83e+01, 2.71e+00]    [0.00e+00, 1.52e+01, 2.71e+00]    [1.60e+01, 1.52e+01, 1.49e+01, 1.57e+01]    
12000     [2.87e+01, 1.12e+01, 2.71e+00]    [0.00e+00, 1.65e+01, 2.71e+00]    [1.66e+01, 1.65e+01, 1.57e+01, 1.62e+01]    
13000     [2.99e+01, 1.36e+01, 2.71e+00]    [0.00e+00, 1.55e+01, 2.71e+00]    [1.57e+01, 1.55e+01, 1.64e+01, 1.68e+01]    
14000     [2.87e+01, 1.18e+01, 2.72e+00]    [0.00e+00, 1.70e+01, 2.72e+00]    [1.61e+01, 1.70e+01, 1.72e+01, 1.69e+01]    
15000     [2.78e+01, 5.83e+00, 2.72e+00]    [0.00e+00, 1.75e+01, 2.72e+00]    [1.67e+01, 1.75e+01, 1.69e+01, 1.68e+01]    
16000     [2.81e+01, 1.09e+01, 2.72e+00]    [0.00e+00, 1.85e+01, 2.72e+00]    [1.78e+01, 1.85e+01, 1.67e+01, 1.67e+01]    
17000     [2.80e+01, 1.05e+01, 2.72e+00]    [0.00e+00, 1.76e+01, 2.72e+00]    [1.69e+01, 1.76e+01, 1.74e+01, 1.75e+01]    
18000     [2.94e+01, 1.67e+01, 2.73e+00]    [0.00e+00, 1.91e+01, 2.73e+00]    [1.87e+01, 1.91e+01, 1.63e+01, 1.67e+01]    
19000     [2.74e+01, 1.12e+01, 2.73e+00]    [0.00e+00, 1.86e+01, 2.73e+00]    [1.86e+01, 1.86e+01, 1.61e+01, 1.68e+01]    
20000     [2.65e+01, 7.30e+00, 2.73e+00]    [0.00e+00, 1.80e+01, 2.73e+00]    [1.83e+01, 1.80e+01, 1.65e+01, 1.73e+01]    
21000     [2.83e+01, 1.39e+01, 2.73e+00]    [0.00e+00, 1.89e+01, 2.73e+00]    [1.91e+01, 1.89e+01, 1.59e+01, 1.68e+01]    
22000     [2.67e+01, 5.72e+00, 2.73e+00]    [0.00e+00, 2.08e+01, 2.73e+00]    [2.15e+01, 2.08e+01, 1.40e+01, 1.55e+01]    
23000     [2.65e+01, 9.31e+00, 2.73e+00]    [0.00e+00, 1.89e+01, 2.73e+00]    [1.98e+01, 1.89e+01, 1.54e+01, 1.67e+01]    
24000     [2.62e+01, 7.63e+00, 2.73e+00]    [0.00e+00, 1.85e+01, 2.73e+00]    [1.94e+01, 1.85e+01, 1.59e+01, 1.73e+01]    
25000     [2.62e+01, 4.88e+00, 2.73e+00]    [0.00e+00, 1.81e+01, 2.73e+00]    [1.87e+01, 1.81e+01, 1.64e+01, 1.73e+01]    
26000     [2.70e+01, 1.12e+01, 2.73e+00]    [0.00e+00, 1.87e+01, 2.73e+00]    [1.97e+01, 1.87e+01, 1.57e+01, 1.72e+01]    
27000     [2.58e+01, 7.48e+00, 2.73e+00]    [0.00e+00, 1.85e+01, 2.73e+00]    [1.97e+01, 1.85e+01, 1.57e+01, 1.71e+01]    
28000     [2.66e+01, 5.25e+00, 2.73e+00]    [0.00e+00, 1.84e+01, 2.73e+00]    [1.89e+01, 1.84e+01, 1.64e+01, 1.71e+01]    
29000     [2.71e+01, 1.14e+01, 2.73e+00]    [0.00e+00, 1.89e+01, 2.73e+00]    [2.01e+01, 1.89e+01, 1.56e+01, 1.71e+01]    
30000     [2.63e+01, 7.76e+00, 2.73e+00]    [0.00e+00, 1.90e+01, 2.73e+00]    [1.94e+01, 1.90e+01, 1.64e+01, 1.70e+01]    

Best model at step 25000:
  train loss: 3.38e+01
  test loss: 2.08e+01
  test metric: [1.87e+01, 1.81e+01, 1.64e+01, 1.73e+01]

'train' took 46.373175 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.187606 s

'compile' took 0.820170 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.44e+02, 9.29e+01, 2.78e+00]    [0.00e+00, 1.74e+02, 2.78e+00]    [1.74e+02, 1.74e+02, 1.94e+02, 1.94e+02]    
1000      [3.06e+01, 1.05e+01, 2.79e+00]    [0.00e+00, 3.14e+01, 2.79e+00]    [2.40e+01, 3.14e+01, 1.76e+01, 2.65e+01]    
2000      [3.20e+01, 5.45e+00, 2.80e+00]    [0.00e+00, 6.55e+01, 2.80e+00]    [2.41e+01, 6.55e+01, 2.18e+01, 7.32e+01]    
3000      [2.91e+01, 2.06e+00, 2.82e+00]    [0.00e+00, 1.09e+02, 2.82e+00]    [1.72e+01, 1.09e+02, 1.67e+01, 1.32e+02]    
4000      [2.70e+01, 2.47e-01, 2.84e+00]    [0.00e+00, 1.57e+02, 2.84e+00]    [1.37e+01, 1.57e+02, 1.47e+01, 1.91e+02]    
5000      [2.83e+01, 5.64e-01, 2.84e+00]    [0.00e+00, 1.56e+02, 2.84e+00]    [1.10e+01, 1.56e+02, 1.37e+01, 1.92e+02]    
6000      [2.66e+01, 4.41e-01, 2.84e+00]    [0.00e+00, 1.66e+02, 2.84e+00]    [1.52e+01, 1.66e+02, 1.24e+01, 2.02e+02]    
7000      [2.98e+01, 9.32e-01, 2.84e+00]    [0.00e+00, 1.61e+02, 2.84e+00]    [1.47e+01, 1.61e+02, 2.34e+01, 1.95e+02]    
8000      [2.68e+01, 3.50e-01, 2.84e+00]    [0.00e+00, 1.92e+02, 2.84e+00]    [1.85e+01, 1.92e+02, 1.31e+01, 2.35e+02]    
9000      [2.53e+01, 2.09e-01, 2.84e+00]    [0.00e+00, 1.80e+02, 2.84e+00]    [1.24e+01, 1.80e+02, 8.91e+00, 2.16e+02]    
10000     [2.79e+01, 6.80e-01, 2.84e+00]    [0.00e+00, 2.00e+02, 2.84e+00]    [2.17e+01, 2.00e+02, 1.76e+01, 2.43e+02]    
11000     [2.79e+01, 6.25e-01, 2.83e+00]    [0.00e+00, 1.73e+02, 2.83e+00]    [1.23e+01, 1.73e+02, 1.88e+01, 2.06e+02]    
12000     [2.53e+01, 4.50e-01, 2.83e+00]    [0.00e+00, 1.90e+02, 2.83e+00]    [1.44e+01, 1.90e+02, 9.41e+00, 2.30e+02]    
13000     [2.71e+01, 6.52e-01, 2.83e+00]    [0.00e+00, 2.00e+02, 2.83e+00]    [2.00e+01, 2.00e+02, 1.43e+01, 2.40e+02]    
14000     [2.34e+01, 1.05e-01, 2.83e+00]    [0.00e+00, 1.88e+02, 2.83e+00]    [1.12e+01, 1.88e+02, 5.80e+00, 2.26e+02]    
15000     [2.46e+01, 2.61e-01, 2.82e+00]    [0.00e+00, 1.83e+02, 2.82e+00]    [1.22e+01, 1.83e+02, 7.39e+00, 2.18e+02]    
16000     [2.39e+01, 2.22e-01, 2.82e+00]    [0.00e+00, 1.85e+02, 2.82e+00]    [1.07e+01, 1.85e+02, 4.45e+00, 2.21e+02]    
17000     [2.29e+01, 8.62e-02, 2.82e+00]    [0.00e+00, 1.91e+02, 2.82e+00]    [9.48e+00, 1.91e+02, 6.51e+00, 2.28e+02]    
18000     [2.72e+01, 5.92e-01, 2.82e+00]    [0.00e+00, 1.74e+02, 2.82e+00]    [1.52e+01, 1.74e+02, 1.90e+01, 2.08e+02]    
19000     [2.44e+01, 3.51e-01, 2.82e+00]    [0.00e+00, 1.84e+02, 2.82e+00]    [1.00e+01, 1.84e+02, 7.89e+00, 2.17e+02]    
20000     [2.51e+01, 3.16e-01, 2.82e+00]    [0.00e+00, 1.80e+02, 2.82e+00]    [1.13e+01, 1.80e+02, 1.25e+01, 2.12e+02]    
21000     [2.46e+01, 3.16e-01, 2.81e+00]    [0.00e+00, 1.83e+02, 2.81e+00]    [9.31e+00, 1.83e+02, 8.66e+00, 2.16e+02]    
22000     [2.41e+01, 2.63e-01, 2.81e+00]    [0.00e+00, 1.84e+02, 2.81e+00]    [9.93e+00, 1.84e+02, 7.62e+00, 2.17e+02]    
23000     [2.57e+01, 4.71e-01, 2.81e+00]    [0.00e+00, 1.77e+02, 2.81e+00]    [1.46e+01, 1.77e+02, 1.67e+01, 2.07e+02]    
24000     [2.45e+01, 4.47e-01, 2.81e+00]    [0.00e+00, 1.81e+02, 2.81e+00]    [1.08e+01, 1.81e+02, 1.02e+01, 2.13e+02]    
25000     [2.22e+01, 1.28e-01, 2.81e+00]    [0.00e+00, 1.91e+02, 2.81e+00]    [9.48e+00, 1.91e+02, 5.71e+00, 2.25e+02]    
26000     [3.13e+01, 1.24e+00, 2.80e+00]    [0.00e+00, 1.59e+02, 2.80e+00]    [2.76e+01, 1.59e+02, 3.35e+01, 1.85e+02]    
27000     [2.22e+01, 9.55e-02, 2.80e+00]    [0.00e+00, 1.86e+02, 2.80e+00]    [1.09e+01, 1.86e+02, 3.53e+00, 2.17e+02]    
28000     [2.34e+01, 3.15e-01, 2.80e+00]    [0.00e+00, 1.94e+02, 2.80e+00]    [1.37e+01, 1.94e+02, 7.42e+00, 2.27e+02]    
29000     [2.37e+01, 2.64e-01, 2.80e+00]    [0.00e+00, 1.80e+02, 2.80e+00]    [1.05e+01, 1.80e+02, 9.09e+00, 2.08e+02]    
30000     [2.29e+01, 2.68e-01, 2.80e+00]    [0.00e+00, 1.81e+02, 2.80e+00]    [1.01e+01, 1.81e+02, 5.89e+00, 2.10e+02]    

Best model at step 27000:
  train loss: 2.51e+01
  test loss: 1.89e+02
  test metric: [1.09e+01, 1.86e+02, 3.53e+00, 2.17e+02]

'train' took 47.498848 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.344077 s

'compile' took 1.006458 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.30e+02, 1.62e+03, 2.69e+00]    [0.00e+00, 1.06e+02, 2.69e+00]    [1.06e+02, 1.06e+02, 6.99e+01, 6.99e+01]    
1000      [5.46e+01, 2.14e+01, 2.68e+00]    [0.00e+00, 6.33e+01, 2.68e+00]    [6.32e+01, 6.33e+01, 1.58e+01, 1.57e+01]    
2000      [3.58e+01, 1.60e+01, 2.68e+00]    [0.00e+00, 3.92e+01, 2.68e+00]    [3.89e+01, 3.92e+01, 1.62e+01, 1.63e+01]    
3000      [3.02e+01, 7.81e+00, 2.67e+00]    [0.00e+00, 2.72e+01, 2.67e+00]    [2.68e+01, 2.72e+01, 1.39e+01, 1.38e+01]    
4000      [2.92e+01, 5.31e+00, 2.67e+00]    [0.00e+00, 2.42e+01, 2.67e+00]    [2.38e+01, 2.42e+01, 1.31e+01, 1.29e+01]    
5000      [3.00e+01, 8.41e+00, 2.67e+00]    [0.00e+00, 2.27e+01, 2.67e+00]    [2.23e+01, 2.27e+01, 1.46e+01, 1.43e+01]    
6000      [2.87e+01, 2.98e+00, 2.67e+00]    [0.00e+00, 2.15e+01, 2.67e+00]    [2.11e+01, 2.15e+01, 1.70e+01, 1.67e+01]    
7000      [2.85e+01, 3.44e+00, 2.67e+00]    [0.00e+00, 2.15e+01, 2.67e+00]    [2.11e+01, 2.15e+01, 1.70e+01, 1.67e+01]    
8000      [3.03e+01, 1.25e+01, 2.66e+00]    [0.00e+00, 2.22e+01, 2.66e+00]    [2.19e+01, 2.22e+01, 1.65e+01, 1.63e+01]    
9000      [2.92e+01, 8.43e+00, 2.66e+00]    [0.00e+00, 2.11e+01, 2.66e+00]    [2.08e+01, 2.11e+01, 1.65e+01, 1.62e+01]    
10000     [2.81e+01, 7.78e+00, 2.66e+00]    [0.00e+00, 2.10e+01, 2.66e+00]    [2.07e+01, 2.10e+01, 1.70e+01, 1.67e+01]    
11000     [2.77e+01, 4.47e+00, 2.66e+00]    [0.00e+00, 2.14e+01, 2.66e+00]    [2.10e+01, 2.14e+01, 1.66e+01, 1.63e+01]    
12000     [2.82e+01, 9.46e+00, 2.66e+00]    [0.00e+00, 2.12e+01, 2.66e+00]    [2.15e+01, 2.12e+01, 1.64e+01, 1.69e+01]    
13000     [2.80e+01, 7.71e+00, 2.66e+00]    [0.00e+00, 2.10e+01, 2.66e+00]    [2.07e+01, 2.10e+01, 1.64e+01, 1.61e+01]    
14000     [2.65e+01, 1.56e+00, 2.65e+00]    [0.00e+00, 2.16e+01, 2.65e+00]    [2.19e+01, 2.16e+01, 1.55e+01, 1.59e+01]    
15000     [2.72e+01, 6.27e+00, 2.65e+00]    [0.00e+00, 2.10e+01, 2.65e+00]    [2.08e+01, 2.10e+01, 1.62e+01, 1.59e+01]    
16000     [2.81e+01, 9.12e+00, 2.65e+00]    [0.00e+00, 2.08e+01, 2.65e+00]    [2.10e+01, 2.08e+01, 1.58e+01, 1.61e+01]    
17000     [2.74e+01, 8.51e+00, 2.65e+00]    [0.00e+00, 2.13e+01, 2.65e+00]    [2.15e+01, 2.13e+01, 1.60e+01, 1.63e+01]    
18000     [2.56e+01, 3.92e-01, 2.65e+00]    [0.00e+00, 2.12e+01, 2.65e+00]    [2.12e+01, 2.12e+01, 1.61e+01, 1.61e+01]    
19000     [2.59e+01, 2.78e+00, 2.65e+00]    [0.00e+00, 2.11e+01, 2.65e+00]    [2.09e+01, 2.11e+01, 1.61e+01, 1.58e+01]    
20000     [2.70e+01, 7.26e+00, 2.64e+00]    [0.00e+00, 2.09e+01, 2.64e+00]    [2.10e+01, 2.09e+01, 1.59e+01, 1.61e+01]    
21000     [2.60e+01, 4.00e+00, 2.64e+00]    [0.00e+00, 2.09e+01, 2.64e+00]    [2.07e+01, 2.09e+01, 1.61e+01, 1.58e+01]    
22000     [2.64e+01, 5.25e+00, 2.64e+00]    [0.00e+00, 2.05e+01, 2.64e+00]    [2.09e+01, 2.05e+01, 1.57e+01, 1.61e+01]    
23000     [2.75e+01, 1.15e+01, 2.64e+00]    [0.00e+00, 2.11e+01, 2.64e+00]    [2.15e+01, 2.11e+01, 1.56e+01, 1.61e+01]    
24000     [2.58e+01, 4.13e+00, 2.63e+00]    [0.00e+00, 2.08e+01, 2.63e+00]    [2.12e+01, 2.08e+01, 1.55e+01, 1.59e+01]    
25000     [2.64e+01, 8.72e+00, 2.63e+00]    [0.00e+00, 2.10e+01, 2.63e+00]    [2.14e+01, 2.10e+01, 1.56e+01, 1.60e+01]    
26000     [2.88e+01, 1.54e+01, 2.63e+00]    [0.00e+00, 2.09e+01, 2.63e+00]    [2.13e+01, 2.09e+01, 1.58e+01, 1.62e+01]    
27000     [2.69e+01, 8.36e+00, 2.62e+00]    [0.00e+00, 2.03e+01, 2.62e+00]    [2.04e+01, 2.03e+01, 1.59e+01, 1.59e+01]    
28000     [2.71e+01, 8.74e+00, 2.62e+00]    [0.00e+00, 2.02e+01, 2.62e+00]    [2.02e+01, 2.02e+01, 1.59e+01, 1.58e+01]    
29000     [2.66e+01, 9.24e+00, 2.62e+00]    [0.00e+00, 2.07e+01, 2.62e+00]    [2.05e+01, 2.07e+01, 1.62e+01, 1.59e+01]    
30000     [2.68e+01, 8.13e+00, 2.61e+00]    [0.00e+00, 2.01e+01, 2.61e+00]    [2.00e+01, 2.01e+01, 1.60e+01, 1.58e+01]    

Best model at step 18000:
  train loss: 2.86e+01
  test loss: 2.39e+01
  test metric: [2.12e+01, 2.12e+01, 1.61e+01, 1.61e+01]

'train' took 47.816723 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.207446 s

'compile' took 0.862728 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.20e+02, 2.26e+02, 2.71e+00]    [0.00e+00, 1.99e+02, 2.71e+00]    [1.99e+02, 1.99e+02, 1.25e+02, 1.25e+02]    
1000      [4.30e+01, 1.75e+01, 2.72e+00]    [0.00e+00, 5.96e+01, 2.72e+00]    [6.00e+01, 5.96e+01, 2.69e+01, 2.69e+01]    
2000      [3.45e+01, 1.22e+01, 2.72e+00]    [0.00e+00, 4.30e+01, 2.72e+00]    [4.36e+01, 4.30e+01, 1.37e+01, 1.42e+01]    
3000      [3.37e+01, 1.24e+01, 2.72e+00]    [0.00e+00, 3.20e+01, 2.72e+00]    [3.29e+01, 3.20e+01, 8.84e+00, 9.32e+00]    
4000      [3.23e+01, 1.00e+01, 2.71e+00]    [0.00e+00, 1.91e+01, 2.71e+00]    [1.88e+01, 1.91e+01, 9.58e+00, 8.25e+00]    
5000      [3.29e+01, 1.42e+01, 2.71e+00]    [0.00e+00, 1.28e+01, 2.71e+00]    [1.30e+01, 1.28e+01, 8.22e+00, 8.59e+00]    
6000      [2.99e+01, 3.78e+00, 2.71e+00]    [0.00e+00, 1.02e+01, 2.71e+00]    [1.24e+01, 1.02e+01, 8.48e+00, 7.86e+00]    
7000      [3.22e+01, 9.17e+00, 2.71e+00]    [0.00e+00, 1.29e+01, 2.71e+00]    [1.48e+01, 1.29e+01, 1.23e+01, 1.04e+01]    
8000      [2.90e+01, 2.92e+00, 2.71e+00]    [0.00e+00, 1.01e+01, 2.71e+00]    [1.23e+01, 1.01e+01, 6.95e+00, 5.48e+00]    
9000      [3.18e+01, 8.28e+00, 2.71e+00]    [0.00e+00, 1.20e+01, 2.71e+00]    [1.44e+01, 1.20e+01, 1.41e+01, 1.20e+01]    
10000     [2.86e+01, 4.37e+00, 2.71e+00]    [0.00e+00, 8.45e+00, 2.71e+00]    [1.11e+01, 8.45e+00, 7.02e+00, 5.50e+00]    
11000     [3.07e+01, 1.20e+01, 2.71e+00]    [0.00e+00, 8.48e+00, 2.71e+00]    [9.59e+00, 8.48e+00, 6.25e+00, 4.51e+00]    
12000     [2.84e+01, 4.87e+00, 2.71e+00]    [0.00e+00, 8.17e+00, 2.71e+00]    [1.10e+01, 8.17e+00, 6.30e+00, 4.96e+00]    
13000     [3.07e+01, 7.70e+00, 2.71e+00]    [0.00e+00, 1.19e+01, 2.71e+00]    [1.47e+01, 1.19e+01, 1.26e+01, 1.05e+01]    
14000     [2.96e+01, 6.73e+00, 2.71e+00]    [0.00e+00, 1.15e+01, 2.71e+00]    [1.44e+01, 1.15e+01, 1.12e+01, 8.96e+00]    
15000     [2.79e+01, 3.60e+00, 2.71e+00]    [0.00e+00, 1.03e+01, 2.71e+00]    [1.33e+01, 1.03e+01, 8.96e+00, 6.52e+00]    
16000     [2.97e+01, 7.66e+00, 2.72e+00]    [0.00e+00, 1.14e+01, 2.72e+00]    [1.44e+01, 1.14e+01, 1.14e+01, 9.12e+00]    
17000     [2.88e+01, 6.08e+00, 2.72e+00]    [0.00e+00, 7.77e+00, 2.72e+00]    [1.08e+01, 7.77e+00, 7.05e+00, 4.95e+00]    
18000     [2.96e+01, 8.01e+00, 2.72e+00]    [0.00e+00, 1.26e+01, 2.72e+00]    [1.57e+01, 1.26e+01, 1.12e+01, 8.66e+00]    
19000     [2.73e+01, 3.40e+00, 2.72e+00]    [0.00e+00, 8.21e+00, 2.72e+00]    [1.14e+01, 8.21e+00, 8.59e+00, 5.59e+00]    
20000     [2.66e+01, 1.02e+00, 2.71e+00]    [0.00e+00, 1.12e+01, 2.71e+00]    [1.45e+01, 1.12e+01, 8.02e+00, 3.77e+00]    
21000     [3.25e+01, 1.61e+01, 2.71e+00]    [0.00e+00, 1.47e+01, 2.71e+00]    [1.78e+01, 1.47e+01, 1.68e+01, 1.48e+01]    
22000     [2.84e+01, 8.00e+00, 2.71e+00]    [0.00e+00, 1.37e+01, 2.71e+00]    [1.69e+01, 1.37e+01, 1.09e+01, 7.48e+00]    
23000     [2.95e+01, 1.08e+01, 2.71e+00]    [0.00e+00, 1.19e+01, 2.71e+00]    [1.35e+01, 1.19e+01, 7.21e+00, 1.83e+00]    
24000     [2.83e+01, 8.30e+00, 2.71e+00]    [0.00e+00, 1.03e+01, 2.71e+00]    [1.19e+01, 1.03e+01, 8.77e+00, 3.00e+00]    
25000     [2.57e+01, 1.18e+00, 2.71e+00]    [0.00e+00, 1.06e+01, 2.71e+00]    [1.42e+01, 1.06e+01, 8.46e+00, 3.41e+00]    
26000     [2.65e+01, 4.77e+00, 2.71e+00]    [0.00e+00, 1.09e+01, 2.71e+00]    [1.45e+01, 1.09e+01, 7.76e+00, 4.40e+00]    
27000     [2.52e+01, 2.17e+00, 2.71e+00]    [0.00e+00, 1.06e+01, 2.71e+00]    [1.43e+01, 1.06e+01, 8.65e+00, 3.39e+00]    
28000     [2.51e+01, 1.11e+00, 2.71e+00]    [0.00e+00, 1.09e+01, 2.71e+00]    [1.46e+01, 1.09e+01, 7.93e+00, 2.83e+00]    
29000     [2.52e+01, 1.76e+00, 2.71e+00]    [0.00e+00, 1.03e+01, 2.71e+00]    [1.40e+01, 1.03e+01, 8.05e+00, 3.68e+00]    
30000     [2.96e+01, 1.24e+01, 2.71e+00]    [0.00e+00, 1.55e+01, 2.71e+00]    [1.87e+01, 1.55e+01, 1.36e+01, 9.55e+00]    

Best model at step 28000:
  train loss: 2.89e+01
  test loss: 1.36e+01
  test metric: [1.46e+01, 1.09e+01, 7.93e+00, 2.83e+00]

'train' took 47.340567 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.189969 s

'compile' took 0.809653 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.70e+02, 2.75e+02, 2.69e+00]    [0.00e+00, 3.46e+02, 2.69e+00]    [3.46e+02, 3.46e+02, 5.55e+02, 5.55e+02]    
1000      [4.05e+01, 3.00e+01, 2.68e+00]    [0.00e+00, 4.92e+01, 2.68e+00]    [5.14e+01, 4.92e+01, 7.33e+00, 6.88e+00]    
2000      [2.91e+01, 1.30e+01, 2.68e+00]    [0.00e+00, 3.29e+01, 2.68e+00]    [2.92e+01, 3.29e+01, 9.56e+00, 1.53e+01]    
3000      [2.87e+01, 1.20e+01, 2.68e+00]    [0.00e+00, 2.31e+01, 2.68e+00]    [1.86e+01, 2.31e+01, 1.19e+01, 1.44e+01]    
4000      [2.91e+01, 1.09e+01, 2.67e+00]    [0.00e+00, 1.82e+01, 2.67e+00]    [1.59e+01, 1.82e+01, 1.22e+01, 1.24e+01]    
5000      [2.96e+01, 8.80e+00, 2.67e+00]    [0.00e+00, 2.95e+01, 2.67e+00]    [2.31e+01, 2.95e+01, 1.75e+01, 3.04e+01]    
6000      [2.79e+01, 6.55e+00, 2.67e+00]    [0.00e+00, 2.75e+01, 2.67e+00]    [2.04e+01, 2.75e+01, 1.08e+01, 2.32e+01]    
7000      [2.90e+01, 8.88e+00, 2.67e+00]    [0.00e+00, 2.02e+01, 2.67e+00]    [1.93e+01, 2.02e+01, 9.30e+00, 8.97e+00]    
8000      [3.06e+01, 1.13e+01, 2.66e+00]    [0.00e+00, 1.80e+01, 2.66e+00]    [2.11e+01, 1.80e+01, 1.21e+01, 9.28e+00]    
9000      [2.72e+01, 5.72e+00, 2.66e+00]    [0.00e+00, 2.98e+01, 2.66e+00]    [2.15e+01, 2.98e+01, 1.26e+01, 2.79e+01]    
10000     [2.91e+01, 9.39e+00, 2.66e+00]    [0.00e+00, 1.94e+01, 2.66e+00]    [1.91e+01, 1.94e+01, 1.16e+01, 1.06e+01]    
11000     [2.78e+01, 7.78e+00, 2.65e+00]    [0.00e+00, 3.16e+01, 2.65e+00]    [2.29e+01, 3.16e+01, 1.63e+01, 3.30e+01]    
12000     [2.96e+01, 1.06e+01, 2.65e+00]    [0.00e+00, 3.43e+01, 2.65e+00]    [2.54e+01, 3.43e+01, 2.11e+01, 3.93e+01]    
13000     [2.94e+01, 1.03e+01, 2.65e+00]    [0.00e+00, 3.42e+01, 2.65e+00]    [2.51e+01, 3.42e+01, 2.13e+01, 3.98e+01]    
14000     [2.63e+01, 6.50e+00, 2.64e+00]    [0.00e+00, 2.36e+01, 2.64e+00]    [1.55e+01, 2.36e+01, 1.20e+01, 1.92e+01]    
15000     [3.03e+01, 1.11e+01, 2.64e+00]    [0.00e+00, 3.39e+01, 2.64e+00]    [2.49e+01, 3.39e+01, 2.05e+01, 3.94e+01]    
16000     [2.55e+01, 4.51e+00, 2.64e+00]    [0.00e+00, 2.58e+01, 2.64e+00]    [1.70e+01, 2.58e+01, 1.13e+01, 2.29e+01]    
17000     [2.55e+01, 5.03e+00, 2.64e+00]    [0.00e+00, 2.24e+01, 2.64e+00]    [1.54e+01, 2.24e+01, 1.20e+01, 1.66e+01]    
18000     [2.62e+01, 5.68e+00, 2.63e+00]    [0.00e+00, 2.07e+01, 2.63e+00]    [1.71e+01, 2.07e+01, 1.09e+01, 1.39e+01]    
19000     [2.50e+01, 4.59e+00, 2.63e+00]    [0.00e+00, 2.30e+01, 2.63e+00]    [1.51e+01, 2.30e+01, 1.22e+01, 1.76e+01]    
20000     [2.57e+01, 5.87e+00, 2.63e+00]    [0.00e+00, 2.18e+01, 2.63e+00]    [1.59e+01, 2.18e+01, 1.13e+01, 1.57e+01]    
21000     [2.61e+01, 6.05e+00, 2.63e+00]    [0.00e+00, 2.87e+01, 2.63e+00]    [2.10e+01, 2.87e+01, 1.33e+01, 2.94e+01]    
22000     [2.70e+01, 7.59e+00, 2.63e+00]    [0.00e+00, 2.93e+01, 2.63e+00]    [2.14e+01, 2.93e+01, 1.42e+01, 3.01e+01]    
23000     [2.69e+01, 6.74e+00, 2.63e+00]    [0.00e+00, 1.90e+01, 2.63e+00]    [1.83e+01, 1.90e+01, 1.05e+01, 1.20e+01]    
24000     [2.43e+01, 3.22e+00, 2.62e+00]    [0.00e+00, 2.40e+01, 2.62e+00]    [1.69e+01, 2.40e+01, 1.05e+01, 2.00e+01]    
25000     [2.87e+01, 9.59e+00, 2.62e+00]    [0.00e+00, 3.25e+01, 2.62e+00]    [2.47e+01, 3.25e+01, 2.02e+01, 3.74e+01]    
26000     [2.47e+01, 4.28e+00, 2.62e+00]    [0.00e+00, 2.33e+01, 2.62e+00]    [1.63e+01, 2.33e+01, 1.09e+01, 1.83e+01]    
27000     [2.82e+01, 8.69e+00, 2.62e+00]    [0.00e+00, 3.26e+01, 2.62e+00]    [2.52e+01, 3.26e+01, 2.05e+01, 3.79e+01]    
28000     [2.48e+01, 3.94e+00, 2.62e+00]    [0.00e+00, 2.10e+01, 2.62e+00]    [1.65e+01, 2.10e+01, 1.09e+01, 1.45e+01]    
29000     [2.50e+01, 4.55e+00, 2.62e+00]    [0.00e+00, 2.12e+01, 2.62e+00]    [1.62e+01, 2.12e+01, 1.10e+01, 1.45e+01]    
30000     [2.48e+01, 4.22e+00, 2.61e+00]    [0.00e+00, 2.60e+01, 2.61e+00]    [1.96e+01, 2.60e+01, 1.07e+01, 2.39e+01]    

Best model at step 24000:
  train loss: 3.01e+01
  test loss: 2.67e+01
  test metric: [1.69e+01, 2.40e+01, 1.05e+01, 2.00e+01]

'train' took 46.032399 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.201488 s

'compile' took 0.845996 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.68e+02, 9.59e+01, 2.68e+00]    [0.00e+00, 2.71e+02, 2.68e+00]    [2.71e+02, 2.71e+02, 2.49e+02, 2.49e+02]    
1000      [3.13e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 1.48e+01, 2.69e+00]    [1.48e+01, 1.48e+01, 1.33e+01, 1.28e+01]    
2000      [3.19e+01, 2.56e+00, 2.71e+00]    [0.00e+00, 2.45e+01, 2.71e+00]    [1.26e+01, 2.45e+01, 1.09e+01, 1.03e+01]    
3000      [3.22e+01, 1.26e+00, 2.72e+00]    [0.00e+00, 5.42e+01, 2.72e+00]    [1.93e+01, 5.42e+01, 1.70e+01, 3.08e+01]    
4000      [2.88e+01, 5.12e-01, 2.72e+00]    [0.00e+00, 5.78e+01, 2.72e+00]    [1.23e+01, 5.78e+01, 9.43e+00, 3.15e+01]    
5000      [2.95e+01, 3.33e-01, 2.72e+00]    [0.00e+00, 4.97e+01, 2.72e+00]    [1.38e+01, 4.97e+01, 1.47e+01, 2.80e+01]    
6000      [2.72e+01, 2.51e-01, 2.73e+00]    [0.00e+00, 7.02e+01, 2.73e+00]    [1.12e+01, 7.02e+01, 8.78e+00, 4.56e+01]    
7000      [2.67e+01, 4.09e-01, 2.73e+00]    [0.00e+00, 6.96e+01, 2.73e+00]    [1.28e+01, 6.96e+01, 7.93e+00, 4.21e+01]    
8000      [2.65e+01, 5.97e-01, 2.73e+00]    [0.00e+00, 7.14e+01, 2.73e+00]    [9.28e+00, 7.14e+01, 6.65e+00, 4.55e+01]    
9000      [2.56e+01, 1.75e-01, 2.72e+00]    [0.00e+00, 7.86e+01, 2.72e+00]    [9.65e+00, 7.86e+01, 6.69e+00, 5.02e+01]    
10000     [2.87e+01, 7.10e-01, 2.72e+00]    [0.00e+00, 9.64e+01, 2.72e+00]    [1.96e+01, 9.64e+01, 1.57e+01, 6.88e+01]    
11000     [2.54e+01, 2.47e-01, 2.72e+00]    [0.00e+00, 7.93e+01, 2.72e+00]    [8.99e+00, 7.93e+01, 6.19e+00, 5.15e+01]    
12000     [2.58e+01, 3.23e-01, 2.71e+00]    [0.00e+00, 8.94e+01, 2.71e+00]    [1.27e+01, 8.94e+01, 8.71e+00, 6.16e+01]    
13000     [2.44e+01, 1.11e-01, 2.71e+00]    [0.00e+00, 8.61e+01, 2.71e+00]    [8.94e+00, 8.61e+01, 7.86e+00, 5.75e+01]    
14000     [2.47e+01, 1.91e-01, 2.71e+00]    [0.00e+00, 8.16e+01, 2.71e+00]    [9.18e+00, 8.16e+01, 5.88e+00, 5.34e+01]    
15000     [2.41e+01, 2.59e-01, 2.70e+00]    [0.00e+00, 8.67e+01, 2.70e+00]    [8.50e+00, 8.67e+01, 6.78e+00, 5.90e+01]    
16000     [2.38e+01, 1.07e-01, 2.70e+00]    [0.00e+00, 8.74e+01, 2.70e+00]    [8.34e+00, 8.74e+01, 6.37e+00, 6.04e+01]    
17000     [2.62e+01, 1.59e-01, 2.70e+00]    [0.00e+00, 9.85e+01, 2.70e+00]    [1.54e+01, 9.85e+01, 1.17e+01, 7.58e+01]    
18000     [2.47e+01, 3.89e-01, 2.70e+00]    [0.00e+00, 8.49e+01, 2.70e+00]    [8.76e+00, 8.49e+01, 6.94e+00, 5.86e+01]    
19000     [2.47e+01, 3.15e-01, 2.69e+00]    [0.00e+00, 9.73e+01, 2.69e+00]    [1.15e+01, 9.73e+01, 8.01e+00, 7.16e+01]    
20000     [2.46e+01, 3.17e-01, 2.69e+00]    [0.00e+00, 8.71e+01, 2.69e+00]    [9.25e+00, 8.71e+01, 8.20e+00, 6.01e+01]    
21000     [2.38e+01, 7.98e-02, 2.69e+00]    [0.00e+00, 9.01e+01, 2.69e+00]    [9.23e+00, 9.01e+01, 6.68e+00, 6.29e+01]    
22000     [2.45e+01, 3.11e-01, 2.69e+00]    [0.00e+00, 8.55e+01, 2.69e+00]    [8.91e+00, 8.55e+01, 8.27e+00, 5.89e+01]    
23000     [2.81e+01, 7.43e-01, 2.68e+00]    [0.00e+00, 7.27e+01, 2.68e+00]    [1.58e+01, 7.27e+01, 2.07e+01, 4.64e+01]    
24000     [2.41e+01, 3.39e-01, 2.68e+00]    [0.00e+00, 9.46e+01, 2.68e+00]    [1.14e+01, 9.46e+01, 7.41e+00, 6.89e+01]    
25000     [2.27e+01, 1.21e-01, 2.68e+00]    [0.00e+00, 8.91e+01, 2.68e+00]    [7.55e+00, 8.91e+01, 6.12e+00, 6.35e+01]    
26000     [2.30e+01, 1.05e-01, 2.68e+00]    [0.00e+00, 8.54e+01, 2.68e+00]    [8.18e+00, 8.54e+01, 5.02e+00, 5.93e+01]    
27000     [2.51e+01, 1.81e-01, 2.68e+00]    [0.00e+00, 9.56e+01, 2.68e+00]    [1.48e+01, 9.56e+01, 1.07e+01, 7.31e+01]    
28000     [2.40e+01, 3.15e-01, 2.68e+00]    [0.00e+00, 8.01e+01, 2.68e+00]    [8.72e+00, 8.01e+01, 7.07e+00, 5.31e+01]    
29000     [2.71e+01, 8.16e-01, 2.68e+00]    [0.00e+00, 6.76e+01, 2.68e+00]    [1.45e+01, 6.76e+01, 1.82e+01, 4.20e+01]    
30000     [2.33e+01, 2.43e-01, 2.67e+00]    [0.00e+00, 8.81e+01, 2.67e+00]    [1.08e+01, 8.81e+01, 6.85e+00, 6.12e+01]    

Best model at step 25000:
  train loss: 2.55e+01
  test loss: 9.18e+01
  test metric: [7.55e+00, 8.91e+01, 6.12e+00, 6.35e+01]

'train' took 47.681311 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.212255 s

'compile' took 1.306186 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.40e+02, 7.68e+02, 2.75e+00]    [0.00e+00, 2.17e+02, 2.75e+00]    [2.17e+02, 2.17e+02, 2.48e+02, 2.48e+02]    
1000      [5.63e+01, 1.64e+01, 2.72e+00]    [0.00e+00, 6.23e+01, 2.72e+00]    [6.23e+01, 6.23e+01, 1.67e+01, 1.67e+01]    
2000      [3.91e+01, 2.39e+01, 2.70e+00]    [0.00e+00, 3.53e+01, 2.70e+00]    [3.52e+01, 3.53e+01, 1.57e+01, 1.57e+01]    
3000      [2.97e+01, 9.31e+00, 2.69e+00]    [0.00e+00, 2.61e+01, 2.69e+00]    [2.60e+01, 2.61e+01, 1.52e+01, 1.51e+01]    
4000      [3.01e+01, 1.45e+01, 2.68e+00]    [0.00e+00, 2.50e+01, 2.68e+00]    [2.51e+01, 2.50e+01, 1.50e+01, 1.51e+01]    
5000      [2.79e+01, 8.78e+00, 2.68e+00]    [0.00e+00, 2.46e+01, 2.68e+00]    [2.47e+01, 2.46e+01, 1.46e+01, 1.48e+01]    
6000      [2.94e+01, 1.53e+01, 2.68e+00]    [0.00e+00, 2.39e+01, 2.68e+00]    [2.37e+01, 2.39e+01, 1.54e+01, 1.52e+01]    
7000      [2.69e+01, 6.19e+00, 2.68e+00]    [0.00e+00, 2.40e+01, 2.68e+00]    [2.42e+01, 2.40e+01, 1.53e+01, 1.56e+01]    
8000      [2.60e+01, 4.32e+00, 2.68e+00]    [0.00e+00, 2.43e+01, 2.68e+00]    [2.44e+01, 2.43e+01, 1.56e+01, 1.59e+01]    
9000      [2.69e+01, 7.23e+00, 2.68e+00]    [0.00e+00, 2.46e+01, 2.68e+00]    [2.44e+01, 2.46e+01, 1.60e+01, 1.57e+01]    
10000     [2.99e+01, 1.60e+01, 2.68e+00]    [0.00e+00, 2.43e+01, 2.68e+00]    [2.42e+01, 2.43e+01, 1.63e+01, 1.62e+01]    
11000     [2.66e+01, 6.96e+00, 2.68e+00]    [0.00e+00, 2.45e+01, 2.68e+00]    [2.43e+01, 2.45e+01, 1.64e+01, 1.61e+01]    
12000     [2.54e+01, 3.47e+00, 2.68e+00]    [0.00e+00, 2.46e+01, 2.68e+00]    [2.48e+01, 2.46e+01, 1.61e+01, 1.65e+01]    
13000     [2.73e+01, 1.07e+01, 2.68e+00]    [0.00e+00, 2.53e+01, 2.68e+00]    [2.51e+01, 2.53e+01, 1.57e+01, 1.54e+01]    
14000     [2.70e+01, 8.97e+00, 2.68e+00]    [0.00e+00, 2.44e+01, 2.68e+00]    [2.46e+01, 2.44e+01, 1.64e+01, 1.68e+01]    
15000     [2.72e+01, 1.15e+01, 2.68e+00]    [0.00e+00, 2.48e+01, 2.68e+00]    [2.45e+01, 2.48e+01, 1.71e+01, 1.67e+01]    
16000     [2.56e+01, 6.08e+00, 2.67e+00]    [0.00e+00, 2.46e+01, 2.67e+00]    [2.45e+01, 2.46e+01, 1.71e+01, 1.69e+01]    
17000     [2.63e+01, 9.70e+00, 2.67e+00]    [0.00e+00, 2.46e+01, 2.67e+00]    [2.45e+01, 2.46e+01, 1.71e+01, 1.71e+01]    
18000     [2.65e+01, 7.66e+00, 2.67e+00]    [0.00e+00, 2.47e+01, 2.67e+00]    [2.44e+01, 2.47e+01, 1.71e+01, 1.67e+01]    
19000     [2.72e+01, 1.06e+01, 2.67e+00]    [0.00e+00, 2.46e+01, 2.67e+00]    [2.43e+01, 2.46e+01, 1.71e+01, 1.68e+01]    
20000     [2.57e+01, 8.22e+00, 2.67e+00]    [0.00e+00, 2.44e+01, 2.67e+00]    [2.43e+01, 2.44e+01, 1.75e+01, 1.74e+01]    
21000     [2.57e+01, 6.16e+00, 2.67e+00]    [0.00e+00, 2.44e+01, 2.67e+00]    [2.40e+01, 2.44e+01, 1.78e+01, 1.73e+01]    
22000     [2.56e+01, 5.44e+00, 2.67e+00]    [0.00e+00, 2.41e+01, 2.67e+00]    [2.38e+01, 2.41e+01, 1.78e+01, 1.75e+01]    
23000     [2.71e+01, 1.11e+01, 2.67e+00]    [0.00e+00, 2.39e+01, 2.67e+00]    [2.37e+01, 2.39e+01, 1.78e+01, 1.77e+01]    
24000     [2.84e+01, 1.49e+01, 2.67e+00]    [0.00e+00, 2.38e+01, 2.67e+00]    [2.34e+01, 2.38e+01, 1.80e+01, 1.77e+01]    
25000     [2.55e+01, 6.64e+00, 2.67e+00]    [0.00e+00, 2.38e+01, 2.67e+00]    [2.34e+01, 2.38e+01, 1.82e+01, 1.78e+01]    
26000     [2.81e+01, 1.43e+01, 2.67e+00]    [0.00e+00, 2.40e+01, 2.67e+00]    [2.35e+01, 2.40e+01, 1.78e+01, 1.73e+01]    
27000     [2.48e+01, 4.81e+00, 2.66e+00]    [0.00e+00, 2.36e+01, 2.66e+00]    [2.31e+01, 2.36e+01, 1.85e+01, 1.80e+01]    
28000     [2.53e+01, 7.54e+00, 2.66e+00]    [0.00e+00, 2.38e+01, 2.66e+00]    [2.34e+01, 2.38e+01, 1.87e+01, 1.83e+01]    
29000     [2.71e+01, 1.17e+01, 2.66e+00]    [0.00e+00, 2.37e+01, 2.66e+00]    [2.31e+01, 2.37e+01, 1.85e+01, 1.80e+01]    
30000     [2.62e+01, 1.07e+01, 2.66e+00]    [0.00e+00, 2.34e+01, 2.66e+00]    [2.34e+01, 2.34e+01, 1.86e+01, 1.87e+01]    

Best model at step 12000:
  train loss: 3.15e+01
  test loss: 2.73e+01
  test metric: [2.48e+01, 2.46e+01, 1.61e+01, 1.65e+01]

'train' took 51.903084 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.182687 s

'compile' took 0.807570 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.60e+02, 6.25e+02, 2.72e+00]    [0.00e+00, 2.39e+02, 2.72e+00]    [2.39e+02, 2.39e+02, 3.22e+02, 3.22e+02]    
1000      [3.85e+01, 3.32e+01, 2.69e+00]    [0.00e+00, 4.50e+01, 2.69e+00]    [4.50e+01, 4.50e+01, 1.50e+01, 1.48e+01]    
2000      [3.04e+01, 1.68e+01, 2.68e+00]    [0.00e+00, 2.85e+01, 2.68e+00]    [2.83e+01, 2.85e+01, 7.78e+00, 7.79e+00]    
3000      [3.03e+01, 1.38e+01, 2.67e+00]    [0.00e+00, 2.07e+01, 2.67e+00]    [2.05e+01, 2.07e+01, 6.09e+00, 6.23e+00]    
4000      [3.05e+01, 1.07e+01, 2.67e+00]    [0.00e+00, 2.00e+01, 2.67e+00]    [1.99e+01, 2.00e+01, 8.95e+00, 9.12e+00]    
5000      [2.92e+01, 7.45e+00, 2.67e+00]    [0.00e+00, 1.73e+01, 2.67e+00]    [1.72e+01, 1.73e+01, 1.00e+01, 1.01e+01]    
6000      [2.93e+01, 8.45e+00, 2.67e+00]    [0.00e+00, 1.57e+01, 2.67e+00]    [1.56e+01, 1.57e+01, 1.02e+01, 1.04e+01]    
7000      [2.80e+01, 5.92e+00, 2.67e+00]    [0.00e+00, 1.55e+01, 2.67e+00]    [1.56e+01, 1.55e+01, 9.82e+00, 9.82e+00]    
8000      [2.85e+01, 8.32e+00, 2.67e+00]    [0.00e+00, 1.69e+01, 2.67e+00]    [1.71e+01, 1.69e+01, 9.47e+00, 9.09e+00]    
9000      [2.70e+01, 4.36e+00, 2.67e+00]    [0.00e+00, 1.44e+01, 2.67e+00]    [1.48e+01, 1.44e+01, 9.02e+00, 9.37e+00]    
10000     [2.81e+01, 9.11e+00, 2.68e+00]    [0.00e+00, 1.53e+01, 2.68e+00]    [1.59e+01, 1.53e+01, 9.00e+00, 8.26e+00]    
11000     [2.97e+01, 1.28e+01, 2.68e+00]    [0.00e+00, 1.59e+01, 2.68e+00]    [1.68e+01, 1.59e+01, 9.37e+00, 7.86e+00]    
12000     [2.82e+01, 1.25e+01, 2.68e+00]    [0.00e+00, 1.37e+01, 2.68e+00]    [1.36e+01, 1.37e+01, 8.45e+00, 8.91e+00]    
13000     [3.16e+01, 1.71e+01, 2.68e+00]    [0.00e+00, 1.63e+01, 2.68e+00]    [1.76e+01, 1.63e+01, 1.11e+01, 8.00e+00]    
14000     [2.75e+01, 1.13e+01, 2.68e+00]    [0.00e+00, 1.43e+01, 2.68e+00]    [1.36e+01, 1.43e+01, 7.44e+00, 8.23e+00]    
15000     [2.70e+01, 1.03e+01, 2.68e+00]    [0.00e+00, 1.46e+01, 2.68e+00]    [1.32e+01, 1.46e+01, 7.42e+00, 8.79e+00]    
16000     [2.63e+01, 5.10e+00, 2.68e+00]    [0.00e+00, 1.31e+01, 2.68e+00]    [1.48e+01, 1.31e+01, 6.91e+00, 9.76e+00]    
17000     [2.54e+01, 4.59e+00, 2.68e+00]    [0.00e+00, 1.43e+01, 2.68e+00]    [1.38e+01, 1.43e+01, 6.36e+00, 9.60e+00]    
18000     [2.73e+01, 7.09e+00, 2.68e+00]    [0.00e+00, 1.26e+01, 2.68e+00]    [1.53e+01, 1.26e+01, 7.75e+00, 1.13e+01]    
19000     [2.68e+01, 8.98e+00, 2.68e+00]    [0.00e+00, 1.59e+01, 2.68e+00]    [1.22e+01, 1.59e+01, 7.35e+00, 1.18e+01]    
20000     [2.78e+01, 1.24e+01, 2.68e+00]    [0.00e+00, 1.60e+01, 2.68e+00]    [1.18e+01, 1.60e+01, 7.47e+00, 1.16e+01]    
21000     [2.72e+01, 1.04e+01, 2.68e+00]    [0.00e+00, 1.54e+01, 2.68e+00]    [1.20e+01, 1.54e+01, 6.84e+00, 1.16e+01]    
22000     [2.61e+01, 5.11e+00, 2.68e+00]    [0.00e+00, 1.27e+01, 2.68e+00]    [1.36e+01, 1.27e+01, 6.39e+00, 1.14e+01]    
23000     [2.79e+01, 1.22e+01, 2.68e+00]    [0.00e+00, 1.53e+01, 2.68e+00]    [1.16e+01, 1.53e+01, 6.90e+00, 1.23e+01]    
24000     [2.54e+01, 2.52e+00, 2.68e+00]    [0.00e+00, 1.33e+01, 2.68e+00]    [1.30e+01, 1.33e+01, 6.59e+00, 1.24e+01]    
25000     [2.70e+01, 6.46e+00, 2.69e+00]    [0.00e+00, 1.17e+01, 2.69e+00]    [1.41e+01, 1.17e+01, 7.45e+00, 1.23e+01]    
26000     [2.61e+01, 3.46e+00, 2.69e+00]    [0.00e+00, 1.21e+01, 2.69e+00]    [1.34e+01, 1.21e+01, 7.45e+00, 1.31e+01]    
27000     [2.80e+01, 1.16e+01, 2.69e+00]    [0.00e+00, 1.52e+01, 2.69e+00]    [1.16e+01, 1.52e+01, 7.24e+00, 1.37e+01]    
28000     [2.73e+01, 9.04e+00, 2.69e+00]    [0.00e+00, 1.15e+01, 2.69e+00]    [1.46e+01, 1.15e+01, 8.41e+00, 1.34e+01]    
29000     [2.85e+01, 1.13e+01, 2.69e+00]    [0.00e+00, 1.19e+01, 2.69e+00]    [1.55e+01, 1.19e+01, 9.59e+00, 1.31e+01]    
30000     [2.57e+01, 5.80e+00, 2.69e+00]    [0.00e+00, 1.44e+01, 2.69e+00]    [1.25e+01, 1.44e+01, 6.98e+00, 1.30e+01]    

Best model at step 24000:
  train loss: 3.06e+01
  test loss: 1.59e+01
  test metric: [1.30e+01, 1.33e+01, 6.59e+00, 1.24e+01]

'train' took 48.051369 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.185237 s

'compile' took 0.912994 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.70e+02, 1.90e+02, 2.65e+00]    [0.00e+00, 1.41e+02, 2.65e+00]    [1.41e+02, 1.41e+02, 7.87e+01, 7.87e+01]    
1000      [3.34e+01, 1.83e+01, 2.65e+00]    [0.00e+00, 3.82e+01, 2.65e+00]    [3.67e+01, 3.82e+01, 1.83e+01, 1.46e+01]    
2000      [2.83e+01, 1.03e+01, 2.65e+00]    [0.00e+00, 3.20e+01, 2.65e+00]    [2.82e+01, 3.20e+01, 1.82e+01, 2.62e+01]    
3000      [2.70e+01, 5.79e+00, 2.65e+00]    [0.00e+00, 2.73e+01, 2.65e+00]    [2.22e+01, 2.73e+01, 1.22e+01, 2.04e+01]    
4000      [2.79e+01, 6.75e+00, 2.65e+00]    [0.00e+00, 2.39e+01, 2.65e+00]    [1.82e+01, 2.39e+01, 1.36e+01, 1.69e+01]    
5000      [2.96e+01, 8.37e+00, 2.65e+00]    [0.00e+00, 3.40e+01, 2.65e+00]    [2.77e+01, 3.40e+01, 2.14e+01, 3.78e+01]    
6000      [2.73e+01, 5.04e+00, 2.64e+00]    [0.00e+00, 2.39e+01, 2.64e+00]    [1.76e+01, 2.39e+01, 1.37e+01, 1.70e+01]    
7000      [2.70e+01, 4.95e+00, 2.64e+00]    [0.00e+00, 2.53e+01, 2.64e+00]    [1.90e+01, 2.53e+01, 1.20e+01, 1.95e+01]    
8000      [2.66e+01, 3.46e+00, 2.64e+00]    [0.00e+00, 2.88e+01, 2.64e+00]    [2.24e+01, 2.88e+01, 1.25e+01, 2.73e+01]    
9000      [2.73e+01, 4.75e+00, 2.64e+00]    [0.00e+00, 2.28e+01, 2.64e+00]    [1.73e+01, 2.28e+01, 1.31e+01, 1.69e+01]    
10000     [2.63e+01, 2.20e+00, 2.64e+00]    [0.00e+00, 2.52e+01, 2.64e+00]    [1.90e+01, 2.52e+01, 1.20e+01, 2.22e+01]    
11000     [2.73e+01, 3.15e+00, 2.64e+00]    [0.00e+00, 2.27e+01, 2.64e+00]    [1.64e+01, 2.27e+01, 1.39e+01, 1.73e+01]    
12000     [2.74e+01, 3.64e+00, 2.63e+00]    [0.00e+00, 2.35e+01, 2.63e+00]    [1.70e+01, 2.35e+01, 1.32e+01, 1.76e+01]    
13000     [2.64e+01, 2.03e+00, 2.63e+00]    [0.00e+00, 2.42e+01, 2.63e+00]    [1.79e+01, 2.42e+01, 1.24e+01, 1.87e+01]    
14000     [2.83e+01, 4.45e+00, 2.63e+00]    [0.00e+00, 2.14e+01, 2.63e+00]    [1.87e+01, 2.14e+01, 1.19e+01, 1.33e+01]    
15000     [2.80e+01, 3.77e+00, 2.63e+00]    [0.00e+00, 2.16e+01, 2.63e+00]    [1.89e+01, 2.16e+01, 1.18e+01, 1.31e+01]    
16000     [2.61e+01, 1.92e+00, 2.63e+00]    [0.00e+00, 2.68e+01, 2.63e+00]    [2.08e+01, 2.68e+01, 1.14e+01, 2.17e+01]    
17000     [2.60e+01, 1.90e+00, 2.63e+00]    [0.00e+00, 2.62e+01, 2.63e+00]    [2.04e+01, 2.62e+01, 1.15e+01, 2.09e+01]    
18000     [2.81e+01, 4.73e+00, 2.62e+00]    [0.00e+00, 2.27e+01, 2.62e+00]    [1.83e+01, 2.27e+01, 1.26e+01, 1.32e+01]    
19000     [2.64e+01, 1.35e+00, 2.62e+00]    [0.00e+00, 2.40e+01, 2.62e+00]    [1.87e+01, 2.40e+01, 1.20e+01, 1.53e+01]    
20000     [2.62e+01, 1.56e+00, 2.62e+00]    [0.00e+00, 2.44e+01, 2.62e+00]    [1.92e+01, 2.44e+01, 1.18e+01, 1.59e+01]    
21000     [2.67e+01, 2.61e+00, 2.62e+00]    [0.00e+00, 2.40e+01, 2.62e+00]    [1.91e+01, 2.40e+01, 1.23e+01, 1.43e+01]    
22000     [2.56e+01, 1.03e+00, 2.62e+00]    [0.00e+00, 2.52e+01, 2.62e+00]    [2.05e+01, 2.52e+01, 1.14e+01, 1.64e+01]    
23000     [2.75e+01, 4.30e+00, 2.62e+00]    [0.00e+00, 2.90e+01, 2.62e+00]    [2.45e+01, 2.90e+01, 1.39e+01, 2.49e+01]    
24000     [2.79e+01, 4.83e+00, 2.61e+00]    [0.00e+00, 3.08e+01, 2.61e+00]    [2.64e+01, 3.08e+01, 1.63e+01, 2.78e+01]    
25000     [2.71e+01, 3.07e+00, 2.61e+00]    [0.00e+00, 2.22e+01, 2.61e+00]    [1.99e+01, 2.22e+01, 1.28e+01, 1.12e+01]    
26000     [2.54e+01, 7.17e-01, 2.61e+00]    [0.00e+00, 2.69e+01, 2.61e+00]    [2.30e+01, 2.69e+01, 1.15e+01, 1.77e+01]    
27000     [2.75e+01, 4.60e+00, 2.61e+00]    [0.00e+00, 3.01e+01, 2.61e+00]    [2.63e+01, 3.01e+01, 1.50e+01, 2.47e+01]    
28000     [2.74e+01, 4.03e+00, 2.61e+00]    [0.00e+00, 2.09e+01, 2.61e+00]    [2.18e+01, 2.09e+01, 1.20e+01, 1.08e+01]    
29000     [2.72e+01, 4.75e+00, 2.61e+00]    [0.00e+00, 3.13e+01, 2.61e+00]    [2.77e+01, 3.13e+01, 1.64e+01, 2.61e+01]    
30000     [2.81e+01, 5.06e+00, 2.61e+00]    [0.00e+00, 1.99e+01, 2.61e+00]    [2.27e+01, 1.99e+01, 1.19e+01, 1.17e+01]    

Best model at step 26000:
  train loss: 2.87e+01
  test loss: 2.95e+01
  test metric: [2.30e+01, 2.69e+01, 1.15e+01, 1.77e+01]

'train' took 53.756361 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.297206 s

'compile' took 1.170210 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.77e+02, 5.25e+02, 2.67e+00]    [0.00e+00, 1.17e+02, 2.67e+00]    [1.17e+02, 1.17e+02, 3.14e+01, 3.14e+01]    
1000      [4.10e+01, 2.72e+01, 2.64e+00]    [0.00e+00, 4.91e+01, 2.64e+00]    [4.90e+01, 4.91e+01, 2.24e+01, 2.23e+01]    
2000      [3.30e+01, 1.41e+01, 2.64e+00]    [0.00e+00, 3.24e+01, 2.64e+00]    [3.23e+01, 3.24e+01, 1.94e+01, 1.93e+01]    
3000      [3.02e+01, 1.27e+01, 2.64e+00]    [0.00e+00, 2.43e+01, 2.64e+00]    [2.44e+01, 2.43e+01, 1.73e+01, 1.75e+01]    
4000      [2.98e+01, 1.21e+01, 2.65e+00]    [0.00e+00, 2.07e+01, 2.65e+00]    [2.10e+01, 2.07e+01, 1.55e+01, 1.57e+01]    
5000      [2.87e+01, 9.57e+00, 2.65e+00]    [0.00e+00, 1.87e+01, 2.65e+00]    [1.91e+01, 1.87e+01, 1.57e+01, 1.61e+01]    
6000      [2.89e+01, 1.16e+01, 2.65e+00]    [0.00e+00, 1.88e+01, 2.65e+00]    [1.88e+01, 1.88e+01, 1.61e+01, 1.59e+01]    
7000      [2.66e+01, 5.84e+00, 2.65e+00]    [0.00e+00, 1.88e+01, 2.65e+00]    [1.92e+01, 1.88e+01, 1.57e+01, 1.60e+01]    
8000      [2.70e+01, 8.14e+00, 2.65e+00]    [0.00e+00, 1.89e+01, 2.65e+00]    [1.94e+01, 1.89e+01, 1.60e+01, 1.64e+01]    
9000      [2.66e+01, 6.05e+00, 2.65e+00]    [0.00e+00, 1.92e+01, 2.65e+00]    [1.97e+01, 1.92e+01, 1.61e+01, 1.65e+01]    
10000     [2.90e+01, 1.58e+01, 2.65e+00]    [0.00e+00, 1.96e+01, 2.65e+00]    [2.01e+01, 1.96e+01, 1.59e+01, 1.62e+01]    
11000     [2.80e+01, 1.39e+01, 2.65e+00]    [0.00e+00, 1.99e+01, 2.65e+00]    [2.04e+01, 1.99e+01, 1.60e+01, 1.62e+01]    
12000     [2.56e+01, 4.28e+00, 2.65e+00]    [0.00e+00, 1.93e+01, 2.65e+00]    [1.98e+01, 1.93e+01, 1.68e+01, 1.71e+01]    
13000     [2.55e+01, 6.20e+00, 2.66e+00]    [0.00e+00, 1.97e+01, 2.66e+00]    [2.02e+01, 1.97e+01, 1.67e+01, 1.70e+01]    
14000     [2.52e+01, 2.71e+00, 2.66e+00]    [0.00e+00, 1.97e+01, 2.66e+00]    [2.03e+01, 1.97e+01, 1.69e+01, 1.72e+01]    
15000     [2.52e+01, 5.25e+00, 2.66e+00]    [0.00e+00, 2.01e+01, 2.66e+00]    [2.07e+01, 2.01e+01, 1.70e+01, 1.73e+01]    
16000     [2.61e+01, 6.96e+00, 2.67e+00]    [0.00e+00, 2.00e+01, 2.67e+00]    [2.06e+01, 2.00e+01, 1.76e+01, 1.79e+01]    
17000     [2.75e+01, 1.18e+01, 2.67e+00]    [0.00e+00, 2.03e+01, 2.67e+00]    [2.08e+01, 2.03e+01, 1.77e+01, 1.79e+01]    
18000     [2.78e+01, 1.21e+01, 2.67e+00]    [0.00e+00, 2.03e+01, 2.67e+00]    [2.09e+01, 2.03e+01, 1.81e+01, 1.84e+01]    
19000     [2.62e+01, 9.38e+00, 2.67e+00]    [0.00e+00, 2.04e+01, 2.67e+00]    [2.10e+01, 2.04e+01, 1.81e+01, 1.83e+01]    
20000     [2.54e+01, 5.12e+00, 2.68e+00]    [0.00e+00, 2.03e+01, 2.68e+00]    [2.09e+01, 2.03e+01, 1.86e+01, 1.89e+01]    
21000     [2.46e+01, 4.24e+00, 2.68e+00]    [0.00e+00, 2.06e+01, 2.68e+00]    [2.12e+01, 2.06e+01, 1.87e+01, 1.89e+01]    
22000     [2.68e+01, 1.12e+01, 2.68e+00]    [0.00e+00, 2.09e+01, 2.68e+00]    [2.15e+01, 2.09e+01, 1.85e+01, 1.86e+01]    
23000     [2.46e+01, 4.45e+00, 2.69e+00]    [0.00e+00, 2.07e+01, 2.69e+00]    [2.13e+01, 2.07e+01, 1.86e+01, 1.88e+01]    
24000     [2.51e+01, 5.28e+00, 2.69e+00]    [0.00e+00, 2.04e+01, 2.69e+00]    [2.10e+01, 2.04e+01, 1.88e+01, 1.91e+01]    
25000     [2.51e+01, 4.96e+00, 2.69e+00]    [0.00e+00, 2.04e+01, 2.69e+00]    [2.10e+01, 2.04e+01, 1.89e+01, 1.91e+01]    
26000     [2.50e+01, 4.82e+00, 2.69e+00]    [0.00e+00, 2.04e+01, 2.69e+00]    [2.10e+01, 2.04e+01, 1.88e+01, 1.90e+01]    
27000     [2.64e+01, 9.84e+00, 2.69e+00]    [0.00e+00, 2.00e+01, 2.69e+00]    [2.06e+01, 2.00e+01, 1.91e+01, 1.93e+01]    
28000     [2.60e+01, 7.70e+00, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.07e+01, 2.01e+01, 1.90e+01, 1.93e+01]    
29000     [2.54e+01, 7.28e+00, 2.69e+00]    [0.00e+00, 1.98e+01, 2.69e+00]    [2.04e+01, 1.98e+01, 1.91e+01, 1.94e+01]    
30000     [2.49e+01, 4.94e+00, 2.69e+00]    [0.00e+00, 1.99e+01, 2.69e+00]    [2.05e+01, 1.99e+01, 1.90e+01, 1.93e+01]    

Best model at step 14000:
  train loss: 3.05e+01
  test loss: 2.24e+01
  test metric: [2.03e+01, 1.97e+01, 1.69e+01, 1.72e+01]

'train' took 57.914737 s

[18.056717615028422, 186.2546737945927, 21.210917619271637, 10.923687366722763, 24.035138532999856, 89.08972096015388, 24.62149090829166, 13.25801353677423, 26.90020234221351, 19.748977660488013]
sigma_y 7 43.40995403365367 52.10771483042663
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.220511 s

'compile' took 1.753120 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.18e+02, 3.52e+02, 2.73e+00]    [0.00e+00, 1.40e+02, 2.73e+00]    [1.40e+02, 1.40e+02, 1.17e+02, 1.17e+02]    
1000      [3.87e+01, 3.26e+01, 2.71e+00]    [0.00e+00, 3.23e+01, 2.71e+00]    [3.19e+01, 3.23e+01, 2.06e+01, 2.00e+01]    
2000      [2.96e+01, 1.77e+01, 2.71e+00]    [0.00e+00, 2.49e+01, 2.71e+00]    [2.44e+01, 2.49e+01, 1.69e+01, 1.64e+01]    
3000      [2.79e+01, 1.29e+01, 2.71e+00]    [0.00e+00, 1.91e+01, 2.71e+00]    [1.86e+01, 1.91e+01, 1.88e+01, 1.82e+01]    
4000      [2.63e+01, 8.51e+00, 2.70e+00]    [0.00e+00, 1.73e+01, 2.70e+00]    [1.68e+01, 1.73e+01, 1.95e+01, 1.89e+01]    
5000      [2.66e+01, 9.55e+00, 2.70e+00]    [0.00e+00, 1.75e+01, 2.70e+00]    [1.72e+01, 1.75e+01, 1.91e+01, 1.86e+01]    
6000      [2.62e+01, 9.19e+00, 2.70e+00]    [0.00e+00, 1.73e+01, 2.70e+00]    [1.80e+01, 1.73e+01, 1.86e+01, 1.89e+01]    
7000      [2.60e+01, 8.94e+00, 2.70e+00]    [0.00e+00, 1.75e+01, 2.70e+00]    [1.83e+01, 1.75e+01, 1.84e+01, 1.88e+01]    
8000      [3.12e+01, 2.16e+01, 2.70e+00]    [0.00e+00, 1.85e+01, 2.70e+00]    [1.83e+01, 1.85e+01, 1.87e+01, 1.85e+01]    
9000      [2.59e+01, 6.37e+00, 2.69e+00]    [0.00e+00, 1.92e+01, 2.69e+00]    [1.82e+01, 1.92e+01, 1.87e+01, 1.77e+01]    
10000     [2.73e+01, 1.26e+01, 2.69e+00]    [0.00e+00, 1.86e+01, 2.69e+00]    [1.99e+01, 1.86e+01, 1.73e+01, 1.84e+01]    
11000     [2.61e+01, 8.18e+00, 2.69e+00]    [0.00e+00, 1.86e+01, 2.69e+00]    [1.88e+01, 1.86e+01, 1.84e+01, 1.84e+01]    
12000     [2.62e+01, 7.79e+00, 2.69e+00]    [0.00e+00, 1.92e+01, 2.69e+00]    [1.86e+01, 1.92e+01, 1.88e+01, 1.81e+01]    
13000     [2.75e+01, 1.17e+01, 2.69e+00]    [0.00e+00, 1.98e+01, 2.69e+00]    [1.86e+01, 1.98e+01, 1.89e+01, 1.78e+01]    
14000     [2.67e+01, 1.13e+01, 2.69e+00]    [0.00e+00, 1.94e+01, 2.69e+00]    [2.01e+01, 1.94e+01, 1.82e+01, 1.89e+01]    
15000     [2.88e+01, 1.57e+01, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.05e+01, 2.01e+01, 1.83e+01, 1.87e+01]    
16000     [2.73e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 2.03e+01, 2.69e+00]    [2.10e+01, 2.03e+01, 1.81e+01, 1.88e+01]    
17000     [2.64e+01, 9.16e+00, 2.69e+00]    [0.00e+00, 2.06e+01, 2.69e+00]    [2.05e+01, 2.06e+01, 1.87e+01, 1.89e+01]    
18000     [2.55e+01, 5.94e+00, 2.69e+00]    [0.00e+00, 2.07e+01, 2.69e+00]    [2.00e+01, 2.07e+01, 1.95e+01, 1.91e+01]    
19000     [2.65e+01, 7.57e+00, 2.69e+00]    [0.00e+00, 2.03e+01, 2.69e+00]    [2.00e+01, 2.03e+01, 1.91e+01, 1.91e+01]    
20000     [2.54e+01, 4.07e+00, 2.69e+00]    [0.00e+00, 2.06e+01, 2.69e+00]    [2.16e+01, 2.06e+01, 1.78e+01, 1.89e+01]    
21000     [3.11e+01, 2.11e+01, 2.68e+00]    [0.00e+00, 2.16e+01, 2.68e+00]    [2.23e+01, 2.16e+01, 1.79e+01, 1.86e+01]    
22000     [2.47e+01, 2.59e+00, 2.68e+00]    [0.00e+00, 2.12e+01, 2.68e+00]    [2.04e+01, 2.12e+01, 1.92e+01, 1.87e+01]    
23000     [2.76e+01, 1.30e+01, 2.68e+00]    [0.00e+00, 2.13e+01, 2.68e+00]    [2.15e+01, 2.13e+01, 1.84e+01, 1.86e+01]    
24000     [2.58e+01, 5.54e+00, 2.68e+00]    [0.00e+00, 2.07e+01, 2.68e+00]    [1.99e+01, 2.07e+01, 1.93e+01, 1.88e+01]    
25000     [2.53e+01, 4.53e+00, 2.68e+00]    [0.00e+00, 2.07e+01, 2.68e+00]    [2.02e+01, 2.07e+01, 1.91e+01, 1.88e+01]    
26000     [2.69e+01, 1.12e+01, 2.68e+00]    [0.00e+00, 2.13e+01, 2.68e+00]    [2.09e+01, 2.13e+01, 1.89e+01, 1.86e+01]    
27000     [2.50e+01, 3.48e+00, 2.67e+00]    [0.00e+00, 2.08e+01, 2.67e+00]    [2.01e+01, 2.08e+01, 1.92e+01, 1.88e+01]    
28000     [2.48e+01, 5.86e+00, 2.67e+00]    [0.00e+00, 2.11e+01, 2.67e+00]    [2.07e+01, 2.11e+01, 1.90e+01, 1.86e+01]    
29000     [2.78e+01, 1.36e+01, 2.67e+00]    [0.00e+00, 2.13e+01, 2.67e+00]    [2.14e+01, 2.13e+01, 1.85e+01, 1.86e+01]    
30000     [2.75e+01, 1.29e+01, 2.67e+00]    [0.00e+00, 2.14e+01, 2.67e+00]    [2.16e+01, 2.14e+01, 1.85e+01, 1.87e+01]    

Best model at step 22000:
  train loss: 3.00e+01
  test loss: 2.39e+01
  test metric: [2.04e+01, 2.12e+01, 1.92e+01, 1.87e+01]

'train' took 49.289229 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.200495 s

'compile' took 0.904870 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.40e+02, 1.31e+02, 2.87e+00]    [0.00e+00, 6.22e+02, 2.87e+00]    [6.22e+02, 6.22e+02, 4.47e+02, 4.47e+02]    
1000      [3.04e+01, 1.02e+01, 2.87e+00]    [0.00e+00, 2.00e+01, 2.87e+00]    [1.38e+01, 2.00e+01, 1.14e+01, 2.05e+01]    
2000      [3.29e+01, 4.81e+00, 2.86e+00]    [0.00e+00, 2.06e+01, 2.86e+00]    [1.68e+01, 2.06e+01, 1.71e+01, 3.39e+01]    
3000      [3.22e+01, 2.51e+00, 2.87e+00]    [0.00e+00, 2.87e+01, 2.87e+00]    [2.15e+01, 2.87e+01, 1.80e+01, 1.87e+01]    
4000      [3.11e+01, 1.79e+00, 2.87e+00]    [0.00e+00, 7.09e+01, 2.87e+00]    [1.97e+01, 7.09e+01, 1.54e+01, 5.99e+01]    
5000      [2.76e+01, 1.13e+00, 2.88e+00]    [0.00e+00, 1.12e+02, 2.88e+00]    [1.34e+01, 1.12e+02, 9.30e+00, 1.16e+02]    
6000      [2.80e+01, 6.21e-01, 2.89e+00]    [0.00e+00, 1.72e+02, 2.89e+00]    [1.40e+01, 1.72e+02, 1.27e+01, 2.00e+02]    
7000      [2.78e+01, 2.78e-01, 2.89e+00]    [0.00e+00, 2.10e+02, 2.89e+00]    [1.63e+01, 2.10e+02, 1.23e+01, 2.51e+02]    
8000      [2.85e+01, 6.96e-01, 2.90e+00]    [0.00e+00, 2.02e+02, 2.90e+00]    [1.27e+01, 2.02e+02, 1.39e+01, 2.43e+02]    
9000      [2.73e+01, 6.65e-01, 2.89e+00]    [0.00e+00, 2.17e+02, 2.89e+00]    [9.96e+00, 2.17e+02, 9.81e+00, 2.60e+02]    
10000     [2.58e+01, 1.39e-01, 2.89e+00]    [0.00e+00, 2.27e+02, 2.89e+00]    [1.28e+01, 2.27e+02, 7.67e+00, 2.70e+02]    
11000     [2.62e+01, 4.49e-01, 2.89e+00]    [0.00e+00, 2.48e+02, 2.89e+00]    [1.31e+01, 2.48e+02, 1.04e+01, 2.91e+02]    
12000     [2.71e+01, 5.28e-01, 2.89e+00]    [0.00e+00, 2.39e+02, 2.89e+00]    [1.28e+01, 2.39e+02, 1.46e+01, 2.79e+02]    
13000     [2.50e+01, 2.03e-01, 2.89e+00]    [0.00e+00, 2.51e+02, 2.89e+00]    [1.35e+01, 2.51e+02, 7.37e+00, 2.92e+02]    
14000     [2.43e+01, 7.37e-02, 2.89e+00]    [0.00e+00, 2.64e+02, 2.89e+00]    [1.02e+01, 2.64e+02, 8.22e+00, 3.05e+02]    
15000     [2.48e+01, 3.76e-01, 2.89e+00]    [0.00e+00, 2.70e+02, 2.89e+00]    [1.22e+01, 2.70e+02, 9.09e+00, 3.08e+02]    
16000     [2.56e+01, 6.43e-01, 2.88e+00]    [0.00e+00, 2.77e+02, 2.88e+00]    [1.39e+01, 2.77e+02, 1.12e+01, 3.11e+02]    
17000     [2.45e+01, 3.48e-01, 2.88e+00]    [0.00e+00, 2.79e+02, 2.88e+00]    [1.15e+01, 2.79e+02, 9.76e+00, 3.13e+02]    
18000     [2.63e+01, 8.44e-01, 2.88e+00]    [0.00e+00, 2.89e+02, 2.88e+00]    [1.78e+01, 2.89e+02, 1.15e+01, 3.22e+02]    
19000     [2.44e+01, 1.39e-01, 2.88e+00]    [0.00e+00, 2.77e+02, 2.88e+00]    [1.28e+01, 2.77e+02, 8.71e+00, 3.07e+02]    
20000     [2.68e+01, 5.27e-01, 2.88e+00]    [0.00e+00, 2.70e+02, 2.88e+00]    [1.45e+01, 2.70e+02, 2.00e+01, 2.98e+02]    
21000     [2.35e+01, 2.94e-01, 2.87e+00]    [0.00e+00, 2.88e+02, 2.87e+00]    [1.36e+01, 2.88e+02, 6.25e+00, 3.18e+02]    
22000     [2.36e+01, 1.03e-01, 2.87e+00]    [0.00e+00, 2.92e+02, 2.87e+00]    [1.22e+01, 2.92e+02, 7.80e+00, 3.23e+02]    
23000     [2.46e+01, 3.96e-01, 2.87e+00]    [0.00e+00, 3.01e+02, 2.87e+00]    [1.41e+01, 3.01e+02, 9.58e+00, 3.30e+02]    
24000     [2.45e+01, 1.93e-01, 2.87e+00]    [0.00e+00, 2.88e+02, 2.87e+00]    [1.39e+01, 2.88e+02, 1.07e+01, 3.15e+02]    
25000     [2.31e+01, 2.65e-01, 2.87e+00]    [0.00e+00, 3.00e+02, 2.87e+00]    [1.24e+01, 3.00e+02, 6.98e+00, 3.27e+02]    
26000     [2.30e+01, 2.56e-01, 2.87e+00]    [0.00e+00, 3.02e+02, 2.87e+00]    [1.29e+01, 3.02e+02, 6.78e+00, 3.28e+02]    
27000     [2.35e+01, 1.08e-01, 2.86e+00]    [0.00e+00, 3.08e+02, 2.86e+00]    [1.25e+01, 3.08e+02, 8.36e+00, 3.36e+02]    
28000     [2.41e+01, 3.33e-01, 2.86e+00]    [0.00e+00, 3.13e+02, 2.86e+00]    [1.41e+01, 3.13e+02, 9.79e+00, 3.39e+02]    
29000     [2.77e+01, 1.03e+00, 2.86e+00]    [0.00e+00, 3.27e+02, 2.86e+00]    [2.43e+01, 3.27e+02, 1.71e+01, 3.54e+02]    
30000     [2.62e+01, 8.69e-01, 2.86e+00]    [0.00e+00, 3.24e+02, 2.86e+00]    [2.10e+01, 3.24e+02, 1.30e+01, 3.51e+02]    

Best model at step 26000:
  train loss: 2.61e+01
  test loss: 3.05e+02
  test metric: [1.29e+01, 3.02e+02, 6.78e+00, 3.28e+02]

'train' took 47.766399 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.181425 s

'compile' took 0.834428 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.47e+02, 6.95e+02, 2.75e+00]    [0.00e+00, 2.33e+02, 2.75e+00]    [2.33e+02, 2.33e+02, 3.09e+02, 3.09e+02]    
1000      [4.66e+01, 3.38e+01, 2.73e+00]    [0.00e+00, 5.57e+01, 2.73e+00]    [5.57e+01, 5.57e+01, 1.70e+01, 1.70e+01]    
2000      [2.96e+01, 1.25e+01, 2.73e+00]    [0.00e+00, 3.20e+01, 2.73e+00]    [3.22e+01, 3.20e+01, 1.23e+01, 1.24e+01]    
3000      [3.27e+01, 2.48e+01, 2.73e+00]    [0.00e+00, 2.71e+01, 2.73e+00]    [2.73e+01, 2.71e+01, 1.03e+01, 1.03e+01]    
4000      [2.76e+01, 1.06e+01, 2.72e+00]    [0.00e+00, 2.37e+01, 2.72e+00]    [2.39e+01, 2.37e+01, 1.10e+01, 1.10e+01]    
5000      [2.76e+01, 1.16e+01, 2.72e+00]    [0.00e+00, 2.16e+01, 2.72e+00]    [2.18e+01, 2.16e+01, 1.20e+01, 1.20e+01]    
6000      [2.69e+01, 9.40e+00, 2.72e+00]    [0.00e+00, 2.05e+01, 2.72e+00]    [2.06e+01, 2.05e+01, 1.23e+01, 1.22e+01]    
7000      [2.71e+01, 1.22e+01, 2.72e+00]    [0.00e+00, 1.98e+01, 2.72e+00]    [1.99e+01, 1.98e+01, 1.27e+01, 1.26e+01]    
8000      [2.89e+01, 1.45e+01, 2.72e+00]    [0.00e+00, 1.93e+01, 2.72e+00]    [1.94e+01, 1.93e+01, 1.23e+01, 1.22e+01]    
9000      [2.66e+01, 1.17e+01, 2.72e+00]    [0.00e+00, 1.83e+01, 2.72e+00]    [1.85e+01, 1.83e+01, 1.37e+01, 1.36e+01]    
10000     [2.64e+01, 9.28e+00, 2.71e+00]    [0.00e+00, 1.78e+01, 2.71e+00]    [1.79e+01, 1.78e+01, 1.39e+01, 1.37e+01]    
11000     [2.76e+01, 1.31e+01, 2.71e+00]    [0.00e+00, 1.75e+01, 2.71e+00]    [1.77e+01, 1.75e+01, 1.42e+01, 1.40e+01]    
12000     [2.65e+01, 9.18e+00, 2.71e+00]    [0.00e+00, 1.79e+01, 2.71e+00]    [1.82e+01, 1.79e+01, 1.37e+01, 1.37e+01]    
13000     [2.63e+01, 8.95e+00, 2.71e+00]    [0.00e+00, 1.83e+01, 2.71e+00]    [1.86e+01, 1.83e+01, 1.33e+01, 1.33e+01]    
14000     [2.67e+01, 1.24e+01, 2.71e+00]    [0.00e+00, 1.79e+01, 2.71e+00]    [1.83e+01, 1.79e+01, 1.41e+01, 1.40e+01]    
15000     [2.54e+01, 6.33e+00, 2.71e+00]    [0.00e+00, 1.77e+01, 2.71e+00]    [1.81e+01, 1.77e+01, 1.41e+01, 1.41e+01]    
16000     [2.70e+01, 1.04e+01, 2.71e+00]    [0.00e+00, 1.73e+01, 2.71e+00]    [1.77e+01, 1.73e+01, 1.44e+01, 1.44e+01]    
17000     [2.78e+01, 1.30e+01, 2.71e+00]    [0.00e+00, 1.78e+01, 2.71e+00]    [1.82e+01, 1.78e+01, 1.41e+01, 1.41e+01]    
18000     [2.77e+01, 1.03e+01, 2.71e+00]    [0.00e+00, 1.79e+01, 2.71e+00]    [1.83e+01, 1.79e+01, 1.42e+01, 1.42e+01]    
19000     [2.66e+01, 8.29e+00, 2.70e+00]    [0.00e+00, 1.81e+01, 2.70e+00]    [1.86e+01, 1.81e+01, 1.43e+01, 1.42e+01]    
20000     [2.49e+01, 3.23e+00, 2.70e+00]    [0.00e+00, 1.84e+01, 2.70e+00]    [1.88e+01, 1.84e+01, 1.42e+01, 1.42e+01]    
21000     [2.65e+01, 7.82e+00, 2.70e+00]    [0.00e+00, 1.83e+01, 2.70e+00]    [1.88e+01, 1.83e+01, 1.45e+01, 1.45e+01]    
22000     [2.74e+01, 9.34e+00, 2.70e+00]    [0.00e+00, 1.85e+01, 2.70e+00]    [1.90e+01, 1.85e+01, 1.45e+01, 1.45e+01]    
23000     [2.80e+01, 1.07e+01, 2.70e+00]    [0.00e+00, 1.75e+01, 2.70e+00]    [1.78e+01, 1.75e+01, 1.59e+01, 1.57e+01]    
24000     [2.52e+01, 4.13e+00, 2.70e+00]    [0.00e+00, 1.86e+01, 2.70e+00]    [1.92e+01, 1.86e+01, 1.46e+01, 1.46e+01]    
25000     [2.65e+01, 6.43e+00, 2.69e+00]    [0.00e+00, 1.81e+01, 2.69e+00]    [1.87e+01, 1.81e+01, 1.49e+01, 1.49e+01]    
26000     [2.58e+01, 5.47e+00, 2.69e+00]    [0.00e+00, 1.74e+01, 2.69e+00]    [1.76e+01, 1.74e+01, 1.62e+01, 1.58e+01]    
27000     [2.55e+01, 6.36e+00, 2.69e+00]    [0.00e+00, 1.79e+01, 2.69e+00]    [1.86e+01, 1.79e+01, 1.56e+01, 1.56e+01]    
28000     [2.47e+01, 4.03e+00, 2.68e+00]    [0.00e+00, 1.79e+01, 2.68e+00]    [1.86e+01, 1.79e+01, 1.56e+01, 1.56e+01]    
29000     [2.75e+01, 1.22e+01, 2.68e+00]    [0.00e+00, 1.82e+01, 2.68e+00]    [1.89e+01, 1.82e+01, 1.55e+01, 1.55e+01]    
30000     [2.46e+01, 2.61e+00, 2.68e+00]    [0.00e+00, 1.78e+01, 2.68e+00]    [1.85e+01, 1.78e+01, 1.54e+01, 1.54e+01]    

Best model at step 30000:
  train loss: 2.98e+01
  test loss: 2.05e+01
  test metric: [1.85e+01, 1.78e+01, 1.54e+01, 1.54e+01]

'train' took 51.642145 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.182539 s

'compile' took 0.839301 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.81e+02, 2.61e+02, 2.70e+00]    [0.00e+00, 2.07e+02, 2.70e+00]    [2.07e+02, 2.07e+02, 1.36e+02, 1.36e+02]    
1000      [3.53e+01, 3.29e+01, 2.67e+00]    [0.00e+00, 3.79e+01, 2.67e+00]    [3.81e+01, 3.79e+01, 6.08e+00, 5.97e+00]    
2000      [3.17e+01, 1.32e+01, 2.69e+00]    [0.00e+00, 2.18e+01, 2.69e+00]    [2.35e+01, 2.18e+01, 1.48e+01, 1.58e+01]    
3000      [3.09e+01, 8.42e+00, 2.70e+00]    [0.00e+00, 1.64e+01, 2.70e+00]    [1.74e+01, 1.64e+01, 1.28e+01, 1.17e+01]    
4000      [3.16e+01, 3.56e+00, 2.71e+00]    [0.00e+00, 1.90e+01, 2.71e+00]    [1.51e+01, 1.90e+01, 1.57e+01, 1.06e+01]    
5000      [3.21e+01, 8.14e+00, 2.72e+00]    [0.00e+00, 1.60e+01, 2.72e+00]    [1.05e+01, 1.60e+01, 1.03e+01, 8.67e+00]    
6000      [3.04e+01, 5.97e+00, 2.73e+00]    [0.00e+00, 1.61e+01, 2.73e+00]    [1.03e+01, 1.61e+01, 1.02e+01, 9.77e+00]    
7000      [2.82e+01, 2.11e+00, 2.73e+00]    [0.00e+00, 1.87e+01, 2.73e+00]    [1.25e+01, 1.87e+01, 1.41e+01, 1.41e+01]    
8000      [3.03e+01, 8.03e+00, 2.74e+00]    [0.00e+00, 1.54e+01, 2.74e+00]    [9.28e+00, 1.54e+01, 5.88e+00, 9.32e+00]    
9000      [2.76e+01, 2.99e+00, 2.74e+00]    [0.00e+00, 1.67e+01, 2.74e+00]    [1.23e+01, 1.67e+01, 8.98e+00, 1.23e+01]    
10000     [2.84e+01, 5.78e+00, 2.74e+00]    [0.00e+00, 1.91e+01, 2.74e+00]    [1.47e+01, 1.91e+01, 1.47e+01, 1.77e+01]    
11000     [2.91e+01, 8.20e+00, 2.75e+00]    [0.00e+00, 1.52e+01, 2.75e+00]    [9.64e+00, 1.52e+01, 4.53e+00, 1.01e+01]    
12000     [2.83e+01, 7.71e+00, 2.75e+00]    [0.00e+00, 2.11e+01, 2.75e+00]    [1.53e+01, 2.11e+01, 1.59e+01, 1.76e+01]    
13000     [2.76e+01, 6.09e+00, 2.75e+00]    [0.00e+00, 1.74e+01, 2.75e+00]    [1.07e+01, 1.74e+01, 5.98e+00, 9.03e+00]    
14000     [2.85e+01, 5.42e+00, 2.75e+00]    [0.00e+00, 2.35e+01, 2.75e+00]    [1.73e+01, 2.35e+01, 1.29e+01, 1.30e+01]    
15000     [2.79e+01, 8.87e+00, 2.75e+00]    [0.00e+00, 1.73e+01, 2.75e+00]    [1.01e+01, 1.73e+01, 6.23e+00, 1.02e+01]    
16000     [2.60e+01, 2.09e+00, 2.75e+00]    [0.00e+00, 1.91e+01, 2.75e+00]    [1.05e+01, 1.91e+01, 9.36e+00, 9.16e+00]    
17000     [2.58e+01, 2.69e+00, 2.75e+00]    [0.00e+00, 2.23e+01, 2.75e+00]    [1.43e+01, 2.23e+01, 1.16e+01, 1.03e+01]    
18000     [2.56e+01, 4.02e+00, 2.75e+00]    [0.00e+00, 2.11e+01, 2.75e+00]    [1.27e+01, 2.11e+01, 1.24e+01, 1.08e+01]    
19000     [2.51e+01, 1.29e+00, 2.75e+00]    [0.00e+00, 2.02e+01, 2.75e+00]    [1.08e+01, 2.02e+01, 9.23e+00, 8.29e+00]    
20000     [2.65e+01, 6.27e+00, 2.75e+00]    [0.00e+00, 2.01e+01, 2.75e+00]    [9.95e+00, 2.01e+01, 6.88e+00, 1.01e+01]    
21000     [2.47e+01, 3.68e+00, 2.75e+00]    [0.00e+00, 2.40e+01, 2.75e+00]    [1.33e+01, 2.40e+01, 1.05e+01, 9.35e+00]    
22000     [2.49e+01, 1.83e+00, 2.75e+00]    [0.00e+00, 2.33e+01, 2.75e+00]    [1.25e+01, 2.33e+01, 7.62e+00, 9.68e+00]    
23000     [2.68e+01, 8.74e+00, 2.75e+00]    [0.00e+00, 2.58e+01, 2.75e+00]    [1.45e+01, 2.58e+01, 1.39e+01, 1.21e+01]    
24000     [2.44e+01, 1.11e+00, 2.75e+00]    [0.00e+00, 2.39e+01, 2.75e+00]    [1.21e+01, 2.39e+01, 9.03e+00, 9.06e+00]    
25000     [2.71e+01, 8.53e+00, 2.75e+00]    [0.00e+00, 2.14e+01, 2.75e+00]    [9.05e+00, 2.14e+01, 6.36e+00, 1.32e+01]    
26000     [2.80e+01, 1.16e+01, 2.74e+00]    [0.00e+00, 2.28e+01, 2.74e+00]    [8.83e+00, 2.28e+01, 7.01e+00, 1.32e+01]    
27000     [2.71e+01, 8.98e+00, 2.74e+00]    [0.00e+00, 2.76e+01, 2.74e+00]    [1.52e+01, 2.76e+01, 1.36e+01, 1.33e+01]    
28000     [2.44e+01, 3.86e+00, 2.74e+00]    [0.00e+00, 2.57e+01, 2.74e+00]    [1.23e+01, 2.57e+01, 1.01e+01, 1.01e+01]    
29000     [2.45e+01, 2.81e+00, 2.74e+00]    [0.00e+00, 2.46e+01, 2.74e+00]    [1.10e+01, 2.46e+01, 6.61e+00, 1.13e+01]    
30000     [2.55e+01, 5.33e+00, 2.73e+00]    [0.00e+00, 2.34e+01, 2.73e+00]    [9.57e+00, 2.34e+01, 6.29e+00, 1.24e+01]    

Best model at step 24000:
  train loss: 2.83e+01
  test loss: 2.67e+01
  test metric: [1.21e+01, 2.39e+01, 9.03e+00, 9.06e+00]

'train' took 53.318692 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.200433 s

'compile' took 0.875368 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.66e+02, 3.05e+02, 2.66e+00]    [0.00e+00, 2.66e+02, 2.66e+00]    [2.66e+02, 2.66e+02, 2.89e+02, 2.89e+02]    
1000      [3.51e+01, 2.07e+01, 2.67e+00]    [0.00e+00, 4.56e+01, 2.67e+00]    [4.09e+01, 4.56e+01, 1.78e+01, 8.34e+00]    
2000      [3.28e+01, 1.45e+01, 2.67e+00]    [0.00e+00, 4.02e+01, 2.67e+00]    [3.43e+01, 4.02e+01, 2.07e+01, 3.39e+01]    
3000      [3.01e+01, 7.20e+00, 2.67e+00]    [0.00e+00, 3.01e+01, 2.67e+00]    [2.23e+01, 3.01e+01, 1.62e+01, 3.04e+01]    
4000      [3.05e+01, 3.32e+00, 2.68e+00]    [0.00e+00, 2.59e+01, 2.68e+00]    [1.64e+01, 2.59e+01, 1.31e+01, 2.67e+01]    
5000      [3.01e+01, 2.64e+00, 2.68e+00]    [0.00e+00, 2.88e+01, 2.68e+00]    [1.88e+01, 2.88e+01, 1.35e+01, 3.03e+01]    
6000      [2.98e+01, 2.10e+00, 2.68e+00]    [0.00e+00, 2.65e+01, 2.68e+00]    [1.53e+01, 2.65e+01, 1.36e+01, 2.38e+01]    
7000      [3.06e+01, 3.82e+00, 2.68e+00]    [0.00e+00, 3.33e+01, 2.68e+00]    [2.14e+01, 3.33e+01, 1.71e+01, 3.59e+01]    
8000      [3.11e+01, 5.04e+00, 2.68e+00]    [0.00e+00, 3.37e+01, 2.68e+00]    [2.21e+01, 3.37e+01, 1.88e+01, 3.73e+01]    
9000      [3.01e+01, 4.44e+00, 2.68e+00]    [0.00e+00, 3.22e+01, 2.68e+00]    [2.19e+01, 3.22e+01, 1.73e+01, 3.56e+01]    
10000     [2.88e+01, 2.51e+00, 2.68e+00]    [0.00e+00, 2.62e+01, 2.68e+00]    [1.72e+01, 2.62e+01, 1.33e+01, 2.36e+01]    
11000     [2.96e+01, 3.89e+00, 2.68e+00]    [0.00e+00, 2.33e+01, 2.68e+00]    [1.69e+01, 2.33e+01, 1.36e+01, 1.84e+01]    
12000     [2.84e+01, 2.69e+00, 2.68e+00]    [0.00e+00, 3.11e+01, 2.68e+00]    [2.28e+01, 3.11e+01, 1.51e+01, 3.21e+01]    
13000     [2.85e+01, 3.30e+00, 2.69e+00]    [0.00e+00, 2.70e+01, 2.69e+00]    [1.90e+01, 2.70e+01, 1.27e+01, 2.19e+01]    
14000     [2.74e+01, 1.72e+00, 2.69e+00]    [0.00e+00, 2.62e+01, 2.69e+00]    [1.86e+01, 2.62e+01, 1.34e+01, 2.09e+01]    
15000     [2.92e+01, 3.50e+00, 2.69e+00]    [0.00e+00, 2.02e+01, 2.69e+00]    [2.14e+01, 2.02e+01, 1.29e+01, 1.39e+01]    
16000     [2.74e+01, 2.44e+00, 2.69e+00]    [0.00e+00, 2.78e+01, 2.69e+00]    [2.10e+01, 2.78e+01, 1.31e+01, 2.32e+01]    
17000     [2.65e+01, 1.02e+00, 2.69e+00]    [0.00e+00, 2.67e+01, 2.69e+00]    [2.03e+01, 2.67e+01, 1.34e+01, 2.06e+01]    
18000     [2.73e+01, 2.55e+00, 2.69e+00]    [0.00e+00, 2.88e+01, 2.69e+00]    [2.27e+01, 2.88e+01, 1.35e+01, 2.46e+01]    
19000     [2.91e+01, 4.61e+00, 2.69e+00]    [0.00e+00, 3.32e+01, 2.69e+00]    [2.73e+01, 3.32e+01, 1.85e+01, 3.34e+01]    
20000     [2.64e+01, 6.47e-01, 2.69e+00]    [0.00e+00, 2.47e+01, 2.69e+00]    [1.94e+01, 2.47e+01, 1.45e+01, 1.86e+01]    
21000     [2.80e+01, 3.06e+00, 2.69e+00]    [0.00e+00, 2.16e+01, 2.69e+00]    [2.03e+01, 2.16e+01, 1.40e+01, 1.44e+01]    
22000     [2.66e+01, 1.46e+00, 2.69e+00]    [0.00e+00, 2.85e+01, 2.69e+00]    [2.36e+01, 2.85e+01, 1.40e+01, 2.41e+01]    
23000     [2.71e+01, 2.99e+00, 2.69e+00]    [0.00e+00, 2.88e+01, 2.69e+00]    [2.41e+01, 2.88e+01, 1.45e+01, 2.49e+01]    
24000     [2.62e+01, 1.57e+00, 2.69e+00]    [0.00e+00, 2.75e+01, 2.69e+00]    [2.31e+01, 2.75e+01, 1.36e+01, 2.18e+01]    
25000     [2.55e+01, 5.96e-01, 2.69e+00]    [0.00e+00, 2.71e+01, 2.69e+00]    [2.30e+01, 2.71e+01, 1.34e+01, 2.08e+01]    
26000     [2.63e+01, 1.88e+00, 2.69e+00]    [0.00e+00, 2.30e+01, 2.69e+00]    [1.94e+01, 2.30e+01, 1.59e+01, 1.50e+01]    
27000     [2.56e+01, 1.56e+00, 2.69e+00]    [0.00e+00, 2.65e+01, 2.69e+00]    [2.31e+01, 2.65e+01, 1.36e+01, 1.96e+01]    
28000     [2.74e+01, 3.75e+00, 2.69e+00]    [0.00e+00, 1.99e+01, 2.69e+00]    [2.22e+01, 1.99e+01, 1.38e+01, 1.42e+01]    
29000     [2.62e+01, 2.23e+00, 2.68e+00]    [0.00e+00, 2.16e+01, 2.68e+00]    [2.07e+01, 2.16e+01, 1.52e+01, 1.41e+01]    
30000     [2.58e+01, 2.34e+00, 2.68e+00]    [0.00e+00, 2.67e+01, 2.68e+00]    [2.42e+01, 2.67e+01, 1.37e+01, 1.97e+01]    

Best model at step 25000:
  train loss: 2.88e+01
  test loss: 2.98e+01
  test metric: [2.30e+01, 2.71e+01, 1.34e+01, 2.08e+01]

'train' took 56.647604 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.182164 s

'compile' took 0.895327 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.74e+02, 1.03e+02, 2.74e+00]    [0.00e+00, 3.26e+02, 2.74e+00]    [3.26e+02, 3.26e+02, 3.78e+02, 3.78e+02]    
1000      [3.06e+01, 1.19e+01, 2.73e+00]    [0.00e+00, 1.32e+01, 2.73e+00]    [1.31e+01, 1.32e+01, 8.37e+00, 9.94e+00]    
2000      [3.16e+01, 5.73e+00, 2.73e+00]    [0.00e+00, 2.43e+01, 2.73e+00]    [1.60e+01, 2.43e+01, 1.68e+01, 3.77e+00]    
3000      [3.21e+01, 7.81e-01, 2.75e+00]    [0.00e+00, 4.52e+01, 2.75e+00]    [1.73e+01, 4.52e+01, 2.00e+01, 1.27e+01]    
4000      [2.95e+01, 3.61e-01, 2.76e+00]    [0.00e+00, 8.69e+01, 2.76e+00]    [1.91e+01, 8.69e+01, 1.51e+01, 4.20e+01]    
5000      [2.84e+01, 7.03e-01, 2.76e+00]    [0.00e+00, 7.78e+01, 2.76e+00]    [7.66e+00, 7.78e+01, 6.61e+00, 2.64e+01]    
6000      [2.84e+01, 8.48e-01, 2.77e+00]    [0.00e+00, 1.10e+02, 2.77e+00]    [1.82e+01, 1.10e+02, 1.38e+01, 5.68e+01]    
7000      [2.96e+01, 7.49e-01, 2.77e+00]    [0.00e+00, 1.20e+02, 2.77e+00]    [2.28e+01, 1.20e+02, 2.16e+01, 7.07e+01]    
8000      [2.53e+01, 1.80e-01, 2.77e+00]    [0.00e+00, 1.07e+02, 2.77e+00]    [9.38e+00, 1.07e+02, 6.32e+00, 5.57e+01]    
9000      [2.56e+01, 2.11e-01, 2.77e+00]    [0.00e+00, 9.89e+01, 2.77e+00]    [6.92e+00, 9.89e+01, 3.39e+00, 4.55e+01]    
10000     [2.64e+01, 2.33e-01, 2.76e+00]    [0.00e+00, 9.31e+01, 2.76e+00]    [7.57e+00, 9.31e+01, 1.00e+01, 3.97e+01]    
11000     [2.82e+01, 7.12e-01, 2.76e+00]    [0.00e+00, 1.19e+02, 2.76e+00]    [1.96e+01, 1.19e+02, 1.86e+01, 7.45e+01]    
12000     [2.54e+01, 4.36e-01, 2.76e+00]    [0.00e+00, 9.56e+01, 2.76e+00]    [5.24e+00, 9.56e+01, 4.88e+00, 4.66e+01]    
13000     [2.49e+01, 5.05e-01, 2.76e+00]    [0.00e+00, 1.09e+02, 2.76e+00]    [1.09e+01, 1.09e+02, 5.94e+00, 6.09e+01]    
14000     [2.74e+01, 7.11e-01, 2.76e+00]    [0.00e+00, 1.16e+02, 2.76e+00]    [1.82e+01, 1.16e+02, 1.64e+01, 7.25e+01]    
15000     [2.52e+01, 3.78e-01, 2.76e+00]    [0.00e+00, 1.09e+02, 2.76e+00]    [1.21e+01, 1.09e+02, 7.88e+00, 6.38e+01]    
16000     [2.43e+01, 2.12e-01, 2.75e+00]    [0.00e+00, 1.05e+02, 2.75e+00]    [9.05e+00, 1.05e+02, 6.66e+00, 6.35e+01]    
17000     [2.76e+01, 7.51e-01, 2.75e+00]    [0.00e+00, 1.15e+02, 2.75e+00]    [1.85e+01, 1.15e+02, 1.89e+01, 7.89e+01]    
18000     [2.61e+01, 4.20e-01, 2.75e+00]    [0.00e+00, 8.86e+01, 2.75e+00]    [1.02e+01, 8.86e+01, 9.47e+00, 4.34e+01]    
19000     [2.46e+01, 3.25e-01, 2.75e+00]    [0.00e+00, 1.05e+02, 2.75e+00]    [1.05e+01, 1.05e+02, 8.18e+00, 6.63e+01]    
20000     [2.36e+01, 2.75e-01, 2.74e+00]    [0.00e+00, 1.01e+02, 2.74e+00]    [8.75e+00, 1.01e+02, 3.38e+00, 5.99e+01]    
21000     [2.39e+01, 8.57e-01, 2.74e+00]    [0.00e+00, 1.00e+02, 2.74e+00]    [9.26e+00, 1.00e+02, 4.32e+00, 5.77e+01]    
22000     [2.44e+01, 2.60e-01, 2.74e+00]    [0.00e+00, 8.61e+01, 2.74e+00]    [7.24e+00, 8.61e+01, 7.63e+00, 4.75e+01]    
23000     [2.58e+01, 7.37e-01, 2.74e+00]    [0.00e+00, 8.03e+01, 2.74e+00]    [9.52e+00, 8.03e+01, 8.96e+00, 4.68e+01]    
24000     [2.72e+01, 6.48e-01, 2.73e+00]    [0.00e+00, 7.30e+01, 2.73e+00]    [1.70e+01, 7.30e+01, 1.86e+01, 3.53e+01]    
25000     [2.38e+01, 4.69e-01, 2.73e+00]    [0.00e+00, 8.32e+01, 2.73e+00]    [5.36e+00, 8.32e+01, 5.06e+00, 5.42e+01]    
26000     [2.37e+01, 3.75e-01, 2.73e+00]    [0.00e+00, 9.52e+01, 2.73e+00]    [1.08e+01, 9.52e+01, 7.40e+00, 7.07e+01]    
27000     [2.56e+01, 7.77e-01, 2.73e+00]    [0.00e+00, 9.93e+01, 2.73e+00]    [1.60e+01, 9.93e+01, 1.29e+01, 7.77e+01]    
28000     [2.42e+01, 7.12e-01, 2.73e+00]    [0.00e+00, 9.50e+01, 2.73e+00]    [1.24e+01, 9.50e+01, 8.80e+00, 7.46e+01]    
29000     [2.67e+01, 8.86e-01, 2.72e+00]    [0.00e+00, 6.75e+01, 2.72e+00]    [1.60e+01, 6.75e+01, 1.63e+01, 4.45e+01]    
30000     [2.32e+01, 6.67e-01, 2.72e+00]    [0.00e+00, 9.02e+01, 2.72e+00]    [9.30e+00, 9.02e+01, 4.84e+00, 7.06e+01]    

Best model at step 30000:
  train loss: 2.66e+01
  test loss: 9.29e+01
  test metric: [9.30e+00, 9.02e+01, 4.84e+00, 7.06e+01]

'train' took 54.257359 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.224371 s

'compile' took 1.023235 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.71e+02, 2.95e+02, 2.70e+00]    [0.00e+00, 1.59e+02, 2.70e+00]    [1.59e+02, 1.59e+02, 1.18e+02, 1.18e+02]    
1000      [5.01e+01, 2.25e+01, 2.67e+00]    [0.00e+00, 6.80e+01, 2.67e+00]    [6.81e+01, 6.80e+01, 2.55e+01, 2.58e+01]    
2000      [3.32e+01, 1.76e+01, 2.67e+00]    [0.00e+00, 3.88e+01, 2.67e+00]    [3.90e+01, 3.88e+01, 1.85e+01, 1.88e+01]    
3000      [2.87e+01, 9.57e+00, 2.66e+00]    [0.00e+00, 2.99e+01, 2.66e+00]    [3.01e+01, 2.99e+01, 1.38e+01, 1.40e+01]    
4000      [2.80e+01, 1.40e+01, 2.66e+00]    [0.00e+00, 2.34e+01, 2.66e+00]    [2.36e+01, 2.34e+01, 1.60e+01, 1.63e+01]    
5000      [2.69e+01, 9.02e+00, 2.65e+00]    [0.00e+00, 2.08e+01, 2.65e+00]    [2.07e+01, 2.08e+01, 1.62e+01, 1.60e+01]    
6000      [2.66e+01, 9.25e+00, 2.65e+00]    [0.00e+00, 2.21e+01, 2.65e+00]    [2.20e+01, 2.21e+01, 1.51e+01, 1.50e+01]    
7000      [2.61e+01, 7.08e+00, 2.64e+00]    [0.00e+00, 1.99e+01, 2.64e+00]    [1.98e+01, 1.99e+01, 1.67e+01, 1.66e+01]    
8000      [2.91e+01, 1.37e+01, 2.64e+00]    [0.00e+00, 2.02e+01, 2.64e+00]    [2.01e+01, 2.02e+01, 1.68e+01, 1.67e+01]    
9000      [2.61e+01, 7.71e+00, 2.63e+00]    [0.00e+00, 2.12e+01, 2.63e+00]    [2.11e+01, 2.12e+01, 1.68e+01, 1.67e+01]    
10000     [2.84e+01, 1.16e+01, 2.63e+00]    [0.00e+00, 2.15e+01, 2.63e+00]    [2.15e+01, 2.15e+01, 1.65e+01, 1.64e+01]    
11000     [2.63e+01, 6.38e+00, 2.63e+00]    [0.00e+00, 2.14e+01, 2.63e+00]    [2.13e+01, 2.14e+01, 1.69e+01, 1.68e+01]    
12000     [2.81e+01, 1.16e+01, 2.63e+00]    [0.00e+00, 2.23e+01, 2.63e+00]    [2.22e+01, 2.23e+01, 1.63e+01, 1.63e+01]    
13000     [2.70e+01, 8.11e+00, 2.62e+00]    [0.00e+00, 2.18e+01, 2.62e+00]    [2.17e+01, 2.18e+01, 1.67e+01, 1.66e+01]    
14000     [2.75e+01, 9.37e+00, 2.62e+00]    [0.00e+00, 2.17e+01, 2.62e+00]    [2.16e+01, 2.17e+01, 1.66e+01, 1.65e+01]    
15000     [2.65e+01, 6.55e+00, 2.62e+00]    [0.00e+00, 2.15e+01, 2.62e+00]    [2.14e+01, 2.15e+01, 1.68e+01, 1.66e+01]    
16000     [2.55e+01, 2.51e+00, 2.62e+00]    [0.00e+00, 2.10e+01, 2.62e+00]    [2.09e+01, 2.10e+01, 1.70e+01, 1.68e+01]    
17000     [2.60e+01, 4.91e+00, 2.62e+00]    [0.00e+00, 2.15e+01, 2.62e+00]    [2.15e+01, 2.15e+01, 1.67e+01, 1.66e+01]    
18000     [2.61e+01, 4.53e+00, 2.61e+00]    [0.00e+00, 2.12e+01, 2.61e+00]    [2.11e+01, 2.12e+01, 1.66e+01, 1.64e+01]    
19000     [2.70e+01, 7.17e+00, 2.61e+00]    [0.00e+00, 2.16e+01, 2.61e+00]    [2.15e+01, 2.16e+01, 1.64e+01, 1.63e+01]    
20000     [2.57e+01, 3.71e+00, 2.61e+00]    [0.00e+00, 2.12e+01, 2.61e+00]    [2.12e+01, 2.12e+01, 1.65e+01, 1.63e+01]    
21000     [2.67e+01, 6.37e+00, 2.61e+00]    [0.00e+00, 2.09e+01, 2.61e+00]    [2.08e+01, 2.09e+01, 1.70e+01, 1.68e+01]    
22000     [2.80e+01, 1.09e+01, 2.60e+00]    [0.00e+00, 2.23e+01, 2.60e+00]    [2.22e+01, 2.23e+01, 1.63e+01, 1.62e+01]    
23000     [2.85e+01, 1.22e+01, 2.60e+00]    [0.00e+00, 2.24e+01, 2.60e+00]    [2.23e+01, 2.24e+01, 1.63e+01, 1.61e+01]    
24000     [2.83e+01, 1.20e+01, 2.60e+00]    [0.00e+00, 2.23e+01, 2.60e+00]    [2.22e+01, 2.23e+01, 1.65e+01, 1.63e+01]    
25000     [2.79e+01, 1.11e+01, 2.59e+00]    [0.00e+00, 2.24e+01, 2.59e+00]    [2.22e+01, 2.24e+01, 1.64e+01, 1.63e+01]    
26000     [2.48e+01, 3.27e+00, 2.59e+00]    [0.00e+00, 2.25e+01, 2.59e+00]    [2.23e+01, 2.25e+01, 1.63e+01, 1.61e+01]    
27000     [2.65e+01, 6.75e+00, 2.58e+00]    [0.00e+00, 2.18e+01, 2.58e+00]    [2.17e+01, 2.18e+01, 1.67e+01, 1.65e+01]    
28000     [2.56e+01, 5.13e+00, 2.58e+00]    [0.00e+00, 2.31e+01, 2.58e+00]    [2.30e+01, 2.31e+01, 1.60e+01, 1.58e+01]    
29000     [2.57e+01, 5.47e+00, 2.58e+00]    [0.00e+00, 2.26e+01, 2.58e+00]    [2.25e+01, 2.26e+01, 1.65e+01, 1.63e+01]    
30000     [2.71e+01, 8.06e+00, 2.57e+00]    [0.00e+00, 2.24e+01, 2.57e+00]    [2.23e+01, 2.24e+01, 1.64e+01, 1.62e+01]    

Best model at step 16000:
  train loss: 3.06e+01
  test loss: 2.36e+01
  test metric: [2.09e+01, 2.10e+01, 1.70e+01, 1.68e+01]

'train' took 55.423523 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.186092 s

'compile' took 0.931980 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.09e+02, 1.25e+03, 2.78e+00]    [0.00e+00, 1.59e+02, 2.78e+00]    [1.59e+02, 1.59e+02, 7.98e+01, 7.98e+01]    
1000      [5.26e+01, 3.28e+01, 2.76e+00]    [0.00e+00, 6.11e+01, 2.76e+00]    [6.11e+01, 6.11e+01, 2.15e+01, 2.16e+01]    
2000      [3.34e+01, 1.81e+01, 2.76e+00]    [0.00e+00, 3.26e+01, 2.76e+00]    [3.27e+01, 3.26e+01, 1.64e+01, 1.64e+01]    
3000      [3.17e+01, 1.49e+01, 2.76e+00]    [0.00e+00, 2.10e+01, 2.76e+00]    [2.12e+01, 2.10e+01, 1.14e+01, 1.12e+01]    
4000      [3.07e+01, 1.30e+01, 2.76e+00]    [0.00e+00, 1.79e+01, 2.76e+00]    [1.81e+01, 1.79e+01, 7.28e+00, 7.38e+00]    
5000      [3.17e+01, 1.91e+01, 2.76e+00]    [0.00e+00, 1.76e+01, 2.76e+00]    [1.81e+01, 1.76e+01, 1.02e+01, 1.07e+01]    
6000      [2.95e+01, 1.07e+01, 2.76e+00]    [0.00e+00, 1.79e+01, 2.76e+00]    [1.86e+01, 1.79e+01, 9.76e+00, 1.07e+01]    
7000      [2.85e+01, 9.21e+00, 2.76e+00]    [0.00e+00, 1.76e+01, 2.76e+00]    [1.85e+01, 1.76e+01, 1.07e+01, 1.17e+01]    
8000      [2.88e+01, 1.08e+01, 2.77e+00]    [0.00e+00, 1.73e+01, 2.77e+00]    [1.84e+01, 1.73e+01, 1.03e+01, 1.12e+01]    
9000      [2.94e+01, 1.34e+01, 2.77e+00]    [0.00e+00, 1.68e+01, 2.77e+00]    [1.81e+01, 1.68e+01, 1.04e+01, 1.12e+01]    
10000     [2.90e+01, 1.28e+01, 2.77e+00]    [0.00e+00, 1.63e+01, 2.77e+00]    [1.77e+01, 1.63e+01, 9.79e+00, 1.08e+01]    
11000     [2.76e+01, 8.08e+00, 2.77e+00]    [0.00e+00, 1.56e+01, 2.77e+00]    [1.72e+01, 1.56e+01, 1.01e+01, 1.11e+01]    
12000     [2.71e+01, 8.10e+00, 2.78e+00]    [0.00e+00, 1.49e+01, 2.78e+00]    [1.68e+01, 1.49e+01, 9.78e+00, 1.15e+01]    
13000     [2.65e+01, 5.10e+00, 2.78e+00]    [0.00e+00, 1.45e+01, 2.78e+00]    [1.73e+01, 1.45e+01, 8.72e+00, 1.12e+01]    
14000     [2.72e+01, 8.24e+00, 2.79e+00]    [0.00e+00, 1.37e+01, 2.79e+00]    [1.65e+01, 1.37e+01, 8.86e+00, 1.10e+01]    
15000     [2.78e+01, 9.57e+00, 2.79e+00]    [0.00e+00, 1.30e+01, 2.79e+00]    [1.61e+01, 1.30e+01, 8.60e+00, 1.10e+01]    
16000     [2.85e+01, 1.10e+01, 2.79e+00]    [0.00e+00, 1.23e+01, 2.79e+00]    [1.59e+01, 1.23e+01, 8.19e+00, 1.15e+01]    
17000     [2.66e+01, 5.97e+00, 2.80e+00]    [0.00e+00, 1.26e+01, 2.80e+00]    [1.51e+01, 1.26e+01, 8.58e+00, 1.00e+01]    
18000     [2.69e+01, 6.21e+00, 2.80e+00]    [0.00e+00, 1.19e+01, 2.80e+00]    [1.51e+01, 1.19e+01, 7.91e+00, 1.01e+01]    
19000     [2.67e+01, 5.75e+00, 2.80e+00]    [0.00e+00, 1.10e+01, 2.80e+00]    [1.52e+01, 1.10e+01, 6.94e+00, 1.03e+01]    
20000     [2.83e+01, 9.76e+00, 2.81e+00]    [0.00e+00, 1.07e+01, 2.81e+00]    [1.56e+01, 1.07e+01, 5.53e+00, 1.03e+01]    
21000     [2.81e+01, 9.33e+00, 2.81e+00]    [0.00e+00, 1.09e+01, 2.81e+00]    [1.43e+01, 1.09e+01, 7.38e+00, 9.99e+00]    
22000     [2.82e+01, 1.02e+01, 2.81e+00]    [0.00e+00, 1.00e+01, 2.81e+00]    [1.57e+01, 1.00e+01, 5.50e+00, 1.04e+01]    
23000     [2.70e+01, 6.31e+00, 2.82e+00]    [0.00e+00, 9.64e+00, 2.82e+00]    [1.58e+01, 9.64e+00, 5.67e+00, 9.71e+00]    
24000     [2.59e+01, 3.87e+00, 2.82e+00]    [0.00e+00, 9.53e+00, 2.82e+00]    [1.48e+01, 9.53e+00, 6.12e+00, 9.57e+00]    
25000     [2.62e+01, 3.85e+00, 2.82e+00]    [0.00e+00, 9.85e+00, 2.82e+00]    [1.37e+01, 9.85e+00, 6.88e+00, 9.68e+00]    
26000     [2.56e+01, 3.24e+00, 2.82e+00]    [0.00e+00, 9.64e+00, 2.82e+00]    [1.34e+01, 9.64e+00, 6.68e+00, 9.64e+00]    
27000     [2.64e+01, 4.51e+00, 2.83e+00]    [0.00e+00, 9.79e+00, 2.83e+00]    [1.37e+01, 9.79e+00, 6.63e+00, 8.88e+00]    
28000     [2.72e+01, 7.53e+00, 2.83e+00]    [0.00e+00, 1.04e+01, 2.83e+00]    [1.29e+01, 1.04e+01, 7.08e+00, 8.56e+00]    
29000     [2.54e+01, 1.16e+00, 2.83e+00]    [0.00e+00, 1.01e+01, 2.83e+00]    [1.36e+01, 1.01e+01, 6.31e+00, 9.02e+00]    
30000     [2.60e+01, 3.25e+00, 2.84e+00]    [0.00e+00, 1.02e+01, 2.84e+00]    [1.40e+01, 1.02e+01, 6.04e+00, 9.00e+00]    

Best model at step 29000:
  train loss: 2.94e+01
  test loss: 1.29e+01
  test metric: [1.36e+01, 1.01e+01, 6.31e+00, 9.02e+00]

'train' took 54.211611 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.219545 s

'compile' took 0.938355 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.83e+02, 2.34e+02, 2.71e+00]    [0.00e+00, 1.09e+02, 2.71e+00]    [1.09e+02, 1.09e+02, 4.62e+01, 4.62e+01]    
1000      [3.55e+01, 1.90e+01, 2.70e+00]    [0.00e+00, 4.31e+01, 2.70e+00]    [4.06e+01, 4.31e+01, 1.23e+01, 1.17e+01]    
2000      [3.22e+01, 1.58e+01, 2.70e+00]    [0.00e+00, 1.68e+01, 2.70e+00]    [2.11e+01, 1.68e+01, 1.23e+01, 1.33e+01]    
3000      [3.04e+01, 7.22e+00, 2.71e+00]    [0.00e+00, 1.11e+01, 2.71e+00]    [1.08e+01, 1.11e+01, 1.37e+01, 1.32e+01]    
4000      [3.09e+01, 3.64e+00, 2.71e+00]    [0.00e+00, 1.87e+01, 2.71e+00]    [1.30e+01, 1.87e+01, 1.39e+01, 2.02e+01]    
5000      [3.23e+01, 4.17e+00, 2.72e+00]    [0.00e+00, 1.02e+01, 2.72e+00]    [1.38e+01, 1.02e+01, 1.41e+01, 1.26e+01]    
6000      [3.09e+01, 2.43e+00, 2.72e+00]    [0.00e+00, 1.36e+01, 2.72e+00]    [1.30e+01, 1.36e+01, 1.29e+01, 1.19e+01]    
7000      [3.14e+01, 3.13e+00, 2.72e+00]    [0.00e+00, 1.28e+01, 2.72e+00]    [1.49e+01, 1.28e+01, 1.39e+01, 1.29e+01]    
8000      [2.93e+01, 1.23e+00, 2.72e+00]    [0.00e+00, 1.76e+01, 2.72e+00]    [1.14e+01, 1.76e+01, 1.43e+01, 1.31e+01]    
9000      [3.11e+01, 4.27e+00, 2.72e+00]    [0.00e+00, 2.47e+01, 2.72e+00]    [1.89e+01, 2.47e+01, 1.69e+01, 2.23e+01]    
10000     [2.84e+01, 5.09e-01, 2.73e+00]    [0.00e+00, 1.79e+01, 2.73e+00]    [1.27e+01, 1.79e+01, 1.41e+01, 1.37e+01]    
11000     [2.84e+01, 1.28e+00, 2.73e+00]    [0.00e+00, 1.91e+01, 2.73e+00]    [1.46e+01, 1.91e+01, 1.39e+01, 1.56e+01]    
12000     [3.22e+01, 5.76e+00, 2.73e+00]    [0.00e+00, 1.54e+01, 2.73e+00]    [1.79e+01, 1.54e+01, 1.65e+01, 1.40e+01]    
13000     [2.81e+01, 1.17e+00, 2.73e+00]    [0.00e+00, 1.39e+01, 2.73e+00]    [1.24e+01, 1.39e+01, 1.45e+01, 1.37e+01]    
14000     [2.89e+01, 2.83e+00, 2.73e+00]    [0.00e+00, 1.98e+01, 2.73e+00]    [1.70e+01, 1.98e+01, 1.51e+01, 1.64e+01]    
15000     [2.83e+01, 1.99e+00, 2.73e+00]    [0.00e+00, 1.26e+01, 2.73e+00]    [1.37e+01, 1.26e+01, 1.41e+01, 1.46e+01]    
16000     [2.72e+01, 8.69e-01, 2.74e+00]    [0.00e+00, 1.32e+01, 2.74e+00]    [1.29e+01, 1.32e+01, 1.50e+01, 1.44e+01]    
17000     [2.94e+01, 3.43e+00, 2.74e+00]    [0.00e+00, 1.77e+01, 2.74e+00]    [1.84e+01, 1.77e+01, 1.49e+01, 1.52e+01]    
18000     [2.93e+01, 3.92e+00, 2.74e+00]    [0.00e+00, 2.16e+01, 2.74e+00]    [2.18e+01, 2.16e+01, 1.87e+01, 1.86e+01]    
19000     [2.68e+01, 1.53e+00, 2.74e+00]    [0.00e+00, 1.29e+01, 2.74e+00]    [1.49e+01, 1.29e+01, 1.44e+01, 1.50e+01]    
20000     [2.72e+01, 2.11e+00, 2.75e+00]    [0.00e+00, 1.59e+01, 2.75e+00]    [1.53e+01, 1.59e+01, 1.44e+01, 1.41e+01]    
21000     [2.63e+01, 1.56e+00, 2.75e+00]    [0.00e+00, 1.48e+01, 2.75e+00]    [1.51e+01, 1.48e+01, 1.47e+01, 1.41e+01]    
22000     [2.57e+01, 8.02e-01, 2.75e+00]    [0.00e+00, 1.47e+01, 2.75e+00]    [1.62e+01, 1.47e+01, 1.44e+01, 1.43e+01]    
23000     [2.56e+01, 6.37e-01, 2.75e+00]    [0.00e+00, 1.42e+01, 2.75e+00]    [1.73e+01, 1.42e+01, 1.41e+01, 1.45e+01]    
24000     [3.12e+01, 8.40e+00, 2.75e+00]    [0.00e+00, 2.39e+01, 2.75e+00]    [2.82e+01, 2.39e+01, 2.66e+01, 2.11e+01]    
25000     [2.84e+01, 5.21e+00, 2.75e+00]    [0.00e+00, 1.72e+01, 2.75e+00]    [2.31e+01, 1.72e+01, 1.77e+01, 1.42e+01]    
26000     [2.78e+01, 4.37e+00, 2.76e+00]    [0.00e+00, 1.67e+01, 2.76e+00]    [2.40e+01, 1.67e+01, 1.81e+01, 1.44e+01]    
27000     [2.53e+01, 1.70e+00, 2.76e+00]    [0.00e+00, 2.01e+01, 2.76e+00]    [1.69e+01, 2.01e+01, 1.58e+01, 1.71e+01]    
28000     [2.51e+01, 1.35e+00, 2.76e+00]    [0.00e+00, 1.68e+01, 2.76e+00]    [2.00e+01, 1.68e+01, 1.48e+01, 1.43e+01]    
29000     [2.49e+01, 1.68e+00, 2.76e+00]    [0.00e+00, 1.85e+01, 2.76e+00]    [1.96e+01, 1.85e+01, 1.52e+01, 1.52e+01]    
30000     [2.80e+01, 5.60e+00, 2.76e+00]    [0.00e+00, 2.69e+01, 2.76e+00]    [2.24e+01, 2.69e+01, 1.57e+01, 2.87e+01]    

Best model at step 23000:
  train loss: 2.90e+01
  test loss: 1.70e+01
  test metric: [1.73e+01, 1.42e+01, 1.41e+01, 1.45e+01]

'train' took 48.981060 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.181886 s

'compile' took 0.876766 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.30e+02, 3.77e+02, 2.71e+00]    [0.00e+00, 1.02e+02, 2.71e+00]    [1.02e+02, 1.02e+02, 2.87e+01, 2.87e+01]    
1000      [4.40e+01, 3.23e+01, 2.68e+00]    [0.00e+00, 4.75e+01, 2.68e+00]    [4.74e+01, 4.75e+01, 1.92e+01, 1.90e+01]    
2000      [3.24e+01, 1.76e+01, 2.69e+00]    [0.00e+00, 2.95e+01, 2.69e+00]    [2.94e+01, 2.95e+01, 1.18e+01, 1.13e+01]    
3000      [2.99e+01, 1.28e+01, 2.69e+00]    [0.00e+00, 2.50e+01, 2.69e+00]    [2.49e+01, 2.50e+01, 1.25e+01, 1.20e+01]    
4000      [2.88e+01, 1.17e+01, 2.69e+00]    [0.00e+00, 2.39e+01, 2.69e+00]    [2.38e+01, 2.39e+01, 1.36e+01, 1.30e+01]    
5000      [2.76e+01, 7.05e+00, 2.69e+00]    [0.00e+00, 2.38e+01, 2.69e+00]    [2.37e+01, 2.38e+01, 1.35e+01, 1.30e+01]    
6000      [2.70e+01, 6.76e+00, 2.69e+00]    [0.00e+00, 2.25e+01, 2.69e+00]    [2.24e+01, 2.25e+01, 1.48e+01, 1.40e+01]    
7000      [2.68e+01, 7.20e+00, 2.69e+00]    [0.00e+00, 2.18e+01, 2.69e+00]    [2.17e+01, 2.18e+01, 1.52e+01, 1.43e+01]    
8000      [2.84e+01, 1.37e+01, 2.69e+00]    [0.00e+00, 2.04e+01, 2.69e+00]    [2.13e+01, 2.04e+01, 1.50e+01, 1.53e+01]    
9000      [2.97e+01, 1.81e+01, 2.68e+00]    [0.00e+00, 2.09e+01, 2.68e+00]    [2.08e+01, 2.09e+01, 1.56e+01, 1.48e+01]    
10000     [2.99e+01, 1.80e+01, 2.68e+00]    [0.00e+00, 2.01e+01, 2.68e+00]    [2.05e+01, 2.01e+01, 1.55e+01, 1.53e+01]    
11000     [2.71e+01, 1.13e+01, 2.68e+00]    [0.00e+00, 1.97e+01, 2.68e+00]    [1.97e+01, 1.97e+01, 1.61e+01, 1.55e+01]    
12000     [2.57e+01, 6.89e+00, 2.68e+00]    [0.00e+00, 1.85e+01, 2.68e+00]    [1.98e+01, 1.85e+01, 1.56e+01, 1.63e+01]    
13000     [2.52e+01, 5.09e+00, 2.68e+00]    [0.00e+00, 1.81e+01, 2.68e+00]    [1.93e+01, 1.81e+01, 1.59e+01, 1.65e+01]    
14000     [2.59e+01, 7.97e+00, 2.68e+00]    [0.00e+00, 1.76e+01, 2.68e+00]    [1.92e+01, 1.76e+01, 1.58e+01, 1.67e+01]    
15000     [2.64e+01, 1.07e+01, 2.68e+00]    [0.00e+00, 1.78e+01, 2.68e+00]    [1.87e+01, 1.78e+01, 1.63e+01, 1.66e+01]    
16000     [2.56e+01, 7.14e+00, 2.68e+00]    [0.00e+00, 1.75e+01, 2.68e+00]    [1.82e+01, 1.75e+01, 1.67e+01, 1.69e+01]    
17000     [2.63e+01, 1.07e+01, 2.68e+00]    [0.00e+00, 1.73e+01, 2.68e+00]    [1.78e+01, 1.73e+01, 1.71e+01, 1.71e+01]    
18000     [2.56e+01, 7.92e+00, 2.68e+00]    [0.00e+00, 1.64e+01, 2.68e+00]    [1.83e+01, 1.64e+01, 1.66e+01, 1.77e+01]    
19000     [2.64e+01, 1.16e+01, 2.68e+00]    [0.00e+00, 1.68e+01, 2.68e+00]    [1.77e+01, 1.68e+01, 1.72e+01, 1.75e+01]    
20000     [2.69e+01, 1.17e+01, 2.68e+00]    [0.00e+00, 1.65e+01, 2.68e+00]    [1.78e+01, 1.65e+01, 1.71e+01, 1.76e+01]    
21000     [2.75e+01, 1.35e+01, 2.68e+00]    [0.00e+00, 1.69e+01, 2.68e+00]    [1.82e+01, 1.69e+01, 1.68e+01, 1.72e+01]    
22000     [2.44e+01, 4.24e+00, 2.68e+00]    [0.00e+00, 1.68e+01, 2.68e+00]    [1.76e+01, 1.68e+01, 1.74e+01, 1.74e+01]    
23000     [2.96e+01, 1.96e+01, 2.68e+00]    [0.00e+00, 1.73e+01, 2.68e+00]    [1.83e+01, 1.73e+01, 1.71e+01, 1.72e+01]    
24000     [2.98e+01, 2.05e+01, 2.68e+00]    [0.00e+00, 1.82e+01, 2.68e+00]    [1.92e+01, 1.82e+01, 1.68e+01, 1.65e+01]    
25000     [2.67e+01, 1.18e+01, 2.68e+00]    [0.00e+00, 1.75e+01, 2.68e+00]    [1.84e+01, 1.75e+01, 1.72e+01, 1.72e+01]    
26000     [2.70e+01, 1.27e+01, 2.68e+00]    [0.00e+00, 1.77e+01, 2.68e+00]    [1.86e+01, 1.77e+01, 1.72e+01, 1.73e+01]    
27000     [2.51e+01, 7.44e+00, 2.68e+00]    [0.00e+00, 1.85e+01, 2.68e+00]    [1.94e+01, 1.85e+01, 1.68e+01, 1.64e+01]    
28000     [2.45e+01, 6.10e+00, 2.68e+00]    [0.00e+00, 1.79e+01, 2.68e+00]    [1.80e+01, 1.79e+01, 1.79e+01, 1.72e+01]    
29000     [2.55e+01, 9.31e+00, 2.68e+00]    [0.00e+00, 1.80e+01, 2.68e+00]    [1.90e+01, 1.80e+01, 1.71e+01, 1.70e+01]    
30000     [2.58e+01, 1.00e+01, 2.69e+00]    [0.00e+00, 1.79e+01, 2.69e+00]    [1.89e+01, 1.79e+01, 1.74e+01, 1.74e+01]    

Best model at step 22000:
  train loss: 3.13e+01
  test loss: 1.95e+01
  test metric: [1.76e+01, 1.68e+01, 1.74e+01, 1.74e+01]

'train' took 50.682887 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[21.172363587812768, 301.9862599762019, 17.78115339246713, 23.919704505870712, 27.124560747492595, 90.22112867690838, 20.996794329346145, 10.069898596171754, 14.222724740473772, 16.798190135476954]
sigma_y 8 54.429277868822204 85.3282814528134
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.186426 s

'compile' took 0.826844 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.83e+02, 3.58e+02, 2.65e+00]    [0.00e+00, 4.46e+02, 2.65e+00]    [4.46e+02, 4.46e+02, 4.10e+02, 4.10e+02]    
1000      [3.94e+01, 2.44e+01, 2.66e+00]    [0.00e+00, 3.33e+01, 2.66e+00]    [3.35e+01, 3.33e+01, 2.67e+01, 2.66e+01]    
2000      [3.36e+01, 1.12e+01, 2.67e+00]    [0.00e+00, 2.07e+01, 2.67e+00]    [2.10e+01, 2.07e+01, 1.31e+01, 1.29e+01]    
3000      [3.15e+01, 1.02e+01, 2.67e+00]    [0.00e+00, 1.47e+01, 2.67e+00]    [1.51e+01, 1.47e+01, 1.19e+01, 1.20e+01]    
4000      [3.01e+01, 9.96e+00, 2.67e+00]    [0.00e+00, 1.10e+01, 2.67e+00]    [1.16e+01, 1.10e+01, 1.21e+01, 1.23e+01]    
5000      [2.97e+01, 6.05e+00, 2.68e+00]    [0.00e+00, 8.57e+00, 2.68e+00]    [8.54e+00, 8.57e+00, 1.35e+01, 1.33e+01]    
6000      [3.18e+01, 1.39e+01, 2.68e+00]    [0.00e+00, 9.89e+00, 2.68e+00]    [9.86e+00, 9.89e+00, 1.30e+01, 1.28e+01]    
7000      [3.07e+01, 1.06e+01, 2.68e+00]    [0.00e+00, 1.08e+01, 2.68e+00]    [1.09e+01, 1.08e+01, 1.25e+01, 1.23e+01]    
8000      [3.11e+01, 8.42e+00, 2.69e+00]    [0.00e+00, 9.68e+00, 2.69e+00]    [9.90e+00, 9.68e+00, 1.29e+01, 1.28e+01]    
9000      [2.95e+01, 6.47e+00, 2.69e+00]    [0.00e+00, 1.11e+01, 2.69e+00]    [1.10e+01, 1.11e+01, 1.25e+01, 1.24e+01]    
10000     [3.16e+01, 9.74e+00, 2.69e+00]    [0.00e+00, 1.07e+01, 2.69e+00]    [1.09e+01, 1.07e+01, 1.22e+01, 1.23e+01]    
11000     [2.97e+01, 8.73e+00, 2.69e+00]    [0.00e+00, 1.26e+01, 2.69e+00]    [1.34e+01, 1.26e+01, 1.12e+01, 1.19e+01]    
12000     [2.99e+01, 6.19e+00, 2.69e+00]    [0.00e+00, 1.15e+01, 2.69e+00]    [1.20e+01, 1.15e+01, 1.20e+01, 1.24e+01]    
13000     [2.94e+01, 4.29e+00, 2.70e+00]    [0.00e+00, 1.18e+01, 2.70e+00]    [1.28e+01, 1.18e+01, 1.17e+01, 1.24e+01]    
14000     [2.88e+01, 8.64e+00, 2.70e+00]    [0.00e+00, 1.26e+01, 2.70e+00]    [1.37e+01, 1.26e+01, 1.17e+01, 1.25e+01]    
15000     [2.94e+01, 5.29e+00, 2.70e+00]    [0.00e+00, 1.22e+01, 2.70e+00]    [1.25e+01, 1.22e+01, 1.25e+01, 1.25e+01]    
16000     [2.80e+01, 5.78e+00, 2.70e+00]    [0.00e+00, 1.29e+01, 2.70e+00]    [1.43e+01, 1.29e+01, 1.18e+01, 1.28e+01]    
17000     [3.00e+01, 1.13e+01, 2.70e+00]    [0.00e+00, 1.34e+01, 2.70e+00]    [1.43e+01, 1.34e+01, 1.23e+01, 1.28e+01]    
18000     [2.76e+01, 2.07e+00, 2.70e+00]    [0.00e+00, 1.31e+01, 2.70e+00]    [1.33e+01, 1.31e+01, 1.30e+01, 1.27e+01]    
19000     [2.83e+01, 5.12e+00, 2.70e+00]    [0.00e+00, 1.30e+01, 2.70e+00]    [1.36e+01, 1.30e+01, 1.29e+01, 1.28e+01]    
20000     [2.78e+01, 6.35e+00, 2.70e+00]    [0.00e+00, 1.37e+01, 2.70e+00]    [1.48e+01, 1.37e+01, 1.26e+01, 1.29e+01]    
21000     [2.67e+01, 2.94e+00, 2.70e+00]    [0.00e+00, 1.36e+01, 2.70e+00]    [1.47e+01, 1.36e+01, 1.28e+01, 1.30e+01]    
22000     [2.90e+01, 7.41e+00, 2.70e+00]    [0.00e+00, 1.35e+01, 2.70e+00]    [1.35e+01, 1.35e+01, 1.36e+01, 1.30e+01]    
23000     [2.70e+01, 3.32e+00, 2.70e+00]    [0.00e+00, 1.36e+01, 2.70e+00]    [1.44e+01, 1.36e+01, 1.33e+01, 1.31e+01]    
24000     [2.68e+01, 2.48e+00, 2.70e+00]    [0.00e+00, 1.41e+01, 2.70e+00]    [1.40e+01, 1.41e+01, 1.39e+01, 1.35e+01]    
25000     [2.61e+01, 1.14e+00, 2.70e+00]    [0.00e+00, 1.40e+01, 2.70e+00]    [1.40e+01, 1.40e+01, 1.42e+01, 1.33e+01]    
26000     [2.90e+01, 1.10e+01, 2.70e+00]    [0.00e+00, 1.49e+01, 2.70e+00]    [1.49e+01, 1.49e+01, 1.40e+01, 1.31e+01]    
27000     [2.76e+01, 7.78e+00, 2.70e+00]    [0.00e+00, 1.46e+01, 2.70e+00]    [1.55e+01, 1.46e+01, 1.37e+01, 1.34e+01]    
28000     [2.76e+01, 7.59e+00, 2.70e+00]    [0.00e+00, 1.50e+01, 2.70e+00]    [1.49e+01, 1.50e+01, 1.44e+01, 1.34e+01]    
29000     [2.80e+01, 8.37e+00, 2.70e+00]    [0.00e+00, 1.42e+01, 2.70e+00]    [1.44e+01, 1.42e+01, 1.48e+01, 1.37e+01]    
30000     [2.56e+01, 2.46e+00, 2.70e+00]    [0.00e+00, 1.48e+01, 2.70e+00]    [1.58e+01, 1.48e+01, 1.42e+01, 1.36e+01]    

Best model at step 25000:
  train loss: 2.99e+01
  test loss: 1.67e+01
  test metric: [1.40e+01, 1.40e+01, 1.42e+01, 1.33e+01]

'train' took 46.566647 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.187148 s

'compile' took 0.818546 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.30e+02, 1.70e+02, 2.74e+00]    [0.00e+00, 8.63e+02, 2.74e+00]    [8.63e+02, 8.63e+02, 1.14e+03, 1.14e+03]    
1000      [2.87e+01, 1.23e+01, 2.76e+00]    [0.00e+00, 2.43e+01, 2.76e+00]    [8.57e+00, 2.43e+01, 3.55e+00, 2.48e+01]    
2000      [2.85e+01, 8.25e+00, 2.76e+00]    [0.00e+00, 1.46e+01, 2.76e+00]    [7.66e+00, 1.46e+01, 6.47e+00, 2.19e+01]    
3000      [3.07e+01, 6.79e+00, 2.75e+00]    [0.00e+00, 1.16e+01, 2.75e+00]    [1.15e+01, 1.16e+01, 1.26e+01, 1.07e+01]    
4000      [2.91e+01, 4.06e+00, 2.74e+00]    [0.00e+00, 1.62e+01, 2.74e+00]    [9.05e+00, 1.62e+01, 6.35e+00, 2.16e+01]    
5000      [3.04e+01, 4.61e+00, 2.74e+00]    [0.00e+00, 1.10e+01, 2.74e+00]    [8.78e+00, 1.10e+01, 9.05e+00, 1.15e+01]    
6000      [3.61e+01, 8.19e+00, 2.73e+00]    [0.00e+00, 4.29e+01, 2.73e+00]    [2.77e+01, 4.29e+01, 3.84e+01, 5.49e+01]    
7000      [3.19e+01, 4.41e+00, 2.72e+00]    [0.00e+00, 9.76e+00, 2.72e+00]    [1.38e+01, 9.76e+00, 1.34e+01, 5.10e+00]    
8000      [2.88e+01, 2.83e+00, 2.72e+00]    [0.00e+00, 1.47e+01, 2.72e+00]    [6.21e+00, 1.47e+01, 5.50e+00, 1.72e+01]    
9000      [2.92e+01, 2.85e+00, 2.71e+00]    [0.00e+00, 1.26e+01, 2.71e+00]    [6.10e+00, 1.26e+01, 6.39e+00, 1.44e+01]    
10000     [2.98e+01, 2.53e+00, 2.71e+00]    [0.00e+00, 1.01e+01, 2.71e+00]    [9.41e+00, 1.01e+01, 8.31e+00, 9.37e+00]    
11000     [2.85e+01, 2.70e+00, 2.71e+00]    [0.00e+00, 2.25e+01, 2.71e+00]    [6.18e+00, 2.25e+01, 7.93e+00, 2.32e+01]    
12000     [3.21e+01, 5.25e+00, 2.70e+00]    [0.00e+00, 3.53e+01, 2.70e+00]    [1.83e+01, 3.53e+01, 2.31e+01, 3.86e+01]    
13000     [2.75e+01, 2.48e+00, 2.70e+00]    [0.00e+00, 2.23e+01, 2.70e+00]    [6.53e+00, 2.23e+01, 6.12e+00, 1.98e+01]    
14000     [2.91e+01, 2.24e+00, 2.70e+00]    [0.00e+00, 9.46e+00, 2.70e+00]    [1.17e+01, 9.46e+00, 9.11e+00, 6.93e+00]    
15000     [2.85e+01, 3.13e+00, 2.70e+00]    [0.00e+00, 2.62e+01, 2.70e+00]    [1.19e+01, 2.62e+01, 1.14e+01, 2.08e+01]    
16000     [2.69e+01, 2.01e+00, 2.70e+00]    [0.00e+00, 1.95e+01, 2.70e+00]    [7.81e+00, 1.95e+01, 5.99e+00, 1.16e+01]    
17000     [2.71e+01, 2.14e+00, 2.70e+00]    [0.00e+00, 1.90e+01, 2.70e+00]    [9.45e+00, 1.90e+01, 8.37e+00, 1.11e+01]    
18000     [2.79e+01, 2.27e+00, 2.69e+00]    [0.00e+00, 1.76e+01, 2.69e+00]    [1.20e+01, 1.76e+01, 9.49e+00, 1.01e+01]    
19000     [2.53e+01, 6.20e-01, 2.69e+00]    [0.00e+00, 1.30e+01, 2.69e+00]    [4.74e+00, 1.30e+01, 3.25e+00, 9.38e+00]    
20000     [2.91e+01, 3.16e+00, 2.69e+00]    [0.00e+00, 3.00e+01, 2.69e+00]    [1.64e+01, 3.00e+01, 1.42e+01, 2.44e+01]    
21000     [2.97e+01, 3.77e+00, 2.69e+00]    [0.00e+00, 1.58e+01, 2.69e+00]    [1.90e+01, 1.58e+01, 1.98e+01, 8.30e+00]    
22000     [2.75e+01, 2.21e+00, 2.69e+00]    [0.00e+00, 1.35e+01, 2.69e+00]    [1.39e+01, 1.35e+01, 1.25e+01, 9.77e+00]    
23000     [2.67e+01, 1.67e+00, 2.69e+00]    [0.00e+00, 1.61e+01, 2.69e+00]    [1.26e+01, 1.61e+01, 1.06e+01, 1.31e+01]    
24000     [2.61e+01, 1.53e+00, 2.69e+00]    [0.00e+00, 2.04e+01, 2.69e+00]    [1.03e+01, 2.04e+01, 7.75e+00, 1.71e+01]    
25000     [2.42e+01, 5.19e-01, 2.69e+00]    [0.00e+00, 3.07e+01, 2.69e+00]    [4.68e+00, 3.07e+01, 4.26e+00, 2.93e+01]    
26000     [2.40e+01, 1.64e-01, 2.68e+00]    [0.00e+00, 3.28e+01, 2.68e+00]    [5.65e+00, 3.28e+01, 3.81e+00, 3.25e+01]    
27000     [2.56e+01, 1.23e+00, 2.68e+00]    [0.00e+00, 2.57e+01, 2.68e+00]    [9.36e+00, 2.57e+01, 7.12e+00, 2.42e+01]    
28000     [2.39e+01, 2.35e-01, 2.68e+00]    [0.00e+00, 3.48e+01, 2.68e+00]    [4.71e+00, 3.48e+01, 4.11e+00, 3.63e+01]    
29000     [2.50e+01, 1.04e+00, 2.68e+00]    [0.00e+00, 3.03e+01, 2.68e+00]    [8.05e+00, 3.03e+01, 4.74e+00, 2.92e+01]    
30000     [2.57e+01, 2.35e+00, 2.68e+00]    [0.00e+00, 4.48e+01, 2.68e+00]    [1.12e+01, 4.48e+01, 9.10e+00, 4.93e+01]    

Best model at step 28000:
  train loss: 2.68e+01
  test loss: 3.75e+01
  test metric: [4.71e+00, 3.48e+01, 4.11e+00, 3.63e+01]

'train' took 46.324206 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.183656 s

'compile' took 0.804905 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.13e+02, 8.74e+02, 2.72e+00]    [0.00e+00, 1.18e+02, 2.72e+00]    [1.18e+02, 1.18e+02, 8.67e+01, 8.67e+01]    
1000      [3.90e+01, 2.95e+01, 2.71e+00]    [0.00e+00, 4.30e+01, 2.71e+00]    [4.33e+01, 4.30e+01, 1.87e+01, 1.93e+01]    
2000      [3.18e+01, 2.29e+01, 2.71e+00]    [0.00e+00, 2.23e+01, 2.71e+00]    [2.30e+01, 2.23e+01, 1.32e+01, 1.40e+01]    
3000      [3.36e+01, 2.23e+01, 2.71e+00]    [0.00e+00, 1.85e+01, 2.71e+00]    [1.80e+01, 1.85e+01, 1.27e+01, 1.16e+01]    
4000      [3.14e+01, 1.75e+01, 2.71e+00]    [0.00e+00, 1.70e+01, 2.71e+00]    [1.64e+01, 1.70e+01, 1.25e+01, 1.17e+01]    
5000      [2.97e+01, 8.68e+00, 2.71e+00]    [0.00e+00, 1.45e+01, 2.71e+00]    [1.37e+01, 1.45e+01, 1.39e+01, 1.28e+01]    
6000      [3.00e+01, 9.90e+00, 2.71e+00]    [0.00e+00, 1.51e+01, 2.71e+00]    [1.43e+01, 1.51e+01, 1.32e+01, 1.20e+01]    
7000      [3.28e+01, 1.60e+01, 2.71e+00]    [0.00e+00, 1.53e+01, 2.71e+00]    [1.44e+01, 1.53e+01, 1.25e+01, 1.14e+01]    
8000      [2.86e+01, 5.89e+00, 2.70e+00]    [0.00e+00, 1.64e+01, 2.70e+00]    [1.56e+01, 1.64e+01, 1.18e+01, 1.12e+01]    
9000      [2.88e+01, 4.88e+00, 2.70e+00]    [0.00e+00, 1.74e+01, 2.70e+00]    [1.65e+01, 1.74e+01, 1.16e+01, 1.14e+01]    
10000     [3.16e+01, 1.60e+01, 2.70e+00]    [0.00e+00, 1.63e+01, 2.70e+00]    [1.54e+01, 1.63e+01, 1.24e+01, 1.18e+01]    
11000     [3.12e+01, 1.15e+01, 2.70e+00]    [0.00e+00, 1.60e+01, 2.70e+00]    [1.52e+01, 1.60e+01, 1.25e+01, 1.16e+01]    
12000     [2.91e+01, 5.79e+00, 2.70e+00]    [0.00e+00, 1.54e+01, 2.70e+00]    [1.45e+01, 1.54e+01, 1.29e+01, 1.19e+01]    
13000     [2.92e+01, 7.12e+00, 2.70e+00]    [0.00e+00, 1.68e+01, 2.70e+00]    [1.60e+01, 1.68e+01, 1.21e+01, 1.14e+01]    
14000     [3.02e+01, 8.95e+00, 2.69e+00]    [0.00e+00, 1.63e+01, 2.69e+00]    [1.55e+01, 1.63e+01, 1.23e+01, 1.15e+01]    
15000     [2.92e+01, 9.06e+00, 2.69e+00]    [0.00e+00, 1.65e+01, 2.69e+00]    [1.57e+01, 1.65e+01, 1.23e+01, 1.18e+01]    
16000     [2.82e+01, 6.55e+00, 2.69e+00]    [0.00e+00, 1.61e+01, 2.69e+00]    [1.53e+01, 1.61e+01, 1.25e+01, 1.19e+01]    
17000     [2.85e+01, 4.33e+00, 2.69e+00]    [0.00e+00, 1.55e+01, 2.69e+00]    [1.47e+01, 1.55e+01, 1.26e+01, 1.19e+01]    
18000     [2.84e+01, 4.74e+00, 2.69e+00]    [0.00e+00, 1.57e+01, 2.69e+00]    [1.49e+01, 1.57e+01, 1.24e+01, 1.17e+01]    
19000     [2.73e+01, 3.20e+00, 2.69e+00]    [0.00e+00, 1.64e+01, 2.69e+00]    [1.56e+01, 1.64e+01, 1.22e+01, 1.17e+01]    
20000     [2.88e+01, 8.11e+00, 2.68e+00]    [0.00e+00, 1.57e+01, 2.68e+00]    [1.49e+01, 1.57e+01, 1.26e+01, 1.20e+01]    
21000     [3.15e+01, 1.08e+01, 2.68e+00]    [0.00e+00, 1.50e+01, 2.68e+00]    [1.42e+01, 1.50e+01, 1.27e+01, 1.17e+01]    
22000     [2.75e+01, 6.20e+00, 2.68e+00]    [0.00e+00, 1.57e+01, 2.68e+00]    [1.48e+01, 1.57e+01, 1.24e+01, 1.18e+01]    
23000     [2.83e+01, 7.34e+00, 2.68e+00]    [0.00e+00, 1.49e+01, 2.68e+00]    [1.41e+01, 1.49e+01, 1.26e+01, 1.18e+01]    
24000     [3.08e+01, 9.41e+00, 2.68e+00]    [0.00e+00, 1.51e+01, 2.68e+00]    [1.43e+01, 1.51e+01, 1.22e+01, 1.14e+01]    
25000     [2.74e+01, 5.03e+00, 2.67e+00]    [0.00e+00, 1.43e+01, 2.67e+00]    [1.35e+01, 1.43e+01, 1.24e+01, 1.15e+01]    
26000     [2.94e+01, 7.53e+00, 2.67e+00]    [0.00e+00, 1.48e+01, 2.67e+00]    [1.39e+01, 1.48e+01, 1.20e+01, 1.11e+01]    
27000     [2.81e+01, 5.23e+00, 2.67e+00]    [0.00e+00, 1.44e+01, 2.67e+00]    [1.36e+01, 1.44e+01, 1.20e+01, 1.10e+01]    
28000     [2.89e+01, 6.81e+00, 2.66e+00]    [0.00e+00, 1.42e+01, 2.66e+00]    [1.33e+01, 1.42e+01, 1.19e+01, 1.08e+01]    
29000     [2.79e+01, 5.17e+00, 2.66e+00]    [0.00e+00, 1.41e+01, 2.66e+00]    [1.32e+01, 1.41e+01, 1.17e+01, 1.07e+01]    
30000     [2.90e+01, 1.02e+01, 2.66e+00]    [0.00e+00, 1.44e+01, 2.66e+00]    [1.35e+01, 1.44e+01, 1.17e+01, 1.09e+01]    

Best model at step 19000:
  train loss: 3.32e+01
  test loss: 1.91e+01
  test metric: [1.56e+01, 1.64e+01, 1.22e+01, 1.17e+01]

'train' took 47.151677 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.170190 s

'compile' took 0.814128 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.58e+02, 6.85e+02, 2.75e+00]    [0.00e+00, 4.51e+02, 2.75e+00]    [4.51e+02, 4.51e+02, 6.92e+02, 6.92e+02]    
1000      [3.88e+01, 2.64e+01, 2.73e+00]    [0.00e+00, 3.06e+01, 2.73e+00]    [3.11e+01, 3.06e+01, 1.41e+01, 1.49e+01]    
2000      [3.36e+01, 1.83e+01, 2.73e+00]    [0.00e+00, 1.81e+01, 2.73e+00]    [1.99e+01, 1.81e+01, 1.22e+01, 1.20e+01]    
3000      [3.48e+01, 1.76e+01, 2.74e+00]    [0.00e+00, 1.33e+01, 2.74e+00]    [1.11e+01, 1.33e+01, 3.97e+00, 6.80e+00]    
4000      [3.39e+01, 1.34e+01, 2.74e+00]    [0.00e+00, 1.42e+01, 2.74e+00]    [1.66e+01, 1.42e+01, 1.70e+01, 1.29e+01]    
5000      [3.07e+01, 4.48e+00, 2.74e+00]    [0.00e+00, 1.08e+01, 2.74e+00]    [1.29e+01, 1.08e+01, 1.13e+01, 8.00e+00]    
6000      [3.51e+01, 1.37e+01, 2.74e+00]    [0.00e+00, 9.18e+00, 2.74e+00]    [6.71e+00, 9.18e+00, 7.20e+00, 8.51e+00]    
7000      [3.14e+01, 4.26e+00, 2.74e+00]    [0.00e+00, 1.14e+01, 2.74e+00]    [1.36e+01, 1.14e+01, 1.60e+01, 1.31e+01]    
8000      [3.27e+01, 7.87e+00, 2.74e+00]    [0.00e+00, 8.03e+00, 2.74e+00]    [9.71e+00, 8.03e+00, 7.26e+00, 9.16e+00]    
9000      [3.35e+01, 7.53e+00, 2.74e+00]    [0.00e+00, 1.38e+01, 2.74e+00]    [1.50e+01, 1.38e+01, 1.88e+01, 1.52e+01]    
10000     [3.30e+01, 7.38e+00, 2.74e+00]    [0.00e+00, 1.33e+01, 2.74e+00]    [1.48e+01, 1.33e+01, 1.70e+01, 1.38e+01]    
11000     [3.53e+01, 1.14e+01, 2.74e+00]    [0.00e+00, 1.49e+01, 2.74e+00]    [1.62e+01, 1.49e+01, 2.09e+01, 1.74e+01]    
12000     [2.96e+01, 2.62e+00, 2.74e+00]    [0.00e+00, 9.59e+00, 2.74e+00]    [1.08e+01, 9.59e+00, 1.00e+01, 8.72e+00]    
13000     [3.30e+01, 9.62e+00, 2.73e+00]    [0.00e+00, 1.14e+01, 2.73e+00]    [7.66e+00, 1.14e+01, 5.92e+00, 8.08e+00]    
14000     [2.94e+01, 1.16e+00, 2.73e+00]    [0.00e+00, 1.06e+01, 2.73e+00]    [1.15e+01, 1.06e+01, 7.29e+00, 7.23e+00]    
15000     [3.30e+01, 9.99e+00, 2.73e+00]    [0.00e+00, 1.34e+01, 2.73e+00]    [9.20e+00, 1.34e+01, 4.72e+00, 7.96e+00]    
16000     [2.93e+01, 1.93e+00, 2.73e+00]    [0.00e+00, 1.18e+01, 2.73e+00]    [1.23e+01, 1.18e+01, 4.98e+00, 7.42e+00]    
17000     [2.99e+01, 4.08e+00, 2.73e+00]    [0.00e+00, 8.99e+00, 2.73e+00]    [7.72e+00, 8.99e+00, 4.78e+00, 7.74e+00]    
18000     [2.85e+01, 2.02e+00, 2.73e+00]    [0.00e+00, 1.06e+01, 2.73e+00]    [1.07e+01, 1.06e+01, 6.69e+00, 7.03e+00]    
19000     [2.88e+01, 2.28e+00, 2.73e+00]    [0.00e+00, 9.96e+00, 2.73e+00]    [9.80e+00, 9.96e+00, 7.53e+00, 7.01e+00]    
20000     [2.84e+01, 1.86e+00, 2.73e+00]    [0.00e+00, 1.12e+01, 2.73e+00]    [1.08e+01, 1.12e+01, 5.76e+00, 7.32e+00]    
21000     [3.13e+01, 6.45e+00, 2.73e+00]    [0.00e+00, 1.47e+01, 2.73e+00]    [1.37e+01, 1.47e+01, 9.09e+00, 7.05e+00]    
22000     [2.95e+01, 3.71e+00, 2.73e+00]    [0.00e+00, 1.40e+01, 2.73e+00]    [1.27e+01, 1.40e+01, 5.53e+00, 7.65e+00]    
23000     [3.13e+01, 7.88e+00, 2.73e+00]    [0.00e+00, 1.40e+01, 2.73e+00]    [1.22e+01, 1.40e+01, 1.13e+01, 7.44e+00]    
24000     [3.05e+01, 6.85e+00, 2.73e+00]    [0.00e+00, 1.43e+01, 2.73e+00]    [1.21e+01, 1.43e+01, 9.65e+00, 7.03e+00]    
25000     [2.80e+01, 2.37e+00, 2.73e+00]    [0.00e+00, 1.59e+01, 2.73e+00]    [8.59e+00, 1.59e+01, 2.90e+00, 8.18e+00]    
26000     [2.81e+01, 3.45e+00, 2.73e+00]    [0.00e+00, 1.70e+01, 2.73e+00]    [8.03e+00, 1.70e+01, 3.52e+00, 8.37e+00]    
27000     [2.93e+01, 5.48e+00, 2.73e+00]    [0.00e+00, 1.40e+01, 2.73e+00]    [1.01e+01, 1.40e+01, 8.32e+00, 7.61e+00]    
28000     [2.67e+01, 1.68e+00, 2.73e+00]    [0.00e+00, 1.58e+01, 2.73e+00]    [8.87e+00, 1.58e+01, 3.02e+00, 9.84e+00]    
29000     [2.95e+01, 7.13e+00, 2.73e+00]    [0.00e+00, 2.39e+01, 2.73e+00]    [1.14e+01, 2.39e+01, 1.36e+00, 1.11e+01]    
30000     [2.70e+01, 1.84e+00, 2.73e+00]    [0.00e+00, 1.93e+01, 2.73e+00]    [7.63e+00, 1.93e+01, 3.70e+00, 9.77e+00]    

Best model at step 28000:
  train loss: 3.11e+01
  test loss: 1.85e+01
  test metric: [8.87e+00, 1.58e+01, 3.02e+00, 9.84e+00]

'train' took 47.499830 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.196432 s

'compile' took 0.937237 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.51e+02, 1.49e+02, 2.75e+00]    [0.00e+00, 2.82e+02, 2.75e+00]    [2.82e+02, 2.82e+02, 3.73e+02, 3.73e+02]    
1000      [3.07e+01, 1.57e+01, 2.74e+00]    [0.00e+00, 4.02e+01, 2.74e+00]    [3.45e+01, 4.02e+01, 1.02e+01, 5.68e+00]    
2000      [2.85e+01, 1.11e+01, 2.74e+00]    [0.00e+00, 3.16e+01, 2.74e+00]    [2.28e+01, 3.16e+01, 9.05e+00, 1.92e+01]    
3000      [2.84e+01, 1.05e+01, 2.73e+00]    [0.00e+00, 2.02e+01, 2.73e+00]    [1.79e+01, 2.02e+01, 1.12e+01, 1.32e+01]    
4000      [2.98e+01, 1.02e+01, 2.73e+00]    [0.00e+00, 1.75e+01, 2.73e+00]    [1.90e+01, 1.75e+01, 1.13e+01, 9.79e+00]    
5000      [2.78e+01, 6.40e+00, 2.73e+00]    [0.00e+00, 2.65e+01, 2.73e+00]    [1.57e+01, 2.65e+01, 9.59e+00, 2.23e+01]    
6000      [2.79e+01, 4.81e+00, 2.73e+00]    [0.00e+00, 2.68e+01, 2.73e+00]    [1.59e+01, 2.68e+01, 9.47e+00, 2.24e+01]    
7000      [2.97e+01, 6.74e+00, 2.73e+00]    [0.00e+00, 2.17e+01, 2.73e+00]    [1.59e+01, 2.17e+01, 9.54e+00, 1.39e+01]    
8000      [3.31e+01, 9.45e+00, 2.72e+00]    [0.00e+00, 4.03e+01, 2.72e+00]    [2.91e+01, 4.03e+01, 2.50e+01, 4.64e+01]    
9000      [3.04e+01, 6.00e+00, 2.72e+00]    [0.00e+00, 3.33e+01, 2.72e+00]    [2.21e+01, 3.33e+01, 1.26e+01, 3.22e+01]    
10000     [2.82e+01, 2.60e+00, 2.72e+00]    [0.00e+00, 2.60e+01, 2.72e+00]    [1.51e+01, 2.60e+01, 1.20e+01, 1.78e+01]    
11000     [3.00e+01, 4.16e+00, 2.72e+00]    [0.00e+00, 2.28e+01, 2.72e+00]    [1.92e+01, 2.28e+01, 9.63e+00, 1.25e+01]    
12000     [2.93e+01, 3.72e+00, 2.72e+00]    [0.00e+00, 3.22e+01, 2.72e+00]    [2.06e+01, 3.22e+01, 9.93e+00, 2.64e+01]    
13000     [2.87e+01, 2.51e+00, 2.72e+00]    [0.00e+00, 2.82e+01, 2.72e+00]    [1.69e+01, 2.82e+01, 1.25e+01, 1.82e+01]    
14000     [2.80e+01, 1.42e+00, 2.72e+00]    [0.00e+00, 2.83e+01, 2.72e+00]    [1.71e+01, 2.83e+01, 1.29e+01, 1.80e+01]    
15000     [2.78e+01, 1.37e+00, 2.72e+00]    [0.00e+00, 2.88e+01, 2.72e+00]    [1.73e+01, 2.88e+01, 1.33e+01, 1.86e+01]    
16000     [3.17e+01, 5.38e+00, 2.72e+00]    [0.00e+00, 1.94e+01, 2.72e+00]    [2.62e+01, 1.94e+01, 1.39e+01, 1.14e+01]    
17000     [2.85e+01, 2.42e+00, 2.72e+00]    [0.00e+00, 2.67e+01, 2.72e+00]    [1.96e+01, 2.67e+01, 1.11e+01, 1.50e+01]    
18000     [2.72e+01, 9.34e-01, 2.72e+00]    [0.00e+00, 2.97e+01, 2.72e+00]    [1.83e+01, 2.97e+01, 1.28e+01, 1.94e+01]    
19000     [3.20e+01, 6.16e+00, 2.71e+00]    [0.00e+00, 1.73e+01, 2.71e+00]    [2.87e+01, 1.73e+01, 1.72e+01, 1.38e+01]    
20000     [2.82e+01, 2.66e+00, 2.71e+00]    [0.00e+00, 3.51e+01, 2.71e+00]    [2.37e+01, 3.51e+01, 1.09e+01, 2.82e+01]    
21000     [2.82e+01, 2.68e+00, 2.71e+00]    [0.00e+00, 3.49e+01, 2.71e+00]    [2.37e+01, 3.49e+01, 1.12e+01, 2.80e+01]    
22000     [2.74e+01, 2.17e+00, 2.71e+00]    [0.00e+00, 2.85e+01, 2.71e+00]    [1.83e+01, 2.85e+01, 1.36e+01, 1.72e+01]    
23000     [2.78e+01, 2.52e+00, 2.71e+00]    [0.00e+00, 3.55e+01, 2.71e+00]    [2.44e+01, 3.55e+01, 1.18e+01, 2.89e+01]    
24000     [2.73e+01, 2.34e+00, 2.71e+00]    [0.00e+00, 3.34e+01, 2.71e+00]    [2.25e+01, 3.34e+01, 1.14e+01, 2.46e+01]    
25000     [2.68e+01, 2.00e+00, 2.71e+00]    [0.00e+00, 3.35e+01, 2.71e+00]    [2.27e+01, 3.35e+01, 1.15e+01, 2.44e+01]    
26000     [2.68e+01, 2.39e+00, 2.71e+00]    [0.00e+00, 3.29e+01, 2.71e+00]    [2.22e+01, 3.29e+01, 1.22e+01, 2.35e+01]    
27000     [2.66e+01, 1.47e+00, 2.71e+00]    [0.00e+00, 2.61e+01, 2.71e+00]    [2.18e+01, 2.61e+01, 1.24e+01, 1.36e+01]    
28000     [2.68e+01, 2.38e+00, 2.71e+00]    [0.00e+00, 3.65e+01, 2.71e+00]    [2.60e+01, 3.65e+01, 1.24e+01, 2.87e+01]    
29000     [2.70e+01, 2.38e+00, 2.71e+00]    [0.00e+00, 2.31e+01, 2.71e+00]    [2.49e+01, 2.31e+01, 1.26e+01, 1.25e+01]    
30000     [2.59e+01, 1.71e+00, 2.71e+00]    [0.00e+00, 2.75e+01, 2.71e+00]    [2.12e+01, 2.75e+01, 1.36e+01, 1.44e+01]    

Best model at step 30000:
  train loss: 3.04e+01
  test loss: 3.02e+01
  test metric: [2.12e+01, 2.75e+01, 1.36e+01, 1.44e+01]

'train' took 49.259331 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.185339 s

'compile' took 0.817838 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.05e+02, 1.63e+02, 2.74e+00]    [0.00e+00, 4.64e+02, 2.74e+00]    [4.64e+02, 4.64e+02, 2.77e+02, 2.77e+02]    
1000      [3.25e+01, 2.43e+01, 2.73e+00]    [0.00e+00, 2.28e+01, 2.73e+00]    [2.16e+01, 2.28e+01, 8.71e+00, 7.82e+00]    
2000      [2.92e+01, 1.29e+01, 2.73e+00]    [0.00e+00, 1.02e+01, 2.73e+00]    [7.95e+00, 1.02e+01, 6.58e+00, 9.36e+00]    
3000      [2.88e+01, 8.67e+00, 2.74e+00]    [0.00e+00, 1.68e+01, 2.74e+00]    [1.38e+01, 1.68e+01, 8.34e+00, 1.31e+01]    
4000      [2.93e+01, 9.01e+00, 2.75e+00]    [0.00e+00, 2.33e+01, 2.75e+00]    [1.83e+01, 2.33e+01, 1.38e+01, 1.98e+01]    
5000      [2.90e+01, 8.40e+00, 2.76e+00]    [0.00e+00, 2.28e+01, 2.76e+00]    [1.60e+01, 2.28e+01, 1.27e+01, 2.01e+01]    
6000      [2.95e+01, 8.23e+00, 2.77e+00]    [0.00e+00, 2.50e+01, 2.77e+00]    [1.73e+01, 2.50e+01, 1.45e+01, 2.22e+01]    
7000      [3.06e+01, 7.32e+00, 2.78e+00]    [0.00e+00, 1.54e+01, 2.78e+00]    [1.01e+01, 1.54e+01, 2.83e+00, 1.12e+01]    
8000      [2.79e+01, 2.85e+00, 2.78e+00]    [0.00e+00, 1.96e+01, 2.78e+00]    [1.19e+01, 1.96e+01, 7.52e+00, 1.55e+01]    
9000      [2.90e+01, 4.00e+00, 2.79e+00]    [0.00e+00, 1.62e+01, 2.79e+00]    [8.98e+00, 1.62e+01, 4.27e+00, 1.23e+01]    
10000     [2.81e+01, 3.80e+00, 2.80e+00]    [0.00e+00, 2.30e+01, 2.80e+00]    [1.47e+01, 2.30e+01, 1.13e+01, 1.80e+01]    
11000     [2.70e+01, 2.07e+00, 2.81e+00]    [0.00e+00, 1.95e+01, 2.81e+00]    [1.18e+01, 1.95e+01, 8.29e+00, 1.53e+01]    
12000     [2.68e+01, 1.34e+00, 2.82e+00]    [0.00e+00, 2.01e+01, 2.82e+00]    [1.21e+01, 2.01e+01, 8.31e+00, 1.49e+01]    
13000     [2.63e+01, 1.14e+00, 2.83e+00]    [0.00e+00, 1.70e+01, 2.83e+00]    [8.42e+00, 1.70e+01, 5.30e+00, 1.36e+01]    
14000     [2.79e+01, 3.73e+00, 2.83e+00]    [0.00e+00, 1.60e+01, 2.83e+00]    [3.91e+00, 1.60e+01, 2.30e+00, 7.34e+00]    
15000     [2.76e+01, 5.94e+00, 2.84e+00]    [0.00e+00, 2.27e+01, 2.84e+00]    [1.29e+01, 2.27e+01, 1.31e+01, 1.85e+01]    
16000     [2.51e+01, 1.33e+00, 2.84e+00]    [0.00e+00, 1.76e+01, 2.84e+00]    [8.58e+00, 1.76e+01, 6.86e+00, 1.36e+01]    
17000     [2.62e+01, 3.47e+00, 2.84e+00]    [0.00e+00, 1.60e+01, 2.84e+00]    [6.35e+00, 1.60e+01, 2.76e+00, 8.57e+00]    
18000     [2.61e+01, 4.23e+00, 2.84e+00]    [0.00e+00, 2.12e+01, 2.84e+00]    [1.14e+01, 2.12e+01, 1.11e+01, 1.49e+01]    
19000     [2.61e+01, 4.52e+00, 2.84e+00]    [0.00e+00, 2.09e+01, 2.84e+00]    [1.11e+01, 2.09e+01, 1.13e+01, 1.46e+01]    
20000     [2.72e+01, 4.64e+00, 2.84e+00]    [0.00e+00, 1.66e+01, 2.84e+00]    [3.67e+00, 1.66e+01, 2.76e+00, 8.00e+00]    
21000     [2.62e+01, 2.99e+00, 2.84e+00]    [0.00e+00, 1.64e+01, 2.84e+00]    [3.67e+00, 1.64e+01, 3.03e+00, 7.48e+00]    
22000     [2.90e+01, 6.82e+00, 2.84e+00]    [0.00e+00, 1.62e+01, 2.84e+00]    [8.68e+00, 1.62e+01, 3.14e+00, 1.08e+01]    
23000     [2.61e+01, 5.18e+00, 2.83e+00]    [0.00e+00, 2.17e+01, 2.83e+00]    [1.23e+01, 2.17e+01, 1.19e+01, 1.41e+01]    
24000     [2.38e+01, 2.27e-01, 2.83e+00]    [0.00e+00, 1.70e+01, 2.83e+00]    [7.88e+00, 1.70e+01, 4.49e+00, 1.04e+01]    
25000     [2.59e+01, 5.51e+00, 2.83e+00]    [0.00e+00, 2.23e+01, 2.83e+00]    [1.22e+01, 2.23e+01, 1.19e+01, 1.40e+01]    
26000     [2.66e+01, 4.14e+00, 2.82e+00]    [0.00e+00, 1.51e+01, 2.82e+00]    [4.75e+00, 1.51e+01, 3.12e+00, 8.57e+00]    
27000     [3.02e+01, 1.13e+01, 2.82e+00]    [0.00e+00, 3.00e+01, 2.82e+00]    [1.89e+01, 3.00e+01, 2.14e+01, 2.19e+01]    
28000     [2.44e+01, 3.06e+00, 2.82e+00]    [0.00e+00, 2.06e+01, 2.82e+00]    [1.08e+01, 2.06e+01, 8.76e+00, 1.13e+01]    
29000     [2.51e+01, 3.33e+00, 2.81e+00]    [0.00e+00, 1.47e+01, 2.81e+00]    [4.30e+00, 1.47e+01, 3.40e+00, 9.92e+00]    
30000     [2.66e+01, 6.73e+00, 2.81e+00]    [0.00e+00, 2.46e+01, 2.81e+00]    [1.38e+01, 2.46e+01, 1.43e+01, 1.54e+01]    

Best model at step 24000:
  train loss: 2.69e+01
  test loss: 1.98e+01
  test metric: [7.88e+00, 1.70e+01, 4.49e+00, 1.04e+01]

'train' took 47.064093 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.164345 s

'compile' took 0.782719 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.72e+02, 2.42e+02, 2.63e+00]    [0.00e+00, 7.59e+01, 2.63e+00]    [7.59e+01, 7.59e+01, 3.03e+01, 3.03e+01]    
1000      [3.88e+01, 3.07e+01, 2.62e+00]    [0.00e+00, 4.00e+01, 2.62e+00]    [3.98e+01, 4.00e+01, 1.60e+01, 1.58e+01]    
2000      [3.30e+01, 1.38e+01, 2.62e+00]    [0.00e+00, 2.23e+01, 2.62e+00]    [2.24e+01, 2.23e+01, 1.81e+01, 1.81e+01]    
3000      [3.01e+01, 5.31e+00, 2.61e+00]    [0.00e+00, 2.21e+01, 2.61e+00]    [2.26e+01, 2.21e+01, 1.76e+01, 1.80e+01]    
4000      [2.94e+01, 8.20e+00, 2.61e+00]    [0.00e+00, 2.27e+01, 2.61e+00]    [2.28e+01, 2.27e+01, 1.80e+01, 1.80e+01]    
5000      [2.87e+01, 6.01e+00, 2.61e+00]    [0.00e+00, 2.28e+01, 2.61e+00]    [2.34e+01, 2.28e+01, 1.75e+01, 1.81e+01]    
6000      [2.86e+01, 6.79e+00, 2.61e+00]    [0.00e+00, 2.26e+01, 2.61e+00]    [2.30e+01, 2.26e+01, 1.78e+01, 1.81e+01]    
7000      [2.85e+01, 8.16e+00, 2.61e+00]    [0.00e+00, 2.24e+01, 2.61e+00]    [2.30e+01, 2.24e+01, 1.76e+01, 1.81e+01]    
8000      [2.74e+01, 2.90e+00, 2.61e+00]    [0.00e+00, 2.20e+01, 2.61e+00]    [2.28e+01, 2.20e+01, 1.72e+01, 1.79e+01]    
9000      [2.92e+01, 1.23e+01, 2.61e+00]    [0.00e+00, 2.25e+01, 2.61e+00]    [2.33e+01, 2.25e+01, 1.71e+01, 1.77e+01]    
10000     [2.87e+01, 9.72e+00, 2.61e+00]    [0.00e+00, 2.22e+01, 2.61e+00]    [2.18e+01, 2.22e+01, 1.79e+01, 1.71e+01]    
11000     [2.70e+01, 4.83e+00, 2.61e+00]    [0.00e+00, 2.18e+01, 2.61e+00]    [2.27e+01, 2.18e+01, 1.72e+01, 1.79e+01]    
12000     [2.84e+01, 1.09e+01, 2.61e+00]    [0.00e+00, 2.20e+01, 2.61e+00]    [2.23e+01, 2.20e+01, 1.77e+01, 1.77e+01]    
13000     [2.69e+01, 3.47e+00, 2.61e+00]    [0.00e+00, 2.15e+01, 2.61e+00]    [2.25e+01, 2.15e+01, 1.73e+01, 1.81e+01]    
14000     [2.87e+01, 1.22e+01, 2.61e+00]    [0.00e+00, 2.21e+01, 2.61e+00]    [2.21e+01, 2.21e+01, 1.79e+01, 1.75e+01]    
15000     [2.87e+01, 1.13e+01, 2.61e+00]    [0.00e+00, 2.25e+01, 2.61e+00]    [2.15e+01, 2.25e+01, 1.78e+01, 1.64e+01]    
16000     [2.70e+01, 8.01e+00, 2.61e+00]    [0.00e+00, 2.18e+01, 2.61e+00]    [2.29e+01, 2.18e+01, 1.73e+01, 1.80e+01]    
17000     [2.72e+01, 9.20e+00, 2.61e+00]    [0.00e+00, 2.17e+01, 2.61e+00]    [2.24e+01, 2.17e+01, 1.77e+01, 1.80e+01]    
18000     [2.57e+01, 2.96e+00, 2.61e+00]    [0.00e+00, 2.16e+01, 2.61e+00]    [2.22e+01, 2.16e+01, 1.77e+01, 1.79e+01]    
19000     [2.60e+01, 3.49e+00, 2.61e+00]    [0.00e+00, 2.18e+01, 2.61e+00]    [2.19e+01, 2.18e+01, 1.79e+01, 1.76e+01]    
20000     [2.74e+01, 8.64e+00, 2.61e+00]    [0.00e+00, 2.13e+01, 2.61e+00]    [2.21e+01, 2.13e+01, 1.74e+01, 1.79e+01]    
21000     [2.62e+01, 7.02e+00, 2.61e+00]    [0.00e+00, 2.13e+01, 2.61e+00]    [2.28e+01, 2.13e+01, 1.72e+01, 1.84e+01]    
22000     [2.66e+01, 6.83e+00, 2.60e+00]    [0.00e+00, 2.19e+01, 2.60e+00]    [2.17e+01, 2.19e+01, 1.78e+01, 1.72e+01]    
23000     [2.64e+01, 6.98e+00, 2.60e+00]    [0.00e+00, 2.17e+01, 2.60e+00]    [2.22e+01, 2.17e+01, 1.74e+01, 1.75e+01]    
24000     [2.54e+01, 5.07e+00, 2.60e+00]    [0.00e+00, 2.18e+01, 2.60e+00]    [2.26e+01, 2.18e+01, 1.73e+01, 1.77e+01]    
25000     [2.54e+01, 5.22e+00, 2.60e+00]    [0.00e+00, 2.16e+01, 2.60e+00]    [2.31e+01, 2.16e+01, 1.68e+01, 1.80e+01]    
26000     [2.46e+01, 6.35e-01, 2.60e+00]    [0.00e+00, 2.18e+01, 2.60e+00]    [2.28e+01, 2.18e+01, 1.69e+01, 1.75e+01]    
27000     [2.69e+01, 7.79e+00, 2.60e+00]    [0.00e+00, 2.27e+01, 2.60e+00]    [2.23e+01, 2.27e+01, 1.75e+01, 1.64e+01]    
28000     [2.57e+01, 5.85e+00, 2.60e+00]    [0.00e+00, 2.30e+01, 2.60e+00]    [2.26e+01, 2.30e+01, 1.73e+01, 1.63e+01]    
29000     [2.46e+01, 2.63e+00, 2.59e+00]    [0.00e+00, 2.27e+01, 2.59e+00]    [2.44e+01, 2.27e+01, 1.57e+01, 1.72e+01]    
30000     [2.44e+01, 9.92e-01, 2.59e+00]    [0.00e+00, 2.43e+01, 2.59e+00]    [2.37e+01, 2.43e+01, 1.61e+01, 1.50e+01]    

Best model at step 26000:
  train loss: 2.79e+01
  test loss: 2.44e+01
  test metric: [2.28e+01, 2.18e+01, 1.69e+01, 1.75e+01]

'train' took 47.240738 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.170597 s

'compile' took 0.822045 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.81e+02, 6.22e+02, 2.76e+00]    [0.00e+00, 1.72e+02, 2.76e+00]    [1.72e+02, 1.72e+02, 7.92e+01, 7.92e+01]    
1000      [3.74e+01, 2.36e+01, 2.75e+00]    [0.00e+00, 4.51e+01, 2.75e+00]    [4.54e+01, 4.51e+01, 1.94e+01, 1.95e+01]    
2000      [2.97e+01, 1.48e+01, 2.74e+00]    [0.00e+00, 2.25e+01, 2.74e+00]    [2.27e+01, 2.25e+01, 1.40e+01, 1.40e+01]    
3000      [2.94e+01, 1.40e+01, 2.73e+00]    [0.00e+00, 1.59e+01, 2.73e+00]    [1.60e+01, 1.59e+01, 1.15e+01, 1.14e+01]    
4000      [2.90e+01, 1.06e+01, 2.73e+00]    [0.00e+00, 1.58e+01, 2.73e+00]    [1.58e+01, 1.58e+01, 1.11e+01, 1.12e+01]    
5000      [2.82e+01, 6.34e+00, 2.73e+00]    [0.00e+00, 1.69e+01, 2.73e+00]    [1.69e+01, 1.69e+01, 1.26e+01, 1.30e+01]    
6000      [2.84e+01, 7.37e+00, 2.73e+00]    [0.00e+00, 1.73e+01, 2.73e+00]    [1.71e+01, 1.73e+01, 1.45e+01, 1.48e+01]    
7000      [2.85e+01, 6.98e+00, 2.72e+00]    [0.00e+00, 1.74e+01, 2.72e+00]    [1.74e+01, 1.74e+01, 1.37e+01, 1.42e+01]    
8000      [2.83e+01, 9.03e+00, 2.72e+00]    [0.00e+00, 1.74e+01, 2.72e+00]    [1.73e+01, 1.74e+01, 1.39e+01, 1.44e+01]    
9000      [2.76e+01, 5.76e+00, 2.72e+00]    [0.00e+00, 1.67e+01, 2.72e+00]    [1.65e+01, 1.67e+01, 1.44e+01, 1.47e+01]    
10000     [2.98e+01, 1.26e+01, 2.72e+00]    [0.00e+00, 1.69e+01, 2.72e+00]    [1.68e+01, 1.69e+01, 1.30e+01, 1.34e+01]    
11000     [2.77e+01, 6.42e+00, 2.72e+00]    [0.00e+00, 1.70e+01, 2.72e+00]    [1.69e+01, 1.70e+01, 1.31e+01, 1.35e+01]    
12000     [2.76e+01, 8.78e+00, 2.71e+00]    [0.00e+00, 1.62e+01, 2.71e+00]    [1.61e+01, 1.62e+01, 1.39e+01, 1.43e+01]    
13000     [3.01e+01, 1.38e+01, 2.71e+00]    [0.00e+00, 1.61e+01, 2.71e+00]    [1.59e+01, 1.61e+01, 1.31e+01, 1.32e+01]    
14000     [2.67e+01, 4.56e+00, 2.71e+00]    [0.00e+00, 1.67e+01, 2.71e+00]    [1.65e+01, 1.67e+01, 1.28e+01, 1.30e+01]    
15000     [2.79e+01, 9.85e+00, 2.71e+00]    [0.00e+00, 1.59e+01, 2.71e+00]    [1.56e+01, 1.59e+01, 1.33e+01, 1.36e+01]    
16000     [2.93e+01, 1.46e+01, 2.70e+00]    [0.00e+00, 1.60e+01, 2.70e+00]    [1.58e+01, 1.60e+01, 1.31e+01, 1.34e+01]    
17000     [2.67e+01, 7.38e+00, 2.70e+00]    [0.00e+00, 1.56e+01, 2.70e+00]    [1.53e+01, 1.56e+01, 1.33e+01, 1.33e+01]    
18000     [2.77e+01, 1.05e+01, 2.70e+00]    [0.00e+00, 1.58e+01, 2.70e+00]    [1.55e+01, 1.58e+01, 1.28e+01, 1.29e+01]    
19000     [2.64e+01, 6.39e+00, 2.70e+00]    [0.00e+00, 1.52e+01, 2.70e+00]    [1.54e+01, 1.52e+01, 1.27e+01, 1.32e+01]    
20000     [2.95e+01, 1.50e+01, 2.69e+00]    [0.00e+00, 1.52e+01, 2.69e+00]    [1.57e+01, 1.52e+01, 1.23e+01, 1.31e+01]    
21000     [2.68e+01, 8.79e+00, 2.69e+00]    [0.00e+00, 1.53e+01, 2.69e+00]    [1.55e+01, 1.53e+01, 1.24e+01, 1.28e+01]    
22000     [2.75e+01, 1.07e+01, 2.69e+00]    [0.00e+00, 1.51e+01, 2.69e+00]    [1.55e+01, 1.51e+01, 1.23e+01, 1.29e+01]    
23000     [2.92e+01, 1.22e+01, 2.68e+00]    [0.00e+00, 1.50e+01, 2.68e+00]    [1.61e+01, 1.50e+01, 1.14e+01, 1.25e+01]    
24000     [2.71e+01, 9.64e+00, 2.68e+00]    [0.00e+00, 1.53e+01, 2.68e+00]    [1.59e+01, 1.53e+01, 1.12e+01, 1.20e+01]    
25000     [2.71e+01, 1.05e+01, 2.68e+00]    [0.00e+00, 1.50e+01, 2.68e+00]    [1.59e+01, 1.50e+01, 1.16e+01, 1.25e+01]    
26000     [2.72e+01, 1.05e+01, 2.67e+00]    [0.00e+00, 1.48e+01, 2.67e+00]    [1.58e+01, 1.48e+01, 1.14e+01, 1.24e+01]    
27000     [2.64e+01, 5.56e+00, 2.67e+00]    [0.00e+00, 1.53e+01, 2.67e+00]    [1.63e+01, 1.53e+01, 1.07e+01, 1.14e+01]    
28000     [2.80e+01, 9.32e+00, 2.67e+00]    [0.00e+00, 1.52e+01, 2.67e+00]    [1.64e+01, 1.52e+01, 1.11e+01, 1.15e+01]    
29000     [2.64e+01, 5.52e+00, 2.66e+00]    [0.00e+00, 1.52e+01, 2.66e+00]    [1.73e+01, 1.52e+01, 1.01e+01, 1.17e+01]    
30000     [2.69e+01, 1.09e+01, 2.66e+00]    [0.00e+00, 1.54e+01, 2.66e+00]    [1.65e+01, 1.54e+01, 1.09e+01, 1.16e+01]    

Best model at step 14000:
  train loss: 3.39e+01
  test loss: 1.94e+01
  test metric: [1.65e+01, 1.67e+01, 1.28e+01, 1.30e+01]

'train' took 47.079037 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.181499 s

'compile' took 0.815516 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.53e+02, 1.44e+02, 2.62e+00]    [0.00e+00, 1.09e+02, 2.62e+00]    [1.09e+02, 1.09e+02, 3.13e+01, 3.13e+01]    
1000      [4.40e+01, 3.40e+01, 2.61e+00]    [0.00e+00, 5.69e+01, 2.61e+00]    [5.71e+01, 5.69e+01, 1.30e+01, 1.29e+01]    
2000      [2.96e+01, 1.77e+01, 2.62e+00]    [0.00e+00, 3.35e+01, 2.62e+00]    [3.38e+01, 3.35e+01, 1.44e+01, 1.43e+01]    
3000      [2.86e+01, 1.09e+01, 2.62e+00]    [0.00e+00, 2.08e+01, 2.62e+00]    [2.12e+01, 2.08e+01, 1.48e+01, 1.48e+01]    
4000      [3.03e+01, 1.55e+01, 2.62e+00]    [0.00e+00, 1.43e+01, 2.62e+00]    [1.44e+01, 1.43e+01, 1.58e+01, 1.55e+01]    
5000      [3.24e+01, 1.54e+01, 2.62e+00]    [0.00e+00, 1.39e+01, 2.62e+00]    [1.40e+01, 1.39e+01, 1.35e+01, 1.33e+01]    
6000      [3.18e+01, 1.30e+01, 2.62e+00]    [0.00e+00, 1.35e+01, 2.62e+00]    [1.35e+01, 1.35e+01, 1.33e+01, 1.32e+01]    
7000      [3.11e+01, 1.10e+01, 2.62e+00]    [0.00e+00, 1.37e+01, 2.62e+00]    [1.36e+01, 1.37e+01, 1.34e+01, 1.31e+01]    
8000      [3.18e+01, 1.06e+01, 2.62e+00]    [0.00e+00, 1.44e+01, 2.62e+00]    [1.43e+01, 1.44e+01, 1.32e+01, 1.30e+01]    
9000      [2.92e+01, 4.53e+00, 2.62e+00]    [0.00e+00, 1.46e+01, 2.62e+00]    [1.45e+01, 1.46e+01, 1.36e+01, 1.35e+01]    
10000     [3.18e+01, 1.02e+01, 2.62e+00]    [0.00e+00, 1.51e+01, 2.62e+00]    [1.50e+01, 1.51e+01, 1.37e+01, 1.35e+01]    
11000     [2.87e+01, 3.05e+00, 2.62e+00]    [0.00e+00, 1.53e+01, 2.62e+00]    [1.51e+01, 1.53e+01, 1.44e+01, 1.43e+01]    
12000     [3.12e+01, 1.13e+01, 2.62e+00]    [0.00e+00, 1.53e+01, 2.62e+00]    [1.51e+01, 1.53e+01, 1.49e+01, 1.48e+01]    
13000     [2.87e+01, 4.06e+00, 2.62e+00]    [0.00e+00, 1.56e+01, 2.62e+00]    [1.54e+01, 1.56e+01, 1.50e+01, 1.48e+01]    
14000     [2.79e+01, 2.50e+00, 2.62e+00]    [0.00e+00, 1.59e+01, 2.62e+00]    [1.60e+01, 1.59e+01, 1.46e+01, 1.47e+01]    
15000     [3.03e+01, 9.27e+00, 2.62e+00]    [0.00e+00, 1.64e+01, 2.62e+00]    [1.66e+01, 1.64e+01, 1.44e+01, 1.45e+01]    
16000     [2.88e+01, 7.09e+00, 2.62e+00]    [0.00e+00, 1.67e+01, 2.62e+00]    [1.69e+01, 1.67e+01, 1.46e+01, 1.47e+01]    
17000     [2.88e+01, 7.05e+00, 2.62e+00]    [0.00e+00, 1.69e+01, 2.62e+00]    [1.71e+01, 1.69e+01, 1.48e+01, 1.49e+01]    
18000     [2.79e+01, 2.96e+00, 2.62e+00]    [0.00e+00, 1.70e+01, 2.62e+00]    [1.72e+01, 1.70e+01, 1.49e+01, 1.50e+01]    
19000     [2.88e+01, 9.28e+00, 2.62e+00]    [0.00e+00, 1.77e+01, 2.62e+00]    [1.79e+01, 1.77e+01, 1.44e+01, 1.44e+01]    
20000     [2.83e+01, 5.53e+00, 2.62e+00]    [0.00e+00, 1.73e+01, 2.62e+00]    [1.75e+01, 1.73e+01, 1.50e+01, 1.50e+01]    
21000     [2.80e+01, 5.82e+00, 2.62e+00]    [0.00e+00, 1.81e+01, 2.62e+00]    [1.82e+01, 1.81e+01, 1.44e+01, 1.43e+01]    
22000     [2.86e+01, 7.32e+00, 2.62e+00]    [0.00e+00, 1.81e+01, 2.62e+00]    [1.83e+01, 1.81e+01, 1.45e+01, 1.44e+01]    
23000     [2.97e+01, 1.12e+01, 2.62e+00]    [0.00e+00, 1.82e+01, 2.62e+00]    [1.84e+01, 1.82e+01, 1.47e+01, 1.46e+01]    
24000     [2.75e+01, 7.05e+00, 2.61e+00]    [0.00e+00, 1.87e+01, 2.61e+00]    [1.89e+01, 1.87e+01, 1.43e+01, 1.42e+01]    
25000     [2.77e+01, 6.22e+00, 2.61e+00]    [0.00e+00, 1.90e+01, 2.61e+00]    [1.92e+01, 1.90e+01, 1.42e+01, 1.40e+01]    
26000     [2.95e+01, 1.17e+01, 2.61e+00]    [0.00e+00, 1.91e+01, 2.61e+00]    [1.93e+01, 1.91e+01, 1.45e+01, 1.43e+01]    
27000     [2.80e+01, 6.67e+00, 2.61e+00]    [0.00e+00, 1.96e+01, 2.61e+00]    [1.98e+01, 1.96e+01, 1.42e+01, 1.39e+01]    
28000     [2.56e+01, 2.12e+00, 2.61e+00]    [0.00e+00, 1.95e+01, 2.61e+00]    [1.97e+01, 1.95e+01, 1.42e+01, 1.39e+01]    
29000     [2.83e+01, 7.27e+00, 2.61e+00]    [0.00e+00, 1.98e+01, 2.61e+00]    [2.00e+01, 1.98e+01, 1.42e+01, 1.38e+01]    
30000     [2.66e+01, 5.75e+00, 2.61e+00]    [0.00e+00, 1.95e+01, 2.61e+00]    [1.97e+01, 1.95e+01, 1.43e+01, 1.40e+01]    

Best model at step 28000:
  train loss: 3.03e+01
  test loss: 2.21e+01
  test metric: [1.97e+01, 1.95e+01, 1.42e+01, 1.39e+01]

'train' took 46.759212 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.172943 s

'compile' took 0.797462 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.95e+02, 4.68e+02, 2.76e+00]    [0.00e+00, 8.96e+01, 2.76e+00]    [8.96e+01, 8.96e+01, 2.44e+01, 2.44e+01]    
1000      [3.61e+01, 2.08e+01, 2.76e+00]    [0.00e+00, 5.00e+01, 2.76e+00]    [5.01e+01, 5.00e+01, 1.93e+01, 1.94e+01]    
2000      [3.08e+01, 1.84e+01, 2.77e+00]    [0.00e+00, 3.27e+01, 2.77e+00]    [3.29e+01, 3.27e+01, 1.44e+01, 1.47e+01]    
3000      [2.86e+01, 8.12e+00, 2.78e+00]    [0.00e+00, 2.62e+01, 2.78e+00]    [2.66e+01, 2.62e+01, 1.44e+01, 1.47e+01]    
4000      [2.78e+01, 9.51e+00, 2.78e+00]    [0.00e+00, 2.63e+01, 2.78e+00]    [2.68e+01, 2.63e+01, 1.49e+01, 1.52e+01]    
5000      [2.78e+01, 9.24e+00, 2.78e+00]    [0.00e+00, 2.59e+01, 2.78e+00]    [2.64e+01, 2.59e+01, 1.51e+01, 1.54e+01]    
6000      [2.70e+01, 7.38e+00, 2.78e+00]    [0.00e+00, 2.58e+01, 2.78e+00]    [2.63e+01, 2.58e+01, 1.56e+01, 1.59e+01]    
7000      [2.62e+01, 5.99e+00, 2.78e+00]    [0.00e+00, 2.56e+01, 2.78e+00]    [2.61e+01, 2.56e+01, 1.63e+01, 1.66e+01]    
8000      [2.57e+01, 7.24e+00, 2.78e+00]    [0.00e+00, 2.46e+01, 2.78e+00]    [2.51e+01, 2.46e+01, 1.72e+01, 1.75e+01]    
9000      [2.62e+01, 8.96e+00, 2.78e+00]    [0.00e+00, 2.47e+01, 2.78e+00]    [2.50e+01, 2.47e+01, 1.75e+01, 1.75e+01]    
10000     [2.51e+01, 6.67e+00, 2.78e+00]    [0.00e+00, 2.42e+01, 2.78e+00]    [2.42e+01, 2.42e+01, 1.77e+01, 1.74e+01]    
11000     [2.61e+01, 7.73e+00, 2.78e+00]    [0.00e+00, 2.32e+01, 2.78e+00]    [2.32e+01, 2.32e+01, 1.81e+01, 1.78e+01]    
12000     [2.83e+01, 1.26e+01, 2.78e+00]    [0.00e+00, 2.23e+01, 2.78e+00]    [2.25e+01, 2.23e+01, 1.84e+01, 1.83e+01]    
13000     [2.68e+01, 1.03e+01, 2.77e+00]    [0.00e+00, 2.20e+01, 2.77e+00]    [2.21e+01, 2.20e+01, 1.86e+01, 1.83e+01]    
14000     [2.63e+01, 8.81e+00, 2.77e+00]    [0.00e+00, 2.23e+01, 2.77e+00]    [2.24e+01, 2.23e+01, 1.79e+01, 1.76e+01]    
15000     [2.60e+01, 7.44e+00, 2.77e+00]    [0.00e+00, 2.22e+01, 2.77e+00]    [2.22e+01, 2.22e+01, 1.78e+01, 1.76e+01]    
16000     [2.57e+01, 9.43e+00, 2.77e+00]    [0.00e+00, 2.26e+01, 2.77e+00]    [2.26e+01, 2.26e+01, 1.73e+01, 1.70e+01]    
17000     [2.63e+01, 8.20e+00, 2.77e+00]    [0.00e+00, 2.16e+01, 2.77e+00]    [2.17e+01, 2.16e+01, 1.79e+01, 1.76e+01]    
18000     [2.64e+01, 8.48e+00, 2.76e+00]    [0.00e+00, 2.11e+01, 2.76e+00]    [2.12e+01, 2.11e+01, 1.81e+01, 1.78e+01]    
19000     [2.39e+01, 3.15e+00, 2.76e+00]    [0.00e+00, 2.16e+01, 2.76e+00]    [2.17e+01, 2.16e+01, 1.74e+01, 1.71e+01]    
20000     [2.46e+01, 8.42e+00, 2.76e+00]    [0.00e+00, 2.21e+01, 2.76e+00]    [2.22e+01, 2.21e+01, 1.70e+01, 1.67e+01]    
21000     [2.51e+01, 6.28e+00, 2.76e+00]    [0.00e+00, 2.11e+01, 2.76e+00]    [2.13e+01, 2.11e+01, 1.77e+01, 1.74e+01]    
22000     [2.55e+01, 7.23e+00, 2.75e+00]    [0.00e+00, 2.06e+01, 2.75e+00]    [2.07e+01, 2.06e+01, 1.81e+01, 1.79e+01]    
23000     [2.60e+01, 8.66e+00, 2.75e+00]    [0.00e+00, 2.09e+01, 2.75e+00]    [2.10e+01, 2.09e+01, 1.77e+01, 1.75e+01]    
24000     [2.48e+01, 8.95e+00, 2.75e+00]    [0.00e+00, 2.22e+01, 2.75e+00]    [2.23e+01, 2.22e+01, 1.64e+01, 1.61e+01]    
25000     [2.34e+01, 4.88e+00, 2.74e+00]    [0.00e+00, 2.15e+01, 2.74e+00]    [2.17e+01, 2.15e+01, 1.68e+01, 1.65e+01]    
26000     [2.68e+01, 1.03e+01, 2.74e+00]    [0.00e+00, 2.02e+01, 2.74e+00]    [2.04e+01, 2.02e+01, 1.76e+01, 1.74e+01]    
27000     [2.45e+01, 5.25e+00, 2.74e+00]    [0.00e+00, 2.10e+01, 2.74e+00]    [2.12e+01, 2.10e+01, 1.67e+01, 1.65e+01]    
28000     [2.45e+01, 5.29e+00, 2.74e+00]    [0.00e+00, 2.09e+01, 2.74e+00]    [2.11e+01, 2.09e+01, 1.68e+01, 1.66e+01]    
29000     [2.44e+01, 5.48e+00, 2.74e+00]    [0.00e+00, 2.09e+01, 2.74e+00]    [2.11e+01, 2.09e+01, 1.64e+01, 1.62e+01]    
30000     [2.48e+01, 5.67e+00, 2.73e+00]    [0.00e+00, 2.07e+01, 2.73e+00]    [2.09e+01, 2.07e+01, 1.66e+01, 1.64e+01]    

Best model at step 19000:
  train loss: 2.99e+01
  test loss: 2.44e+01
  test metric: [2.17e+01, 2.16e+01, 1.74e+01, 1.71e+01]

'train' took 45.896650 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[13.97680434814895, 34.849345154999185, 16.427906624582437, 15.75485990131916, 27.540445771897275, 16.997898053308962, 21.83497957954279, 16.71682179714168, 19.537381013728083, 21.643020293102126]
sigma_y 9 20.527946253777067 6.063881679093434
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.176506 s

'compile' took 0.797467 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.93e+02, 4.81e+02, 2.72e+00]    [0.00e+00, 1.98e+02, 2.72e+00]    [1.98e+02, 1.98e+02, 1.81e+02, 1.81e+02]    
1000      [3.97e+01, 2.89e+01, 2.70e+00]    [0.00e+00, 3.65e+01, 2.70e+00]    [3.69e+01, 3.65e+01, 2.17e+01, 2.20e+01]    
2000      [3.31e+01, 1.82e+01, 2.70e+00]    [0.00e+00, 1.81e+01, 2.70e+00]    [1.86e+01, 1.81e+01, 1.12e+01, 1.16e+01]    
3000      [3.05e+01, 1.43e+01, 2.69e+00]    [0.00e+00, 1.13e+01, 2.69e+00]    [1.18e+01, 1.13e+01, 9.84e+00, 1.01e+01]    
4000      [3.26e+01, 1.92e+01, 2.69e+00]    [0.00e+00, 8.28e+00, 2.69e+00]    [8.49e+00, 8.28e+00, 1.09e+01, 1.09e+01]    
5000      [2.83e+01, 7.53e+00, 2.69e+00]    [0.00e+00, 8.77e+00, 2.69e+00]    [8.58e+00, 8.77e+00, 1.11e+01, 1.10e+01]    
6000      [3.04e+01, 1.54e+01, 2.69e+00]    [0.00e+00, 1.08e+01, 2.69e+00]    [1.02e+01, 1.08e+01, 1.06e+01, 1.02e+01]    
7000      [2.77e+01, 7.35e+00, 2.69e+00]    [0.00e+00, 1.04e+01, 2.69e+00]    [1.00e+01, 1.04e+01, 1.15e+01, 1.12e+01]    
8000      [2.93e+01, 1.25e+01, 2.69e+00]    [0.00e+00, 1.23e+01, 2.69e+00]    [1.17e+01, 1.23e+01, 1.09e+01, 1.05e+01]    
9000      [2.78e+01, 6.89e+00, 2.69e+00]    [0.00e+00, 1.22e+01, 2.69e+00]    [1.17e+01, 1.22e+01, 1.12e+01, 1.08e+01]    
10000     [2.72e+01, 4.30e+00, 2.69e+00]    [0.00e+00, 1.32e+01, 2.69e+00]    [1.27e+01, 1.32e+01, 1.11e+01, 1.08e+01]    
11000     [2.85e+01, 8.59e+00, 2.69e+00]    [0.00e+00, 1.45e+01, 2.69e+00]    [1.40e+01, 1.45e+01, 1.09e+01, 1.07e+01]    
12000     [2.77e+01, 6.64e+00, 2.69e+00]    [0.00e+00, 1.49e+01, 2.69e+00]    [1.44e+01, 1.49e+01, 1.11e+01, 1.09e+01]    
13000     [2.99e+01, 8.27e+00, 2.69e+00]    [0.00e+00, 1.39e+01, 2.69e+00]    [1.35e+01, 1.39e+01, 1.20e+01, 1.19e+01]    
14000     [2.80e+01, 4.63e+00, 2.69e+00]    [0.00e+00, 1.45e+01, 2.69e+00]    [1.40e+01, 1.45e+01, 1.20e+01, 1.18e+01]    
15000     [2.98e+01, 7.22e+00, 2.69e+00]    [0.00e+00, 1.45e+01, 2.69e+00]    [1.41e+01, 1.45e+01, 1.22e+01, 1.19e+01]    
16000     [2.95e+01, 6.63e+00, 2.69e+00]    [0.00e+00, 1.44e+01, 2.69e+00]    [1.41e+01, 1.44e+01, 1.27e+01, 1.26e+01]    
17000     [2.93e+01, 5.85e+00, 2.69e+00]    [0.00e+00, 1.41e+01, 2.69e+00]    [1.42e+01, 1.41e+01, 1.28e+01, 1.31e+01]    
18000     [2.74e+01, 1.42e+00, 2.69e+00]    [0.00e+00, 1.40e+01, 2.69e+00]    [1.45e+01, 1.40e+01, 1.32e+01, 1.37e+01]    
19000     [2.82e+01, 3.30e+00, 2.69e+00]    [0.00e+00, 1.36e+01, 2.69e+00]    [1.42e+01, 1.36e+01, 1.38e+01, 1.45e+01]    
20000     [2.83e+01, 3.15e+00, 2.69e+00]    [0.00e+00, 1.42e+01, 2.69e+00]    [1.50e+01, 1.42e+01, 1.33e+01, 1.41e+01]    
21000     [2.80e+01, 2.82e+00, 2.69e+00]    [0.00e+00, 1.44e+01, 2.69e+00]    [1.53e+01, 1.44e+01, 1.36e+01, 1.44e+01]    
22000     [2.84e+01, 3.68e+00, 2.69e+00]    [0.00e+00, 1.52e+01, 2.69e+00]    [1.62e+01, 1.52e+01, 1.34e+01, 1.41e+01]    
23000     [2.95e+01, 7.56e+00, 2.69e+00]    [0.00e+00, 1.65e+01, 2.69e+00]    [1.75e+01, 1.65e+01, 1.30e+01, 1.37e+01]    
24000     [2.71e+01, 1.62e+00, 2.69e+00]    [0.00e+00, 1.56e+01, 2.69e+00]    [1.67e+01, 1.56e+01, 1.36e+01, 1.44e+01]    
25000     [3.05e+01, 9.87e+00, 2.69e+00]    [0.00e+00, 1.66e+01, 2.69e+00]    [1.76e+01, 1.66e+01, 1.35e+01, 1.43e+01]    
26000     [3.10e+01, 9.86e+00, 2.69e+00]    [0.00e+00, 1.55e+01, 2.69e+00]    [1.65e+01, 1.55e+01, 1.41e+01, 1.47e+01]    
27000     [2.78e+01, 4.49e+00, 2.69e+00]    [0.00e+00, 1.70e+01, 2.69e+00]    [1.80e+01, 1.70e+01, 1.35e+01, 1.41e+01]    
28000     [2.88e+01, 7.52e+00, 2.69e+00]    [0.00e+00, 1.71e+01, 2.69e+00]    [1.82e+01, 1.71e+01, 1.38e+01, 1.44e+01]    
29000     [2.65e+01, 1.39e+00, 2.69e+00]    [0.00e+00, 1.63e+01, 2.69e+00]    [1.74e+01, 1.63e+01, 1.45e+01, 1.51e+01]    
30000     [3.04e+01, 9.41e+00, 2.69e+00]    [0.00e+00, 1.61e+01, 2.69e+00]    [1.72e+01, 1.61e+01, 1.47e+01, 1.52e+01]    

Best model at step 29000:
  train loss: 3.06e+01
  test loss: 1.90e+01
  test metric: [1.74e+01, 1.63e+01, 1.45e+01, 1.51e+01]

'train' took 46.847545 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.168493 s

'compile' took 0.791875 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.65e+02, 1.59e+02, 2.66e+00]    [0.00e+00, 1.26e+03, 2.66e+00]    [1.26e+03, 1.26e+03, 1.39e+03, 1.39e+03]    
1000      [2.85e+01, 1.42e+01, 2.65e+00]    [0.00e+00, 2.55e+01, 2.65e+00]    [1.83e+01, 2.55e+01, 1.53e+01, 3.45e+01]    
2000      [2.71e+01, 1.16e+01, 2.64e+00]    [0.00e+00, 2.52e+01, 2.64e+00]    [1.55e+01, 2.52e+01, 1.39e+01, 3.29e+01]    
3000      [2.83e+01, 1.07e+01, 2.64e+00]    [0.00e+00, 2.02e+01, 2.64e+00]    [1.51e+01, 2.02e+01, 2.02e+01, 2.12e+01]    
4000      [2.95e+01, 1.00e+01, 2.63e+00]    [0.00e+00, 1.68e+01, 2.63e+00]    [2.11e+01, 1.68e+01, 2.14e+01, 1.35e+01]    
5000      [3.07e+01, 1.01e+01, 2.63e+00]    [0.00e+00, 3.69e+01, 2.63e+00]    [2.30e+01, 3.69e+01, 2.70e+01, 5.02e+01]    
6000      [3.01e+01, 8.85e+00, 2.63e+00]    [0.00e+00, 3.14e+01, 2.63e+00]    [2.08e+01, 3.14e+01, 2.18e+01, 4.36e+01]    
7000      [3.08e+01, 8.32e+00, 2.63e+00]    [0.00e+00, 3.18e+01, 2.63e+00]    [2.18e+01, 3.18e+01, 2.45e+01, 4.34e+01]    
8000      [2.80e+01, 5.74e+00, 2.63e+00]    [0.00e+00, 2.00e+01, 2.63e+00]    [1.69e+01, 2.00e+01, 1.10e+01, 2.40e+01]    
9000      [3.06e+01, 6.47e+00, 2.63e+00]    [0.00e+00, 2.36e+01, 2.63e+00]    [2.04e+01, 2.36e+01, 1.98e+01, 3.22e+01]    
10000     [2.81e+01, 4.25e+00, 2.64e+00]    [0.00e+00, 1.28e+01, 2.64e+00]    [1.45e+01, 1.28e+01, 1.37e+01, 8.50e+00]    
11000     [2.80e+01, 3.35e+00, 2.64e+00]    [0.00e+00, 7.80e+00, 2.64e+00]    [1.28e+01, 7.80e+00, 8.90e+00, 6.68e+00]    
12000     [2.79e+01, 2.68e+00, 2.65e+00]    [0.00e+00, 8.83e+00, 2.65e+00]    [1.09e+01, 8.83e+00, 8.85e+00, 7.35e+00]    
13000     [2.72e+01, 1.92e+00, 2.65e+00]    [0.00e+00, 1.47e+01, 2.65e+00]    [1.14e+01, 1.47e+01, 8.16e+00, 1.19e+01]    
14000     [3.08e+01, 3.33e+00, 2.66e+00]    [0.00e+00, 3.93e+01, 2.66e+00]    [2.48e+01, 3.93e+01, 1.90e+01, 4.05e+01]    
15000     [2.74e+01, 1.55e+00, 2.66e+00]    [0.00e+00, 3.42e+01, 2.66e+00]    [1.34e+01, 3.42e+01, 9.06e+00, 3.54e+01]    
16000     [2.76e+01, 1.52e+00, 2.67e+00]    [0.00e+00, 4.23e+01, 2.67e+00]    [1.24e+01, 4.23e+01, 1.08e+01, 4.30e+01]    
17000     [3.08e+01, 2.84e+00, 2.67e+00]    [0.00e+00, 6.64e+01, 2.67e+00]    [2.64e+01, 6.64e+01, 2.31e+01, 6.99e+01]    
18000     [3.48e+01, 5.55e+00, 2.67e+00]    [0.00e+00, 8.61e+01, 2.67e+00]    [4.01e+01, 8.61e+01, 3.60e+01, 9.37e+01]    
19000     [2.78e+01, 1.80e+00, 2.68e+00]    [0.00e+00, 4.08e+01, 2.68e+00]    [1.17e+01, 4.08e+01, 1.11e+01, 4.37e+01]    
20000     [2.68e+01, 1.54e+00, 2.68e+00]    [0.00e+00, 7.14e+01, 2.68e+00]    [1.36e+01, 7.14e+01, 9.44e+00, 7.59e+01]    
21000     [2.70e+01, 1.43e+00, 2.68e+00]    [0.00e+00, 5.84e+01, 2.68e+00]    [9.66e+00, 5.84e+01, 8.84e+00, 6.21e+01]    
22000     [2.50e+01, 2.31e-01, 2.69e+00]    [0.00e+00, 8.03e+01, 2.69e+00]    [7.39e+00, 8.03e+01, 5.70e+00, 8.42e+01]    
23000     [2.80e+01, 1.91e+00, 2.69e+00]    [0.00e+00, 5.85e+01, 2.69e+00]    [1.68e+01, 5.85e+01, 1.68e+01, 6.48e+01]    
24000     [2.70e+01, 2.15e+00, 2.69e+00]    [0.00e+00, 7.14e+01, 2.69e+00]    [9.54e+00, 7.14e+01, 8.84e+00, 7.81e+01]    
25000     [2.60e+01, 1.20e+00, 2.69e+00]    [0.00e+00, 9.99e+01, 2.69e+00]    [1.57e+01, 9.99e+01, 1.27e+01, 1.06e+02]    
26000     [2.47e+01, 3.41e-01, 2.69e+00]    [0.00e+00, 8.40e+01, 2.69e+00]    [6.39e+00, 8.40e+01, 2.66e+00, 8.78e+01]    
27000     [2.41e+01, 4.75e-01, 2.69e+00]    [0.00e+00, 9.24e+01, 2.69e+00]    [6.88e+00, 9.24e+01, 3.42e+00, 9.73e+01]    
28000     [2.70e+01, 2.02e+00, 2.69e+00]    [0.00e+00, 7.67e+01, 2.69e+00]    [1.30e+01, 7.67e+01, 1.23e+01, 8.09e+01]    
29000     [2.52e+01, 1.14e+00, 2.69e+00]    [0.00e+00, 8.66e+01, 2.69e+00]    [6.46e+00, 8.66e+01, 6.01e+00, 8.86e+01]    
30000     [2.55e+01, 1.05e+00, 2.68e+00]    [0.00e+00, 8.57e+01, 2.68e+00]    [8.00e+00, 8.57e+01, 8.50e+00, 8.74e+01]    

Best model at step 27000:
  train loss: 2.72e+01
  test loss: 9.51e+01
  test metric: [6.88e+00, 9.24e+01, 3.42e+00, 9.73e+01]

'train' took 46.048365 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.178553 s

'compile' took 0.787932 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.23e+02, 7.41e+02, 2.74e+00]    [0.00e+00, 1.43e+02, 2.74e+00]    [1.43e+02, 1.43e+02, 2.07e+01, 2.07e+01]    
1000      [4.90e+01, 2.77e+01, 2.70e+00]    [0.00e+00, 7.32e+01, 2.70e+00]    [7.32e+01, 7.32e+01, 9.90e+00, 9.96e+00]    
2000      [3.20e+01, 1.28e+01, 2.70e+00]    [0.00e+00, 3.94e+01, 2.70e+00]    [3.96e+01, 3.94e+01, 6.97e+00, 7.06e+00]    
3000      [3.02e+01, 1.39e+01, 2.70e+00]    [0.00e+00, 2.80e+01, 2.70e+00]    [2.83e+01, 2.80e+01, 7.97e+00, 7.90e+00]    
4000      [3.06e+01, 1.41e+01, 2.70e+00]    [0.00e+00, 2.51e+01, 2.70e+00]    [2.55e+01, 2.51e+01, 7.73e+00, 7.67e+00]    
5000      [2.83e+01, 9.31e+00, 2.69e+00]    [0.00e+00, 2.39e+01, 2.69e+00]    [2.44e+01, 2.39e+01, 7.70e+00, 7.67e+00]    
6000      [2.80e+01, 8.38e+00, 2.69e+00]    [0.00e+00, 2.29e+01, 2.69e+00]    [2.35e+01, 2.29e+01, 7.95e+00, 7.96e+00]    
7000      [2.92e+01, 1.27e+01, 2.69e+00]    [0.00e+00, 2.24e+01, 2.69e+00]    [2.31e+01, 2.24e+01, 8.18e+00, 8.22e+00]    
8000      [2.88e+01, 1.34e+01, 2.69e+00]    [0.00e+00, 2.18e+01, 2.69e+00]    [2.26e+01, 2.18e+01, 8.62e+00, 8.65e+00]    
9000      [2.86e+01, 1.23e+01, 2.69e+00]    [0.00e+00, 2.12e+01, 2.69e+00]    [2.21e+01, 2.12e+01, 8.90e+00, 8.93e+00]    
10000     [2.74e+01, 6.96e+00, 2.69e+00]    [0.00e+00, 2.13e+01, 2.69e+00]    [2.22e+01, 2.13e+01, 9.26e+00, 9.30e+00]    
11000     [3.06e+01, 1.39e+01, 2.69e+00]    [0.00e+00, 2.13e+01, 2.69e+00]    [2.23e+01, 2.13e+01, 9.84e+00, 9.87e+00]    
12000     [3.01e+01, 1.18e+01, 2.69e+00]    [0.00e+00, 2.09e+01, 2.69e+00]    [2.20e+01, 2.09e+01, 1.03e+01, 1.03e+01]    
13000     [2.80e+01, 9.60e+00, 2.69e+00]    [0.00e+00, 2.11e+01, 2.69e+00]    [2.22e+01, 2.11e+01, 1.09e+01, 1.10e+01]    
14000     [2.82e+01, 9.41e+00, 2.69e+00]    [0.00e+00, 2.06e+01, 2.69e+00]    [2.19e+01, 2.06e+01, 1.12e+01, 1.13e+01]    
15000     [2.79e+01, 6.21e+00, 2.69e+00]    [0.00e+00, 2.01e+01, 2.69e+00]    [2.15e+01, 2.01e+01, 1.15e+01, 1.16e+01]    
16000     [3.04e+01, 1.14e+01, 2.69e+00]    [0.00e+00, 1.97e+01, 2.69e+00]    [2.12e+01, 1.97e+01, 1.19e+01, 1.19e+01]    
17000     [3.03e+01, 1.35e+01, 2.69e+00]    [0.00e+00, 1.99e+01, 2.69e+00]    [2.15e+01, 1.99e+01, 1.23e+01, 1.24e+01]    
18000     [2.91e+01, 1.08e+01, 2.69e+00]    [0.00e+00, 1.96e+01, 2.69e+00]    [2.14e+01, 1.96e+01, 1.27e+01, 1.28e+01]    
19000     [3.32e+01, 1.53e+01, 2.69e+00]    [0.00e+00, 1.88e+01, 2.69e+00]    [2.07e+01, 1.88e+01, 1.30e+01, 1.31e+01]    
20000     [2.95e+01, 1.04e+01, 2.69e+00]    [0.00e+00, 1.88e+01, 2.69e+00]    [2.09e+01, 1.88e+01, 1.31e+01, 1.31e+01]    
21000     [3.02e+01, 9.02e+00, 2.69e+00]    [0.00e+00, 1.84e+01, 2.69e+00]    [2.06e+01, 1.84e+01, 1.36e+01, 1.36e+01]    
22000     [2.99e+01, 8.78e+00, 2.69e+00]    [0.00e+00, 1.82e+01, 2.69e+00]    [2.05e+01, 1.82e+01, 1.33e+01, 1.34e+01]    
23000     [2.91e+01, 1.05e+01, 2.69e+00]    [0.00e+00, 1.85e+01, 2.69e+00]    [2.09e+01, 1.85e+01, 1.32e+01, 1.33e+01]    
24000     [2.76e+01, 6.33e+00, 2.69e+00]    [0.00e+00, 1.83e+01, 2.69e+00]    [2.08e+01, 1.83e+01, 1.29e+01, 1.30e+01]    
25000     [2.86e+01, 9.63e+00, 2.68e+00]    [0.00e+00, 1.82e+01, 2.68e+00]    [2.09e+01, 1.82e+01, 1.29e+01, 1.31e+01]    
26000     [2.73e+01, 4.82e+00, 2.68e+00]    [0.00e+00, 1.77e+01, 2.68e+00]    [2.04e+01, 1.77e+01, 1.27e+01, 1.28e+01]    
27000     [2.88e+01, 7.25e+00, 2.68e+00]    [0.00e+00, 1.74e+01, 2.68e+00]    [2.03e+01, 1.74e+01, 1.29e+01, 1.30e+01]    
28000     [2.83e+01, 7.48e+00, 2.68e+00]    [0.00e+00, 1.73e+01, 2.68e+00]    [2.03e+01, 1.73e+01, 1.27e+01, 1.29e+01]    
29000     [2.98e+01, 9.04e+00, 2.68e+00]    [0.00e+00, 1.68e+01, 2.68e+00]    [1.99e+01, 1.68e+01, 1.28e+01, 1.29e+01]    
30000     [2.72e+01, 3.77e+00, 2.67e+00]    [0.00e+00, 1.66e+01, 2.67e+00]    [1.98e+01, 1.66e+01, 1.27e+01, 1.29e+01]    

Best model at step 30000:
  train loss: 3.37e+01
  test loss: 1.93e+01
  test metric: [1.98e+01, 1.66e+01, 1.27e+01, 1.29e+01]

'train' took 46.466087 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.178688 s

'compile' took 0.785541 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.19e+02, 4.73e+02, 2.83e+00]    [0.00e+00, 3.30e+02, 2.83e+00]    [3.30e+02, 3.30e+02, 2.99e+02, 2.99e+02]    
1000      [3.52e+01, 2.45e+01, 2.82e+00]    [0.00e+00, 1.61e+01, 2.82e+00]    [1.64e+01, 1.61e+01, 1.52e+01, 1.52e+01]    
2000      [3.11e+01, 1.07e+01, 2.81e+00]    [0.00e+00, 2.24e+00, 2.81e+00]    [1.82e+00, 2.24e+00, 1.70e+00, 2.02e+00]    
3000      [3.07e+01, 1.02e+01, 2.81e+00]    [0.00e+00, 6.25e+00, 2.81e+00]    [7.46e+00, 6.25e+00, 8.38e+00, 6.56e+00]    
4000      [3.01e+01, 9.30e+00, 2.81e+00]    [0.00e+00, 8.41e+00, 2.81e+00]    [9.64e+00, 8.41e+00, 1.20e+01, 1.00e+01]    
5000      [3.08e+01, 1.01e+01, 2.80e+00]    [0.00e+00, 1.12e+01, 2.80e+00]    [1.28e+01, 1.12e+01, 1.43e+01, 1.21e+01]    
6000      [3.19e+01, 1.34e+01, 2.80e+00]    [0.00e+00, 7.74e+00, 2.80e+00]    [5.95e+00, 7.74e+00, 4.92e+00, 4.08e+00]    
7000      [3.12e+01, 1.29e+01, 2.80e+00]    [0.00e+00, 9.23e+00, 2.80e+00]    [8.05e+00, 9.23e+00, 6.13e+00, 5.62e+00]    
8000      [3.09e+01, 1.14e+01, 2.80e+00]    [0.00e+00, 7.89e+00, 2.80e+00]    [7.00e+00, 7.89e+00, 5.55e+00, 6.37e+00]    
9000      [3.06e+01, 8.53e+00, 2.80e+00]    [0.00e+00, 1.15e+01, 2.80e+00]    [1.35e+01, 1.15e+01, 1.39e+01, 1.08e+01]    
10000     [3.01e+01, 1.01e+01, 2.80e+00]    [0.00e+00, 7.83e+00, 2.80e+00]    [5.31e+00, 7.83e+00, 5.67e+00, 7.10e+00]    
11000     [2.79e+01, 4.37e+00, 2.80e+00]    [0.00e+00, 6.07e+00, 2.80e+00]    [9.13e+00, 6.07e+00, 5.11e+00, 6.78e+00]    
12000     [2.87e+01, 4.79e+00, 2.81e+00]    [0.00e+00, 7.71e+00, 2.81e+00]    [1.11e+01, 7.71e+00, 7.20e+00, 6.71e+00]    
13000     [2.78e+01, 2.91e+00, 2.81e+00]    [0.00e+00, 6.05e+00, 2.81e+00]    [9.22e+00, 6.05e+00, 5.50e+00, 6.85e+00]    
14000     [3.22e+01, 1.03e+01, 2.81e+00]    [0.00e+00, 1.16e+01, 2.81e+00]    [1.48e+01, 1.16e+01, 1.44e+01, 1.15e+01]    
15000     [3.03e+01, 7.41e+00, 2.81e+00]    [0.00e+00, 9.46e+00, 2.81e+00]    [1.30e+01, 9.46e+00, 9.82e+00, 8.64e+00]    
16000     [3.03e+01, 9.12e+00, 2.81e+00]    [0.00e+00, 1.13e+01, 2.81e+00]    [8.93e+00, 1.13e+01, 2.19e+00, 1.02e+01]    
17000     [2.75e+01, 2.92e+00, 2.82e+00]    [0.00e+00, 6.41e+00, 2.82e+00]    [1.00e+01, 6.41e+00, 2.76e+00, 7.49e+00]    
18000     [2.81e+01, 5.06e+00, 2.82e+00]    [0.00e+00, 9.69e+00, 2.82e+00]    [7.40e+00, 9.69e+00, 3.48e+00, 7.92e+00]    
19000     [2.69e+01, 2.93e+00, 2.82e+00]    [0.00e+00, 6.63e+00, 2.82e+00]    [9.74e+00, 6.63e+00, 3.23e+00, 7.91e+00]    
20000     [2.68e+01, 3.89e+00, 2.82e+00]    [0.00e+00, 6.56e+00, 2.82e+00]    [8.70e+00, 6.56e+00, 4.30e+00, 7.82e+00]    
21000     [2.75e+01, 4.85e+00, 2.82e+00]    [0.00e+00, 9.74e+00, 2.82e+00]    [6.93e+00, 9.74e+00, 3.38e+00, 8.61e+00]    
22000     [2.63e+01, 2.58e+00, 2.82e+00]    [0.00e+00, 7.78e+00, 2.82e+00]    [6.23e+00, 7.78e+00, 4.20e+00, 7.94e+00]    
23000     [2.95e+01, 8.51e+00, 2.82e+00]    [0.00e+00, 1.06e+01, 2.82e+00]    [1.26e+01, 1.06e+01, 9.68e+00, 8.35e+00]    
24000     [2.67e+01, 4.17e+00, 2.83e+00]    [0.00e+00, 1.19e+01, 2.83e+00]    [8.12e+00, 1.19e+01, 3.95e+00, 7.28e+00]    
25000     [2.58e+01, 3.16e+00, 2.83e+00]    [0.00e+00, 7.44e+00, 2.83e+00]    [9.31e+00, 7.44e+00, 4.32e+00, 7.53e+00]    
26000     [2.77e+01, 6.34e+00, 2.83e+00]    [0.00e+00, 1.43e+01, 2.83e+00]    [9.47e+00, 1.43e+01, 4.43e+00, 9.84e+00]    
27000     [2.72e+01, 5.44e+00, 2.83e+00]    [0.00e+00, 9.56e+00, 2.83e+00]    [1.08e+01, 9.56e+00, 6.32e+00, 6.29e+00]    
28000     [2.62e+01, 4.29e+00, 2.83e+00]    [0.00e+00, 8.81e+00, 2.83e+00]    [9.99e+00, 8.81e+00, 5.65e+00, 7.41e+00]    
29000     [2.68e+01, 5.65e+00, 2.83e+00]    [0.00e+00, 1.64e+01, 2.83e+00]    [1.04e+01, 1.64e+01, 4.85e+00, 8.98e+00]    
30000     [2.47e+01, 1.53e+00, 2.83e+00]    [0.00e+00, 1.29e+01, 2.83e+00]    [7.78e+00, 1.29e+01, 6.43e+00, 6.73e+00]    

Best model at step 30000:
  train loss: 2.90e+01
  test loss: 1.57e+01
  test metric: [7.78e+00, 1.29e+01, 6.43e+00, 6.73e+00]

'train' took 46.328765 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.181315 s

'compile' took 0.788695 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.44e+02, 2.13e+02, 2.78e+00]    [0.00e+00, 3.57e+02, 2.78e+00]    [3.57e+02, 3.57e+02, 4.01e+02, 4.01e+02]    
1000      [3.37e+01, 1.38e+01, 2.78e+00]    [0.00e+00, 3.45e+01, 2.78e+00]    [2.60e+01, 3.45e+01, 9.71e+00, 8.06e+00]    
2000      [3.21e+01, 1.27e+01, 2.78e+00]    [0.00e+00, 1.44e+01, 2.78e+00]    [1.78e+01, 1.44e+01, 1.18e+01, 8.16e+00]    
3000      [2.99e+01, 6.72e+00, 2.78e+00]    [0.00e+00, 2.90e+01, 2.78e+00]    [1.47e+01, 2.90e+01, 6.17e+00, 2.75e+01]    
4000      [3.19e+01, 8.13e+00, 2.78e+00]    [0.00e+00, 4.09e+01, 2.78e+00]    [2.54e+01, 4.09e+01, 1.96e+01, 4.56e+01]    
5000      [3.07e+01, 4.93e+00, 2.78e+00]    [0.00e+00, 3.64e+01, 2.78e+00]    [2.02e+01, 3.64e+01, 1.15e+01, 3.55e+01]    
6000      [3.75e+01, 1.30e+01, 2.78e+00]    [0.00e+00, 5.73e+01, 2.78e+00]    [4.09e+01, 5.73e+01, 4.13e+01, 6.89e+01]    
7000      [3.24e+01, 5.94e+00, 2.79e+00]    [0.00e+00, 1.94e+01, 2.79e+00]    [2.26e+01, 1.94e+01, 1.15e+01, 1.05e+01]    
8000      [3.27e+01, 7.09e+00, 2.79e+00]    [0.00e+00, 4.41e+01, 2.79e+00]    [2.87e+01, 4.41e+01, 1.88e+01, 4.54e+01]    
9000      [3.35e+01, 8.17e+00, 2.80e+00]    [0.00e+00, 4.77e+01, 2.80e+00]    [3.31e+01, 4.77e+01, 2.55e+01, 5.14e+01]    
10000     [2.94e+01, 3.79e+00, 2.80e+00]    [0.00e+00, 3.54e+01, 2.80e+00]    [2.20e+01, 3.54e+01, 8.66e+00, 3.11e+01]    
11000     [3.09e+01, 6.29e+00, 2.81e+00]    [0.00e+00, 3.62e+01, 2.81e+00]    [2.33e+01, 3.62e+01, 1.04e+01, 3.36e+01]    
12000     [3.00e+01, 5.18e+00, 2.81e+00]    [0.00e+00, 3.79e+01, 2.81e+00]    [2.59e+01, 3.79e+01, 1.35e+01, 3.58e+01]    
13000     [2.72e+01, 1.74e+00, 2.82e+00]    [0.00e+00, 2.15e+01, 2.82e+00]    [1.66e+01, 2.15e+01, 6.49e+00, 1.11e+01]    
14000     [2.71e+01, 2.63e+00, 2.82e+00]    [0.00e+00, 2.62e+01, 2.82e+00]    [1.66e+01, 2.62e+01, 6.02e+00, 1.79e+01]    
15000     [2.73e+01, 2.48e+00, 2.82e+00]    [0.00e+00, 1.42e+01, 2.82e+00]    [2.05e+01, 1.42e+01, 6.33e+00, 9.66e+00]    
16000     [2.69e+01, 2.88e+00, 2.83e+00]    [0.00e+00, 2.56e+01, 2.83e+00]    [1.88e+01, 2.56e+01, 5.54e+00, 1.68e+01]    
17000     [2.76e+01, 4.24e+00, 2.83e+00]    [0.00e+00, 2.81e+01, 2.83e+00]    [2.18e+01, 2.81e+01, 7.91e+00, 2.29e+01]    
18000     [2.62e+01, 1.61e+00, 2.83e+00]    [0.00e+00, 1.14e+01, 2.83e+00]    [1.79e+01, 1.14e+01, 5.42e+00, 1.04e+01]    
19000     [2.58e+01, 1.86e+00, 2.83e+00]    [0.00e+00, 1.84e+01, 2.83e+00]    [1.55e+01, 1.84e+01, 5.39e+00, 1.26e+01]    
20000     [2.83e+01, 5.97e+00, 2.84e+00]    [0.00e+00, 2.31e+01, 2.84e+00]    [2.03e+01, 2.31e+01, 8.31e+00, 2.03e+01]    
21000     [2.71e+01, 3.52e+00, 2.84e+00]    [0.00e+00, 1.74e+01, 2.84e+00]    [2.08e+01, 1.74e+01, 1.03e+01, 1.36e+01]    
22000     [2.54e+01, 2.11e+00, 2.84e+00]    [0.00e+00, 1.44e+01, 2.84e+00]    [1.47e+01, 1.44e+01, 5.53e+00, 1.15e+01]    
23000     [2.68e+01, 4.73e+00, 2.84e+00]    [0.00e+00, 1.91e+01, 2.84e+00]    [1.79e+01, 1.91e+01, 6.26e+00, 1.58e+01]    
24000     [2.44e+01, 8.34e-01, 2.84e+00]    [0.00e+00, 1.01e+01, 2.84e+00]    [1.15e+01, 1.01e+01, 8.05e+00, 1.19e+01]    
25000     [2.73e+01, 5.08e+00, 2.84e+00]    [0.00e+00, 1.87e+01, 2.84e+00]    [1.84e+01, 1.87e+01, 7.48e+00, 1.56e+01]    
26000     [2.95e+01, 7.47e+00, 2.84e+00]    [0.00e+00, 2.80e+01, 2.84e+00]    [2.60e+01, 2.80e+01, 1.92e+01, 2.83e+01]    
27000     [2.48e+01, 2.92e+00, 2.84e+00]    [0.00e+00, 1.09e+01, 2.84e+00]    [1.19e+01, 1.09e+01, 7.36e+00, 1.15e+01]    
28000     [2.77e+01, 5.68e+00, 2.83e+00]    [0.00e+00, 2.26e+01, 2.83e+00]    [2.25e+01, 2.26e+01, 1.38e+01, 2.04e+01]    
29000     [2.40e+01, 1.30e+00, 2.83e+00]    [0.00e+00, 1.31e+01, 2.83e+00]    [1.30e+01, 1.31e+01, 5.14e+00, 1.16e+01]    
30000     [2.39e+01, 1.12e+00, 2.83e+00]    [0.00e+00, 1.51e+01, 2.83e+00]    [1.44e+01, 1.51e+01, 4.78e+00, 1.25e+01]    

Best model at step 30000:
  train loss: 2.79e+01
  test loss: 1.80e+01
  test metric: [1.44e+01, 1.51e+01, 4.78e+00, 1.25e+01]

'train' took 46.723850 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.192964 s

'compile' took 0.821867 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.66e+02, 5.28e+02, 2.73e+00]    [0.00e+00, 4.74e+02, 2.73e+00]    [4.74e+02, 4.74e+02, 2.21e+02, 2.21e+02]    
1000      [3.30e+01, 2.77e+01, 2.72e+00]    [0.00e+00, 2.70e+01, 2.72e+00]    [2.84e+01, 2.70e+01, 2.34e+01, 2.35e+01]    
2000      [3.01e+01, 1.44e+01, 2.73e+00]    [0.00e+00, 1.35e+01, 2.73e+00]    [1.58e+01, 1.35e+01, 1.84e+01, 1.76e+01]    
3000      [3.06e+01, 1.44e+01, 2.73e+00]    [0.00e+00, 1.55e+01, 2.73e+00]    [1.33e+01, 1.55e+01, 2.08e+01, 1.76e+01]    
4000      [2.80e+01, 9.51e+00, 2.74e+00]    [0.00e+00, 1.24e+01, 2.74e+00]    [9.46e+00, 1.24e+01, 9.54e+00, 5.62e+00]    
5000      [3.03e+01, 1.34e+01, 2.74e+00]    [0.00e+00, 9.71e+00, 2.74e+00]    [6.00e+00, 9.71e+00, 4.49e+00, 3.30e+00]    
6000      [3.05e+01, 1.35e+01, 2.74e+00]    [0.00e+00, 1.72e+01, 2.74e+00]    [1.29e+01, 1.72e+01, 1.47e+01, 1.10e+01]    
7000      [3.00e+01, 1.16e+01, 2.74e+00]    [0.00e+00, 1.85e+01, 2.74e+00]    [1.22e+01, 1.85e+01, 1.37e+01, 9.23e+00]    
8000      [2.77e+01, 7.00e+00, 2.75e+00]    [0.00e+00, 1.61e+01, 2.75e+00]    [8.17e+00, 1.61e+01, 6.59e+00, 3.19e+00]    
9000      [2.92e+01, 1.01e+01, 2.75e+00]    [0.00e+00, 1.48e+01, 2.75e+00]    [5.64e+00, 1.48e+01, 4.15e+00, 6.79e+00]    
10000     [2.97e+01, 1.00e+01, 2.75e+00]    [0.00e+00, 2.08e+01, 2.75e+00]    [1.07e+01, 2.08e+01, 1.24e+01, 6.98e+00]    
11000     [2.77e+01, 7.11e+00, 2.76e+00]    [0.00e+00, 1.79e+01, 2.76e+00]    [5.57e+00, 1.79e+01, 2.94e+00, 5.74e+00]    
12000     [2.75e+01, 6.58e+00, 2.76e+00]    [0.00e+00, 1.86e+01, 2.76e+00]    [4.96e+00, 1.86e+01, 2.11e+00, 6.40e+00]    
13000     [2.75e+01, 6.39e+00, 2.76e+00]    [0.00e+00, 1.94e+01, 2.76e+00]    [4.37e+00, 1.94e+01, 1.43e+00, 7.21e+00]    
14000     [2.78e+01, 5.50e+00, 2.76e+00]    [0.00e+00, 2.42e+01, 2.76e+00]    [7.48e+00, 2.42e+01, 8.88e+00, 4.09e+00]    
15000     [2.69e+01, 5.32e+00, 2.77e+00]    [0.00e+00, 2.25e+01, 2.77e+00]    [4.37e+00, 2.25e+01, 2.88e+00, 7.28e+00]    
16000     [2.93e+01, 7.37e+00, 2.77e+00]    [0.00e+00, 2.74e+01, 2.77e+00]    [8.44e+00, 2.74e+01, 1.20e+01, 7.25e+00]    
17000     [2.70e+01, 3.96e+00, 2.77e+00]    [0.00e+00, 2.61e+01, 2.77e+00]    [6.04e+00, 2.61e+01, 7.69e+00, 5.64e+00]    
18000     [2.95e+01, 7.19e+00, 2.77e+00]    [0.00e+00, 2.97e+01, 2.77e+00]    [9.22e+00, 2.97e+01, 1.21e+01, 8.30e+00]    
19000     [2.81e+01, 5.65e+00, 2.78e+00]    [0.00e+00, 2.47e+01, 2.78e+00]    [2.84e+00, 2.47e+01, 6.20e-01, 1.10e+01]    
20000     [2.86e+01, 6.87e+00, 2.78e+00]    [0.00e+00, 2.64e+01, 2.78e+00]    [3.11e+00, 2.64e+01, 1.05e+00, 1.25e+01]    
21000     [2.89e+01, 6.65e+00, 2.78e+00]    [0.00e+00, 3.36e+01, 2.78e+00]    [9.14e+00, 3.36e+01, 1.06e+01, 8.56e+00]    
22000     [2.76e+01, 5.90e+00, 2.78e+00]    [0.00e+00, 2.96e+01, 2.78e+00]    [3.99e+00, 2.96e+01, 6.53e-01, 1.27e+01]    
23000     [2.83e+01, 6.11e+00, 2.78e+00]    [0.00e+00, 3.54e+01, 2.78e+00]    [8.75e+00, 3.54e+01, 1.03e+01, 8.78e+00]    
24000     [2.82e+01, 6.55e+00, 2.78e+00]    [0.00e+00, 3.67e+01, 2.78e+00]    [8.81e+00, 3.67e+01, 1.03e+01, 9.04e+00]    
25000     [2.72e+01, 5.46e+00, 2.78e+00]    [0.00e+00, 3.71e+01, 2.78e+00]    [8.02e+00, 3.71e+01, 8.83e+00, 8.91e+00]    
26000     [2.56e+01, 2.44e+00, 2.78e+00]    [0.00e+00, 3.63e+01, 2.78e+00]    [6.67e+00, 3.63e+01, 6.54e+00, 9.40e+00]    
27000     [2.67e+01, 5.06e+00, 2.78e+00]    [0.00e+00, 3.92e+01, 2.78e+00]    [8.50e+00, 3.92e+01, 8.51e+00, 9.46e+00]    
28000     [2.59e+01, 3.79e+00, 2.78e+00]    [0.00e+00, 3.45e+01, 2.78e+00]    [3.38e+00, 3.45e+01, 1.46e+00, 1.41e+01]    
29000     [2.69e+01, 6.02e+00, 2.78e+00]    [0.00e+00, 3.95e+01, 2.78e+00]    [7.93e+00, 3.95e+01, 9.47e+00, 8.44e+00]    
30000     [2.72e+01, 6.80e+00, 2.78e+00]    [0.00e+00, 3.46e+01, 2.78e+00]    [2.93e+00, 3.46e+01, 1.81e+00, 1.73e+01]    

Best model at step 26000:
  train loss: 3.08e+01
  test loss: 3.91e+01
  test metric: [6.67e+00, 3.63e+01, 6.54e+00, 9.40e+00]

'train' took 47.146675 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.169896 s

'compile' took 0.794266 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.84e+02, 2.74e+02, 2.63e+00]    [0.00e+00, 9.33e+01, 2.63e+00]    [9.33e+01, 9.33e+01, 3.87e+01, 3.87e+01]    
1000      [4.36e+01, 3.64e+01, 2.60e+00]    [0.00e+00, 5.25e+01, 2.60e+00]    [5.24e+01, 5.25e+01, 1.16e+01, 1.15e+01]    
2000      [3.15e+01, 2.06e+01, 2.59e+00]    [0.00e+00, 2.70e+01, 2.59e+00]    [2.70e+01, 2.70e+01, 1.34e+01, 1.33e+01]    
3000      [3.04e+01, 1.91e+01, 2.59e+00]    [0.00e+00, 1.91e+01, 2.59e+00]    [1.92e+01, 1.91e+01, 1.26e+01, 1.26e+01]    
4000      [2.97e+01, 1.67e+01, 2.59e+00]    [0.00e+00, 1.50e+01, 2.59e+00]    [1.51e+01, 1.50e+01, 1.37e+01, 1.37e+01]    
5000      [2.79e+01, 1.22e+01, 2.58e+00]    [0.00e+00, 1.38e+01, 2.58e+00]    [1.39e+01, 1.38e+01, 1.29e+01, 1.29e+01]    
6000      [2.94e+01, 1.61e+01, 2.58e+00]    [0.00e+00, 1.30e+01, 2.58e+00]    [1.31e+01, 1.30e+01, 1.28e+01, 1.28e+01]    
7000      [2.93e+01, 1.61e+01, 2.58e+00]    [0.00e+00, 1.29e+01, 2.58e+00]    [1.31e+01, 1.29e+01, 1.21e+01, 1.22e+01]    
8000      [2.90e+01, 1.54e+01, 2.58e+00]    [0.00e+00, 1.32e+01, 2.58e+00]    [1.35e+01, 1.32e+01, 1.16e+01, 1.16e+01]    
9000      [3.01e+01, 1.65e+01, 2.58e+00]    [0.00e+00, 1.23e+01, 2.58e+00]    [1.25e+01, 1.23e+01, 1.22e+01, 1.22e+01]    
10000     [2.77e+01, 1.26e+01, 2.57e+00]    [0.00e+00, 1.40e+01, 2.57e+00]    [1.42e+01, 1.40e+01, 1.17e+01, 1.18e+01]    
11000     [2.85e+01, 1.28e+01, 2.57e+00]    [0.00e+00, 1.38e+01, 2.57e+00]    [1.41e+01, 1.38e+01, 1.20e+01, 1.21e+01]    
12000     [2.89e+01, 1.29e+01, 2.57e+00]    [0.00e+00, 1.38e+01, 2.57e+00]    [1.40e+01, 1.38e+01, 1.22e+01, 1.23e+01]    
13000     [2.85e+01, 1.17e+01, 2.57e+00]    [0.00e+00, 1.51e+01, 2.57e+00]    [1.54e+01, 1.51e+01, 1.15e+01, 1.16e+01]    
14000     [2.88e+01, 1.17e+01, 2.57e+00]    [0.00e+00, 1.54e+01, 2.57e+00]    [1.57e+01, 1.54e+01, 1.13e+01, 1.14e+01]    
15000     [2.88e+01, 1.47e+01, 2.56e+00]    [0.00e+00, 1.42e+01, 2.56e+00]    [1.45e+01, 1.42e+01, 1.24e+01, 1.24e+01]    
16000     [2.94e+01, 1.31e+01, 2.56e+00]    [0.00e+00, 1.46e+01, 2.56e+00]    [1.49e+01, 1.46e+01, 1.17e+01, 1.18e+01]    
17000     [2.76e+01, 1.19e+01, 2.56e+00]    [0.00e+00, 1.45e+01, 2.56e+00]    [1.49e+01, 1.45e+01, 1.20e+01, 1.21e+01]    
18000     [2.65e+01, 7.19e+00, 2.56e+00]    [0.00e+00, 1.45e+01, 2.56e+00]    [1.49e+01, 1.45e+01, 1.18e+01, 1.19e+01]    
19000     [2.89e+01, 1.50e+01, 2.56e+00]    [0.00e+00, 1.56e+01, 2.56e+00]    [1.59e+01, 1.56e+01, 1.12e+01, 1.13e+01]    
20000     [2.90e+01, 1.50e+01, 2.56e+00]    [0.00e+00, 1.56e+01, 2.56e+00]    [1.60e+01, 1.56e+01, 1.12e+01, 1.13e+01]    
21000     [2.84e+01, 1.06e+01, 2.55e+00]    [0.00e+00, 1.37e+01, 2.55e+00]    [1.41e+01, 1.37e+01, 1.21e+01, 1.23e+01]    
22000     [2.89e+01, 1.38e+01, 2.55e+00]    [0.00e+00, 1.42e+01, 2.55e+00]    [1.46e+01, 1.42e+01, 1.20e+01, 1.21e+01]    
23000     [2.88e+01, 1.17e+01, 2.55e+00]    [0.00e+00, 1.40e+01, 2.55e+00]    [1.45e+01, 1.40e+01, 1.20e+01, 1.21e+01]    
24000     [2.78e+01, 8.93e+00, 2.55e+00]    [0.00e+00, 1.40e+01, 2.55e+00]    [1.45e+01, 1.40e+01, 1.20e+01, 1.22e+01]    
25000     [2.58e+01, 5.44e+00, 2.55e+00]    [0.00e+00, 1.35e+01, 2.55e+00]    [1.40e+01, 1.35e+01, 1.25e+01, 1.27e+01]    
26000     [2.78e+01, 1.17e+01, 2.54e+00]    [0.00e+00, 1.43e+01, 2.54e+00]    [1.49e+01, 1.43e+01, 1.18e+01, 1.20e+01]    
27000     [2.72e+01, 7.16e+00, 2.54e+00]    [0.00e+00, 1.43e+01, 2.54e+00]    [1.49e+01, 1.43e+01, 1.15e+01, 1.17e+01]    
28000     [2.68e+01, 6.66e+00, 2.54e+00]    [0.00e+00, 1.35e+01, 2.54e+00]    [1.43e+01, 1.35e+01, 1.21e+01, 1.23e+01]    
29000     [2.67e+01, 9.38e+00, 2.54e+00]    [0.00e+00, 1.38e+01, 2.54e+00]    [1.46e+01, 1.38e+01, 1.21e+01, 1.23e+01]    
30000     [2.56e+01, 7.30e+00, 2.54e+00]    [0.00e+00, 1.50e+01, 2.54e+00]    [1.59e+01, 1.50e+01, 1.12e+01, 1.14e+01]    

Best model at step 25000:
  train loss: 3.38e+01
  test loss: 1.60e+01
  test metric: [1.40e+01, 1.35e+01, 1.25e+01, 1.27e+01]

'train' took 46.434392 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.185870 s

'compile' took 0.819286 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.01e+02, 9.44e+02, 2.73e+00]    [0.00e+00, 2.94e+02, 2.73e+00]    [2.94e+02, 2.94e+02, 2.99e+02, 2.99e+02]    
1000      [4.39e+01, 3.16e+01, 2.71e+00]    [0.00e+00, 4.31e+01, 2.71e+00]    [4.33e+01, 4.31e+01, 1.71e+01, 1.73e+01]    
2000      [3.09e+01, 1.27e+01, 2.71e+00]    [0.00e+00, 1.89e+01, 2.71e+00]    [1.89e+01, 1.89e+01, 1.70e+01, 1.69e+01]    
3000      [3.06e+01, 1.22e+01, 2.72e+00]    [0.00e+00, 1.80e+01, 2.72e+00]    [1.75e+01, 1.80e+01, 1.20e+01, 1.17e+01]    
4000      [2.96e+01, 9.35e+00, 2.72e+00]    [0.00e+00, 1.75e+01, 2.72e+00]    [1.68e+01, 1.75e+01, 9.82e+00, 9.31e+00]    
5000      [2.90e+01, 8.16e+00, 2.72e+00]    [0.00e+00, 1.68e+01, 2.72e+00]    [1.57e+01, 1.68e+01, 1.09e+01, 1.02e+01]    
6000      [2.96e+01, 1.14e+01, 2.73e+00]    [0.00e+00, 1.76e+01, 2.73e+00]    [1.62e+01, 1.76e+01, 1.05e+01, 9.65e+00]    
7000      [3.05e+01, 1.33e+01, 2.73e+00]    [0.00e+00, 1.72e+01, 2.73e+00]    [1.55e+01, 1.72e+01, 1.11e+01, 1.00e+01]    
8000      [2.77e+01, 8.73e+00, 2.74e+00]    [0.00e+00, 1.81e+01, 2.74e+00]    [1.62e+01, 1.81e+01, 1.13e+01, 1.02e+01]    
9000      [2.76e+01, 9.65e+00, 2.74e+00]    [0.00e+00, 1.76e+01, 2.74e+00]    [1.70e+01, 1.76e+01, 1.10e+01, 1.18e+01]    
10000     [2.92e+01, 1.43e+01, 2.74e+00]    [0.00e+00, 1.78e+01, 2.74e+00]    [1.70e+01, 1.78e+01, 1.19e+01, 1.27e+01]    
11000     [2.83e+01, 1.19e+01, 2.75e+00]    [0.00e+00, 1.79e+01, 2.75e+00]    [1.67e+01, 1.79e+01, 1.21e+01, 1.27e+01]    
12000     [2.80e+01, 7.27e+00, 2.75e+00]    [0.00e+00, 1.80e+01, 2.75e+00]    [1.54e+01, 1.80e+01, 1.26e+01, 1.14e+01]    
13000     [2.75e+01, 1.03e+01, 2.76e+00]    [0.00e+00, 1.77e+01, 2.76e+00]    [1.58e+01, 1.77e+01, 1.24e+01, 1.23e+01]    
14000     [2.69e+01, 9.04e+00, 2.76e+00]    [0.00e+00, 1.72e+01, 2.76e+00]    [1.51e+01, 1.72e+01, 1.24e+01, 1.20e+01]    
15000     [2.85e+01, 9.27e+00, 2.76e+00]    [0.00e+00, 1.57e+01, 2.76e+00]    [1.46e+01, 1.57e+01, 1.14e+01, 1.18e+01]    
16000     [2.75e+01, 6.57e+00, 2.77e+00]    [0.00e+00, 1.64e+01, 2.77e+00]    [1.36e+01, 1.64e+01, 1.20e+01, 1.09e+01]    
17000     [2.63e+01, 4.82e+00, 2.77e+00]    [0.00e+00, 1.72e+01, 2.77e+00]    [1.28e+01, 1.72e+01, 1.27e+01, 9.99e+00]    
18000     [2.78e+01, 1.19e+01, 2.77e+00]    [0.00e+00, 1.65e+01, 2.77e+00]    [1.38e+01, 1.65e+01, 1.21e+01, 1.13e+01]    
19000     [2.62e+01, 7.96e+00, 2.77e+00]    [0.00e+00, 1.55e+01, 2.77e+00]    [1.45e+01, 1.55e+01, 1.11e+01, 1.18e+01]    
20000     [2.61e+01, 8.08e+00, 2.77e+00]    [0.00e+00, 1.66e+01, 2.77e+00]    [1.35e+01, 1.66e+01, 1.17e+01, 1.07e+01]    
21000     [2.67e+01, 1.03e+01, 2.77e+00]    [0.00e+00, 1.61e+01, 2.77e+00]    [1.39e+01, 1.61e+01, 1.10e+01, 1.09e+01]    
22000     [2.66e+01, 9.38e+00, 2.78e+00]    [0.00e+00, 1.62e+01, 2.78e+00]    [1.36e+01, 1.62e+01, 1.09e+01, 1.06e+01]    
23000     [2.56e+01, 6.95e+00, 2.78e+00]    [0.00e+00, 1.63e+01, 2.78e+00]    [1.31e+01, 1.63e+01, 1.10e+01, 1.01e+01]    
24000     [2.47e+01, 4.05e+00, 2.78e+00]    [0.00e+00, 1.57e+01, 2.78e+00]    [1.36e+01, 1.57e+01, 1.02e+01, 1.04e+01]    
25000     [2.55e+01, 7.04e+00, 2.78e+00]    [0.00e+00, 1.57e+01, 2.78e+00]    [1.35e+01, 1.57e+01, 1.00e+01, 1.02e+01]    
26000     [2.77e+01, 9.32e+00, 2.78e+00]    [0.00e+00, 1.46e+01, 2.78e+00]    [1.34e+01, 1.46e+01, 8.96e+00, 9.90e+00]    
27000     [2.51e+01, 5.54e+00, 2.78e+00]    [0.00e+00, 1.65e+01, 2.78e+00]    [1.29e+01, 1.65e+01, 1.02e+01, 9.57e+00]    
28000     [2.50e+01, 5.79e+00, 2.78e+00]    [0.00e+00, 1.56e+01, 2.78e+00]    [1.30e+01, 1.56e+01, 9.65e+00, 9.73e+00]    
29000     [2.69e+01, 9.78e+00, 2.78e+00]    [0.00e+00, 1.50e+01, 2.78e+00]    [1.40e+01, 1.50e+01, 8.88e+00, 1.04e+01]    
30000     [2.47e+01, 3.64e+00, 2.78e+00]    [0.00e+00, 1.53e+01, 2.78e+00]    [1.27e+01, 1.53e+01, 9.29e+00, 9.35e+00]    

Best model at step 30000:
  train loss: 3.11e+01
  test loss: 1.81e+01
  test metric: [1.27e+01, 1.53e+01, 9.29e+00, 9.35e+00]

'train' took 46.720845 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.176567 s

'compile' took 0.803814 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.79e+02, 4.90e+02, 2.70e+00]    [0.00e+00, 1.08e+02, 2.70e+00]    [1.08e+02, 1.08e+02, 2.83e+01, 2.83e+01]    
1000      [4.52e+01, 3.39e+01, 2.68e+00]    [0.00e+00, 4.99e+01, 2.68e+00]    [5.01e+01, 4.99e+01, 3.11e+00, 3.02e+00]    
2000      [3.09e+01, 2.02e+01, 2.69e+00]    [0.00e+00, 1.86e+01, 2.69e+00]    [1.89e+01, 1.86e+01, 1.26e+01, 1.24e+01]    
3000      [2.91e+01, 1.14e+01, 2.69e+00]    [0.00e+00, 9.99e+00, 2.69e+00]    [1.01e+01, 9.99e+00, 6.06e+00, 5.69e+00]    
4000      [2.92e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 8.53e+00, 2.69e+00]    [8.65e+00, 8.53e+00, 4.12e+00, 4.27e+00]    
5000      [2.88e+01, 1.07e+01, 2.69e+00]    [0.00e+00, 9.86e+00, 2.69e+00]    [9.78e+00, 9.86e+00, 5.77e+00, 6.16e+00]    
6000      [3.00e+01, 1.50e+01, 2.69e+00]    [0.00e+00, 1.04e+01, 2.69e+00]    [1.02e+01, 1.04e+01, 7.40e+00, 8.10e+00]    
7000      [2.78e+01, 8.37e+00, 2.69e+00]    [0.00e+00, 1.09e+01, 2.69e+00]    [1.07e+01, 1.09e+01, 8.08e+00, 9.01e+00]    
8000      [2.82e+01, 7.15e+00, 2.70e+00]    [0.00e+00, 1.20e+01, 2.70e+00]    [1.16e+01, 1.20e+01, 9.10e+00, 1.03e+01]    
9000      [3.08e+01, 1.18e+01, 2.70e+00]    [0.00e+00, 1.28e+01, 2.70e+00]    [1.22e+01, 1.28e+01, 1.01e+01, 1.18e+01]    
10000     [2.86e+01, 8.21e+00, 2.71e+00]    [0.00e+00, 1.30e+01, 2.71e+00]    [1.22e+01, 1.30e+01, 1.10e+01, 1.28e+01]    
11000     [3.00e+01, 9.27e+00, 2.71e+00]    [0.00e+00, 1.27e+01, 2.71e+00]    [1.17e+01, 1.27e+01, 1.02e+01, 1.22e+01]    
12000     [2.99e+01, 7.75e+00, 2.72e+00]    [0.00e+00, 1.30e+01, 2.72e+00]    [1.18e+01, 1.30e+01, 9.64e+00, 1.16e+01]    
13000     [3.00e+01, 7.74e+00, 2.72e+00]    [0.00e+00, 1.30e+01, 2.72e+00]    [1.15e+01, 1.30e+01, 9.46e+00, 1.14e+01]    
14000     [2.81e+01, 4.33e+00, 2.73e+00]    [0.00e+00, 1.28e+01, 2.73e+00]    [1.14e+01, 1.28e+01, 9.33e+00, 1.19e+01]    
15000     [2.96e+01, 4.96e+00, 2.73e+00]    [0.00e+00, 1.33e+01, 2.73e+00]    [1.22e+01, 1.33e+01, 8.17e+00, 1.12e+01]    
16000     [2.83e+01, 5.02e+00, 2.74e+00]    [0.00e+00, 1.31e+01, 2.74e+00]    [1.20e+01, 1.31e+01, 7.71e+00, 1.18e+01]    
17000     [2.87e+01, 5.75e+00, 2.74e+00]    [0.00e+00, 1.31e+01, 2.74e+00]    [1.21e+01, 1.31e+01, 6.87e+00, 1.16e+01]    
18000     [2.76e+01, 1.67e+00, 2.74e+00]    [0.00e+00, 1.28e+01, 2.74e+00]    [1.18e+01, 1.28e+01, 6.12e+00, 1.14e+01]    
19000     [2.78e+01, 3.12e+00, 2.74e+00]    [0.00e+00, 1.29e+01, 2.74e+00]    [1.20e+01, 1.29e+01, 5.87e+00, 1.20e+01]    
20000     [2.70e+01, 1.11e+00, 2.74e+00]    [0.00e+00, 1.34e+01, 2.74e+00]    [1.27e+01, 1.34e+01, 5.86e+00, 1.19e+01]    
21000     [2.74e+01, 2.89e+00, 2.75e+00]    [0.00e+00, 1.28e+01, 2.75e+00]    [1.19e+01, 1.28e+01, 5.91e+00, 1.24e+01]    
22000     [2.90e+01, 6.86e+00, 2.75e+00]    [0.00e+00, 1.34e+01, 2.75e+00]    [1.29e+01, 1.34e+01, 6.55e+00, 1.16e+01]    
23000     [2.90e+01, 7.78e+00, 2.75e+00]    [0.00e+00, 1.27e+01, 2.75e+00]    [1.22e+01, 1.27e+01, 6.54e+00, 1.33e+01]    
24000     [2.69e+01, 3.30e+00, 2.75e+00]    [0.00e+00, 1.31e+01, 2.75e+00]    [1.28e+01, 1.31e+01, 7.04e+00, 1.31e+01]    
25000     [2.64e+01, 3.22e+00, 2.75e+00]    [0.00e+00, 1.41e+01, 2.75e+00]    [1.38e+01, 1.41e+01, 8.11e+00, 1.23e+01]    
26000     [2.84e+01, 7.97e+00, 2.75e+00]    [0.00e+00, 1.46e+01, 2.75e+00]    [1.40e+01, 1.46e+01, 8.96e+00, 1.21e+01]    
27000     [2.67e+01, 5.06e+00, 2.74e+00]    [0.00e+00, 1.38e+01, 2.74e+00]    [1.32e+01, 1.38e+01, 8.78e+00, 1.33e+01]    
28000     [2.64e+01, 3.90e+00, 2.74e+00]    [0.00e+00, 1.46e+01, 2.74e+00]    [1.39e+01, 1.46e+01, 9.96e+00, 1.25e+01]    
29000     [2.62e+01, 4.58e+00, 2.74e+00]    [0.00e+00, 1.42e+01, 2.74e+00]    [1.35e+01, 1.42e+01, 9.46e+00, 1.36e+01]    
30000     [2.67e+01, 6.34e+00, 2.74e+00]    [0.00e+00, 1.40e+01, 2.74e+00]    [1.34e+01, 1.40e+01, 1.00e+01, 1.30e+01]    

Best model at step 20000:
  train loss: 3.08e+01
  test loss: 1.62e+01
  test metric: [1.27e+01, 1.34e+01, 5.86e+00, 1.19e+01]

'train' took 59.781521 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.382004 s

'compile' took 1.404813 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.78e+02, 5.96e+02, 2.74e+00]    [0.00e+00, 1.13e+02, 2.74e+00]    [1.13e+02, 1.13e+02, 3.00e+01, 3.00e+01]    
1000      [3.78e+01, 2.71e+01, 2.72e+00]    [0.00e+00, 4.56e+01, 2.72e+00]    [4.57e+01, 4.56e+01, 2.41e+01, 2.43e+01]    
2000      [2.99e+01, 1.11e+01, 2.72e+00]    [0.00e+00, 2.93e+01, 2.72e+00]    [2.91e+01, 2.93e+01, 1.49e+01, 1.45e+01]    
3000      [2.85e+01, 7.43e+00, 2.72e+00]    [0.00e+00, 2.66e+01, 2.72e+00]    [2.65e+01, 2.66e+01, 1.31e+01, 1.28e+01]    
4000      [3.12e+01, 1.77e+01, 2.72e+00]    [0.00e+00, 2.49e+01, 2.72e+00]    [2.49e+01, 2.49e+01, 1.33e+01, 1.30e+01]    
5000      [2.86e+01, 7.91e+00, 2.72e+00]    [0.00e+00, 2.52e+01, 2.72e+00]    [2.53e+01, 2.52e+01, 1.15e+01, 1.14e+01]    
6000      [2.82e+01, 8.45e+00, 2.71e+00]    [0.00e+00, 2.55e+01, 2.71e+00]    [2.57e+01, 2.55e+01, 1.08e+01, 1.08e+01]    
7000      [2.69e+01, 4.19e+00, 2.71e+00]    [0.00e+00, 2.45e+01, 2.71e+00]    [2.48e+01, 2.45e+01, 1.23e+01, 1.24e+01]    
8000      [2.65e+01, 3.81e+00, 2.71e+00]    [0.00e+00, 2.49e+01, 2.71e+00]    [2.53e+01, 2.49e+01, 1.17e+01, 1.18e+01]    
9000      [3.02e+01, 1.46e+01, 2.71e+00]    [0.00e+00, 2.52e+01, 2.71e+00]    [2.57e+01, 2.52e+01, 1.14e+01, 1.16e+01]    
10000     [2.60e+01, 4.11e+00, 2.71e+00]    [0.00e+00, 2.42e+01, 2.71e+00]    [2.49e+01, 2.42e+01, 1.26e+01, 1.29e+01]    
11000     [2.63e+01, 6.62e+00, 2.71e+00]    [0.00e+00, 2.50e+01, 2.71e+00]    [2.58e+01, 2.50e+01, 1.16e+01, 1.20e+01]    
12000     [2.58e+01, 5.18e+00, 2.71e+00]    [0.00e+00, 2.47e+01, 2.71e+00]    [2.56e+01, 2.47e+01, 1.21e+01, 1.25e+01]    
13000     [2.60e+01, 6.82e+00, 2.70e+00]    [0.00e+00, 2.48e+01, 2.70e+00]    [2.58e+01, 2.48e+01, 1.20e+01, 1.24e+01]    
14000     [2.51e+01, 4.14e+00, 2.70e+00]    [0.00e+00, 2.46e+01, 2.70e+00]    [2.56e+01, 2.46e+01, 1.24e+01, 1.28e+01]    
15000     [2.74e+01, 1.06e+01, 2.70e+00]    [0.00e+00, 2.39e+01, 2.70e+00]    [2.50e+01, 2.39e+01, 1.34e+01, 1.38e+01]    
16000     [2.51e+01, 4.95e+00, 2.70e+00]    [0.00e+00, 2.48e+01, 2.70e+00]    [2.59e+01, 2.48e+01, 1.25e+01, 1.29e+01]    
17000     [2.52e+01, 6.56e+00, 2.70e+00]    [0.00e+00, 2.46e+01, 2.70e+00]    [2.58e+01, 2.46e+01, 1.29e+01, 1.32e+01]    
18000     [2.57e+01, 6.94e+00, 2.70e+00]    [0.00e+00, 2.47e+01, 2.70e+00]    [2.59e+01, 2.47e+01, 1.27e+01, 1.30e+01]    
19000     [2.56e+01, 7.77e+00, 2.70e+00]    [0.00e+00, 2.48e+01, 2.70e+00]    [2.61e+01, 2.48e+01, 1.28e+01, 1.31e+01]    
20000     [2.47e+01, 5.11e+00, 2.70e+00]    [0.00e+00, 2.45e+01, 2.70e+00]    [2.58e+01, 2.45e+01, 1.31e+01, 1.34e+01]    
21000     [2.42e+01, 3.65e+00, 2.70e+00]    [0.00e+00, 2.47e+01, 2.70e+00]    [2.60e+01, 2.47e+01, 1.29e+01, 1.32e+01]    
22000     [2.53e+01, 6.57e+00, 2.70e+00]    [0.00e+00, 2.41e+01, 2.70e+00]    [2.54e+01, 2.41e+01, 1.39e+01, 1.42e+01]    
23000     [2.49e+01, 6.02e+00, 2.70e+00]    [0.00e+00, 2.45e+01, 2.70e+00]    [2.58e+01, 2.45e+01, 1.35e+01, 1.38e+01]    
24000     [2.51e+01, 7.94e+00, 2.69e+00]    [0.00e+00, 2.50e+01, 2.69e+00]    [2.64e+01, 2.50e+01, 1.31e+01, 1.34e+01]    
25000     [2.46e+01, 6.95e+00, 2.69e+00]    [0.00e+00, 2.45e+01, 2.69e+00]    [2.59e+01, 2.45e+01, 1.39e+01, 1.42e+01]    
26000     [2.37e+01, 4.35e+00, 2.69e+00]    [0.00e+00, 2.47e+01, 2.69e+00]    [2.62e+01, 2.47e+01, 1.36e+01, 1.39e+01]    
27000     [2.49e+01, 6.60e+00, 2.69e+00]    [0.00e+00, 2.43e+01, 2.69e+00]    [2.58e+01, 2.43e+01, 1.42e+01, 1.44e+01]    
28000     [2.40e+01, 5.37e+00, 2.69e+00]    [0.00e+00, 2.47e+01, 2.69e+00]    [2.63e+01, 2.47e+01, 1.40e+01, 1.43e+01]    
29000     [2.51e+01, 7.41e+00, 2.69e+00]    [0.00e+00, 2.45e+01, 2.69e+00]    [2.60e+01, 2.45e+01, 1.42e+01, 1.45e+01]    
30000     [2.37e+01, 3.95e+00, 2.69e+00]    [0.00e+00, 2.48e+01, 2.69e+00]    [2.63e+01, 2.48e+01, 1.41e+01, 1.43e+01]    

Best model at step 30000:
  train loss: 3.03e+01
  test loss: 2.74e+01
  test metric: [2.63e+01, 2.48e+01, 1.41e+01, 1.43e+01]

'train' took 71.448119 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[16.333318401372264, 92.40160734369685, 16.60167494295129, 12.892394679821631, 15.1355101539706, 36.2878945297814, 13.482428600092046, 15.333374921787364, 13.423874424287716, 24.757660075924193]
sigma_y 10 25.664973807368533 23.271614334485133
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.301196 s

'compile' took 1.139336 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.77e+02, 6.68e+02, 2.81e+00]    [0.00e+00, 7.99e+01, 2.81e+00]    [7.99e+01, 7.99e+01, 6.07e+01, 6.07e+01]    
1000      [3.62e+01, 3.25e+01, 2.80e+00]    [0.00e+00, 4.07e+01, 2.80e+00]    [4.05e+01, 4.07e+01, 2.38e+01, 2.33e+01]    
2000      [3.12e+01, 1.94e+01, 2.81e+00]    [0.00e+00, 2.38e+01, 2.81e+00]    [2.42e+01, 2.38e+01, 1.82e+01, 1.80e+01]    
3000      [2.85e+01, 1.02e+01, 2.81e+00]    [0.00e+00, 1.86e+01, 2.81e+00]    [1.93e+01, 1.86e+01, 1.77e+01, 1.77e+01]    
4000      [2.80e+01, 8.40e+00, 2.81e+00]    [0.00e+00, 1.72e+01, 2.81e+00]    [1.79e+01, 1.72e+01, 1.79e+01, 1.80e+01]    
5000      [2.83e+01, 1.02e+01, 2.80e+00]    [0.00e+00, 1.53e+01, 2.80e+00]    [1.62e+01, 1.53e+01, 1.92e+01, 1.93e+01]    
6000      [2.72e+01, 6.83e+00, 2.80e+00]    [0.00e+00, 1.69e+01, 2.80e+00]    [1.58e+01, 1.69e+01, 1.99e+01, 1.85e+01]    
7000      [2.84e+01, 9.53e+00, 2.80e+00]    [0.00e+00, 1.80e+01, 2.80e+00]    [1.64e+01, 1.80e+01, 1.95e+01, 1.77e+01]    
8000      [2.70e+01, 6.30e+00, 2.80e+00]    [0.00e+00, 2.05e+01, 2.80e+00]    [1.87e+01, 2.05e+01, 1.80e+01, 1.59e+01]    
9000      [2.73e+01, 6.19e+00, 2.80e+00]    [0.00e+00, 2.09e+01, 2.80e+00]    [1.89e+01, 2.09e+01, 1.83e+01, 1.59e+01]    
10000     [2.83e+01, 8.40e+00, 2.80e+00]    [0.00e+00, 2.23e+01, 2.80e+00]    [2.01e+01, 2.23e+01, 1.77e+01, 1.52e+01]    
11000     [2.96e+01, 1.09e+01, 2.80e+00]    [0.00e+00, 2.48e+01, 2.80e+00]    [2.23e+01, 2.48e+01, 1.64e+01, 1.35e+01]    
12000     [2.76e+01, 5.42e+00, 2.80e+00]    [0.00e+00, 2.36e+01, 2.80e+00]    [2.09e+01, 2.36e+01, 1.81e+01, 1.52e+01]    
13000     [2.74e+01, 4.69e+00, 2.79e+00]    [0.00e+00, 2.48e+01, 2.79e+00]    [2.19e+01, 2.48e+01, 1.76e+01, 1.45e+01]    
14000     [3.09e+01, 1.23e+01, 2.79e+00]    [0.00e+00, 2.68e+01, 2.79e+00]    [2.38e+01, 2.68e+01, 1.68e+01, 1.33e+01]    
15000     [2.69e+01, 3.15e+00, 2.79e+00]    [0.00e+00, 2.62e+01, 2.79e+00]    [2.31e+01, 2.62e+01, 1.75e+01, 1.38e+01]    
16000     [3.16e+01, 1.15e+01, 2.79e+00]    [0.00e+00, 2.51e+01, 2.79e+00]    [2.19e+01, 2.51e+01, 1.89e+01, 1.51e+01]    
17000     [2.72e+01, 2.20e+00, 2.78e+00]    [0.00e+00, 2.59e+01, 2.78e+00]    [2.26e+01, 2.59e+01, 1.94e+01, 1.56e+01]    
18000     [2.76e+01, 3.40e+00, 2.78e+00]    [0.00e+00, 2.63e+01, 2.78e+00]    [2.29e+01, 2.63e+01, 1.96e+01, 1.56e+01]    
19000     [2.74e+01, 3.43e+00, 2.78e+00]    [0.00e+00, 2.67e+01, 2.78e+00]    [2.32e+01, 2.67e+01, 1.97e+01, 1.55e+01]    
20000     [3.23e+01, 1.23e+01, 2.78e+00]    [0.00e+00, 2.57e+01, 2.78e+00]    [2.21e+01, 2.57e+01, 2.06e+01, 1.62e+01]    
21000     [3.03e+01, 9.25e+00, 2.77e+00]    [0.00e+00, 2.63e+01, 2.77e+00]    [2.25e+01, 2.63e+01, 2.06e+01, 1.60e+01]    
22000     [2.75e+01, 4.35e+00, 2.77e+00]    [0.00e+00, 2.69e+01, 2.77e+00]    [2.31e+01, 2.69e+01, 2.06e+01, 1.59e+01]    
23000     [2.88e+01, 7.49e+00, 2.77e+00]    [0.00e+00, 2.63e+01, 2.77e+00]    [2.25e+01, 2.63e+01, 2.13e+01, 1.65e+01]    
24000     [2.94e+01, 9.12e+00, 2.77e+00]    [0.00e+00, 2.79e+01, 2.77e+00]    [2.38e+01, 2.79e+01, 2.08e+01, 1.58e+01]    
25000     [2.63e+01, 2.52e+00, 2.76e+00]    [0.00e+00, 2.74e+01, 2.76e+00]    [2.33e+01, 2.74e+01, 2.14e+01, 1.63e+01]    
26000     [2.77e+01, 6.38e+00, 2.76e+00]    [0.00e+00, 2.66e+01, 2.76e+00]    [2.25e+01, 2.66e+01, 2.21e+01, 1.69e+01]    
27000     [2.56e+01, 2.78e+00, 2.76e+00]    [0.00e+00, 2.71e+01, 2.76e+00]    [2.28e+01, 2.71e+01, 2.22e+01, 1.68e+01]    
28000     [2.53e+01, 1.08e+00, 2.76e+00]    [0.00e+00, 2.78e+01, 2.76e+00]    [2.33e+01, 2.78e+01, 2.20e+01, 1.63e+01]    
29000     [2.88e+01, 9.36e+00, 2.76e+00]    [0.00e+00, 2.88e+01, 2.76e+00]    [2.42e+01, 2.88e+01, 2.19e+01, 1.60e+01]    
30000     [2.54e+01, 2.29e+00, 2.75e+00]    [0.00e+00, 2.85e+01, 2.75e+00]    [2.39e+01, 2.85e+01, 2.22e+01, 1.61e+01]    

Best model at step 28000:
  train loss: 2.91e+01
  test loss: 3.06e+01
  test metric: [2.33e+01, 2.78e+01, 2.20e+01, 1.63e+01]

'train' took 66.045637 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.232851 s

'compile' took 1.204071 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [8.28e+02, 3.73e+02, 2.84e+00]    [0.00e+00, 2.76e+03, 2.84e+00]    [2.76e+03, 2.76e+03, 2.11e+03, 2.11e+03]    
1000      [3.06e+01, 1.21e+01, 2.83e+00]    [0.00e+00, 1.59e+01, 2.83e+00]    [1.70e+01, 1.59e+01, 1.15e+01, 1.66e+01]    
2000      [2.80e+01, 8.56e+00, 2.83e+00]    [0.00e+00, 3.07e+01, 2.83e+00]    [2.14e+01, 3.07e+01, 1.22e+01, 4.14e+01]    
3000      [3.06e+01, 9.44e+00, 2.82e+00]    [0.00e+00, 4.88e+01, 2.82e+00]    [2.90e+01, 4.88e+01, 2.99e+01, 5.61e+01]    
4000      [2.81e+01, 7.03e+00, 2.82e+00]    [0.00e+00, 2.24e+01, 2.82e+00]    [1.45e+01, 2.24e+01, 1.71e+01, 2.19e+01]    
5000      [3.05e+01, 7.76e+00, 2.82e+00]    [0.00e+00, 4.66e+01, 2.82e+00]    [2.76e+01, 4.66e+01, 2.99e+01, 5.24e+01]    
6000      [3.05e+01, 7.31e+00, 2.82e+00]    [0.00e+00, 4.37e+01, 2.82e+00]    [2.52e+01, 4.37e+01, 2.73e+01, 4.90e+01]    
7000      [3.18e+01, 6.56e+00, 2.81e+00]    [0.00e+00, 1.28e+01, 2.81e+00]    [3.39e+01, 1.28e+01, 2.32e+01, 1.24e+01]    
8000      [2.75e+01, 4.52e+00, 2.82e+00]    [0.00e+00, 2.02e+01, 2.82e+00]    [1.48e+01, 2.02e+01, 1.09e+01, 2.30e+01]    
9000      [2.74e+01, 4.01e+00, 2.82e+00]    [0.00e+00, 2.11e+01, 2.82e+00]    [1.26e+01, 2.11e+01, 8.89e+00, 2.48e+01]    
10000     [2.99e+01, 4.65e+00, 2.82e+00]    [0.00e+00, 4.06e+01, 2.82e+00]    [2.08e+01, 4.06e+01, 2.69e+01, 3.88e+01]    
11000     [2.93e+01, 4.05e+00, 2.82e+00]    [0.00e+00, 3.24e+01, 2.82e+00]    [2.03e+01, 3.24e+01, 2.09e+01, 3.28e+01]    
12000     [2.90e+01, 2.54e+00, 2.83e+00]    [0.00e+00, 9.57e+00, 2.83e+00]    [1.72e+01, 9.57e+00, 1.15e+01, 4.24e+00]    
13000     [2.92e+01, 2.22e+00, 2.83e+00]    [0.00e+00, 1.40e+01, 2.83e+00]    [1.67e+01, 1.40e+01, 1.30e+01, 5.05e+00]    
14000     [3.06e+01, 3.01e+00, 2.84e+00]    [0.00e+00, 3.39e+01, 2.84e+00]    [2.34e+01, 3.39e+01, 3.09e+01, 2.48e+01]    
15000     [2.64e+01, 6.53e-01, 2.84e+00]    [0.00e+00, 1.08e+01, 2.84e+00]    [1.41e+01, 1.08e+01, 7.25e+00, 2.72e+00]    
16000     [2.84e+01, 1.94e+00, 2.85e+00]    [0.00e+00, 2.50e+01, 2.85e+00]    [1.81e+01, 2.50e+01, 1.02e+01, 1.04e+01]    
17000     [2.73e+01, 1.38e+00, 2.85e+00]    [0.00e+00, 2.95e+01, 2.85e+00]    [1.56e+01, 2.95e+01, 9.82e+00, 1.23e+01]    
18000     [2.59e+01, 7.37e-01, 2.85e+00]    [0.00e+00, 2.70e+01, 2.85e+00]    [9.84e+00, 2.70e+01, 9.19e+00, 9.47e+00]    
19000     [2.97e+01, 3.01e+00, 2.85e+00]    [0.00e+00, 2.58e+01, 2.85e+00]    [2.92e+01, 2.58e+01, 3.08e+01, 9.42e+00]    
20000     [2.55e+01, 8.54e-01, 2.85e+00]    [0.00e+00, 1.48e+01, 2.85e+00]    [1.13e+01, 1.48e+01, 9.56e+00, 7.13e+00]    
21000     [2.64e+01, 1.53e+00, 2.85e+00]    [0.00e+00, 4.14e+01, 2.85e+00]    [1.66e+01, 4.14e+01, 1.14e+01, 2.13e+01]    
22000     [2.60e+01, 1.44e+00, 2.85e+00]    [0.00e+00, 1.13e+01, 2.85e+00]    [1.24e+01, 1.13e+01, 1.47e+01, 8.86e+00]    
23000     [2.49e+01, 7.57e-01, 2.85e+00]    [0.00e+00, 1.44e+01, 2.85e+00]    [1.13e+01, 1.44e+01, 9.92e+00, 7.76e+00]    
24000     [2.82e+01, 2.98e+00, 2.85e+00]    [0.00e+00, 5.16e+01, 2.85e+00]    [2.66e+01, 5.16e+01, 1.75e+01, 3.25e+01]    
25000     [2.44e+01, 1.06e+00, 2.84e+00]    [0.00e+00, 1.61e+01, 2.84e+00]    [1.17e+01, 1.61e+01, 5.27e+00, 6.60e+00]    
26000     [2.63e+01, 1.40e+00, 2.84e+00]    [0.00e+00, 1.09e+01, 2.84e+00]    [1.68e+01, 1.09e+01, 1.79e+01, 7.78e+00]    
27000     [2.59e+01, 1.83e+00, 2.84e+00]    [0.00e+00, 3.83e+01, 2.84e+00]    [1.62e+01, 3.83e+01, 1.14e+01, 2.10e+01]    
28000     [2.46e+01, 1.00e+00, 2.84e+00]    [0.00e+00, 3.10e+01, 2.84e+00]    [1.12e+01, 3.10e+01, 8.12e+00, 1.55e+01]    
29000     [2.35e+01, 4.96e-01, 2.84e+00]    [0.00e+00, 1.67e+01, 2.84e+00]    [6.28e+00, 1.67e+01, 2.79e+00, 9.45e+00]    
30000     [2.38e+01, 1.10e+00, 2.83e+00]    [0.00e+00, 1.99e+01, 2.83e+00]    [4.36e+00, 1.99e+01, 3.34e+00, 1.13e+01]    

Best model at step 29000:
  train loss: 2.69e+01
  test loss: 1.95e+01
  test metric: [6.28e+00, 1.67e+01, 2.79e+00, 9.45e+00]

'train' took 63.707278 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.231384 s

'compile' took 1.052214 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.03e+03, 1.53e+03, 2.89e+00]    [0.00e+00, 1.21e+02, 2.89e+00]    [1.21e+02, 1.21e+02, 9.58e+01, 9.58e+01]    
1000      [4.89e+01, 4.04e+01, 2.87e+00]    [0.00e+00, 6.18e+01, 2.87e+00]    [6.20e+01, 6.18e+01, 1.41e+01, 1.40e+01]    
2000      [2.94e+01, 1.59e+01, 2.87e+00]    [0.00e+00, 2.54e+01, 2.87e+00]    [2.60e+01, 2.54e+01, 1.42e+01, 1.42e+01]    
3000      [3.18e+01, 2.11e+01, 2.87e+00]    [0.00e+00, 1.57e+01, 2.87e+00]    [1.65e+01, 1.57e+01, 1.43e+01, 1.42e+01]    
4000      [2.92e+01, 1.27e+01, 2.88e+00]    [0.00e+00, 1.41e+01, 2.88e+00]    [1.47e+01, 1.41e+01, 1.27e+01, 1.24e+01]    
5000      [2.91e+01, 8.06e+00, 2.88e+00]    [0.00e+00, 1.21e+01, 2.88e+00]    [1.28e+01, 1.21e+01, 1.11e+01, 1.09e+01]    
6000      [2.99e+01, 8.69e+00, 2.88e+00]    [0.00e+00, 1.18e+01, 2.88e+00]    [1.21e+01, 1.18e+01, 1.07e+01, 1.01e+01]    
7000      [3.02e+01, 1.14e+01, 2.88e+00]    [0.00e+00, 1.13e+01, 2.88e+00]    [1.10e+01, 1.13e+01, 1.09e+01, 9.52e+00]    
8000      [3.26e+01, 1.37e+01, 2.88e+00]    [0.00e+00, 1.18e+01, 2.88e+00]    [1.20e+01, 1.18e+01, 1.10e+01, 1.01e+01]    
9000      [3.02e+01, 1.17e+01, 2.88e+00]    [0.00e+00, 1.14e+01, 2.88e+00]    [1.12e+01, 1.14e+01, 1.12e+01, 9.62e+00]    
10000     [3.02e+01, 1.26e+01, 2.88e+00]    [0.00e+00, 1.12e+01, 2.88e+00]    [1.06e+01, 1.12e+01, 1.13e+01, 9.20e+00]    
11000     [2.87e+01, 5.22e+00, 2.87e+00]    [0.00e+00, 1.11e+01, 2.87e+00]    [1.05e+01, 1.11e+01, 1.09e+01, 8.64e+00]    
12000     [2.91e+01, 6.15e+00, 2.87e+00]    [0.00e+00, 1.09e+01, 2.87e+00]    [1.02e+01, 1.09e+01, 1.07e+01, 8.20e+00]    
13000     [3.00e+01, 8.43e+00, 2.87e+00]    [0.00e+00, 1.08e+01, 2.87e+00]    [1.00e+01, 1.08e+01, 1.04e+01, 7.67e+00]    
14000     [2.81e+01, 6.02e+00, 2.87e+00]    [0.00e+00, 1.09e+01, 2.87e+00]    [9.99e+00, 1.09e+01, 1.05e+01, 7.49e+00]    
15000     [3.01e+01, 8.22e+00, 2.87e+00]    [0.00e+00, 1.06e+01, 2.87e+00]    [9.63e+00, 1.06e+01, 1.00e+01, 6.71e+00]    
16000     [2.90e+01, 1.05e+01, 2.87e+00]    [0.00e+00, 1.06e+01, 2.87e+00]    [9.53e+00, 1.06e+01, 1.05e+01, 6.91e+00]    
17000     [3.13e+01, 1.35e+01, 2.87e+00]    [0.00e+00, 1.04e+01, 2.87e+00]    [9.35e+00, 1.04e+01, 1.06e+01, 6.75e+00]    
18000     [3.13e+01, 1.40e+01, 2.86e+00]    [0.00e+00, 1.05e+01, 2.86e+00]    [9.74e+00, 1.05e+01, 1.06e+01, 6.70e+00]    
19000     [3.23e+01, 1.49e+01, 2.86e+00]    [0.00e+00, 1.07e+01, 2.86e+00]    [1.01e+01, 1.07e+01, 1.08e+01, 6.70e+00]    
20000     [3.07e+01, 1.09e+01, 2.86e+00]    [0.00e+00, 1.06e+01, 2.86e+00]    [9.78e+00, 1.06e+01, 1.04e+01, 5.90e+00]    
21000     [2.87e+01, 6.53e+00, 2.86e+00]    [0.00e+00, 1.09e+01, 2.86e+00]    [1.04e+01, 1.09e+01, 1.02e+01, 5.78e+00]    
22000     [2.77e+01, 3.60e+00, 2.86e+00]    [0.00e+00, 1.11e+01, 2.86e+00]    [1.08e+01, 1.11e+01, 1.04e+01, 5.93e+00]    
23000     [3.21e+01, 1.36e+01, 2.86e+00]    [0.00e+00, 1.07e+01, 2.86e+00]    [9.93e+00, 1.07e+01, 1.05e+01, 5.32e+00]    
24000     [2.78e+01, 5.42e+00, 2.86e+00]    [0.00e+00, 1.09e+01, 2.86e+00]    [1.01e+01, 1.09e+01, 1.01e+01, 4.72e+00]    
25000     [3.40e+01, 1.72e+01, 2.85e+00]    [0.00e+00, 1.07e+01, 2.85e+00]    [9.84e+00, 1.07e+01, 1.06e+01, 4.94e+00]    
26000     [2.92e+01, 7.53e+00, 2.85e+00]    [0.00e+00, 1.09e+01, 2.85e+00]    [1.04e+01, 1.09e+01, 1.05e+01, 5.03e+00]    
27000     [2.87e+01, 6.59e+00, 2.85e+00]    [0.00e+00, 1.10e+01, 2.85e+00]    [1.07e+01, 1.10e+01, 1.05e+01, 5.02e+00]    
28000     [2.88e+01, 6.42e+00, 2.85e+00]    [0.00e+00, 1.07e+01, 2.85e+00]    [1.07e+01, 1.07e+01, 1.06e+01, 5.13e+00]    
29000     [3.02e+01, 8.81e+00, 2.85e+00]    [0.00e+00, 1.08e+01, 2.85e+00]    [1.12e+01, 1.08e+01, 1.02e+01, 5.14e+00]    
30000     [2.76e+01, 4.59e+00, 2.84e+00]    [0.00e+00, 1.05e+01, 2.84e+00]    [1.12e+01, 1.05e+01, 1.03e+01, 5.23e+00]    

Best model at step 22000:
  train loss: 3.41e+01
  test loss: 1.40e+01
  test metric: [1.08e+01, 1.11e+01, 1.04e+01, 5.93e+00]

'train' took 64.900363 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.216902 s

'compile' took 1.074106 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.65e+02, 4.21e+02, 2.76e+00]    [0.00e+00, 1.64e+02, 2.76e+00]    [1.64e+02, 1.64e+02, 6.68e+01, 6.68e+01]    
1000      [3.60e+01, 3.03e+01, 2.75e+00]    [0.00e+00, 2.91e+01, 2.75e+00]    [2.90e+01, 2.91e+01, 1.64e+01, 1.61e+01]    
2000      [3.06e+01, 1.61e+01, 2.74e+00]    [0.00e+00, 1.19e+01, 2.74e+00]    [1.24e+01, 1.19e+01, 4.45e+00, 4.70e+00]    
3000      [3.09e+01, 1.75e+01, 2.74e+00]    [0.00e+00, 5.86e+00, 2.74e+00]    [6.51e+00, 5.86e+00, 3.58e+00, 3.79e+00]    
4000      [2.99e+01, 1.56e+01, 2.73e+00]    [0.00e+00, 4.64e+00, 2.73e+00]    [4.86e+00, 4.64e+00, 1.92e+00, 2.04e+00]    
5000      [2.88e+01, 1.26e+01, 2.73e+00]    [0.00e+00, 4.85e+00, 2.73e+00]    [5.06e+00, 4.85e+00, 2.34e+00, 3.14e+00]    
6000      [2.94e+01, 1.31e+01, 2.73e+00]    [0.00e+00, 5.51e+00, 2.73e+00]    [5.71e+00, 5.51e+00, 3.47e+00, 4.49e+00]    
7000      [2.77e+01, 9.96e+00, 2.73e+00]    [0.00e+00, 6.76e+00, 2.73e+00]    [6.95e+00, 6.76e+00, 4.38e+00, 5.56e+00]    
8000      [2.82e+01, 1.01e+01, 2.73e+00]    [0.00e+00, 6.51e+00, 2.73e+00]    [6.74e+00, 6.51e+00, 5.18e+00, 6.63e+00]    
9000      [2.80e+01, 9.59e+00, 2.73e+00]    [0.00e+00, 8.03e+00, 2.73e+00]    [8.28e+00, 8.03e+00, 6.48e+00, 8.25e+00]    
10000     [2.81e+01, 9.37e+00, 2.73e+00]    [0.00e+00, 9.96e+00, 2.73e+00]    [1.02e+01, 9.96e+00, 7.28e+00, 9.31e+00]    
11000     [3.19e+01, 1.56e+01, 2.73e+00]    [0.00e+00, 1.01e+01, 2.73e+00]    [1.04e+01, 1.01e+01, 7.96e+00, 1.04e+01]    
12000     [2.90e+01, 1.12e+01, 2.74e+00]    [0.00e+00, 9.87e+00, 2.74e+00]    [1.02e+01, 9.87e+00, 6.89e+00, 9.84e+00]    
13000     [3.03e+01, 1.12e+01, 2.74e+00]    [0.00e+00, 8.87e+00, 2.74e+00]    [9.30e+00, 8.87e+00, 5.27e+00, 8.66e+00]    
14000     [2.97e+01, 1.00e+01, 2.74e+00]    [0.00e+00, 8.28e+00, 2.74e+00]    [8.76e+00, 8.28e+00, 5.65e+00, 9.44e+00]    
15000     [2.70e+01, 4.76e+00, 2.74e+00]    [0.00e+00, 8.98e+00, 2.74e+00]    [8.72e+00, 8.98e+00, 5.90e+00, 9.35e+00]    
16000     [2.81e+01, 6.69e+00, 2.75e+00]    [0.00e+00, 8.97e+00, 2.75e+00]    [9.46e+00, 8.97e+00, 4.44e+00, 9.04e+00]    
17000     [2.71e+01, 7.11e+00, 2.75e+00]    [0.00e+00, 9.28e+00, 2.75e+00]    [9.77e+00, 9.28e+00, 4.55e+00, 9.66e+00]    
18000     [2.73e+01, 4.69e+00, 2.75e+00]    [0.00e+00, 8.84e+00, 2.75e+00]    [9.32e+00, 8.84e+00, 3.75e+00, 9.12e+00]    
19000     [2.68e+01, 5.41e+00, 2.76e+00]    [0.00e+00, 9.72e+00, 2.76e+00]    [1.01e+01, 9.72e+00, 3.00e+00, 9.06e+00]    
20000     [2.82e+01, 5.52e+00, 2.76e+00]    [0.00e+00, 9.76e+00, 2.76e+00]    [1.00e+01, 9.76e+00, 1.76e+00, 8.15e+00]    
21000     [3.04e+01, 8.84e+00, 2.76e+00]    [0.00e+00, 9.60e+00, 2.76e+00]    [9.78e+00, 9.60e+00, 1.53e+00, 7.67e+00]    
22000     [2.65e+01, 3.46e+00, 2.76e+00]    [0.00e+00, 9.33e+00, 2.76e+00]    [9.41e+00, 9.33e+00, 2.43e+00, 9.07e+00]    
23000     [3.15e+01, 1.13e+01, 2.76e+00]    [0.00e+00, 8.75e+00, 2.76e+00]    [8.73e+00, 8.75e+00, 3.07e+00, 8.46e+00]    
24000     [2.82e+01, 5.29e+00, 2.76e+00]    [0.00e+00, 9.75e+00, 2.76e+00]    [9.46e+00, 9.75e+00, 2.69e+00, 8.41e+00]    
25000     [2.62e+01, 3.46e+00, 2.76e+00]    [0.00e+00, 1.02e+01, 2.76e+00]    [9.63e+00, 1.02e+01, 2.98e+00, 9.21e+00]    
26000     [2.58e+01, 2.75e+00, 2.76e+00]    [0.00e+00, 1.02e+01, 2.76e+00]    [9.41e+00, 1.02e+01, 3.71e+00, 9.37e+00]    
27000     [2.73e+01, 7.30e+00, 2.77e+00]    [0.00e+00, 1.03e+01, 2.77e+00]    [9.27e+00, 1.03e+01, 4.42e+00, 9.98e+00]    
28000     [2.70e+01, 7.24e+00, 2.77e+00]    [0.00e+00, 1.13e+01, 2.77e+00]    [9.80e+00, 1.13e+01, 4.41e+00, 9.53e+00]    
29000     [2.77e+01, 8.33e+00, 2.77e+00]    [0.00e+00, 1.26e+01, 2.77e+00]    [1.06e+01, 1.26e+01, 4.02e+00, 9.42e+00]    
30000     [2.51e+01, 2.07e+00, 2.77e+00]    [0.00e+00, 1.12e+01, 2.77e+00]    [9.07e+00, 1.12e+01, 4.91e+00, 9.63e+00]    

Best model at step 30000:
  train loss: 2.99e+01
  test loss: 1.40e+01
  test metric: [9.07e+00, 1.12e+01, 4.91e+00, 9.63e+00]

'train' took 63.469725 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.262300 s

'compile' took 1.373557 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.13e+02, 2.42e+02, 2.66e+00]    [0.00e+00, 2.90e+02, 2.66e+00]    [2.90e+02, 2.90e+02, 2.31e+02, 2.31e+02]    
1000      [3.14e+01, 2.45e+01, 2.65e+00]    [0.00e+00, 2.55e+01, 2.65e+00]    [3.41e+01, 2.55e+01, 1.28e+01, 1.53e+00]    
2000      [2.97e+01, 1.67e+01, 2.65e+00]    [0.00e+00, 3.09e+01, 2.65e+00]    [1.66e+01, 3.09e+01, 1.90e+01, 3.86e+01]    
3000      [2.65e+01, 1.22e+01, 2.65e+00]    [0.00e+00, 1.26e+01, 2.65e+00]    [9.84e+00, 1.26e+01, 8.22e+00, 1.01e+01]    
4000      [2.97e+01, 1.35e+01, 2.65e+00]    [0.00e+00, 3.21e+01, 2.65e+00]    [1.44e+01, 3.21e+01, 8.08e+00, 2.94e+01]    
5000      [2.82e+01, 1.12e+01, 2.65e+00]    [0.00e+00, 2.00e+01, 2.65e+00]    [1.36e+01, 2.00e+01, 7.76e+00, 1.26e+01]    
6000      [3.00e+01, 1.21e+01, 2.65e+00]    [0.00e+00, 4.77e+01, 2.65e+00]    [2.73e+01, 4.77e+01, 2.12e+01, 4.61e+01]    
7000      [2.70e+01, 8.60e+00, 2.65e+00]    [0.00e+00, 2.73e+01, 2.65e+00]    [1.35e+01, 2.73e+01, 9.33e+00, 1.70e+01]    
8000      [2.72e+01, 7.44e+00, 2.65e+00]    [0.00e+00, 3.66e+01, 2.65e+00]    [1.47e+01, 3.66e+01, 1.08e+01, 2.70e+01]    
9000      [2.79e+01, 7.45e+00, 2.65e+00]    [0.00e+00, 4.39e+01, 2.65e+00]    [2.12e+01, 4.39e+01, 1.26e+01, 3.51e+01]    
10000     [2.79e+01, 6.76e+00, 2.65e+00]    [0.00e+00, 3.98e+01, 2.65e+00]    [1.66e+01, 3.98e+01, 1.30e+01, 2.81e+01]    
11000     [3.11e+01, 9.71e+00, 2.65e+00]    [0.00e+00, 5.57e+01, 2.65e+00]    [3.20e+01, 5.57e+01, 2.26e+01, 4.90e+01]    
12000     [2.78e+01, 5.72e+00, 2.65e+00]    [0.00e+00, 3.22e+01, 2.65e+00]    [1.48e+01, 3.22e+01, 1.35e+01, 2.08e+01]    
13000     [2.93e+01, 6.28e+00, 2.66e+00]    [0.00e+00, 2.27e+01, 2.66e+00]    [2.32e+01, 2.27e+01, 1.57e+01, 1.39e+01]    
14000     [2.91e+01, 5.84e+00, 2.66e+00]    [0.00e+00, 2.33e+01, 2.66e+00]    [2.21e+01, 2.33e+01, 1.55e+01, 1.45e+01]    
15000     [3.23e+01, 8.57e+00, 2.66e+00]    [0.00e+00, 1.49e+01, 2.66e+00]    [2.91e+01, 1.49e+01, 2.23e+01, 1.52e+01]    
16000     [2.65e+01, 3.72e+00, 2.66e+00]    [0.00e+00, 3.38e+01, 2.66e+00]    [1.12e+01, 3.38e+01, 1.47e+01, 2.34e+01]    
17000     [2.68e+01, 3.74e+00, 2.67e+00]    [0.00e+00, 3.86e+01, 2.67e+00]    [1.52e+01, 3.86e+01, 1.22e+01, 2.94e+01]    
18000     [2.78e+01, 3.71e+00, 2.67e+00]    [0.00e+00, 2.55e+01, 2.67e+00]    [1.76e+01, 2.55e+01, 1.16e+01, 1.59e+01]    
19000     [3.55e+01, 1.24e+01, 2.68e+00]    [0.00e+00, 6.70e+01, 2.68e+00]    [4.37e+01, 6.70e+01, 3.97e+01, 6.49e+01]    
20000     [2.72e+01, 4.48e+00, 2.68e+00]    [0.00e+00, 3.29e+01, 2.68e+00]    [1.23e+01, 3.29e+01, 1.15e+01, 2.21e+01]    
21000     [2.77e+01, 3.05e+00, 2.69e+00]    [0.00e+00, 2.33e+01, 2.69e+00]    [1.60e+01, 2.33e+01, 8.54e+00, 1.49e+01]    
22000     [3.12e+01, 7.73e+00, 2.70e+00]    [0.00e+00, 5.61e+01, 2.70e+00]    [3.61e+01, 5.61e+01, 2.86e+01, 5.15e+01]    
23000     [2.90e+01, 5.43e+00, 2.70e+00]    [0.00e+00, 4.40e+01, 2.70e+00]    [2.66e+01, 4.40e+01, 1.61e+01, 3.54e+01]    
24000     [2.64e+01, 2.80e+00, 2.71e+00]    [0.00e+00, 3.15e+01, 2.71e+00]    [1.74e+01, 3.15e+01, 4.95e+00, 2.16e+01]    
25000     [2.62e+01, 1.96e+00, 2.71e+00]    [0.00e+00, 1.40e+01, 2.71e+00]    [1.82e+01, 1.40e+01, 4.10e+00, 1.78e+01]    
26000     [2.68e+01, 4.14e+00, 2.72e+00]    [0.00e+00, 3.14e+01, 2.72e+00]    [2.06e+01, 3.14e+01, 5.69e+00, 2.15e+01]    
27000     [2.51e+01, 2.49e+00, 2.72e+00]    [0.00e+00, 2.01e+01, 2.72e+00]    [1.27e+01, 2.01e+01, 5.33e+00, 1.61e+01]    
28000     [2.79e+01, 4.99e+00, 2.72e+00]    [0.00e+00, 3.15e+01, 2.72e+00]    [2.62e+01, 3.15e+01, 1.44e+01, 2.19e+01]    
29000     [2.62e+01, 3.53e+00, 2.73e+00]    [0.00e+00, 2.73e+01, 2.73e+00]    [2.22e+01, 2.73e+01, 8.10e+00, 1.78e+01]    
30000     [2.91e+01, 7.21e+00, 2.73e+00]    [0.00e+00, 3.89e+01, 2.73e+00]    [3.16e+01, 3.89e+01, 2.15e+01, 2.73e+01]    

Best model at step 27000:
  train loss: 3.03e+01
  test loss: 2.28e+01
  test metric: [1.27e+01, 2.01e+01, 5.33e+00, 1.61e+01]

'train' took 75.451529 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.337614 s

'compile' took 1.247578 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [4.45e+02, 6.84e+02, 2.73e+00]    [0.00e+00, 3.63e+02, 2.73e+00]    [3.63e+02, 3.63e+02, 3.94e+02, 3.94e+02]    
1000      [3.09e+01, 2.18e+01, 2.72e+00]    [0.00e+00, 2.80e+01, 2.72e+00]    [2.84e+01, 2.80e+01, 1.58e+01, 1.53e+01]    
2000      [2.85e+01, 1.14e+01, 2.71e+00]    [0.00e+00, 1.75e+01, 2.71e+00]    [1.80e+01, 1.75e+01, 1.58e+01, 1.51e+01]    
3000      [3.06e+01, 1.58e+01, 2.70e+00]    [0.00e+00, 1.81e+01, 2.70e+00]    [1.84e+01, 1.81e+01, 2.04e+01, 1.91e+01]    
4000      [2.87e+01, 1.19e+01, 2.69e+00]    [0.00e+00, 1.99e+01, 2.69e+00]    [2.03e+01, 1.99e+01, 1.51e+01, 1.34e+01]    
5000      [3.28e+01, 1.75e+01, 2.69e+00]    [0.00e+00, 1.10e+01, 2.69e+00]    [1.15e+01, 1.10e+01, 5.01e+00, 6.75e+00]    
6000      [2.85e+01, 1.01e+01, 2.68e+00]    [0.00e+00, 2.17e+01, 2.68e+00]    [2.23e+01, 2.17e+01, 1.25e+01, 9.92e+00]    
7000      [2.77e+01, 7.11e+00, 2.68e+00]    [0.00e+00, 2.22e+01, 2.68e+00]    [2.28e+01, 2.22e+01, 1.15e+01, 8.65e+00]    
8000      [2.76e+01, 6.40e+00, 2.69e+00]    [0.00e+00, 2.11e+01, 2.69e+00]    [2.18e+01, 2.11e+01, 1.12e+01, 8.54e+00]    
9000      [3.03e+01, 1.09e+01, 2.69e+00]    [0.00e+00, 2.59e+01, 2.69e+00]    [2.66e+01, 2.59e+01, 1.62e+01, 1.24e+01]    
10000     [2.84e+01, 7.26e+00, 2.69e+00]    [0.00e+00, 2.26e+01, 2.69e+00]    [2.32e+01, 2.26e+01, 1.19e+01, 9.10e+00]    
11000     [2.80e+01, 5.40e+00, 2.69e+00]    [0.00e+00, 1.92e+01, 2.69e+00]    [1.99e+01, 1.92e+01, 1.02e+01, 1.06e+01]    
12000     [2.86e+01, 5.96e+00, 2.69e+00]    [0.00e+00, 2.21e+01, 2.69e+00]    [2.27e+01, 2.21e+01, 1.28e+01, 9.61e+00]    
13000     [2.75e+01, 3.86e+00, 2.70e+00]    [0.00e+00, 1.86e+01, 2.70e+00]    [1.93e+01, 1.86e+01, 8.34e+00, 9.88e+00]    
14000     [2.84e+01, 4.73e+00, 2.70e+00]    [0.00e+00, 2.13e+01, 2.70e+00]    [2.18e+01, 2.13e+01, 1.05e+01, 7.59e+00]    
15000     [2.80e+01, 4.26e+00, 2.70e+00]    [0.00e+00, 1.60e+01, 2.70e+00]    [1.66e+01, 1.60e+01, 7.17e+00, 1.21e+01]    
16000     [2.80e+01, 4.99e+00, 2.70e+00]    [0.00e+00, 1.61e+01, 2.70e+00]    [1.57e+01, 1.61e+01, 6.86e+00, 1.20e+01]    
17000     [2.84e+01, 6.20e+00, 2.70e+00]    [0.00e+00, 1.77e+01, 2.70e+00]    [1.40e+01, 1.77e+01, 7.33e+00, 1.01e+01]    
18000     [3.04e+01, 8.32e+00, 2.71e+00]    [0.00e+00, 2.04e+01, 2.71e+00]    [2.07e+01, 2.04e+01, 1.28e+01, 6.66e+00]    
19000     [2.78e+01, 6.02e+00, 2.71e+00]    [0.00e+00, 1.97e+01, 2.71e+00]    [1.38e+01, 1.97e+01, 6.16e+00, 8.43e+00]    
20000     [2.73e+01, 5.00e+00, 2.71e+00]    [0.00e+00, 1.90e+01, 2.71e+00]    [1.37e+01, 1.90e+01, 5.64e+00, 8.89e+00]    
21000     [2.66e+01, 2.49e+00, 2.71e+00]    [0.00e+00, 1.50e+01, 2.71e+00]    [1.55e+01, 1.50e+01, 7.06e+00, 1.14e+01]    
22000     [2.71e+01, 3.38e+00, 2.71e+00]    [0.00e+00, 1.56e+01, 2.71e+00]    [1.62e+01, 1.56e+01, 8.45e+00, 1.01e+01]    
23000     [2.63e+01, 4.31e+00, 2.71e+00]    [0.00e+00, 2.01e+01, 2.71e+00]    [1.20e+01, 2.01e+01, 4.92e+00, 7.02e+00]    
24000     [2.59e+01, 4.30e+00, 2.71e+00]    [0.00e+00, 1.98e+01, 2.71e+00]    [1.21e+01, 1.98e+01, 4.58e+00, 7.19e+00]    
25000     [2.54e+01, 3.55e+00, 2.71e+00]    [0.00e+00, 2.06e+01, 2.71e+00]    [1.22e+01, 2.06e+01, 3.78e+00, 6.59e+00]    
26000     [2.59e+01, 2.76e+00, 2.71e+00]    [0.00e+00, 1.53e+01, 2.71e+00]    [1.54e+01, 1.53e+01, 7.06e+00, 1.13e+01]    
27000     [2.79e+01, 6.52e+00, 2.71e+00]    [0.00e+00, 1.69e+01, 2.71e+00]    [1.74e+01, 1.69e+01, 1.15e+01, 7.98e+00]    
28000     [2.83e+01, 7.52e+00, 2.71e+00]    [0.00e+00, 1.66e+01, 2.71e+00]    [1.72e+01, 1.66e+01, 1.22e+01, 7.75e+00]    
29000     [2.50e+01, 3.57e+00, 2.71e+00]    [0.00e+00, 2.21e+01, 2.71e+00]    [1.16e+01, 2.21e+01, 2.18e+00, 4.78e+00]    
30000     [2.49e+01, 3.55e+00, 2.71e+00]    [0.00e+00, 2.19e+01, 2.71e+00]    [1.09e+01, 2.19e+01, 2.58e+00, 4.81e+00]    

Best model at step 30000:
  train loss: 3.12e+01
  test loss: 2.46e+01
  test metric: [1.09e+01, 2.19e+01, 2.58e+00, 4.81e+00]

'train' took 66.132151 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.490836 s

'compile' took 1.535620 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.48e+02, 9.51e+02, 2.77e+00]    [0.00e+00, 1.25e+02, 2.77e+00]    [1.25e+02, 1.25e+02, 1.49e+01, 1.49e+01]    
1000      [3.57e+01, 2.72e+01, 2.73e+00]    [0.00e+00, 3.80e+01, 2.73e+00]    [3.78e+01, 3.80e+01, 3.14e+00, 2.80e+00]    
2000      [3.36e+01, 2.56e+01, 2.73e+00]    [0.00e+00, 1.82e+01, 2.73e+00]    [1.79e+01, 1.82e+01, 2.14e+00, 2.08e+00]    
3000      [2.98e+01, 1.52e+01, 2.73e+00]    [0.00e+00, 8.14e+00, 2.73e+00]    [7.92e+00, 8.14e+00, 5.80e+00, 4.89e+00]    
4000      [3.34e+01, 2.16e+01, 2.73e+00]    [0.00e+00, 8.94e+00, 2.73e+00]    [1.01e+01, 8.94e+00, 4.12e+00, 4.94e+00]    
5000      [2.89e+01, 1.27e+01, 2.72e+00]    [0.00e+00, 8.97e+00, 2.72e+00]    [9.29e+00, 8.97e+00, 5.77e+00, 5.39e+00]    
6000      [3.11e+01, 1.74e+01, 2.72e+00]    [0.00e+00, 1.01e+01, 2.72e+00]    [9.57e+00, 1.01e+01, 6.05e+00, 4.54e+00]    
7000      [2.96e+01, 1.48e+01, 2.73e+00]    [0.00e+00, 9.38e+00, 2.73e+00]    [1.07e+01, 9.38e+00, 6.44e+00, 7.20e+00]    
8000      [2.74e+01, 8.88e+00, 2.73e+00]    [0.00e+00, 1.03e+01, 2.73e+00]    [1.12e+01, 1.03e+01, 7.92e+00, 8.04e+00]    
9000      [3.11e+01, 1.76e+01, 2.74e+00]    [0.00e+00, 9.85e+00, 2.74e+00]    [1.18e+01, 9.85e+00, 8.03e+00, 9.02e+00]    
10000     [2.77e+01, 8.99e+00, 2.74e+00]    [0.00e+00, 1.06e+01, 2.74e+00]    [1.03e+01, 1.06e+01, 9.96e+00, 8.74e+00]    
11000     [2.82e+01, 7.07e+00, 2.75e+00]    [0.00e+00, 1.01e+01, 2.75e+00]    [9.34e+00, 1.01e+01, 1.02e+01, 8.67e+00]    
12000     [3.20e+01, 1.28e+01, 2.76e+00]    [0.00e+00, 9.88e+00, 2.76e+00]    [9.10e+00, 9.88e+00, 1.01e+01, 8.76e+00]    
13000     [2.86e+01, 7.72e+00, 2.76e+00]    [0.00e+00, 7.86e+00, 2.76e+00]    [1.07e+01, 7.86e+00, 8.83e+00, 9.30e+00]    
14000     [3.01e+01, 7.11e+00, 2.77e+00]    [0.00e+00, 9.78e+00, 2.77e+00]    [8.23e+00, 9.78e+00, 1.03e+01, 8.23e+00]    
15000     [3.05e+01, 1.04e+01, 2.77e+00]    [0.00e+00, 9.40e+00, 2.77e+00]    [8.73e+00, 9.40e+00, 1.03e+01, 8.35e+00]    
16000     [2.95e+01, 7.48e+00, 2.78e+00]    [0.00e+00, 1.02e+01, 2.78e+00]    [8.12e+00, 1.02e+01, 1.04e+01, 7.44e+00]    
17000     [2.89e+01, 6.78e+00, 2.78e+00]    [0.00e+00, 8.45e+00, 2.78e+00]    [1.11e+01, 8.45e+00, 9.50e+00, 8.82e+00]    
18000     [3.06e+01, 6.59e+00, 2.79e+00]    [0.00e+00, 1.07e+01, 2.79e+00]    [1.01e+01, 1.07e+01, 1.08e+01, 8.25e+00]    
19000     [2.93e+01, 3.87e+00, 2.79e+00]    [0.00e+00, 1.02e+01, 2.79e+00]    [1.09e+01, 1.02e+01, 1.01e+01, 7.78e+00]    
20000     [2.95e+01, 5.14e+00, 2.80e+00]    [0.00e+00, 1.08e+01, 2.80e+00]    [1.08e+01, 1.08e+01, 1.01e+01, 7.10e+00]    
21000     [3.21e+01, 1.01e+01, 2.80e+00]    [0.00e+00, 1.04e+01, 2.80e+00]    [1.17e+01, 1.04e+01, 9.56e+00, 6.81e+00]    
22000     [2.71e+01, 3.98e+00, 2.81e+00]    [0.00e+00, 9.36e+00, 2.81e+00]    [1.37e+01, 9.36e+00, 8.76e+00, 6.86e+00]    
23000     [2.84e+01, 7.57e+00, 2.81e+00]    [0.00e+00, 1.17e+01, 2.81e+00]    [1.19e+01, 1.17e+01, 1.00e+01, 6.36e+00]    
24000     [2.92e+01, 9.57e+00, 2.81e+00]    [0.00e+00, 1.22e+01, 2.81e+00]    [1.23e+01, 1.22e+01, 1.02e+01, 6.62e+00]    
25000     [2.78e+01, 4.50e+00, 2.82e+00]    [0.00e+00, 1.11e+01, 2.82e+00]    [1.42e+01, 1.11e+01, 9.09e+00, 6.11e+00]    
26000     [2.89e+01, 9.06e+00, 2.82e+00]    [0.00e+00, 1.30e+01, 2.82e+00]    [1.36e+01, 1.30e+01, 1.05e+01, 7.29e+00]    
27000     [2.64e+01, 3.49e+00, 2.82e+00]    [0.00e+00, 1.24e+01, 2.82e+00]    [1.53e+01, 1.24e+01, 9.72e+00, 6.89e+00]    
28000     [2.55e+01, 1.44e+00, 2.82e+00]    [0.00e+00, 1.33e+01, 2.82e+00]    [1.50e+01, 1.33e+01, 1.02e+01, 7.75e+00]    
29000     [2.73e+01, 6.48e+00, 2.83e+00]    [0.00e+00, 1.46e+01, 2.83e+00]    [1.52e+01, 1.46e+01, 1.15e+01, 9.26e+00]    
30000     [2.90e+01, 1.06e+01, 2.83e+00]    [0.00e+00, 1.55e+01, 2.83e+00]    [1.49e+01, 1.55e+01, 1.23e+01, 1.05e+01]    

Best model at step 28000:
  train loss: 2.97e+01
  test loss: 1.62e+01
  test metric: [1.50e+01, 1.33e+01, 1.02e+01, 7.75e+00]

'train' took 63.789822 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.359347 s

'compile' took 1.216058 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.91e+02, 4.08e+02, 2.72e+00]    [0.00e+00, 1.85e+02, 2.72e+00]    [1.85e+02, 1.85e+02, 1.30e+02, 1.30e+02]    
1000      [3.58e+01, 2.77e+01, 2.72e+00]    [0.00e+00, 5.38e+01, 2.72e+00]    [5.37e+01, 5.38e+01, 1.30e+01, 1.29e+01]    
2000      [2.99e+01, 1.21e+01, 2.73e+00]    [0.00e+00, 2.51e+01, 2.73e+00]    [2.50e+01, 2.51e+01, 3.04e+00, 3.86e+00]    
3000      [2.83e+01, 7.85e+00, 2.73e+00]    [0.00e+00, 1.24e+01, 2.73e+00]    [1.25e+01, 1.24e+01, 1.12e+01, 1.12e+01]    
4000      [2.77e+01, 1.03e+01, 2.73e+00]    [0.00e+00, 1.13e+01, 2.73e+00]    [1.13e+01, 1.13e+01, 1.25e+01, 1.25e+01]    
5000      [2.82e+01, 1.23e+01, 2.72e+00]    [0.00e+00, 1.38e+01, 2.72e+00]    [1.36e+01, 1.38e+01, 1.16e+01, 1.15e+01]    
6000      [2.84e+01, 1.26e+01, 2.72e+00]    [0.00e+00, 1.56e+01, 2.72e+00]    [1.54e+01, 1.56e+01, 1.14e+01, 1.16e+01]    
7000      [2.95e+01, 1.52e+01, 2.72e+00]    [0.00e+00, 1.76e+01, 2.72e+00]    [1.73e+01, 1.76e+01, 1.11e+01, 1.16e+01]    
8000      [2.93e+01, 1.13e+01, 2.72e+00]    [0.00e+00, 1.72e+01, 2.72e+00]    [1.69e+01, 1.72e+01, 1.19e+01, 1.26e+01]    
9000      [2.87e+01, 1.35e+01, 2.72e+00]    [0.00e+00, 2.05e+01, 2.72e+00]    [2.01e+01, 2.05e+01, 1.11e+01, 1.21e+01]    
10000     [2.65e+01, 5.29e+00, 2.72e+00]    [0.00e+00, 2.11e+01, 2.72e+00]    [1.95e+01, 2.11e+01, 1.33e+01, 1.29e+01]    
11000     [3.09e+01, 1.62e+01, 2.72e+00]    [0.00e+00, 2.26e+01, 2.72e+00]    [2.23e+01, 2.26e+01, 1.29e+01, 1.48e+01]    
12000     [3.02e+01, 1.20e+01, 2.72e+00]    [0.00e+00, 2.22e+01, 2.72e+00]    [2.02e+01, 2.22e+01, 1.41e+01, 1.37e+01]    
13000     [2.98e+01, 1.36e+01, 2.72e+00]    [0.00e+00, 2.29e+01, 2.72e+00]    [2.27e+01, 2.29e+01, 1.35e+01, 1.60e+01]    
14000     [2.88e+01, 1.19e+01, 2.72e+00]    [0.00e+00, 2.32e+01, 2.72e+00]    [2.30e+01, 2.32e+01, 1.21e+01, 1.48e+01]    
15000     [2.78e+01, 9.16e+00, 2.72e+00]    [0.00e+00, 2.33e+01, 2.72e+00]    [2.10e+01, 2.33e+01, 1.45e+01, 1.44e+01]    
16000     [2.90e+01, 1.25e+01, 2.72e+00]    [0.00e+00, 2.25e+01, 2.72e+00]    [2.17e+01, 2.25e+01, 1.33e+01, 1.53e+01]    
17000     [2.85e+01, 1.15e+01, 2.72e+00]    [0.00e+00, 2.19e+01, 2.72e+00]    [2.21e+01, 2.19e+01, 1.21e+01, 1.56e+01]    
18000     [2.62e+01, 6.33e+00, 2.72e+00]    [0.00e+00, 2.17e+01, 2.72e+00]    [2.16e+01, 2.17e+01, 1.20e+01, 1.52e+01]    
19000     [2.98e+01, 1.17e+01, 2.72e+00]    [0.00e+00, 2.19e+01, 2.72e+00]    [1.94e+01, 2.19e+01, 1.27e+01, 1.26e+01]    
20000     [2.71e+01, 8.34e+00, 2.72e+00]    [0.00e+00, 2.22e+01, 2.72e+00]    [2.05e+01, 2.22e+01, 1.24e+01, 1.37e+01]    
21000     [2.63e+01, 6.56e+00, 2.72e+00]    [0.00e+00, 2.31e+01, 2.72e+00]    [1.93e+01, 2.31e+01, 1.34e+01, 1.21e+01]    
22000     [2.73e+01, 6.89e+00, 2.72e+00]    [0.00e+00, 2.22e+01, 2.72e+00]    [1.83e+01, 2.22e+01, 1.26e+01, 1.11e+01]    
23000     [2.54e+01, 3.61e+00, 2.72e+00]    [0.00e+00, 2.17e+01, 2.72e+00]    [1.90e+01, 2.17e+01, 1.14e+01, 1.20e+01]    
24000     [2.82e+01, 1.13e+01, 2.71e+00]    [0.00e+00, 2.23e+01, 2.71e+00]    [1.88e+01, 2.23e+01, 1.18e+01, 1.16e+01]    
25000     [2.73e+01, 7.64e+00, 2.71e+00]    [0.00e+00, 2.16e+01, 2.71e+00]    [1.78e+01, 2.16e+01, 1.13e+01, 1.04e+01]    
26000     [2.75e+01, 9.50e+00, 2.71e+00]    [0.00e+00, 2.05e+01, 2.71e+00]    [2.04e+01, 2.05e+01, 9.03e+00, 1.36e+01]    
27000     [2.72e+01, 8.98e+00, 2.71e+00]    [0.00e+00, 2.11e+01, 2.71e+00]    [1.92e+01, 2.11e+01, 9.87e+00, 1.20e+01]    
28000     [2.62e+01, 6.41e+00, 2.71e+00]    [0.00e+00, 2.30e+01, 2.71e+00]    [1.72e+01, 2.30e+01, 1.21e+01, 9.26e+00]    
29000     [2.56e+01, 4.35e+00, 2.71e+00]    [0.00e+00, 2.11e+01, 2.71e+00]    [1.78e+01, 2.11e+01, 9.95e+00, 1.03e+01]    
30000     [2.56e+01, 5.56e+00, 2.71e+00]    [0.00e+00, 2.08e+01, 2.71e+00]    [1.85e+01, 2.08e+01, 9.17e+00, 1.12e+01]    

Best model at step 23000:
  train loss: 3.17e+01
  test loss: 2.44e+01
  test metric: [1.90e+01, 2.17e+01, 1.14e+01, 1.20e+01]

'train' took 63.537098 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.258840 s

'compile' took 1.111349 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.66e+02, 1.77e+02, 2.73e+00]    [0.00e+00, 8.38e+01, 2.73e+00]    [8.38e+01, 8.38e+01, 2.31e+01, 2.31e+01]    
1000      [4.16e+01, 3.31e+01, 2.71e+00]    [0.00e+00, 4.19e+01, 2.71e+00]    [4.20e+01, 4.19e+01, 6.22e+00, 6.27e+00]    
2000      [3.25e+01, 2.73e+01, 2.72e+00]    [0.00e+00, 1.19e+01, 2.72e+00]    [1.23e+01, 1.19e+01, 4.57e+00, 4.33e+00]    
3000      [2.94e+01, 1.47e+01, 2.73e+00]    [0.00e+00, 5.17e+00, 2.73e+00]    [4.91e+00, 5.17e+00, 4.64e+00, 3.80e+00]    
4000      [2.98e+01, 1.38e+01, 2.73e+00]    [0.00e+00, 6.28e+00, 2.73e+00]    [5.94e+00, 6.28e+00, 1.55e+00, 4.78e-01]    
5000      [3.06e+01, 1.61e+01, 2.73e+00]    [0.00e+00, 7.70e+00, 2.73e+00]    [7.29e+00, 7.70e+00, 1.92e+00, 3.13e+00]    
6000      [2.95e+01, 1.05e+01, 2.73e+00]    [0.00e+00, 7.97e+00, 2.73e+00]    [7.50e+00, 7.97e+00, 4.36e+00, 5.72e+00]    
7000      [2.87e+01, 1.04e+01, 2.73e+00]    [0.00e+00, 8.76e+00, 2.73e+00]    [7.80e+00, 8.76e+00, 6.92e+00, 7.94e+00]    
8000      [2.96e+01, 1.15e+01, 2.73e+00]    [0.00e+00, 1.04e+01, 2.73e+00]    [8.86e+00, 1.04e+01, 8.64e+00, 9.30e+00]    
9000      [3.19e+01, 1.38e+01, 2.73e+00]    [0.00e+00, 1.09e+01, 2.73e+00]    [9.29e+00, 1.09e+01, 8.49e+00, 9.18e+00]    
10000     [2.90e+01, 1.01e+01, 2.73e+00]    [0.00e+00, 1.21e+01, 2.73e+00]    [1.04e+01, 1.21e+01, 1.03e+01, 1.11e+01]    
11000     [2.88e+01, 6.14e+00, 2.73e+00]    [0.00e+00, 1.28e+01, 2.73e+00]    [1.09e+01, 1.28e+01, 1.07e+01, 1.15e+01]    
12000     [3.23e+01, 1.35e+01, 2.72e+00]    [0.00e+00, 1.33e+01, 2.72e+00]    [1.11e+01, 1.33e+01, 1.08e+01, 1.16e+01]    
13000     [3.19e+01, 1.20e+01, 2.72e+00]    [0.00e+00, 1.40e+01, 2.72e+00]    [1.16e+01, 1.40e+01, 1.15e+01, 1.24e+01]    
14000     [3.02e+01, 1.14e+01, 2.72e+00]    [0.00e+00, 1.50e+01, 2.72e+00]    [1.22e+01, 1.50e+01, 1.31e+01, 1.39e+01]    
15000     [3.32e+01, 1.44e+01, 2.72e+00]    [0.00e+00, 1.47e+01, 2.72e+00]    [1.15e+01, 1.47e+01, 1.18e+01, 1.27e+01]    
16000     [3.14e+01, 1.10e+01, 2.72e+00]    [0.00e+00, 1.48e+01, 2.72e+00]    [1.11e+01, 1.48e+01, 1.18e+01, 1.27e+01]    
17000     [2.87e+01, 8.23e+00, 2.72e+00]    [0.00e+00, 1.49e+01, 2.72e+00]    [1.08e+01, 1.49e+01, 1.23e+01, 1.32e+01]    
18000     [3.06e+01, 9.35e+00, 2.72e+00]    [0.00e+00, 1.45e+01, 2.72e+00]    [9.80e+00, 1.45e+01, 1.12e+01, 1.23e+01]    
19000     [3.23e+01, 1.31e+01, 2.72e+00]    [0.00e+00, 1.46e+01, 2.72e+00]    [9.95e+00, 1.46e+01, 1.04e+01, 1.22e+01]    
20000     [2.82e+01, 5.42e+00, 2.72e+00]    [0.00e+00, 1.49e+01, 2.72e+00]    [9.89e+00, 1.49e+01, 1.06e+01, 1.26e+01]    
21000     [2.89e+01, 6.95e+00, 2.71e+00]    [0.00e+00, 1.51e+01, 2.71e+00]    [9.94e+00, 1.51e+01, 9.86e+00, 1.24e+01]    
22000     [3.07e+01, 9.74e+00, 2.71e+00]    [0.00e+00, 1.54e+01, 2.71e+00]    [9.86e+00, 1.54e+01, 9.18e+00, 1.21e+01]    
23000     [3.01e+01, 9.04e+00, 2.71e+00]    [0.00e+00, 1.57e+01, 2.71e+00]    [9.68e+00, 1.57e+01, 8.83e+00, 1.20e+01]    
24000     [2.99e+01, 1.17e+01, 2.71e+00]    [0.00e+00, 1.66e+01, 2.71e+00]    [9.96e+00, 1.66e+01, 9.86e+00, 1.31e+01]    
25000     [2.74e+01, 7.45e+00, 2.71e+00]    [0.00e+00, 1.68e+01, 2.71e+00]    [9.91e+00, 1.68e+01, 8.82e+00, 1.27e+01]    
26000     [2.82e+01, 9.07e+00, 2.70e+00]    [0.00e+00, 1.72e+01, 2.70e+00]    [9.69e+00, 1.72e+01, 8.60e+00, 1.26e+01]    
27000     [2.65e+01, 3.60e+00, 2.70e+00]    [0.00e+00, 1.75e+01, 2.70e+00]    [9.47e+00, 1.75e+01, 7.72e+00, 1.22e+01]    
28000     [2.72e+01, 7.59e+00, 2.70e+00]    [0.00e+00, 1.81e+01, 2.70e+00]    [9.45e+00, 1.81e+01, 7.61e+00, 1.24e+01]    
29000     [2.75e+01, 5.14e+00, 2.70e+00]    [0.00e+00, 1.82e+01, 2.70e+00]    [8.86e+00, 1.82e+01, 6.64e+00, 1.16e+01]    
30000     [2.93e+01, 8.62e+00, 2.70e+00]    [0.00e+00, 1.87e+01, 2.70e+00]    [8.81e+00, 1.87e+01, 5.94e+00, 1.14e+01]    

Best model at step 27000:
  train loss: 3.28e+01
  test loss: 2.02e+01
  test metric: [9.47e+00, 1.75e+01, 7.72e+00, 1.22e+01]

'train' took 65.004029 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.261342 s

'compile' took 1.248722 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.84e+02, 4.24e+02, 2.79e+00]    [0.00e+00, 1.02e+02, 2.79e+00]    [1.02e+02, 1.02e+02, 4.78e+00, 4.78e+00]    
1000      [4.91e+01, 3.08e+01, 2.77e+00]    [0.00e+00, 6.16e+01, 2.77e+00]    [6.16e+01, 6.16e+01, 1.85e+01, 1.85e+01]    
2000      [3.38e+01, 1.42e+01, 2.77e+00]    [0.00e+00, 3.75e+01, 2.77e+00]    [3.75e+01, 3.75e+01, 1.02e+01, 1.03e+01]    
3000      [3.17e+01, 5.96e+00, 2.76e+00]    [0.00e+00, 3.29e+01, 2.76e+00]    [3.30e+01, 3.29e+01, 9.49e+00, 9.58e+00]    
4000      [3.05e+01, 4.81e+00, 2.76e+00]    [0.00e+00, 3.15e+01, 2.76e+00]    [3.16e+01, 3.15e+01, 9.42e+00, 9.51e+00]    
5000      [3.15e+01, 1.16e+01, 2.75e+00]    [0.00e+00, 3.06e+01, 2.75e+00]    [3.07e+01, 3.06e+01, 9.64e+00, 9.73e+00]    
6000      [3.05e+01, 9.39e+00, 2.75e+00]    [0.00e+00, 3.00e+01, 2.75e+00]    [3.01e+01, 3.00e+01, 9.83e+00, 9.92e+00]    
7000      [3.16e+01, 1.32e+01, 2.74e+00]    [0.00e+00, 2.98e+01, 2.74e+00]    [2.98e+01, 2.98e+01, 9.73e+00, 9.82e+00]    
8000      [2.80e+01, 4.35e+00, 2.74e+00]    [0.00e+00, 2.99e+01, 2.74e+00]    [3.00e+01, 2.99e+01, 1.03e+01, 1.04e+01]    
9000      [2.81e+01, 6.18e+00, 2.74e+00]    [0.00e+00, 3.01e+01, 2.74e+00]    [3.02e+01, 3.01e+01, 1.06e+01, 1.07e+01]    
10000     [2.86e+01, 9.82e+00, 2.74e+00]    [0.00e+00, 3.02e+01, 2.74e+00]    [3.03e+01, 3.02e+01, 1.06e+01, 1.07e+01]    
11000     [2.77e+01, 8.16e+00, 2.73e+00]    [0.00e+00, 3.02e+01, 2.73e+00]    [3.03e+01, 3.02e+01, 1.07e+01, 1.09e+01]    
12000     [2.83e+01, 1.04e+01, 2.73e+00]    [0.00e+00, 3.02e+01, 2.73e+00]    [3.03e+01, 3.02e+01, 1.12e+01, 1.13e+01]    
13000     [2.79e+01, 9.00e+00, 2.73e+00]    [0.00e+00, 3.04e+01, 2.73e+00]    [3.05e+01, 3.04e+01, 1.11e+01, 1.12e+01]    
14000     [2.60e+01, 4.37e+00, 2.73e+00]    [0.00e+00, 3.03e+01, 2.73e+00]    [3.04e+01, 3.03e+01, 1.14e+01, 1.16e+01]    
15000     [2.63e+01, 6.31e+00, 2.73e+00]    [0.00e+00, 3.04e+01, 2.73e+00]    [3.05e+01, 3.04e+01, 1.17e+01, 1.18e+01]    
16000     [2.74e+01, 8.33e+00, 2.73e+00]    [0.00e+00, 3.04e+01, 2.73e+00]    [3.05e+01, 3.04e+01, 1.20e+01, 1.22e+01]    
17000     [2.54e+01, 5.16e+00, 2.73e+00]    [0.00e+00, 3.05e+01, 2.73e+00]    [3.06e+01, 3.05e+01, 1.19e+01, 1.21e+01]    
18000     [2.54e+01, 4.56e+00, 2.72e+00]    [0.00e+00, 3.04e+01, 2.72e+00]    [3.06e+01, 3.04e+01, 1.21e+01, 1.23e+01]    
19000     [2.84e+01, 9.99e+00, 2.72e+00]    [0.00e+00, 3.04e+01, 2.72e+00]    [3.06e+01, 3.04e+01, 1.19e+01, 1.20e+01]    
20000     [2.50e+01, 3.74e+00, 2.72e+00]    [0.00e+00, 3.05e+01, 2.72e+00]    [3.06e+01, 3.05e+01, 1.24e+01, 1.26e+01]    
21000     [2.78e+01, 9.55e+00, 2.72e+00]    [0.00e+00, 3.05e+01, 2.72e+00]    [3.07e+01, 3.05e+01, 1.23e+01, 1.25e+01]    
22000     [2.65e+01, 8.51e+00, 2.72e+00]    [0.00e+00, 3.04e+01, 2.72e+00]    [3.06e+01, 3.04e+01, 1.27e+01, 1.29e+01]    
23000     [2.77e+01, 9.69e+00, 2.72e+00]    [0.00e+00, 3.05e+01, 2.72e+00]    [3.07e+01, 3.05e+01, 1.26e+01, 1.28e+01]    
24000     [2.54e+01, 5.47e+00, 2.72e+00]    [0.00e+00, 3.05e+01, 2.72e+00]    [3.07e+01, 3.05e+01, 1.29e+01, 1.32e+01]    
25000     [2.71e+01, 8.84e+00, 2.72e+00]    [0.00e+00, 3.06e+01, 2.72e+00]    [3.08e+01, 3.06e+01, 1.30e+01, 1.32e+01]    
26000     [2.59e+01, 7.22e+00, 2.71e+00]    [0.00e+00, 3.06e+01, 2.71e+00]    [3.08e+01, 3.06e+01, 1.35e+01, 1.37e+01]    
27000     [2.72e+01, 9.71e+00, 2.71e+00]    [0.00e+00, 3.06e+01, 2.71e+00]    [3.08e+01, 3.06e+01, 1.37e+01, 1.39e+01]    
28000     [2.61e+01, 7.07e+00, 2.71e+00]    [0.00e+00, 3.05e+01, 2.71e+00]    [3.07e+01, 3.05e+01, 1.34e+01, 1.36e+01]    
29000     [2.67e+01, 9.28e+00, 2.71e+00]    [0.00e+00, 3.06e+01, 2.71e+00]    [3.08e+01, 3.06e+01, 1.39e+01, 1.41e+01]    
30000     [2.51e+01, 5.79e+00, 2.71e+00]    [0.00e+00, 3.07e+01, 2.71e+00]    [3.09e+01, 3.07e+01, 1.38e+01, 1.40e+01]    

Best model at step 20000:
  train loss: 3.14e+01
  test loss: 3.32e+01
  test metric: [3.06e+01, 3.05e+01, 1.24e+01, 1.26e+01]

'train' took 67.565063 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[27.80030780355645, 16.68146840875379, 11.101005385414775, 11.207188385408699, 20.112693647447298, 21.873969310430038, 13.341667136686235, 21.69898387848548, 17.537863569288966, 30.460955270106695]
sigma_y 11 19.18161027955784 6.22819060551643
=======================================================
=======================================================
              Case          n     E (GPa)  ...      Wp/Wt    E* (GPa)      sy/E*
count    95.000000  95.000000   95.000000  ...  95.000000   95.000000  95.000000
mean    274.052632   0.208946  109.209358  ...   0.736768  109.209358   0.013545
std     407.776179   0.177157   66.358723  ...   0.130611   66.358723   0.009893
min       1.000000   0.000000   10.000000  ...   0.455921   10.000000   0.001429
25%      37.500000   0.084688   50.000000  ...   0.640934   50.000000   0.005556
50%      67.000000   0.173476  100.810000  ...   0.741830  100.810000   0.012000
75%      90.500000   0.300000  170.000000  ...   0.834702  170.000000   0.017647
max    1023.000000   0.500000  210.000000  ...   0.971835  210.000000   0.040000

[8 rows x 9 columns]
              Case          n     E (GPa)  ...     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  ...   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499  ...   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848  ...   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000  ...    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500  ...   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000  ...   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000  ...   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000  ...  239.235773  326727.270700   0.971982

[8 rows x 7 columns]

Cross-validation iteration: 1
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.357045 s

'compile' took 1.240858 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.11e+02, 1.99e+02, 2.69e+00]    [0.00e+00, 6.66e+01, 2.69e+00]    [6.66e+01, 6.66e+01, 2.53e+01, 2.53e+01]    
1000      [3.63e+01, 3.09e+01, 2.66e+00]    [0.00e+00, 4.06e+01, 2.66e+00]    [4.07e+01, 4.06e+01, 8.98e+00, 9.06e+00]    
2000      [2.87e+01, 1.00e+01, 2.66e+00]    [0.00e+00, 1.81e+01, 2.66e+00]    [1.83e+01, 1.81e+01, 1.80e+01, 1.81e+01]    
3000      [3.00e+01, 1.09e+01, 2.66e+00]    [0.00e+00, 1.99e+01, 2.66e+00]    [1.97e+01, 1.99e+01, 1.31e+01, 1.29e+01]    
4000      [2.74e+01, 6.21e+00, 2.65e+00]    [0.00e+00, 2.15e+01, 2.65e+00]    [2.12e+01, 2.15e+01, 1.17e+01, 1.15e+01]    
5000      [2.82e+01, 8.22e+00, 2.65e+00]    [0.00e+00, 2.20e+01, 2.65e+00]    [2.17e+01, 2.20e+01, 1.16e+01, 1.13e+01]    
6000      [2.77e+01, 7.62e+00, 2.65e+00]    [0.00e+00, 2.29e+01, 2.65e+00]    [2.26e+01, 2.29e+01, 1.15e+01, 1.11e+01]    
7000      [2.93e+01, 1.04e+01, 2.65e+00]    [0.00e+00, 2.31e+01, 2.65e+00]    [2.28e+01, 2.31e+01, 1.15e+01, 1.10e+01]    
8000      [2.87e+01, 8.66e+00, 2.65e+00]    [0.00e+00, 2.37e+01, 2.65e+00]    [2.32e+01, 2.37e+01, 1.17e+01, 1.10e+01]    
9000      [3.04e+01, 1.10e+01, 2.64e+00]    [0.00e+00, 2.42e+01, 2.64e+00]    [2.37e+01, 2.42e+01, 1.17e+01, 1.10e+01]    
10000     [2.71e+01, 4.95e+00, 2.64e+00]    [0.00e+00, 2.52e+01, 2.64e+00]    [2.47e+01, 2.52e+01, 1.10e+01, 1.02e+01]    
11000     [2.72e+01, 5.33e+00, 2.64e+00]    [0.00e+00, 2.65e+01, 2.64e+00]    [2.60e+01, 2.65e+01, 1.04e+01, 9.46e+00]    
12000     [2.96e+01, 9.73e+00, 2.64e+00]    [0.00e+00, 2.74e+01, 2.64e+00]    [2.67e+01, 2.74e+01, 1.02e+01, 9.16e+00]    
13000     [2.67e+01, 3.15e+00, 2.64e+00]    [0.00e+00, 2.74e+01, 2.64e+00]    [2.67e+01, 2.74e+01, 1.06e+01, 9.46e+00]    
14000     [2.73e+01, 4.49e+00, 2.64e+00]    [0.00e+00, 2.76e+01, 2.64e+00]    [2.69e+01, 2.76e+01, 1.09e+01, 9.58e+00]    
15000     [3.06e+01, 1.10e+01, 2.64e+00]    [0.00e+00, 2.89e+01, 2.64e+00]    [2.80e+01, 2.89e+01, 1.08e+01, 9.32e+00]    
16000     [2.71e+01, 4.99e+00, 2.64e+00]    [0.00e+00, 2.84e+01, 2.64e+00]    [2.75e+01, 2.84e+01, 1.18e+01, 1.02e+01]    
17000     [2.74e+01, 5.65e+00, 2.64e+00]    [0.00e+00, 2.87e+01, 2.64e+00]    [2.77e+01, 2.87e+01, 1.23e+01, 1.05e+01]    
18000     [2.92e+01, 9.02e+00, 2.64e+00]    [0.00e+00, 2.88e+01, 2.64e+00]    [2.77e+01, 2.88e+01, 1.30e+01, 1.10e+01]    
19000     [3.02e+01, 8.16e+00, 2.64e+00]    [0.00e+00, 2.77e+01, 2.64e+00]    [2.65e+01, 2.77e+01, 1.43e+01, 1.22e+01]    
20000     [2.85e+01, 4.95e+00, 2.64e+00]    [0.00e+00, 2.81e+01, 2.64e+00]    [2.68e+01, 2.81e+01, 1.50e+01, 1.26e+01]    
21000     [2.87e+01, 5.41e+00, 2.64e+00]    [0.00e+00, 2.82e+01, 2.64e+00]    [2.68e+01, 2.82e+01, 1.55e+01, 1.29e+01]    
22000     [2.91e+01, 8.38e+00, 2.64e+00]    [0.00e+00, 2.94e+01, 2.64e+00]    [2.79e+01, 2.94e+01, 1.53e+01, 1.26e+01]    
23000     [2.88e+01, 7.34e+00, 2.64e+00]    [0.00e+00, 2.96e+01, 2.64e+00]    [2.80e+01, 2.96e+01, 1.58e+01, 1.29e+01]    
24000     [2.75e+01, 3.79e+00, 2.64e+00]    [0.00e+00, 2.90e+01, 2.64e+00]    [2.74e+01, 2.90e+01, 1.69e+01, 1.38e+01]    
25000     [2.74e+01, 4.05e+00, 2.64e+00]    [0.00e+00, 2.93e+01, 2.64e+00]    [2.75e+01, 2.93e+01, 1.72e+01, 1.39e+01]    
26000     [2.68e+01, 3.22e+00, 2.64e+00]    [0.00e+00, 2.95e+01, 2.64e+00]    [2.77e+01, 2.95e+01, 1.74e+01, 1.39e+01]    
27000     [2.76e+01, 5.62e+00, 2.64e+00]    [0.00e+00, 3.05e+01, 2.64e+00]    [2.85e+01, 3.05e+01, 1.74e+01, 1.36e+01]    
28000     [2.75e+01, 5.38e+00, 2.64e+00]    [0.00e+00, 3.08e+01, 2.64e+00]    [2.87e+01, 3.08e+01, 1.79e+01, 1.38e+01]    
29000     [2.76e+01, 5.85e+00, 2.64e+00]    [0.00e+00, 3.03e+01, 2.64e+00]    [2.81e+01, 3.03e+01, 1.86e+01, 1.43e+01]    
30000     [2.70e+01, 5.13e+00, 2.64e+00]    [0.00e+00, 3.04e+01, 2.64e+00]    [2.81e+01, 3.04e+01, 1.90e+01, 1.44e+01]    

Best model at step 13000:
  train loss: 3.25e+01
  test loss: 3.01e+01
  test metric: [2.67e+01, 2.74e+01, 1.06e+01, 9.46e+00]

'train' took 63.699586 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 2
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.259250 s

'compile' took 1.269091 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [1.66e+02, 1.48e+02, 2.85e+00]    [0.00e+00, 1.38e+02, 2.85e+00]    [1.38e+02, 1.38e+02, 6.42e+00, 6.42e+00]    
1000      [3.01e+01, 1.79e+01, 2.85e+00]    [0.00e+00, 1.78e+01, 2.85e+00]    [1.27e+01, 1.78e+01, 9.78e+00, 7.41e+00]    
2000      [3.10e+01, 1.57e+01, 2.85e+00]    [0.00e+00, 2.53e+01, 2.85e+00]    [2.82e+01, 2.53e+01, 1.57e+01, 9.07e+00]    
3000      [2.80e+01, 1.30e+01, 2.84e+00]    [0.00e+00, 6.14e+00, 2.84e+00]    [8.92e+00, 6.14e+00, 4.50e+00, 2.33e+00]    
4000      [2.73e+01, 1.15e+01, 2.84e+00]    [0.00e+00, 6.71e+00, 2.84e+00]    [8.99e+00, 6.71e+00, 7.94e+00, 8.50e-01]    
5000      [2.64e+01, 8.15e+00, 2.84e+00]    [0.00e+00, 1.16e+01, 2.84e+00]    [1.29e+01, 1.16e+01, 1.29e+01, 6.47e+00]    
6000      [2.88e+01, 1.20e+01, 2.84e+00]    [0.00e+00, 2.01e+01, 2.84e+00]    [2.11e+01, 2.01e+01, 2.03e+01, 1.56e+01]    
7000      [2.87e+01, 1.13e+01, 2.84e+00]    [0.00e+00, 2.10e+01, 2.84e+00]    [2.04e+01, 2.10e+01, 1.92e+01, 1.41e+01]    
8000      [2.81e+01, 1.02e+01, 2.84e+00]    [0.00e+00, 1.96e+01, 2.84e+00]    [1.81e+01, 1.96e+01, 1.81e+01, 1.40e+01]    
9000      [2.73e+01, 8.46e+00, 2.84e+00]    [0.00e+00, 1.77e+01, 2.84e+00]    [1.54e+01, 1.77e+01, 1.43e+01, 1.09e+01]    
10000     [2.82e+01, 8.72e+00, 2.84e+00]    [0.00e+00, 4.79e+00, 2.84e+00]    [6.70e+00, 4.79e+00, 2.43e+00, 3.48e+00]    
11000     [2.77e+01, 7.52e+00, 2.84e+00]    [0.00e+00, 1.98e+01, 2.84e+00]    [1.63e+01, 1.98e+01, 1.42e+01, 1.18e+01]    
12000     [2.69e+01, 4.68e+00, 2.84e+00]    [0.00e+00, 1.46e+01, 2.84e+00]    [1.12e+01, 1.46e+01, 9.10e+00, 6.63e+00]    
13000     [2.82e+01, 7.31e+00, 2.84e+00]    [0.00e+00, 1.99e+01, 2.84e+00]    [1.75e+01, 1.99e+01, 1.61e+01, 1.55e+01]    
14000     [2.72e+01, 4.53e+00, 2.84e+00]    [0.00e+00, 1.34e+01, 2.84e+00]    [1.01e+01, 1.34e+01, 9.44e+00, 7.87e+00]    
15000     [2.82e+01, 4.87e+00, 2.84e+00]    [0.00e+00, 7.20e+00, 2.84e+00]    [7.37e+00, 7.20e+00, 3.10e+00, 5.10e+00]    
16000     [2.79e+01, 5.01e+00, 2.84e+00]    [0.00e+00, 1.86e+01, 2.84e+00]    [1.54e+01, 1.86e+01, 1.34e+01, 1.36e+01]    
17000     [2.92e+01, 6.06e+00, 2.84e+00]    [0.00e+00, 4.74e+00, 2.84e+00]    [3.51e+00, 4.74e+00, 7.65e-01, 4.98e-01]    
18000     [2.74e+01, 3.01e+00, 2.84e+00]    [0.00e+00, 1.17e+01, 2.84e+00]    [7.06e+00, 1.17e+01, 5.36e+00, 2.61e+00]    
19000     [2.72e+01, 4.01e+00, 2.84e+00]    [0.00e+00, 1.89e+01, 2.84e+00]    [1.41e+01, 1.89e+01, 1.21e+01, 9.33e+00]    
20000     [3.09e+01, 9.27e+00, 2.84e+00]    [0.00e+00, 8.80e+00, 2.84e+00]    [6.16e+00, 8.80e+00, 3.75e+00, 3.23e+00]    
21000     [2.83e+01, 5.31e+00, 2.84e+00]    [0.00e+00, 4.52e+00, 2.84e+00]    [1.68e+00, 4.51e+00, 7.90e-01, 3.17e+00]    
22000     [2.67e+01, 3.21e+00, 2.84e+00]    [0.00e+00, 1.80e+01, 2.84e+00]    [1.28e+01, 1.80e+01, 1.18e+01, 7.68e+00]    
23000     [2.80e+01, 5.72e+00, 2.84e+00]    [0.00e+00, 2.22e+01, 2.84e+00]    [1.64e+01, 2.22e+01, 1.57e+01, 1.20e+01]    
24000     [2.71e+01, 3.46e+00, 2.85e+00]    [0.00e+00, 6.23e+00, 2.85e+00]    [3.19e+00, 6.23e+00, 2.41e+00, 2.79e+00]    
25000     [2.71e+01, 3.76e+00, 2.85e+00]    [0.00e+00, 4.97e+00, 2.85e+00]    [1.48e+00, 4.97e+00, 3.53e-02, 3.98e+00]    
26000     [2.56e+01, 2.16e+00, 2.85e+00]    [0.00e+00, 1.34e+01, 2.85e+00]    [8.75e+00, 1.34e+01, 8.00e+00, 3.21e+00]    
27000     [2.73e+01, 3.92e+00, 2.85e+00]    [0.00e+00, 7.03e+00, 2.85e+00]    [4.02e+00, 7.03e+00, 6.43e-01, 9.14e-01]    
28000     [2.71e+01, 4.92e+00, 2.85e+00]    [0.00e+00, 1.86e+01, 2.85e+00]    [1.20e+01, 1.86e+01, 1.17e+01, 6.95e+00]    
29000     [2.70e+01, 4.27e+00, 2.85e+00]    [0.00e+00, 1.81e+01, 2.85e+00]    [1.12e+01, 1.81e+01, 1.10e+01, 6.64e+00]    
30000     [2.54e+01, 1.75e+00, 2.85e+00]    [0.00e+00, 4.58e+00, 2.85e+00]    [3.35e+00, 4.58e+00, 7.51e-01, 2.67e+00]    

Best model at step 30000:
  train loss: 3.00e+01
  test loss: 7.42e+00
  test metric: [3.35e+00, 4.58e+00, 7.51e-01, 2.67e+00]

'train' took 64.049465 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 3
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.291221 s

'compile' took 1.139964 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.35e+02, 2.21e+02, 2.74e+00]    [0.00e+00, 1.47e+02, 2.74e+00]    [1.47e+02, 1.47e+02, 8.81e+00, 8.81e+00]    
1000      [3.41e+01, 2.80e+01, 2.72e+00]    [0.00e+00, 3.82e+01, 2.72e+00]    [3.83e+01, 3.82e+01, 5.45e+00, 5.45e+00]    
2000      [2.93e+01, 1.34e+01, 2.72e+00]    [0.00e+00, 2.10e+01, 2.72e+00]    [2.15e+01, 2.10e+01, 1.85e+00, 1.86e+00]    
3000      [2.83e+01, 1.08e+01, 2.72e+00]    [0.00e+00, 1.62e+01, 2.72e+00]    [1.69e+01, 1.62e+01, 2.28e+00, 2.28e+00]    
4000      [2.91e+01, 1.34e+01, 2.71e+00]    [0.00e+00, 1.43e+01, 2.71e+00]    [1.51e+01, 1.43e+01, 2.98e+00, 2.98e+00]    
5000      [2.84e+01, 1.15e+01, 2.71e+00]    [0.00e+00, 1.26e+01, 2.71e+00]    [1.36e+01, 1.26e+01, 2.84e+00, 2.84e+00]    
6000      [2.69e+01, 6.40e+00, 2.71e+00]    [0.00e+00, 1.17e+01, 2.71e+00]    [1.29e+01, 1.17e+01, 2.87e+00, 2.86e+00]    
7000      [2.79e+01, 8.38e+00, 2.70e+00]    [0.00e+00, 1.11e+01, 2.70e+00]    [1.25e+01, 1.11e+01, 2.84e+00, 2.82e+00]    
8000      [2.82e+01, 9.09e+00, 2.70e+00]    [0.00e+00, 1.09e+01, 2.70e+00]    [1.25e+01, 1.09e+01, 3.00e+00, 2.98e+00]    
9000      [2.68e+01, 7.05e+00, 2.70e+00]    [0.00e+00, 1.11e+01, 2.70e+00]    [1.29e+01, 1.11e+01, 3.22e+00, 3.20e+00]    
10000     [2.82e+01, 1.17e+01, 2.70e+00]    [0.00e+00, 1.07e+01, 2.70e+00]    [1.28e+01, 1.07e+01, 3.54e+00, 3.52e+00]    
11000     [2.70e+01, 7.67e+00, 2.70e+00]    [0.00e+00, 1.05e+01, 2.70e+00]    [1.28e+01, 1.05e+01, 3.53e+00, 3.51e+00]    
12000     [2.72e+01, 5.95e+00, 2.69e+00]    [0.00e+00, 1.01e+01, 2.69e+00]    [1.27e+01, 1.01e+01, 3.53e+00, 3.51e+00]    
13000     [3.02e+01, 1.03e+01, 2.69e+00]    [0.00e+00, 9.97e+00, 2.69e+00]    [1.28e+01, 9.97e+00, 3.67e+00, 3.65e+00]    
14000     [2.69e+01, 7.63e+00, 2.69e+00]    [0.00e+00, 1.04e+01, 2.69e+00]    [1.34e+01, 1.04e+01, 4.00e+00, 3.99e+00]    
15000     [2.93e+01, 8.38e+00, 2.69e+00]    [0.00e+00, 9.67e+00, 2.69e+00]    [1.30e+01, 9.67e+00, 3.81e+00, 3.79e+00]    
16000     [3.13e+01, 1.12e+01, 2.69e+00]    [0.00e+00, 9.62e+00, 2.69e+00]    [1.33e+01, 9.62e+00, 3.73e+00, 3.71e+00]    
17000     [3.05e+01, 1.00e+01, 2.69e+00]    [0.00e+00, 9.27e+00, 2.69e+00]    [1.33e+01, 9.27e+00, 3.79e+00, 3.78e+00]    
18000     [2.83e+01, 5.06e+00, 2.69e+00]    [0.00e+00, 9.25e+00, 2.69e+00]    [1.36e+01, 9.25e+00, 3.88e+00, 3.88e+00]    
19000     [2.84e+01, 7.80e+00, 2.69e+00]    [0.00e+00, 8.98e+00, 2.69e+00]    [1.38e+01, 8.98e+00, 3.96e+00, 3.97e+00]    
20000     [2.74e+01, 6.68e+00, 2.69e+00]    [0.00e+00, 9.28e+00, 2.69e+00]    [1.45e+01, 9.28e+00, 3.83e+00, 3.84e+00]    
21000     [2.79e+01, 7.20e+00, 2.69e+00]    [0.00e+00, 9.21e+00, 2.69e+00]    [1.48e+01, 9.21e+00, 3.60e+00, 3.61e+00]    
22000     [2.88e+01, 6.01e+00, 2.69e+00]    [0.00e+00, 9.42e+00, 2.69e+00]    [1.54e+01, 9.42e+00, 3.36e+00, 3.36e+00]    
23000     [2.78e+01, 3.83e+00, 2.69e+00]    [0.00e+00, 9.83e+00, 2.69e+00]    [1.64e+01, 9.83e+00, 3.34e+00, 3.36e+00]    
24000     [2.77e+01, 7.72e+00, 2.69e+00]    [0.00e+00, 1.05e+01, 2.69e+00]    [1.75e+01, 1.05e+01, 3.44e+00, 3.48e+00]    
25000     [2.89e+01, 6.53e+00, 2.69e+00]    [0.00e+00, 1.08e+01, 2.69e+00]    [1.84e+01, 1.08e+01, 3.06e+00, 3.08e+00]    
26000     [2.87e+01, 9.93e+00, 2.70e+00]    [0.00e+00, 1.12e+01, 2.70e+00]    [1.94e+01, 1.12e+01, 3.04e+00, 3.08e+00]    
27000     [2.87e+01, 9.92e+00, 2.70e+00]    [0.00e+00, 1.08e+01, 2.70e+00]    [1.96e+01, 1.08e+01, 2.77e+00, 2.80e+00]    
28000     [2.61e+01, 4.41e+00, 2.70e+00]    [0.00e+00, 9.59e+00, 2.70e+00]    [1.89e+01, 9.59e+00, 2.04e+00, 2.00e+00]    
29000     [2.82e+01, 6.55e+00, 2.70e+00]    [0.00e+00, 9.29e+00, 2.70e+00]    [1.90e+01, 9.29e+00, 1.55e+00, 1.46e+00]    
30000     [2.63e+01, 5.61e+00, 2.70e+00]    [0.00e+00, 8.89e+00, 2.70e+00]    [1.90e+01, 8.89e+00, 1.26e+00, 1.14e+00]    

Best model at step 28000:
  train loss: 3.32e+01
  test loss: 1.23e+01
  test metric: [1.89e+01, 9.59e+00, 2.04e+00, 2.00e+00]

'train' took 66.015721 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 4
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.259818 s

'compile' took 1.103682 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.42e+02, 3.45e+02, 2.78e+00]    [0.00e+00, 8.09e+01, 2.78e+00]    [8.09e+01, 8.09e+01, 3.07e+01, 3.07e+01]    
1000      [3.48e+01, 2.92e+01, 2.77e+00]    [0.00e+00, 4.01e+01, 2.77e+00]    [4.03e+01, 4.01e+01, 3.55e+00, 3.50e+00]    
2000      [2.88e+01, 1.12e+01, 2.77e+00]    [0.00e+00, 1.26e+01, 2.77e+00]    [1.32e+01, 1.26e+01, 7.15e+00, 7.02e+00]    
3000      [2.86e+01, 1.07e+01, 2.77e+00]    [0.00e+00, 9.12e+00, 2.77e+00]    [9.26e+00, 9.12e+00, 5.64e+00, 4.76e+00]    
4000      [2.86e+01, 1.13e+01, 2.77e+00]    [0.00e+00, 1.08e+01, 2.77e+00]    [1.09e+01, 1.08e+01, 1.82e+00, 7.03e-01]    
5000      [2.73e+01, 7.94e+00, 2.77e+00]    [0.00e+00, 1.33e+01, 2.77e+00]    [1.34e+01, 1.33e+01, 1.65e+00, 3.08e+00]    
6000      [3.03e+01, 1.28e+01, 2.77e+00]    [0.00e+00, 1.50e+01, 2.77e+00]    [1.50e+01, 1.50e+01, 3.99e+00, 5.88e+00]    
7000      [3.13e+01, 1.44e+01, 2.77e+00]    [0.00e+00, 1.65e+01, 2.77e+00]    [1.64e+01, 1.65e+01, 5.46e+00, 8.02e+00]    
8000      [2.87e+01, 8.79e+00, 2.77e+00]    [0.00e+00, 1.65e+01, 2.77e+00]    [1.63e+01, 1.65e+01, 4.72e+00, 8.18e+00]    
9000      [2.77e+01, 6.70e+00, 2.78e+00]    [0.00e+00, 1.75e+01, 2.78e+00]    [1.71e+01, 1.75e+01, 5.21e+00, 9.84e+00]    
10000     [2.86e+01, 9.50e+00, 2.79e+00]    [0.00e+00, 1.95e+01, 2.79e+00]    [1.88e+01, 1.95e+01, 6.53e+00, 1.28e+01]    
11000     [2.77e+01, 6.68e+00, 2.79e+00]    [0.00e+00, 2.02e+01, 2.79e+00]    [1.92e+01, 2.02e+01, 7.15e+00, 1.52e+01]    
12000     [2.92e+01, 9.86e+00, 2.80e+00]    [0.00e+00, 2.04e+01, 2.80e+00]    [1.92e+01, 2.04e+01, 5.93e+00, 1.57e+01]    
13000     [3.20e+01, 1.24e+01, 2.80e+00]    [0.00e+00, 1.92e+01, 2.80e+00]    [1.78e+01, 1.92e+01, 2.66e+00, 1.42e+01]    
14000     [2.67e+01, 4.74e+00, 2.81e+00]    [0.00e+00, 2.01e+01, 2.81e+00]    [1.84e+01, 2.01e+01, 2.24e+00, 1.59e+01]    
15000     [2.92e+01, 1.04e+01, 2.81e+00]    [0.00e+00, 2.04e+01, 2.81e+00]    [1.83e+01, 2.04e+01, 9.87e-01, 1.65e+01]    
16000     [2.58e+01, 2.41e+00, 2.81e+00]    [0.00e+00, 1.94e+01, 2.81e+00]    [1.73e+01, 1.94e+01, 1.19e+00, 1.58e+01]    
17000     [2.75e+01, 7.16e+00, 2.82e+00]    [0.00e+00, 2.00e+01, 2.82e+00]    [1.75e+01, 2.00e+01, 2.24e+00, 1.64e+01]    
18000     [2.71e+01, 5.34e+00, 2.82e+00]    [0.00e+00, 1.93e+01, 2.82e+00]    [1.68e+01, 1.93e+01, 3.55e+00, 1.61e+01]    
19000     [2.58e+01, 2.69e+00, 2.82e+00]    [0.00e+00, 1.97e+01, 2.82e+00]    [1.70e+01, 1.97e+01, 4.17e+00, 1.63e+01]    
20000     [2.59e+01, 3.59e+00, 2.82e+00]    [0.00e+00, 2.02e+01, 2.82e+00]    [1.73e+01, 2.02e+01, 4.03e+00, 1.72e+01]    
21000     [2.92e+01, 1.12e+01, 2.82e+00]    [0.00e+00, 2.04e+01, 2.82e+00]    [1.75e+01, 2.04e+01, 3.61e+00, 1.84e+01]    
22000     [2.61e+01, 4.33e+00, 2.82e+00]    [0.00e+00, 2.00e+01, 2.82e+00]    [1.70e+01, 2.00e+01, 4.21e+00, 1.83e+01]    
23000     [2.67e+01, 7.05e+00, 2.82e+00]    [0.00e+00, 2.05e+01, 2.82e+00]    [1.74e+01, 2.05e+01, 4.45e+00, 1.85e+01]    
24000     [2.63e+01, 5.66e+00, 2.82e+00]    [0.00e+00, 1.94e+01, 2.82e+00]    [1.64e+01, 1.94e+01, 5.19e+00, 1.82e+01]    
25000     [2.61e+01, 6.11e+00, 2.82e+00]    [0.00e+00, 2.04e+01, 2.82e+00]    [1.71e+01, 2.04e+01, 4.90e+00, 1.90e+01]    
26000     [2.69e+01, 7.86e+00, 2.82e+00]    [0.00e+00, 2.03e+01, 2.82e+00]    [1.70e+01, 2.03e+01, 4.70e+00, 1.97e+01]    
27000     [2.60e+01, 5.08e+00, 2.82e+00]    [0.00e+00, 1.98e+01, 2.82e+00]    [1.66e+01, 1.98e+01, 4.70e+00, 1.98e+01]    
28000     [2.53e+01, 4.50e+00, 2.82e+00]    [0.00e+00, 1.95e+01, 2.82e+00]    [1.62e+01, 1.95e+01, 5.14e+00, 1.93e+01]    
29000     [2.50e+01, 4.01e+00, 2.81e+00]    [0.00e+00, 1.95e+01, 2.81e+00]    [1.61e+01, 1.95e+01, 5.20e+00, 1.92e+01]    
30000     [2.48e+01, 3.13e+00, 2.81e+00]    [0.00e+00, 1.96e+01, 2.81e+00]    [1.61e+01, 1.96e+01, 5.22e+00, 1.93e+01]    

Best model at step 30000:
  train loss: 3.07e+01
  test loss: 2.24e+01
  test metric: [1.61e+01, 1.96e+01, 5.22e+00, 1.93e+01]

'train' took 72.900527 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 5
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.262297 s

'compile' took 1.109775 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.67e+02, 5.37e+02, 2.79e+00]    [0.00e+00, 3.37e+03, 2.79e+00]    [3.37e+03, 3.37e+03, 3.30e+03, 3.30e+03]    
1000      [3.00e+01, 1.58e+01, 2.79e+00]    [0.00e+00, 5.24e+01, 2.79e+00]    [2.71e+01, 5.24e+01, 9.49e+00, 3.54e+01]    
2000      [3.13e+01, 1.50e+01, 2.79e+00]    [0.00e+00, 2.45e+01, 2.79e+00]    [1.61e+01, 2.45e+01, 6.91e+00, 1.64e+01]    
3000      [2.77e+01, 1.05e+01, 2.79e+00]    [0.00e+00, 5.00e+01, 2.79e+00]    [2.05e+01, 5.00e+01, 1.24e+01, 4.32e+01]    
4000      [3.19e+01, 1.35e+01, 2.79e+00]    [0.00e+00, 7.93e+01, 2.79e+00]    [4.95e+01, 7.93e+01, 4.27e+01, 7.42e+01]    
5000      [3.00e+01, 1.14e+01, 2.79e+00]    [0.00e+00, 6.98e+01, 2.79e+00]    [4.29e+01, 6.98e+01, 3.63e+01, 6.50e+01]    
6000      [2.68e+01, 7.13e+00, 2.79e+00]    [0.00e+00, 4.04e+01, 2.79e+00]    [1.69e+01, 4.04e+01, 1.05e+01, 3.62e+01]    
7000      [2.75e+01, 7.80e+00, 2.79e+00]    [0.00e+00, 2.66e+01, 2.79e+00]    [5.82e+00, 2.66e+01, 3.89e-01, 2.28e+01]    
8000      [2.74e+01, 7.50e+00, 2.79e+00]    [0.00e+00, 2.06e+01, 2.79e+00]    [3.22e+00, 2.06e+01, 2.67e+00, 1.74e+01]    
9000      [2.73e+01, 6.66e+00, 2.79e+00]    [0.00e+00, 3.53e+01, 2.79e+00]    [1.98e+01, 3.53e+01, 1.39e+01, 3.25e+01]    
10000     [2.68e+01, 6.42e+00, 2.79e+00]    [0.00e+00, 2.00e+01, 2.79e+00]    [7.12e+00, 2.00e+01, 1.66e+00, 1.79e+01]    
11000     [3.13e+01, 9.79e+00, 2.79e+00]    [0.00e+00, 5.64e+01, 2.79e+00]    [4.46e+01, 5.64e+01, 3.86e+01, 5.41e+01]    
12000     [2.72e+01, 5.50e+00, 2.79e+00]    [0.00e+00, 7.55e+00, 2.79e+00]    [6.80e+00, 7.55e+00, 5.24e-01, 5.35e+00]    
13000     [3.05e+01, 8.25e+00, 2.79e+00]    [0.00e+00, 4.51e+01, 2.79e+00]    [3.84e+01, 4.51e+01, 3.19e+01, 4.30e+01]    
14000     [2.64e+01, 4.18e+00, 2.79e+00]    [0.00e+00, 9.39e+00, 2.79e+00]    [6.40e+00, 9.39e+00, 3.85e-01, 7.46e+00]    
15000     [2.93e+01, 6.67e+00, 2.80e+00]    [0.00e+00, 3.19e+01, 2.80e+00]    [2.96e+01, 3.19e+01, 2.28e+01, 3.04e+01]    
16000     [3.04e+01, 7.31e+00, 2.80e+00]    [0.00e+00, 3.68e+01, 2.80e+00]    [3.64e+01, 3.68e+01, 2.93e+01, 3.55e+01]    
17000     [2.82e+01, 4.91e+00, 2.80e+00]    [0.00e+00, 2.38e+01, 2.80e+00]    [2.64e+01, 2.38e+01, 1.86e+01, 2.23e+01]    
18000     [2.79e+01, 4.22e+00, 2.80e+00]    [0.00e+00, 1.87e+01, 2.80e+00]    [2.32e+01, 1.87e+01, 1.50e+01, 1.73e+01]    
19000     [2.65e+01, 2.00e+00, 2.81e+00]    [0.00e+00, 1.48e+00, 2.81e+00]    [7.97e+00, 1.48e+00, 3.30e-01, 5.88e-01]    
20000     [2.78e+01, 3.15e+00, 2.81e+00]    [0.00e+00, 1.15e+01, 2.81e+00]    [2.18e+01, 1.15e+01, 1.28e+01, 1.05e+01]    
21000     [2.81e+01, 2.90e+00, 2.81e+00]    [0.00e+00, 9.81e+00, 2.81e+00]    [2.21e+01, 9.81e+00, 1.27e+01, 8.92e+00]    
22000     [2.95e+01, 4.42e+00, 2.81e+00]    [0.00e+00, 1.62e+01, 2.81e+00]    [3.01e+01, 1.62e+01, 2.00e+01, 1.51e+01]    
23000     [2.81e+01, 2.61e+00, 2.82e+00]    [0.00e+00, 2.30e+01, 2.82e+00]    [1.37e+01, 2.30e+01, 2.72e+00, 2.15e+01]    
24000     [3.01e+01, 4.41e+00, 2.82e+00]    [0.00e+00, 4.05e+01, 2.82e+00]    [2.81e+01, 4.05e+01, 1.58e+01, 3.82e+01]    
25000     [2.75e+01, 2.78e+00, 2.82e+00]    [0.00e+00, 3.08e+01, 2.82e+00]    [1.78e+01, 3.08e+01, 5.09e+00, 2.85e+01]    
26000     [2.71e+01, 2.16e+00, 2.82e+00]    [0.00e+00, 4.94e+00, 2.82e+00]    [1.95e+01, 4.94e+00, 5.97e+00, 2.24e+00]    
27000     [2.99e+01, 4.54e+00, 2.82e+00]    [0.00e+00, 5.00e+01, 2.82e+00]    [3.29e+01, 5.00e+01, 1.83e+01, 4.65e+01]    
28000     [2.63e+01, 1.84e+00, 2.82e+00]    [0.00e+00, 1.59e+01, 2.82e+00]    [1.39e+01, 1.59e+01, 1.11e+00, 1.25e+01]    
29000     [2.78e+01, 3.28e+00, 2.82e+00]    [0.00e+00, 3.77e+00, 2.82e+00]    [2.59e+01, 3.77e+00, 1.05e+01, 3.09e-01]    
30000     [2.66e+01, 2.04e+00, 2.82e+00]    [0.00e+00, 1.60e+01, 2.82e+00]    [1.63e+01, 1.60e+01, 1.06e+00, 1.30e+01]    

Best model at step 28000:
  train loss: 3.10e+01
  test loss: 1.87e+01
  test metric: [1.39e+01, 1.59e+01, 1.11e+00, 1.25e+01]

'train' took 66.715993 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 6
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.257812 s

'compile' took 1.351544 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.79e+02, 3.03e+02, 2.57e+00]    [0.00e+00, 4.30e+02, 2.57e+00]    [4.30e+02, 4.30e+02, 3.15e+02, 3.15e+02]    
1000      [3.28e+01, 2.10e+01, 2.56e+00]    [0.00e+00, 3.34e+01, 2.56e+00]    [3.31e+01, 3.34e+01, 1.29e+00, 9.93e-01]    
2000      [3.29e+01, 1.45e+01, 2.56e+00]    [0.00e+00, 8.02e+00, 2.56e+00]    [7.98e+00, 8.02e+00, 5.16e+00, 5.45e+00]    
3000      [3.07e+01, 8.58e+00, 2.55e+00]    [0.00e+00, 1.81e+01, 2.55e+00]    [1.81e+01, 1.81e+01, 1.12e+01, 1.07e+01]    
4000      [3.52e+01, 1.39e+01, 2.55e+00]    [0.00e+00, 1.03e+01, 2.55e+00]    [9.39e+00, 1.03e+01, 8.34e+00, 7.99e+00]    
5000      [3.01e+01, 7.06e+00, 2.55e+00]    [0.00e+00, 2.29e+01, 2.55e+00]    [2.35e+01, 2.29e+01, 4.28e+00, 2.91e+00]    
6000      [2.97e+01, 6.07e+00, 2.55e+00]    [0.00e+00, 1.86e+01, 2.55e+00]    [1.98e+01, 1.86e+01, 2.94e+00, 5.15e+00]    
7000      [2.94e+01, 7.12e+00, 2.55e+00]    [0.00e+00, 2.47e+01, 2.55e+00]    [2.67e+01, 2.47e+01, 2.63e+00, 9.58e-01]    
8000      [2.93e+01, 6.25e+00, 2.55e+00]    [0.00e+00, 1.73e+01, 2.55e+00]    [2.09e+01, 1.73e+01, 5.68e+00, 1.17e+01]    
9000      [2.94e+01, 7.50e+00, 2.55e+00]    [0.00e+00, 2.25e+01, 2.55e+00]    [2.84e+01, 2.25e+01, 2.52e+00, 7.32e+00]    
10000     [3.03e+01, 1.00e+01, 2.55e+00]    [0.00e+00, 2.05e+01, 2.55e+00]    [2.94e+01, 2.05e+01, 4.26e+00, 1.04e+01]    
11000     [2.84e+01, 6.66e+00, 2.55e+00]    [0.00e+00, 1.84e+01, 2.55e+00]    [2.62e+01, 1.84e+01, 1.75e+00, 1.38e+01]    
12000     [2.83e+01, 6.57e+00, 2.55e+00]    [0.00e+00, 2.26e+01, 2.55e+00]    [2.57e+01, 2.26e+01, 2.53e+00, 1.03e+01]    
13000     [3.01e+01, 1.07e+01, 2.56e+00]    [0.00e+00, 3.82e+01, 2.56e+00]    [1.62e+01, 3.82e+01, 8.26e+00, 2.36e+00]    
14000     [2.82e+01, 6.23e+00, 2.56e+00]    [0.00e+00, 4.00e+01, 2.56e+00]    [1.73e+01, 4.00e+01, 6.68e+00, 3.23e+00]    
15000     [2.92e+01, 8.94e+00, 2.56e+00]    [0.00e+00, 4.49e+01, 2.56e+00]    [1.60e+01, 4.49e+01, 8.37e+00, 6.48e+00]    
16000     [2.87e+01, 6.45e+00, 2.56e+00]    [0.00e+00, 3.39e+01, 2.56e+00]    [2.67e+01, 3.39e+01, 4.93e+00, 3.13e+00]    
17000     [2.77e+01, 5.62e+00, 2.56e+00]    [0.00e+00, 3.85e+01, 2.56e+00]    [2.43e+01, 3.85e+01, 2.92e+00, 8.86e-01]    
18000     [2.73e+01, 4.82e+00, 2.56e+00]    [0.00e+00, 4.87e+01, 2.56e+00]    [1.66e+01, 4.87e+01, 4.70e+00, 1.03e+01]    
19000     [2.77e+01, 4.80e+00, 2.56e+00]    [0.00e+00, 5.32e+01, 2.56e+00]    [1.36e+01, 5.32e+01, 7.24e+00, 1.46e+01]    
20000     [2.69e+01, 3.78e+00, 2.56e+00]    [0.00e+00, 5.20e+01, 2.56e+00]    [1.54e+01, 5.20e+01, 4.58e+00, 1.35e+01]    
21000     [2.61e+01, 2.60e+00, 2.56e+00]    [0.00e+00, 4.97e+01, 2.56e+00]    [1.76e+01, 4.97e+01, 1.15e+00, 1.20e+01]    
22000     [2.62e+01, 2.81e+00, 2.56e+00]    [0.00e+00, 4.94e+01, 2.56e+00]    [1.85e+01, 4.94e+01, 4.11e-01, 1.17e+01]    
23000     [2.96e+01, 8.79e+00, 2.55e+00]    [0.00e+00, 6.23e+01, 2.55e+00]    [1.04e+01, 6.23e+01, 8.79e+00, 2.29e+01]    
24000     [2.63e+01, 3.47e+00, 2.55e+00]    [0.00e+00, 5.14e+01, 2.55e+00]    [1.79e+01, 5.14e+01, 7.89e-01, 1.37e+01]    
25000     [2.66e+01, 3.12e+00, 2.55e+00]    [0.00e+00, 5.79e+01, 2.55e+00]    [1.32e+01, 5.79e+01, 4.21e+00, 1.93e+01]    
26000     [2.67e+01, 3.14e+00, 2.55e+00]    [0.00e+00, 5.97e+01, 2.55e+00]    [1.23e+01, 5.97e+01, 4.88e+00, 2.08e+01]    
27000     [2.62e+01, 2.07e+00, 2.55e+00]    [0.00e+00, 5.41e+01, 2.55e+00]    [1.67e+01, 5.41e+01, 8.93e-01, 1.63e+01]    
28000     [2.57e+01, 1.17e+00, 2.55e+00]    [0.00e+00, 5.78e+01, 2.55e+00]    [1.43e+01, 5.78e+01, 1.60e+00, 1.94e+01]    
29000     [2.67e+01, 3.48e+00, 2.55e+00]    [0.00e+00, 5.41e+01, 2.55e+00]    [1.77e+01, 5.41e+01, 2.64e+00, 1.60e+01]    
30000     [2.67e+01, 3.96e+00, 2.55e+00]    [0.00e+00, 5.55e+01, 2.55e+00]    [1.69e+01, 5.55e+01, 2.08e+00, 1.73e+01]    

Best model at step 28000:
  train loss: 2.94e+01
  test loss: 6.04e+01
  test metric: [1.43e+01, 5.78e+01, 1.60e+00, 1.94e+01]

'train' took 64.543811 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 7
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.275270 s

'compile' took 1.158858 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [3.40e+02, 4.08e+02, 2.59e+00]    [0.00e+00, 2.53e+02, 2.59e+00]    [2.53e+02, 2.53e+02, 1.18e+02, 1.18e+02]    
1000      [3.60e+01, 2.71e+01, 2.57e+00]    [0.00e+00, 3.16e+01, 2.57e+00]    [3.19e+01, 3.16e+01, 8.62e+00, 8.80e+00]    
2000      [3.16e+01, 1.65e+01, 2.57e+00]    [0.00e+00, 2.02e+01, 2.57e+00]    [2.07e+01, 2.02e+01, 1.31e+00, 1.60e+00]    
3000      [2.90e+01, 1.17e+01, 2.57e+00]    [0.00e+00, 8.81e+00, 2.57e+00]    [9.31e+00, 8.81e+00, 4.13e+00, 4.30e+00]    
4000      [2.91e+01, 1.19e+01, 2.56e+00]    [0.00e+00, 6.51e+00, 2.56e+00]    [6.86e+00, 6.51e+00, 5.56e+00, 5.54e+00]    
5000      [2.76e+01, 8.09e+00, 2.56e+00]    [0.00e+00, 6.46e+00, 2.56e+00]    [6.65e+00, 6.46e+00, 5.90e+00, 5.67e+00]    
6000      [2.70e+01, 8.30e+00, 2.55e+00]    [0.00e+00, 7.34e+00, 2.55e+00]    [7.49e+00, 7.34e+00, 4.64e+00, 4.32e+00]    
7000      [2.68e+01, 8.70e+00, 2.55e+00]    [0.00e+00, 6.21e+00, 2.55e+00]    [6.65e+00, 6.21e+00, 5.08e+00, 4.99e+00]    
8000      [2.72e+01, 7.90e+00, 2.55e+00]    [0.00e+00, 6.66e+00, 2.55e+00]    [7.20e+00, 6.66e+00, 4.29e+00, 4.26e+00]    
9000      [2.78e+01, 8.80e+00, 2.54e+00]    [0.00e+00, 5.58e+00, 2.54e+00]    [5.91e+00, 5.58e+00, 5.51e+00, 5.22e+00]    
10000     [2.69e+01, 7.18e+00, 2.54e+00]    [0.00e+00, 7.39e+00, 2.54e+00]    [8.27e+00, 7.39e+00, 3.55e+00, 3.75e+00]    
11000     [2.84e+01, 8.55e+00, 2.54e+00]    [0.00e+00, 5.83e+00, 2.54e+00]    [6.84e+00, 5.83e+00, 5.04e+00, 5.33e+00]    
12000     [3.06e+01, 1.24e+01, 2.54e+00]    [0.00e+00, 5.87e+00, 2.54e+00]    [6.10e+00, 5.87e+00, 5.49e+00, 4.93e+00]    
13000     [2.79e+01, 7.79e+00, 2.54e+00]    [0.00e+00, 5.41e+00, 2.54e+00]    [6.61e+00, 5.41e+00, 4.98e+00, 5.32e+00]    
14000     [2.66e+01, 6.94e+00, 2.54e+00]    [0.00e+00, 6.44e+00, 2.54e+00]    [7.85e+00, 6.44e+00, 3.85e+00, 4.31e+00]    
15000     [2.72e+01, 6.52e+00, 2.54e+00]    [0.00e+00, 5.18e+00, 2.54e+00]    [6.74e+00, 5.18e+00, 4.45e+00, 4.97e+00]    
16000     [2.87e+01, 8.96e+00, 2.54e+00]    [0.00e+00, 5.41e+00, 2.54e+00]    [7.11e+00, 5.41e+00, 3.77e+00, 4.35e+00]    
17000     [2.83e+01, 9.69e+00, 2.54e+00]    [0.00e+00, 6.13e+00, 2.54e+00]    [7.97e+00, 6.13e+00, 2.92e+00, 3.54e+00]    
18000     [2.73e+01, 7.48e+00, 2.54e+00]    [0.00e+00, 6.25e+00, 2.54e+00]    [8.25e+00, 6.25e+00, 2.05e+00, 2.71e+00]    
19000     [3.01e+01, 1.27e+01, 2.54e+00]    [0.00e+00, 6.58e+00, 2.54e+00]    [8.70e+00, 6.58e+00, 1.45e+00, 2.11e+00]    
20000     [2.89e+01, 1.03e+01, 2.54e+00]    [0.00e+00, 5.42e+00, 2.54e+00]    [7.74e+00, 5.42e+00, 1.82e+00, 2.53e+00]    
21000     [2.72e+01, 4.72e+00, 2.54e+00]    [0.00e+00, 3.47e+00, 2.54e+00]    [5.70e+00, 3.47e+00, 2.72e+00, 3.18e+00]    
22000     [2.68e+01, 4.05e+00, 2.54e+00]    [0.00e+00, 4.31e+00, 2.54e+00]    [4.16e+00, 4.31e+00, 3.50e+00, 1.40e+00]    
23000     [2.73e+01, 4.85e+00, 2.54e+00]    [0.00e+00, 2.89e+00, 2.54e+00]    [5.33e+00, 2.89e+00, 2.08e+00, 2.38e+00]    
24000     [2.67e+01, 3.27e+00, 2.54e+00]    [0.00e+00, 2.42e+00, 2.54e+00]    [5.57e+00, 2.42e+00, 1.28e+00, 2.09e+00]    
25000     [2.91e+01, 9.88e+00, 2.54e+00]    [0.00e+00, 4.12e+00, 2.54e+00]    [7.47e+00, 4.12e+00, 3.69e-01, 4.29e-01]    
26000     [2.87e+01, 6.81e+00, 2.55e+00]    [0.00e+00, 1.67e+00, 2.55e+00]    [5.22e+00, 1.67e+00, 7.88e-01, 1.55e+00]    
27000     [2.94e+01, 7.26e+00, 2.55e+00]    [0.00e+00, 2.62e+00, 2.55e+00]    [3.48e+00, 2.62e+00, 1.71e+00, 5.11e-01]    
28000     [2.75e+01, 4.04e+00, 2.55e+00]    [0.00e+00, 2.07e+00, 2.55e+00]    [3.93e+00, 2.07e+00, 1.14e+00, 3.40e-01]    
29000     [2.83e+01, 5.05e+00, 2.55e+00]    [0.00e+00, 1.71e+00, 2.55e+00]    [3.98e+00, 1.71e+00, 7.63e-01, 6.32e-01]    
30000     [2.69e+01, 3.20e+00, 2.56e+00]    [0.00e+00, 2.21e+00, 2.56e+00]    [3.23e+00, 2.21e+00, 1.21e+00, 1.76e+00]    

Best model at step 24000:
  train loss: 3.26e+01
  test loss: 4.97e+00
  test metric: [5.57e+00, 2.42e+00, 1.28e+00, 2.09e+00]

'train' took 67.758566 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 8
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.231384 s

'compile' took 0.983370 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [6.68e+02, 1.04e+03, 2.71e+00]    [0.00e+00, 1.03e+02, 2.71e+00]    [1.03e+02, 1.03e+02, 3.56e+01, 3.56e+01]    
1000      [3.90e+01, 2.82e+01, 2.68e+00]    [0.00e+00, 4.26e+01, 2.68e+00]    [4.26e+01, 4.26e+01, 2.08e+01, 2.09e+01]    
2000      [3.29e+01, 1.15e+01, 2.67e+00]    [0.00e+00, 2.71e+01, 2.67e+00]    [2.74e+01, 2.71e+01, 5.43e+00, 5.52e+00]    
3000      [3.24e+01, 1.15e+01, 2.66e+00]    [0.00e+00, 2.30e+01, 2.66e+00]    [2.34e+01, 2.30e+01, 6.73e+00, 6.85e+00]    
4000      [3.00e+01, 7.68e+00, 2.65e+00]    [0.00e+00, 1.93e+01, 2.65e+00]    [1.99e+01, 1.93e+01, 1.03e+01, 1.05e+01]    
5000      [2.86e+01, 6.45e+00, 2.65e+00]    [0.00e+00, 1.91e+01, 2.65e+00]    [1.98e+01, 1.91e+01, 1.01e+01, 1.03e+01]    
6000      [2.90e+01, 7.48e+00, 2.64e+00]    [0.00e+00, 1.76e+01, 2.64e+00]    [1.84e+01, 1.76e+01, 1.14e+01, 1.18e+01]    
7000      [2.78e+01, 5.56e+00, 2.64e+00]    [0.00e+00, 1.57e+01, 2.64e+00]    [1.66e+01, 1.57e+01, 1.28e+01, 1.31e+01]    
8000      [2.94e+01, 1.01e+01, 2.63e+00]    [0.00e+00, 1.66e+01, 2.63e+00]    [1.76e+01, 1.66e+01, 1.18e+01, 1.23e+01]    
9000      [2.88e+01, 9.53e+00, 2.63e+00]    [0.00e+00, 1.54e+01, 2.63e+00]    [1.65e+01, 1.54e+01, 1.27e+01, 1.32e+01]    
10000     [2.74e+01, 7.13e+00, 2.63e+00]    [0.00e+00, 1.46e+01, 2.63e+00]    [1.48e+01, 1.46e+01, 1.41e+01, 1.35e+01]    
11000     [3.06e+01, 1.27e+01, 2.62e+00]    [0.00e+00, 1.49e+01, 2.62e+00]    [1.48e+01, 1.49e+01, 1.43e+01, 1.34e+01]    
12000     [2.72e+01, 4.96e+00, 2.62e+00]    [0.00e+00, 1.56e+01, 2.62e+00]    [1.49e+01, 1.56e+01, 1.36e+01, 1.21e+01]    
13000     [3.13e+01, 1.40e+01, 2.62e+00]    [0.00e+00, 1.66e+01, 2.62e+00]    [1.59e+01, 1.66e+01, 1.31e+01, 1.15e+01]    
14000     [2.74e+01, 7.27e+00, 2.62e+00]    [0.00e+00, 1.65e+01, 2.62e+00]    [1.58e+01, 1.65e+01, 1.26e+01, 1.10e+01]    
15000     [2.87e+01, 1.01e+01, 2.62e+00]    [0.00e+00, 1.71e+01, 2.62e+00]    [1.65e+01, 1.71e+01, 1.20e+01, 1.03e+01]    
16000     [2.82e+01, 9.23e+00, 2.61e+00]    [0.00e+00, 1.72e+01, 2.61e+00]    [1.66e+01, 1.72e+01, 1.18e+01, 1.00e+01]    
17000     [2.70e+01, 4.55e+00, 2.61e+00]    [0.00e+00, 1.56e+01, 2.61e+00]    [1.50e+01, 1.56e+01, 1.28e+01, 1.10e+01]    
18000     [2.85e+01, 7.52e+00, 2.61e+00]    [0.00e+00, 1.53e+01, 2.61e+00]    [1.48e+01, 1.53e+01, 1.27e+01, 1.09e+01]    
19000     [2.99e+01, 9.94e+00, 2.61e+00]    [0.00e+00, 1.83e+01, 2.61e+00]    [1.78e+01, 1.83e+01, 9.70e+00, 7.80e+00]    
20000     [2.68e+01, 7.04e+00, 2.61e+00]    [0.00e+00, 1.64e+01, 2.61e+00]    [1.61e+01, 1.64e+01, 1.18e+01, 9.96e+00]    
21000     [2.64e+01, 5.96e+00, 2.61e+00]    [0.00e+00, 1.60e+01, 2.61e+00]    [1.59e+01, 1.60e+01, 1.18e+01, 1.01e+01]    
22000     [2.55e+01, 2.33e+00, 2.61e+00]    [0.00e+00, 1.54e+01, 2.61e+00]    [1.56e+01, 1.54e+01, 1.17e+01, 1.02e+01]    
23000     [2.83e+01, 7.38e+00, 2.61e+00]    [0.00e+00, 1.37e+01, 2.61e+00]    [1.43e+01, 1.37e+01, 1.27e+01, 1.14e+01]    
24000     [2.65e+01, 4.42e+00, 2.61e+00]    [0.00e+00, 1.42e+01, 2.61e+00]    [1.54e+01, 1.42e+01, 1.17e+01, 1.07e+01]    
25000     [3.09e+01, 1.18e+01, 2.61e+00]    [0.00e+00, 1.31e+01, 2.61e+00]    [1.49e+01, 1.31e+01, 1.18e+01, 1.13e+01]    
26000     [2.74e+01, 8.02e+00, 2.60e+00]    [0.00e+00, 1.32e+01, 2.60e+00]    [1.58e+01, 1.32e+01, 1.16e+01, 1.17e+01]    
27000     [2.68e+01, 7.35e+00, 2.60e+00]    [0.00e+00, 1.36e+01, 2.60e+00]    [1.70e+01, 1.36e+01, 1.02e+01, 1.09e+01]    
28000     [2.54e+01, 2.95e+00, 2.60e+00]    [0.00e+00, 1.32e+01, 2.60e+00]    [1.47e+01, 1.32e+01, 1.19e+01, 1.06e+01]    
29000     [2.57e+01, 3.89e+00, 2.60e+00]    [0.00e+00, 1.36e+01, 2.60e+00]    [1.49e+01, 1.36e+01, 1.20e+01, 1.02e+01]    
30000     [2.77e+01, 7.64e+00, 2.60e+00]    [0.00e+00, 1.34e+01, 2.60e+00]    [1.48e+01, 1.34e+01, 1.14e+01, 9.69e+00]    

Best model at step 22000:
  train loss: 3.04e+01
  test loss: 1.80e+01
  test metric: [1.56e+01, 1.54e+01, 1.17e+01, 1.02e+01]

'train' took 53.256132 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 9
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.195586 s

'compile' took 0.863678 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [2.43e+02, 2.94e+02, 2.80e+00]    [0.00e+00, 1.22e+02, 2.80e+00]    [1.22e+02, 1.22e+02, 6.04e+00, 6.04e+00]    
1000      [3.93e+01, 2.92e+01, 2.78e+00]    [0.00e+00, 4.66e+01, 2.78e+00]    [4.67e+01, 4.66e+01, 8.73e+00, 8.37e+00]    
2000      [3.36e+01, 1.88e+01, 2.78e+00]    [0.00e+00, 1.30e+01, 2.78e+00]    [1.31e+01, 1.30e+01, 7.44e-01, 4.06e-01]    
3000      [3.04e+01, 7.05e+00, 2.78e+00]    [0.00e+00, 2.90e+00, 2.78e+00]    [2.67e+00, 2.90e+00, 9.44e-01, 6.52e-01]    
4000      [3.51e+01, 1.79e+01, 2.78e+00]    [0.00e+00, 5.45e+00, 2.78e+00]    [5.18e+00, 5.45e+00, 3.85e-01, 6.98e-01]    
5000      [3.05e+01, 7.85e+00, 2.77e+00]    [0.00e+00, 8.56e+00, 2.77e+00]    [8.24e+00, 8.56e+00, 3.55e+00, 3.86e+00]    
6000      [2.87e+01, 5.19e+00, 2.77e+00]    [0.00e+00, 1.10e+01, 2.77e+00]    [1.06e+01, 1.10e+01, 6.16e+00, 6.50e+00]    
7000      [2.87e+01, 6.50e+00, 2.77e+00]    [0.00e+00, 1.24e+01, 2.77e+00]    [1.19e+01, 1.24e+01, 7.66e+00, 8.06e+00]    
8000      [2.90e+01, 4.87e+00, 2.77e+00]    [0.00e+00, 1.30e+01, 2.77e+00]    [1.23e+01, 1.30e+01, 8.33e+00, 8.79e+00]    
9000      [2.89e+01, 6.50e+00, 2.77e+00]    [0.00e+00, 1.43e+01, 2.77e+00]    [1.33e+01, 1.43e+01, 9.46e+00, 1.00e+01]    
10000     [2.92e+01, 6.90e+00, 2.77e+00]    [0.00e+00, 1.54e+01, 2.77e+00]    [1.41e+01, 1.54e+01, 9.83e+00, 1.05e+01]    
11000     [2.89e+01, 6.31e+00, 2.78e+00]    [0.00e+00, 1.56e+01, 2.78e+00]    [1.38e+01, 1.56e+01, 9.37e+00, 1.01e+01]    
12000     [3.24e+01, 1.10e+01, 2.78e+00]    [0.00e+00, 1.51e+01, 2.78e+00]    [1.28e+01, 1.51e+01, 7.58e+00, 8.35e+00]    
13000     [3.01e+01, 5.99e+00, 2.78e+00]    [0.00e+00, 1.55e+01, 2.78e+00]    [1.26e+01, 1.55e+01, 7.45e+00, 8.34e+00]    
14000     [2.87e+01, 6.25e+00, 2.79e+00]    [0.00e+00, 1.57e+01, 2.79e+00]    [1.19e+01, 1.57e+01, 8.48e+00, 9.51e+00]    
15000     [2.87e+01, 3.94e+00, 2.79e+00]    [0.00e+00, 1.58e+01, 2.79e+00]    [1.11e+01, 1.58e+01, 7.34e+00, 8.44e+00]    
16000     [2.94e+01, 8.14e+00, 2.79e+00]    [0.00e+00, 1.68e+01, 2.79e+00]    [1.11e+01, 1.68e+01, 8.24e+00, 9.50e+00]    
17000     [3.02e+01, 7.02e+00, 2.80e+00]    [0.00e+00, 1.71e+01, 2.80e+00]    [1.02e+01, 1.71e+01, 6.32e+00, 7.65e+00]    
18000     [2.83e+01, 4.76e+00, 2.80e+00]    [0.00e+00, 1.75e+01, 2.80e+00]    [9.55e+00, 1.75e+01, 6.45e+00, 7.90e+00]    
19000     [2.98e+01, 1.06e+01, 2.80e+00]    [0.00e+00, 1.83e+01, 2.80e+00]    [9.08e+00, 1.83e+01, 7.61e+00, 9.29e+00]    
20000     [2.67e+01, 2.97e+00, 2.80e+00]    [0.00e+00, 1.79e+01, 2.80e+00]    [7.36e+00, 1.79e+01, 6.68e+00, 8.39e+00]    
21000     [2.62e+01, 2.40e+00, 2.81e+00]    [0.00e+00, 1.77e+01, 2.81e+00]    [6.84e+00, 1.77e+01, 5.86e+00, 8.67e+00]    
22000     [2.83e+01, 7.31e+00, 2.81e+00]    [0.00e+00, 1.74e+01, 2.81e+00]    [6.33e+00, 1.74e+01, 4.46e+00, 8.21e+00]    
23000     [2.65e+01, 4.22e+00, 2.81e+00]    [0.00e+00, 1.79e+01, 2.81e+00]    [6.17e+00, 1.79e+01, 3.89e+00, 8.10e+00]    
24000     [2.67e+01, 4.93e+00, 2.81e+00]    [0.00e+00, 1.82e+01, 2.81e+00]    [5.94e+00, 1.82e+01, 3.04e+00, 7.92e+00]    
25000     [2.81e+01, 8.41e+00, 2.81e+00]    [0.00e+00, 1.89e+01, 2.81e+00]    [7.04e+00, 1.89e+01, 2.62e+00, 9.21e+00]    
26000     [2.56e+01, 4.73e+00, 2.81e+00]    [0.00e+00, 1.86e+01, 2.81e+00]    [5.77e+00, 1.86e+01, 1.22e+00, 7.80e+00]    
27000     [2.69e+01, 6.99e+00, 2.81e+00]    [0.00e+00, 1.89e+01, 2.81e+00]    [5.15e+00, 1.89e+01, 3.56e-01, 7.13e+00]    
28000     [2.56e+01, 3.47e+00, 2.81e+00]    [0.00e+00, 1.97e+01, 2.81e+00]    [5.97e+00, 1.97e+01, 7.12e-02, 8.08e+00]    
29000     [3.11e+01, 1.46e+01, 2.81e+00]    [0.00e+00, 2.06e+01, 2.81e+00]    [6.91e+00, 2.06e+01, 3.41e-01, 9.15e+00]    
30000     [3.02e+01, 1.19e+01, 2.80e+00]    [0.00e+00, 2.07e+01, 2.80e+00]    [6.73e+00, 2.07e+01, 3.19e-01, 8.83e+00]    

Best model at step 21000:
  train loss: 3.14e+01
  test loss: 2.05e+01
  test metric: [6.84e+00, 1.77e+01, 5.86e+00, 8.67e+00]

'train' took 51.849724 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...

Cross-validation iteration: 10
Using TensorFlow 2 backend.

Compiling model...
Building multifidelity neural network...
'build' took 0.199699 s

'compile' took 0.854809 s

Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric                                 
0         [5.81e+02, 8.89e+02, 2.74e+00]    [0.00e+00, 8.20e+01, 2.74e+00]    [8.20e+01, 8.20e+01, 4.38e+01, 4.38e+01]    
1000      [2.95e+01, 1.66e+01, 2.74e+00]    [0.00e+00, 3.90e+01, 2.74e+00]    [3.91e+01, 3.90e+01, 3.95e+00, 3.81e+00]    
2000      [2.89e+01, 1.01e+01, 2.74e+00]    [0.00e+00, 2.73e+01, 2.74e+00]    [2.76e+01, 2.73e+01, 3.69e+00, 3.42e+00]    
3000      [2.74e+01, 9.90e+00, 2.74e+00]    [0.00e+00, 2.79e+01, 2.74e+00]    [2.83e+01, 2.79e+01, 1.06e+00, 7.06e-01]    
4000      [2.63e+01, 6.39e+00, 2.74e+00]    [0.00e+00, 2.78e+01, 2.74e+00]    [2.83e+01, 2.78e+01, 4.78e-01, 8.69e-01]    
5000      [2.61e+01, 7.11e+00, 2.74e+00]    [0.00e+00, 2.72e+01, 2.74e+00]    [2.77e+01, 2.72e+01, 1.51e+00, 1.92e+00]    
6000      [2.61e+01, 6.74e+00, 2.74e+00]    [0.00e+00, 2.69e+01, 2.74e+00]    [2.75e+01, 2.69e+01, 2.23e+00, 2.65e+00]    
7000      [2.82e+01, 1.26e+01, 2.74e+00]    [0.00e+00, 2.65e+01, 2.74e+00]    [2.73e+01, 2.65e+01, 3.69e+00, 4.13e+00]    
8000      [2.52e+01, 5.14e+00, 2.74e+00]    [0.00e+00, 2.65e+01, 2.74e+00]    [2.73e+01, 2.65e+01, 3.47e+00, 3.92e+00]    
9000      [2.51e+01, 6.30e+00, 2.73e+00]    [0.00e+00, 2.64e+01, 2.73e+00]    [2.72e+01, 2.64e+01, 3.74e+00, 4.19e+00]    
10000     [2.56e+01, 6.80e+00, 2.73e+00]    [0.00e+00, 2.61e+01, 2.73e+00]    [2.70e+01, 2.61e+01, 3.87e+00, 4.33e+00]    
11000     [2.51e+01, 7.00e+00, 2.73e+00]    [0.00e+00, 2.60e+01, 2.73e+00]    [2.68e+01, 2.60e+01, 4.22e+00, 4.68e+00]    
12000     [2.78e+01, 1.22e+01, 2.73e+00]    [0.00e+00, 2.57e+01, 2.73e+00]    [2.66e+01, 2.57e+01, 4.80e+00, 5.28e+00]    
13000     [2.51e+01, 6.75e+00, 2.73e+00]    [0.00e+00, 2.55e+01, 2.73e+00]    [2.64e+01, 2.55e+01, 4.65e+00, 5.14e+00]    
14000     [2.67e+01, 8.88e+00, 2.73e+00]    [0.00e+00, 2.53e+01, 2.73e+00]    [2.63e+01, 2.53e+01, 4.42e+00, 4.92e+00]    
15000     [2.66e+01, 8.88e+00, 2.73e+00]    [0.00e+00, 2.51e+01, 2.73e+00]    [2.61e+01, 2.51e+01, 4.54e+00, 5.03e+00]    
16000     [2.57e+01, 8.27e+00, 2.73e+00]    [0.00e+00, 2.50e+01, 2.73e+00]    [2.60e+01, 2.50e+01, 5.03e+00, 5.54e+00]    
17000     [2.50e+01, 5.71e+00, 2.73e+00]    [0.00e+00, 2.47e+01, 2.73e+00]    [2.58e+01, 2.47e+01, 4.72e+00, 5.23e+00]    
18000     [2.79e+01, 1.14e+01, 2.72e+00]    [0.00e+00, 2.43e+01, 2.72e+00]    [2.54e+01, 2.43e+01, 5.57e+00, 6.10e+00]    
19000     [2.77e+01, 1.17e+01, 2.72e+00]    [0.00e+00, 2.42e+01, 2.72e+00]    [2.54e+01, 2.42e+01, 5.46e+00, 6.00e+00]    
20000     [2.51e+01, 7.02e+00, 2.72e+00]    [0.00e+00, 2.41e+01, 2.72e+00]    [2.53e+01, 2.41e+01, 5.28e+00, 5.82e+00]    
21000     [2.74e+01, 1.07e+01, 2.72e+00]    [0.00e+00, 2.38e+01, 2.72e+00]    [2.50e+01, 2.38e+01, 5.58e+00, 6.12e+00]    
22000     [2.54e+01, 6.96e+00, 2.72e+00]    [0.00e+00, 2.37e+01, 2.72e+00]    [2.49e+01, 2.37e+01, 5.45e+00, 5.99e+00]    
23000     [2.76e+01, 1.03e+01, 2.72e+00]    [0.00e+00, 2.34e+01, 2.72e+00]    [2.47e+01, 2.34e+01, 5.78e+00, 6.32e+00]    
24000     [2.47e+01, 5.24e+00, 2.72e+00]    [0.00e+00, 2.33e+01, 2.72e+00]    [2.46e+01, 2.33e+01, 5.42e+00, 5.96e+00]    
25000     [2.47e+01, 4.56e+00, 2.72e+00]    [0.00e+00, 2.34e+01, 2.72e+00]    [2.47e+01, 2.34e+01, 5.20e+00, 5.74e+00]    
26000     [2.58e+01, 7.60e+00, 2.71e+00]    [0.00e+00, 2.31e+01, 2.71e+00]    [2.44e+01, 2.31e+01, 5.61e+00, 6.15e+00]    
27000     [2.43e+01, 3.74e+00, 2.71e+00]    [0.00e+00, 2.31e+01, 2.71e+00]    [2.44e+01, 2.31e+01, 5.35e+00, 5.89e+00]    
28000     [2.66e+01, 8.68e+00, 2.71e+00]    [0.00e+00, 2.28e+01, 2.71e+00]    [2.42e+01, 2.28e+01, 5.30e+00, 5.85e+00]    
29000     [2.61e+01, 7.73e+00, 2.71e+00]    [0.00e+00, 2.27e+01, 2.71e+00]    [2.41e+01, 2.27e+01, 5.91e+00, 6.46e+00]    
30000     [2.48e+01, 5.51e+00, 2.71e+00]    [0.00e+00, 2.27e+01, 2.71e+00]    [2.41e+01, 2.27e+01, 5.45e+00, 6.01e+00]    

Best model at step 27000:
  train loss: 3.07e+01
  test loss: 2.58e+01
  test metric: [2.44e+01, 2.31e+01, 5.35e+00, 5.89e+00]

'train' took 62.006262 s

Saving loss history to loss.dat ...
Saving training data to train.dat ...
Saving test data to test.dat ...
[27.434699856445164, 4.576781428330251, 9.592788515339745, 19.578621248453015, 15.924353736895366, 57.80395874489834, 2.4243285258611063, 15.430425555336248, 17.682358168942024, 23.080416217658815]
sigma_y 12 19.352873199816006 14.791369674237961
=======================================================
=======================================================
